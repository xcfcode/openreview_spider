{"notes": [{"id": "bfn9DplVgh", "original": null, "number": 2, "cdate": 1584009849293, "ddate": null, "tcdate": 1584009849293, "tmdate": 1584009849293, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Public_Comment", "content": {"title": "Related results for graph classification", "comment": "Hello,\nI just discovered your paper, thank you for this very comprehensive work. I wanted to bring to your attention a short paper about node classification, in which we reached very similar conclusions: https://arxiv.org/pdf/1911.05384.pdf\n\nOur main focus was the study of the relative performance of graph neural networks depending on the number of training examples and features in the dataset, which is a different perspective. However, one observation that we made is that even when a lot of training data is available, intertwining propagation and learning layers is not useful. We found that in this case, it was better to use several propagation layers (with no trainable parameters), followed by a non-linear feature extractor (i.e a MLP). It looks very close to your conclusions about graph classification.\n\nWe were not aware of your work when we wrote the paper and therefore did not cite you, but we'll make sure to do it whenever we present it.\n\n"}, "signatures": ["~Cl\u00e9ment_Vignac1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Cl\u00e9ment_Vignac1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504185244, "tmdate": 1576860576261, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Public_Comment"}}}, {"id": "BJxQxeBYwH", "original": "SJg05hyFPB", "number": 2092, "cdate": 1569439722697, "ddate": null, "tcdate": 1569439722697, "tmdate": 1577168261008, "tddate": null, "forum": "BJxQxeBYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "eO0TSjvspL", "original": null, "number": 1, "cdate": 1576798740294, "ddate": null, "tcdate": 1576798740294, "tmdate": 1576800895956, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to split the GNN operations into two parts and study the effects of each part. While two reviewers are positive about this paper, the other reviewer R1 has raised some concerns. During discussion, R1 responded and indicated that his/her concerns were not addressed in author rebuttal. Overall, I feel the paper is borderline and lean towards reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720794, "tmdate": 1576800271691, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Decision"}}}, {"id": "rylyUz9voB", "original": null, "number": 6, "cdate": 1573524039020, "ddate": null, "tcdate": 1573524039020, "tmdate": 1573524039020, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment", "content": {"title": "Revision log", "comment": "This is to log what we have changed in the revision:\n\n1. We added comparisons to RETGK and GNTK as suggested by Reviewer 1.\n2. We clarified a notation as suggested by Reviewer 2.\n3. We added an experiment on varying dataset size according to the comment of Reviewer 3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2092/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2092/Authors|ICLR.cc/2020/Conference/Paper2092/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146448, "tmdate": 1576860542890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment"}}}, {"id": "Bkxh3xqwsH", "original": null, "number": 5, "cdate": 1573523636462, "ddate": null, "tcdate": 1573523636462, "tmdate": 1573523636462, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJg-ri4CYB", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the time and detailed comments. Please find our responses to the comments below.\n\n[Analysis and insights]\n\nTwo types of theoretical analysis are presented in this paper:\n    1) We prove that GFN can be derived by linearizing graph filtering part of GNNs (proposition 1), and leverage this theoretical connection to decouple the two GNN parts and study the importance of them separately.\n    2) We show that GFNs can be a very powerful framework without the restriction on the feature extraction function \u03b3(G, X) and the exact forms of the set function (proposition 2), which is encouraging for future graph function design.\n\nRegarding the gaps between GCN and GFN among datasets, we note that 6 out of 10 datasets, GFN outperforms GNN counterpart in fair comparisons, and also note the gaps are *small* as they are within *1 standard deviation*. We are not convinced if these gaps are substantial, and thus conclude that both methods are on par across the whole set of benchmarks.\n\nThe main insight of this paper is that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.\n\n[Non-linearity in GNN\u2019s middle layers]\n\nWe do account for the non-linearity in GNN\u2019s as our GNN baselines have non-linearity in them. When nonlinearity in GNN\u2019s middle layers are removed, we prove that they can be expressed as a GFN with appropriated graph features (in proposition 1). By comparing GFN and GNN, we are testing the importance of the nonlinearity of the graph filtering function (in GNN\u2019s middle layers).\n\n[More comparisons]\n\nAt the time when this work was conducted, the state-of-the-art of GNN variant was GIN (Xu et al ICLR\u201919), which we compared in this work (among other 7 baselines). We\u2019d also like to point out that our goal is to dissect GNN variants, while both suggested papers are based on graph kernels, which are quadratic to the number of nodes and graphs (e.g. faster RetGKII is generally inferior than much slower RetGKI, and GNTK cannot scale to Reddit datasets). In contrast, our GFN has linear complexity thus in practice very fast/scalable (Figure 2), and the performances are better or comparable averaged over all benchmarks. Nonetheless, we appreciate the reviewer\u2019s time and evaluation thus have added the full comparison and discussion in the revision (Appendix H), in good faith that the reviewer would also appreciate the contributions of our work.\n\nWe\u2019d like to clarify further as necessary, so please feel free to let us know if any of the concerns are not fully addressed."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2092/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2092/Authors|ICLR.cc/2020/Conference/Paper2092/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146448, "tmdate": 1576860542890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment"}}}, {"id": "S1xJYl9vsB", "original": null, "number": 4, "cdate": 1573523574611, "ddate": null, "tcdate": 1573523574611, "tmdate": 1573523574611, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "Hye94W7M5r", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your time and positive feedbacks. Regarding the datasets, we compared 12 social and biological graph datasets (from 188 to 11929 graphs), which are the most widely used standard benchmarks for graph classification task as of today (some existing work does not even include the largest RE-M12K due to scalability issue).\n\nOn the dataset size, we try to incorporate your comments and perform extra experiments. To see how the varying dataset size affects the performance of GFN and GCN, we take the largest RE-M12K dataset (11929 graphs), and randomly sample datasets of different size (from 10% graphs to 100% graphs). We run 10 fold cross validation on each of the dataset, and found that: as dataset sizes increases, it becomes harder to overfit (especially for GFN), but GFN still performs as well as, if not better, than GCN. Details of this experiment are added to the appendix I.\n\nOn the dataset complexity, we fully agree that with more complex tasks/datasets powerful GNNs could probably show better performance. And in fact, that is also part of our goal in publishing our work, to raise the awareness that common graph classification benchmarks are likely inadequate for testing advanced GNN variants. We wish the community as a whole to explore and adopt more convincing benchmarks for testing advanced GNN variants, or include GFN as a standard baseline to provide a sanity check."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2092/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2092/Authors|ICLR.cc/2020/Conference/Paper2092/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146448, "tmdate": 1576860542890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment"}}}, {"id": "HklhfgqDsr", "original": null, "number": 3, "cdate": 1573523475845, "ddate": null, "tcdate": 1573523475845, "tmdate": 1573523526008, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "HJgX4T6CtH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your time and valuable feedbacks. \n\n[More discussion on the observations]\n\nWe agree that there is more than one possibility for the empirical observations. Allow us to re-elaborate our main observation: what our experiments show is that GNN can overfit training set, but it doesn\u2019t generalize better than GFN (GNN with linearized graph filtering function) on a broad set of benchmarks. \n\nOne possibility is the inadequacy of existing graph classification benchmarks, which we are inclined to think it is the case. We have tried our best and tested on the most widely used benchmarks across the spectrum (from 188 to 11929 graphs). We also try to varying the dataset size by subsampling the largest dataset (RE-M12K), and the results can be found in appendix I. We hope, along with the whole community, to realize and adopt more complex real datasets to test if the observation still persists. \n\nThe other possibility is that the linear graph filtering may be a good inductive bias for the tested datasets/problems. This is what other studies on node classification (e.g. Wu et al, ICML\u201919) suggest as well - GNNs are performing low-pass filtering. However, this is again dependant on the tasks and datasets considered.\n\nThe third possibility is that, as suggested by the reviewer, despite GNNs can overfit but they are not capable of capturing the generalizable features (at least not prioritizing to learn those features). To show this is the case, we need to improve existing GNNs (e.g. architecture, objective) so that they can generalize better in existing benchmarks. We have not been able to find new techniques, or existing work, that can identify those more generalizable features.\n\nWe admit that our work has limitations on fully answering these questions, but we believe raising the right question itself (with solid experimental observations) is an important step towards the good answers. We wish our work can raise the awareness of the phenomenon so that it can be better studied in the future. What\u2019s more, the proposed GFN can serve as a fast and accurate approximation to GNN for graph classification task, which we believe is a practical contribution.\n\n[Other datasets and tasks]\n\nIn this work we focus on graph classification problem on 12 datasets as they are the most widely used benchmarks for recently proposed advanced GNN variants. However, we agree that to  further demystify the above possibilities, more work should be done to adopt GFN as baseline and apply it for more datasets/tasks.\n\nToward that end, we conduct experiments on image classification as graph classification on MNIST, where we find significant gap between GCN and GFN (Appendix D), which suggests non-linear graph filtering is important for image classification by treating images as graphs (unlike other natural graph datasets). We wish to conduct more meaningful downstream tasks that use graph neural nets but it requires careful selection and establishing benchmark datasets, thus we defer it for future work.\n\nAs for node classification task, which does not require the graph readout function (i.e., only has graph filtering function), and typically it is tested in the transductive setting (i.e. single graph), thus is a simpler task. Wu et al (ICML\u201919) has shown that fully linearizing GCN yields similar performance in several node classification datasets, which can be seen as a special case of GFN without set function. However, fully linearizing GCN for graph classification (i.e. GLN) significantly degenerates the performance, making it an important distinction between graph and node classification tasks.\n\n[Notation clarification]\n\nWe have updated the draft to clarify \\tilde{A}. Shown below Eq 2, $\\tilde{A} = \\tilde{D}^{-1/2}(A+\\epsilon I)\\tilde{D}^{-1/2}$ is the normalized adjacency matrix, with \\epsilon=1 this is the one proposed in Kipf and Welling (2016). We use this formulation by default (with \\epsilon=1e-8), but want to note that other formulation of \\tilde{A} is also allowed (e.g. different normalized graph Lapacian) under the general framework of GFN."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2092/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2092/Authors|ICLR.cc/2020/Conference/Paper2092/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146448, "tmdate": 1576860542890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment"}}}, {"id": "BJg-ri4CYB", "original": null, "number": 1, "cdate": 1571863353302, "ddate": null, "tcdate": 1571863353302, "tmdate": 1572972384134, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tries to study the importance of different components of GNNs. This paper studies two components 1) graph filtering: aggregation of neighboring features and 2) the aggregation function for the output.\n\nTo study this problem, this paper proposes two models, Graph Feature Network (GFN) and Graph Linear Network (GLN). GFN first uses the adjacency matrix to create several layers of features, then applies a multi-layer fully-connected neural network. GLN is a special case of GFN with the fully-connected neural network being linear.\n\nThis paper conducts experiments on graph classification task and finds GFN gives a reasonable performance, whereas GLN's performance is weaker.\n\n\n\nComments:\nThis paper studies an important problem in GNN, and the proposed method is interesting. However, I cannot accept the paper in the current form because of the following reasons.\n\n1. There is no theoretical analysis in the paper. For example, on some datasets, GFN, GLN, and GNN's performances are close while on other datasets, there are gaps. The current paper does not provide insight.\n\n2. GNN also contains non-linearity in the middle layers. However, the methodology in this paper cannot account for the importance of non-linearity in the middle layers.\n\n3. The experiment section ignores some recent results on graph classification tasks. See:\nhttps://arxiv.org/abs/1809.02670\nhttps://arxiv.org/abs/1905.13192"}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087191868, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Reviewers"], "noninvitees": [], "tcdate": 1570237727829, "tmdate": 1575087191879, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review"}}}, {"id": "HJgX4T6CtH", "original": null, "number": 2, "cdate": 1571900714582, "ddate": null, "tcdate": 1571900714582, "tmdate": 1572972384089, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a dissection analysis of graph neural networks by decomposing GNNs into two parts: a graph filtering function and a set function. Although this decomposition may not be unique in general, as pointed out in the paper, these two parts can help analyze the impact of each part in the GNN model. Two simplified versions of GNN is then proposed by linearizing the graph filtering function and the set function, denoted as GFN and GLN, respectively. Experimental results on benchmarks datasets for graph classification show that GFN can achieve comparable or even better performance compared to recently proposed GNNs with higher computational efficiency. This demonstrates that the current GNN models may be unnecessarily complicated and overkill on graph classification. These empirical results are pretty interesting to the research community, and can encourage other researchers to reflect on existing fancy GNN models whether it's worth having more complex and more computationally expensive models to achieve similar or even inferior performance. Overall, this paper is well-written and the contribution is clear. I would like to recommend a weak accept for this paper. If the suggestions below can be addressed in author response, I would be willing to increase the score.\n\n\nSuggestions for improvement:\n\n1) Considering the experimental results in this paper, it is possible that the existing graph classification tasks are not that difficult so that the simplified GNN variant can also achieve comparable or even better performance (easier to learn). This can be conjectured from the consistently better training performance but comparable testing performance of original GNN. Another possibility is that even the original GNN has larger model capacity, it is not able to capture more useful information from the graph structure, even on tasks that are more challenging than graph classification. However, this paper lacks such in-depth discussions;\n\n2) Besides the graph classification task, it would be better to explore the performance of the simplified GNN on other graph learning tasks, such as node classification, and various downstream tasks using graph neural networks. This can help demystify the question raised in the previous point; 3) The matrix \\tilde{A} in Equation 5 is not well explained (described as \"similar to that in Kipf and Welling (2016)\"). It would be more clear to directly point out that it is the adjacency matrix, as described later in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087191868, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Reviewers"], "noninvitees": [], "tcdate": 1570237727829, "tmdate": 1575087191879, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review"}}}, {"id": "Hye94W7M5r", "original": null, "number": 3, "cdate": 1572118833883, "ddate": null, "tcdate": 1572118833883, "tmdate": 1572972384045, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper dissects the importance of two parts in GCN: 1) nonlinear neighborhood aggregation; 2) nonlinear set function by linearizing the two parts and resulting in Graph Feature Network (GFN) and Graph Linear Network (GLN). It shows empirically that GFN achieves almost the same performance while GLN is much worse, suggesting the nonlinear graph neighborhood aggregation step may be unnecessary. Extensive ablation studies are conducted to single out the effects of various factors.\n\nThe paper studies an interesting problem and sets out a good plan of experiments to verify the hypotheses. The results are interesting: merely constructing graph neighborhood features alone is enough to get comparable performance with GCN since the nonlinearity in the set function is strong enough. The experiments are designed nicely: 1) it compares with various baselines on a variety of popular benchmarks; 2) ablation studies single out the importance of different graph features, such as degree, and multi-hop averages; 3) verifying whether the good performance GFN comes from easier optimization.\n\nThe paper is also clearly written, with clean notations, and well-structured sections.\n\nI think the experiment can be improved by comparing on larger, more complex datasets. Figure 1 seems to suggest GCN is overfitting compared to GFN due to its extra capacity--significantly better training accuracy but slightly worse test accuracy. It is usually the case that larger and more complex datasets require more sophisticated models. But the paper makes a good case for GFN in these datasets for the graph classification task."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087191868, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Reviewers"], "noninvitees": [], "tcdate": 1570237727829, "tmdate": 1575087191879, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Review"}}}, {"id": "B1xn-Pmq5B", "original": null, "number": 2, "cdate": 1572644612390, "ddate": null, "tcdate": 1572644612390, "tmdate": 1572644612390, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "SygrzG5uqH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment", "content": {"title": "It is one instantiation of graph augmented features in our framework", "comment": "Thanks for your interest in our work. It is fair to instantiate graph augmented features with other filters/operators, in our work, we follow Kipf and Welling (2016) and use modified adjacency matrix with renormalization trick, which is shown to be better than Chebyshev polynomials in their work. But I think Chebyshev polynomials can probably be used as another instantiation of the graph augmented features, along with possibly many more. We are different from those GCN based methods (with normalized adj or Chebyshev polynomials) in the sense that we fix graph augmented features in learning time, and treat the graph as a set."}, "signatures": ["ICLR.cc/2020/Conference/Paper2092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2092/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2092/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2092/Authors|ICLR.cc/2020/Conference/Paper2092/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146448, "tmdate": 1576860542890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Official_Comment"}}}, {"id": "SygrzG5uqH", "original": null, "number": 1, "cdate": 1572540940699, "ddate": null, "tcdate": 1572540940699, "tmdate": 1572541044603, "tddate": null, "forum": "BJxQxeBYwH", "replyto": "BJxQxeBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2092/-/Public_Comment", "content": {"title": "Relation to Chebyshev graph convolution", "comment": "Thank you for an interesting paper. I found that Eq. 5 resembles the Chebyshev graph convolution proposed in [1] which you cite in the first sentence only. The nice thing about [1] is that it approximates spectral graph convolution if K is large enough and uses the orthogonal Chebyshev basis, so it's theoretically sound. In your Eq. 5 you just take powers of adjacency matrices, and my hypothesis is that it can lead to unstable training dynamics, which might explain somewhat lower than expected performance when you use K>1. \n\nIt would be interesting to see the connection of your formulation in Eq. 5 to [1].\n\n[1] Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"}, "signatures": ["~Boris_Knyazev1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Knyazev1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification", "authors": ["Ting Chen", "Song Bian", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "biansonghz@gmail.com", "yzsun@cs.ucla.edu"], "keywords": ["graph neural nets", "graph classification", "set function"], "TL;DR": "We propose a dissection of GNNs through linearization of the parts, and find that linear graph filtering with non-linear set function is powerful enough for common graph classification benchmarks.", "abstract": "Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.", "pdf": "/pdf/60f60e3259ea10fe4f9eb2c67202aa7fa1f4f980.pdf", "paperhash": "chen|are_powerful_graph_neural_nets_necessary_a_dissection_on_graph_classification", "original_pdf": "/attachment/57fd0762ccfd69d96c565e1668e9f4f8f76d0563.pdf", "_bibtex": "@misc{\nchen2020are,\ntitle={Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification},\nauthor={Ting Chen and Song Bian and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxQxeBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxQxeBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504185244, "tmdate": 1576860576261, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2092/Authors", "ICLR.cc/2020/Conference/Paper2092/Reviewers", "ICLR.cc/2020/Conference/Paper2092/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2092/-/Public_Comment"}}}], "count": 12}