{"notes": [{"id": "HJxB5sRcFQ", "original": "H1xdAnEXKX", "number": 533, "cdate": 1538087821370, "ddate": null, "tcdate": 1538087821370, "tmdate": 1547282017501, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJeOxbBexN", "original": null, "number": 1, "cdate": 1544732911621, "ddate": null, "tcdate": 1544732911621, "tmdate": 1545354512816, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Meta_Review", "content": {"metareview": "Reviewers agree the paper should be accepted.\nSee reviews below.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Clear accept ratings from reviewers"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper533/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353182630, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353182630}}}, {"id": "Hye5G91CRQ", "original": null, "number": 7, "cdate": 1543531026143, "ddate": null, "tcdate": 1543531026143, "tmdate": 1543531026143, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "B1lzpiiQAX", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Additional experiments, explanation and comparisons makes it a good paper", "comment": "Thank you for your detailed rebuttal and for addressing my concerns and responding to my questions. \n- Specifically I found the additional analysis (both human evaluation and showing results from other baselines) on Clip-art scene generation satisfying. \n- I also found it helpful to look at DCGAN results for all experiments. \n- I also looked at the animation videos to demonstrate the movements of all the graphic elements and it helped me appreciate the the approach / design choices a bit more. \n\nOverall, I think the paper is greatly improved from the time of the submission and contains more exhaustive evaluation with existing work and shows application for a wide variety of tasks. Based on the rebuttal, I am updating my score to 7 (Good paper)"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "S1eLur3cnQ", "original": null, "number": 2, "cdate": 1541223789645, "ddate": null, "tcdate": 1541223789645, "tmdate": 1543530968443, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "content": {"title": "Interesting application of GANS; Applications on interesting datasets but weak comparison with baselines and quantitative analysis. ", "review": "\f\nSummary: This paper presents a novel GAN framework for generating graphic layouts which consists of a set of graphic elements which are geometrically and semantically related. The generator learns a function that maps input layout ( a random set of graphic elements denoted by their classes probabilities and and geometric parameters) and outputs the new contextually refined layout. The paper also explores two choices of discriminators: (1) relation based discriminator which directly extracts the relations among different graphic elements in the parameter space, and (2) wireframe rendering discriminator which maps graphic elements to 2D wireframe images using a differentiable layer followed by a CNN for learning the discriminator. The novel GAN framework is evaluated on several datasets such as MNIST, document layout comparison and clipart abstract scene generation  \n\nPros:\n- The paper is trying to solve an interesting problem of layout generation. While a large body of work has focussed on pixel generation, this paper focuses on graphic layouts which can have a wide range of practical applications. \n- The paper presents a novel architecture by proposing a generator that outputs a graphic layout consisting of class probabilities and polygon keypoints. They also propose a novel discriminator consisting of a differentiable layer that takes the parameters of the output layout and generates a rasterized image representing the wireframe. This is quite neat as it allows to utilize a CNN for learning a discriminator for real / fake prediction. \n- Qualitative results are shown on a wide variety of datasets - from MNIST to clipart scene generation and tangram graphic design generation. I found the clipart scene and tangram graphic design generation experiments quite neat. \n\nCons:\n- While the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation. \n- The paper also doesn\u2019t report results on simple baselines for generating graphic layouts. Why not have a simple regression based baseline for predicting polygon parameters? Or compare with the approach mentioned in [1]\n- Even for generating MNIST digits, the paper doesn\u2019t report numbers on previous methods used for MNIST digit generation. \nInterestingly, only figure 4 shows results from a traditional GAN approach (DCGAN). Why not show the output on other datasets too? \n\nQuestions / Remarks:\n- Why is the input to the GAN not the desired graphic elements and pose the problem as just predicting the polygon keypoints for those graphic elements. I didn\u2019t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input.  \n    - How does this work for the case of clip-art generation for example? The input to the gan is a list of all graphic elements (boy, girl glasses, hat, sun and tree) or a subset of these?\n    - It is also not clear what role the class probabilities are playing this formulation. \n- In section 3.3.2, it\u2019s mentioned that the target image consist of C channels assuming there are C semantic classes for each element. What do you mean by each graphic element having C semantic classes? Also in the formulation discusses in this section, there is no further mention of C. I wasn\u2019t quite clear what the purpose of C channels is then. \n- I found Figure 3b quite interesting - it would have been nice if you expanded on that experiments and the observations you made a little more. \n\n[1] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "cdate": 1542234439582, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745932, "tmdate": 1552335745932, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1emOpxi0m", "original": null, "number": 5, "cdate": 1543339371383, "ddate": null, "tcdate": 1543339371383, "tmdate": 1543339371383, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "rJeEMnsXR7", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Thank you for the new experiments", "comment": "Thank you for the new experiments. I think this makes the paper stronger."}, "signatures": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "rJeEMnsXR7", "original": null, "number": 4, "cdate": 1542859787901, "ddate": null, "tcdate": 1542859787901, "tmdate": 1542859787901, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "HkeCYxhc27", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Responses to Reviewer3:", "comment": "Q: \u201cMy only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.\u201d\n \nA: We added a new experiment of mobile app layout generation by using the RICO dataset (http://interactionmining.org/rico). We showed the results in the appendix due to page limitation. Please see Section 6.6 in the uploaded version for more details. "}, "signatures": ["ICLR.cc/2019/Conference/Paper533/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "B1lzpiiQAX", "original": null, "number": 3, "cdate": 1542859705583, "ddate": null, "tcdate": 1542859705583, "tmdate": 1542859705583, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "S1eLur3cnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Responses to Reviewer2-2:", "comment": "Q: \u201cWhy is the input to the GAN not the desired graphic elements and pose the problem as just predicting the polygon keypoints for those graphic elements. I didn\u2019t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input.  How does this work for the case of clip-art generation for example? The input to the gan is a list of all graphic elements (boy, girl glasses, hat, sun and tree) or a subset of these?\u201d\n \nA: Given a set of desired graphic elements, our LayoutGAN is actually able to predict their geometric parameters. We demonstrated its ability in the perturbation experiment in Figure 7. However, such synthesis process requires human priors in advance, i.e., the class of each graphic element desired by a reasonable layout.  \nOur work goes beyond that. It synthesizes graphic layouts from a set of purely random graphic elements in terms of both geometric parameters and classes. The class of each input element is not predefined but randomly sampled from Uniform distribution, thus the class combination of all the input elements can be in various semantic forms. Take the Clipart experiment as an example, an input set of elements may contain all categories (boy, girl, glasses, hat, sun and tree) or a subset of these with duplicates (boy, girl, glasses, glasses, hat, hat, tree). The model should learn to figure out and adjust the spatial-semantic relations among all elements automatically, and to predict refined class probabilities (can be zero class vector to remove duplicates if necessary) along with geometric parameters for each element to form a reasonable layout.  Predicting class probabilities together with geometric parameters greatly increases the flexibility and applicability of the LayoutGAN on different tasks.\n\nQ: \u201cIt is also not clear what role the class probabilities are playing this formulation. In section 3.3.2, it\u2019s mentioned that the target image consist of C channels assuming there are C semantic classes for each element. What do you mean by each graphic element having C semantic classes? Also in the formulation discusses in this section, there is no further mention of C. I wasn\u2019t quite clear what the purpose of C channels is then.\u201d \n \nA: As a graphic element can be boy, girl, hat, glasses, etc, out of C possible classes, we use a C-dimensional vector to represent the probabilities of one element being a particular class. It serves two roles. 1) It allows the network to model the spatial-semantic relations among different elements, e.g. a hat should precisely appear on top of a boy\u2019s head. 2) In the generation experiment, it allows the network to modify the class of each input random element to produce a layout that follow the ground truth class distribution. \nThe rendered image consists of C channels because we render wireframes that belong to a specific class with predicted probabilities onto a single channel (C equals to the total number of classes), upon which the CNN-based discriminator can be applied to optimize both the geometric parameters and class probabilities of all graphic elements coherently in a differentiable way. Note that if we render all the wireframes onto a single channel, then we lose the semantic information. In other words, the CNN discriminator would not be able to tell if a bounding box represents a hat or the sun.\n \nQ: \u201cI found Figure 3b quite interesting - it would have been nice if you expanded on that experiments and the observations you made a little more.\u201d\n\nA: Thanks. The colors was used to trace the points from initial random positions to final positions. Following your suggestion, we expanded this experiment on both MNIST and tangram generation by visualizing the displacements or the flows between initial and final positions of graphic elements. In particular, we made animation videos to demonstrate the movements of all the graphic elements. Please review them in the following anonymous link: https://sites.google.com/view/supp-videos-for-iclr-2019/home "}, "signatures": ["ICLR.cc/2019/Conference/Paper533/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "SJlQtjjm0X", "original": null, "number": 2, "cdate": 1542859642815, "ddate": null, "tcdate": 1542859642815, "tmdate": 1542859642815, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "S1eLur3cnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Responses to Reviewer2-1:", "comment": "Q: \u201cWhile the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation\u201d\n \nA: Thank you for your suggestions. This work is the first attempt to solve layout synthesis from random input for both Clipart scene generation and tangram graphic design (tangram data are collected and annotated by ourselves, we promise to release it upon acceptance). As no previous methods have focused on these problems, there is a lack of widely-accepted quantitative evaluation metrics for both tasks. To this end, we carried out a user study involving 20 respondents for a subjective evaluation of the generated Clipart abstract scenes. Please see Table 3 in the updated version.\n \nQ: \u201cThe paper also doesn\u2019t report results on simple baselines for generating graphic layouts. Why not have a simple regression based baseline for predicting polygon parameters? Or compare with the approach mentioned in [1]\u201d\n[1] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al\n\nA: Thank you for your suggestions. We have supplemented experiments of generating tangram graphic design sequentially as Wang et al [1] for comparison in the updated version. Specifically, Wang et al [1] generate indoor scenes iteratively by adding objects one-by-one. The choice of such sequential paradigm is partly because the rendering process from geometric parameters (object location) to indoor scene images is not differentiable. Similarly, we would have faced such a problem in our layout design. However, we propose a novel wireframe rendering layer to make the layout rendering process differentiable. Benefiting from it, we can predict a set of graphic elements simultaneously in an end-to-end network. But still, we can adopt the sequential paradigm in Wang et al [1] to our layout design problem by generating graphic elements one-by-one. However, we found such sequential synthesis process suffers from accumulated error, which validates the superiority of the proposed LayoutGAN. Please see Figure 8 for comparisons in the updated version.\n\nQ: \u201cEven for generating MNIST digits, the paper doesn\u2019t report numbers on previous methods used for MNIST digit generation. \n \nA: Our experiment on MNIST serves as sanity test. A 2D point, as the simplest geometric form, is not a desirable element representation for our approach, and we do not expect it to compete with other GANs applied to MNIST. We have reflected this in the updated version. \n\nQ: Interestingly, only figure 4 shows results from a traditional GAN approach (DCGAN). Why not show the output on other datasets too?\u201d\n\nA: Thanks for the suggestion. We added experiments to apply DCGAN to both Clipart abstract scene generation and tangram graphic design task in the updated version. Please see Figure 5 and 8.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "S1eY99iXAX", "original": null, "number": 1, "cdate": 1542859408519, "ddate": null, "tcdate": 1542859408519, "tmdate": 1542859408519, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "rJxm6KDi2X", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "content": {"title": "Responses to Reviewer1:", "comment": "Q: \u201cWhy not ask the generator to generate the rendering instead of class probabilities?\u201d\n \nA: The generator produces geometric layout parameters together with class probabilities. Rendering a wireframe image from the layout parameters is then trivial. Rendering an application-specific layout, e.g., graphic design bitmap, is application-dependent and unnecessarily complex for modeling layout. Does this answer your question?\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616116, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxB5sRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper533/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper533/Authors|ICLR.cc/2019/Conference/Paper533/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers", "ICLR.cc/2019/Conference/Paper533/Authors", "ICLR.cc/2019/Conference/Paper533/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616116}}}, {"id": "rJxm6KDi2X", "original": null, "number": 3, "cdate": 1541269947234, "ddate": null, "tcdate": 1541269947234, "tmdate": 1541533913361, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "content": {"title": "Interesting application of GAN on Layout rendering", "review": "Summary:\nThe paper proposed to use GAN to synthesize graphical layouts. The generator takes a random input and generates class probabilities and geometric parameters based on a self-attention module. The discriminator is based on a differentiable wireframe rendering component (proposed by the paper) to allow back propagation through the rendering module. I found the topic very interesting and the approach seems to make sense.\n\nQuality:\n+ The idea is very interesting and novel.\n\nClarity:\n+ The paper is clearly written and is easy to follow.\n\nOriginality:\n+ I believe the paper is novel. The differentiable wireframe rendering is new and very interesting.\n\nSignificance:\n+ I believe the paper has value to the community.\n- The evaluation of the task seems to be challenging (Inception score may not be appropriate) but since this is probably the first paper to generate layouts, I would not worry too much about the actual accuracy.\n\nQuestion:\nWhy not ask the generator to generate the rendering instead of class probabilities?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "cdate": 1542234439582, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745932, "tmdate": 1552335745932, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeCYxhc27", "original": null, "number": 1, "cdate": 1541222533520, "ddate": null, "tcdate": 1541222533520, "tmdate": 1541533913068, "tddate": null, "forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "content": {"title": "Well written paper", "review": "\nThe authors present a GAN based framework for Graphic Layouts. Instead of considering a graphic layout as a collection of pixels, they treat it as a collection of primitive objects like polygons. The objective is to create an alignment of these objects that mimics some real data distribution.\n\nThe novelty is a differentiable wireframe rendering layer allowing the discriminator to judge alignment. They compare this with a relation based discriminator based on the point net architecture by Qi et al. The experimentation is thorough and demonstrates the importance of their model architecture compared to baseline methods. \n\nOverall, this is a well written paper that proposes and solves a novel problem. My only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper533/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.", "keywords": [], "authorids": ["lijianan15@gmail.com", "jimyang@adobe.com", "hertzman@adobe.com", "jianmzha@adobe.com", "ciom_xtf1@bit.edu.cn"], "authors": ["Jianan Li", "Jimei Yang", "Aaron Hertzmann", "Jianming Zhang", "Tingfa Xu"], "pdf": "/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf", "paperhash": "li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators", "_bibtex": "@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper533/Official_Review", "cdate": 1542234439582, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxB5sRcFQ", "replyto": "HJxB5sRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper533/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745932, "tmdate": 1552335745932, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper533/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}