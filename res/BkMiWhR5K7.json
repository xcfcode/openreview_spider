{"notes": [{"id": "BkMiWhR5K7", "original": "S1xPwZ2YYQ", "number": 1206, "cdate": 1538087939394, "ddate": null, "tcdate": 1538087939394, "tmdate": 1550710455871, "tddate": null, "forum": "BkMiWhR5K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJgXFDcHxV", "original": null, "number": 1, "cdate": 1545082747107, "ddate": null, "tcdate": 1545082747107, "tmdate": 1545354480828, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Meta_Review", "content": {"metareview": "This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available. The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used. This setting appears to be pervasive. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "well written, effective and relevant work on blackbox adversarial example generation"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1206/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352924811, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352924811}}}, {"id": "BJx1HlJJpQ", "original": null, "number": 3, "cdate": 1541496886966, "ddate": null, "tcdate": 1541496886966, "tmdate": 1543345537326, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "content": {"title": "good paper, accept", "review": "This paper formulates the black-box adversarial attack as a gradient estimation\nproblem, and provide some theoretical analysis to show the optimality of an\nexisting gradient estimation method (Neural Evolution Strategies) for black-box\nattacks.\n\nThis paper also proposes two additional methods to reduce the number of queries\nin black-box attack, by exploiting the spacial and temporal correlations in\ngradients. They consider these techniques as priors to gradients, and a bandit\noptimization based method is proposed to update these priors. \n\nThe ideas used in this paper are not entirely new. For example, the main\ngradient estimation method is the same as NES (Ilyas et al. 2017);\ndata-dependent priors using spatially local similarities was used in Chen et\nal. 2017.  But I have no concern with this and the nice thing of this paper is \nto put these tricks under an unified theoretical framework, which I really \nappreciate.\n\nExperiments on black-box attacks to Inception-v3 model show that the proposed\nbandit based attack can significantly reduces the number of queries (2-4 times\nfewer) when compared with NES. \n\nOverall, the paper is well written and ideas are well presented.\nI have a few concerns:\n\n1) In Figure 2, the authors show that there are strong correlations between the\ngradients of current and previous steps. Such correlation heavily depends on\nthe selection of step size.  Imagine that the step size is sufficiently large,\nsuch that when we arrive at a new point for the next iteration, the\noptimization landscape is sufficiently changed and the new gradient is vastly\ndifferent than the previous one. On the other hand, when using a very small\nstep-size close to 0, gradients between consecutive steps will be almost the\nsame. By changing step-size I can show any degree of correlation.  I am not\nsure if the improvement of Bandit_T comes from a specific selection of\nstep-size. More empirical evidence on this need to be shown - for example, run\nBandit_T and NES with different step sizes and observe the number of queries\nrequired.\n\n2) This paper did not compare with many other recent works which claim to\nreduce query numbers significantly in black-box attack. For example, [1]\nproposes \"random feature grouping\" and use PCA for reducing queries, and [2]\nuses a good gradient estimator with autoencoder. I believe the proposed method\ncan beat them, but the authors should include at least one more baseline to \nconvince the readers that the proposed method is indeed a state-of-the-art.\n\n3) Additionally, the results in this paper are only shown on a single model\n(Inception-v3), and it is hard to compare the results directly with many other\nrecent works. I suggest adding at least two more models for comparison (most\nblack-box attack papers also include MNIST and CIFAR, which should be easy to\nadd quickly). These numbers can be put in appendix.\n\nOverall, this is a great paper, offering good insights on black-box adversarial\nattack and provide some interesting theoretical analysis. However currently it\nis still missing some important experimental results as mentioned above, and\nnot ready to be published as a high quality conference paper. I conditionally\naccept this paper as long as sufficient experiments can be added during the\ndiscussion period.\n\n\n[1] Exploring the Space of Black-box Attacks on Deep Neural Networks, by Arjun\nNitin Bhagoji, Warren He, Bo Li and Dawn Song, https://arxiv.org/abs/1712.09491\n(conference version accepted by ECCV 2018)\n\n[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking\nBlack-box Neural Networks, by Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia\nLiu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng,\nhttps://arxiv.org/abs/1805.11770\n\n==========================================\n\nAfter discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments. The paper looks much better than before. Thus I increased my rating.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "cdate": 1542234281275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895991, "tmdate": 1552335895991, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eCfrzsA7", "original": null, "number": 16, "cdate": 1543345430282, "ddate": null, "tcdate": 1543345430282, "tmdate": 1543345430282, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkglK_U5C7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Thanks for the update. I will increase my rating to 7", "comment": "Thanks for fixing this bug in experiments. The results look much reasonable now. I will increase my rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "H1gTyuSq0Q", "original": null, "number": 14, "cdate": 1543292900972, "ddate": null, "tcdate": 1543292900972, "tmdate": 1543297165074, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "SJg2WDBcR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Another note", "comment": "Also note that due to the time constraint in getting a revision in, we actually only compared Tu et al to our second-best method, Bandits_T. We are running a comparison with Bandits_{TD} (both time and data priors) and will revise (if time permits) and/or report back the results.\n\n[EDIT: we have now done so, see comment above]"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "BkglK_U5C7", "original": null, "number": 15, "cdate": 1543297144385, "ddate": null, "tcdate": 1543297144385, "tmdate": 1543297144385, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "SJg2WDBcR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Updated again with time and data priors", "comment": "We have updated the paper again (specifically, the comparison with Tu et al) to reflect experiments we have now run with both the time and data prior (Bandits_{TD}). At 100% success rate with the same experimental design, our method now uses over 6 times fewer queries.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "SJg2WDBcR7", "original": null, "number": 13, "cdate": 1543292676319, "ddate": null, "tcdate": 1543292676319, "tmdate": 1543294545995, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "rkgBbCf90X", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Thank you for pointing this out... have revised the paper and updated the result ", "comment": "First of all, we would like to sincerely thank the reviewer for their continually detailed comments and thorough review---it has been great help in improving the manuscript.\n\nUpon checking the code, we realized that (as the reviewer suggested), we had accidentally reproduced the _targeted_ attacks in the baseline code repository. To account for this, we modified our code to work for targeted attacks, and properly replicated the experimental setup, choosing the correct \\ell_2 perturbation bound, and random target classes as in Tu et al (except for the fact that we use the prepackaged Inception-v3 classifier rather than the downloaded one from Tu et al). We don't tune our hyper parameters at all, and use the same ones that we used for untargeted. \n\nOur method achieves the same success rate at over 3 times the query efficiency at 100% success rate (note that this is higher success rate than Tu et al achieve at the same l2 perturbation bound, since there the authors only bound the mean and not the max), still establishing significant improvement. We have uploaded a revision reflecting these changes. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "rkgBbCf90X", "original": null, "number": 10, "cdate": 1543282173449, "ddate": null, "tcdate": 1543282173449, "tmdate": 1543282173449, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "B1ead6lupQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Thanks for adding these results! They look very good, except for a small concern", "comment": "Dear Paper1206  Authors,\n\nThank you for adding these new results. Figure 7 now shows the cosine similarity under different step sizes, which looks convincing. The newly added experiments on different models (ResNet-50, VGG-16) and different datasets (CIFAR and ImageNet), as well as the comparisons to other state-of-the-art methods make this paper look much stronger than before.\n\nI have a concern regarding the comparison with (Tu et al, 2018). The 100-fold reduction looks to good to be true. Can you confirm that you performed the attack under the same setting? e.g., do you run attacks with the same target labels for both methods, or running untargeted attacks for both? I think it is better to double check this.\n\nI am willing to increase my rating to 7 as long as the above concern can be addressed.\n\nThanks,\nPaper1206 AnonReviewer1\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "B1xIAu-qhm", "original": null, "number": 1, "cdate": 1541179597929, "ddate": null, "tcdate": 1541179597929, "tmdate": 1543209490587, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "content": {"title": "A decent paper", "review": "UPDATE:\n\nI've read the revised version of this paper, I think the concernings have been clarified.\n\n-------\n\nThis paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.\n\nAlthough I think this paper is a decent paper that deserves an acceptance, there are several concernings:\n\n1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become \"vanishingly small\", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).\n\n2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.\n\n3. In the experimental results, what is the difference between one \"query\" and one \"iteration\"? It looks like in one iteration, the Algorithm 2 queries twice?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "cdate": 1542234281275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895991, "tmdate": 1552335895991, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJg_v6iOCQ", "original": null, "number": 8, "cdate": 1543187807709, "ddate": null, "tcdate": 1543187807709, "tmdate": 1543187807709, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "B1xIAu-qhm", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Revision [Reply to R2]", "comment": "Thank you again for the review. We have now posted a revision of our paper, and the summary comment above details all of the changes we've made in response to reviewer comments, including several additional experiments and comparisons with other methods. \n\nTo highlight the changes that are most relevant to your review:\n\n1) We now provide an illustration of the bound in Appendix A in the relevant query regimes\n\n2 and 3) We have clarified some points in the papers based on reviewer comments and added significantly more experimental results---we hope that these results further justify the use of the full 10 pages."}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "SJeCGji_A7", "original": null, "number": 7, "cdate": 1543187222329, "ddate": null, "tcdate": 1543187222329, "tmdate": 1543187222329, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "B1lDhBt52X", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Revision [Reply to R3]", "comment": "We have addressed the above comments in our revision, please see the main comment for more details. Thank you again for the review and suggestions."}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "SyglTvjdR7", "original": null, "number": 6, "cdate": 1543186360361, "ddate": null, "tcdate": 1543186360361, "tmdate": 1543186386423, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BJx1HlJJpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Revision [Reply to R1]", "comment": "We have addressed comments (1), (2), and (3) in our revision (details are in the main comment above). To address the raised points directly:\n\n(1) is now addressed in Figure 7 in Appendix B.3 which shows how the time-dependent trend decays with the step size---even at high step sizes the trend persists. Specifically, we plot a graph identical to Figure 2 but for many different step sizes, from norm around 0.03 all the way to 4.0.\n\n(2) Appendix G now shows a comparison with Tu et al ([2] in the original review). See our main comment above for a summary of the results.\n\n(3) We now include results from ImageNet and CIFAR, with Inception-v3, ResNet, and VGG16 in the appendices (more details in our main comment above).\n\nThank you again for the detailed review and the useful suggestions. \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "rklvTWsO07", "original": null, "number": 5, "cdate": 1543184831268, "ddate": null, "tcdate": 1543184831268, "tmdate": 1543184831268, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Revision", "comment": "We thank all the reviewers again for the helpful responses and revision suggestions. We have posted a revision that we believe addresses all the reviewer comments. \n\nIn addition to adding the suggested edits to the paper for clarity, we have now compared our approach with several datasets, baselines, and classifiers, and established a significant margin over state-of-the-art methods. Specifically, we have made the following updates:\n\n\u2014\u2014\u2014\u2014\u2014\nQuantifying time-dependent prior\n\u2014\u2014\u2014\u2014\u2014\nWe include a graph (in the omitted figures appendix) showing that the successive correlation prior (aka the time-dependent prior) holds true even up to very large step sizes. Specifically, we plot a graph identical to Figure 2 but for many different step sizes, from norm around 0.03 all the way to 4.0.\n\n\u2014\u2014\u2014\u2014\u2014\nOther threat models and datasets\n\u2014\u2014\u2014\u2014\u2014\nWe have added an Appendix F corresponding to ImageNet results for VGG16 and ResNet50 classifiers (along with Inceptionv3 copied from the main text for reference). Our methods still outperform NES on these benchmarks, often by a larger margin than shown for Inception-v3 in Table 1.\n\nWe have added an Appendix E corresponding to a comparison of our methods and NES in the CIFAR l-infinity threat model (for L2, we could not find a reasonable maximum \\epsilon from recent literature) with VGG16, Resnet50, and Inceptionv3 networks. \n\n\u2014\u2014\u2014\u2014- \nComparison with another baseline\n\u2014\u2014\u2014\u2014-\n\nEfficiency compared to Tu et al:\n\u2014\u2014\u2014\u2014\u2014\nWe looked into (Tu et al, 2018) and (Bhagoji et al, 2017) as suggested by reviewer 1 to compare with a baseline; we chose to compare with Tu et al (AutoZOOM) since it was (a) released later, (b) uses a more standard classifier than in Bhagoji et al and (c) does not require access to an external set of representative images (unlike Bhagoji et al, which uses this set to find the PCA components). As such, we have added an Appendix comparing our method to that of Tu et al: achieving the same success rate and using the mean perturbation from Tu et al as our maximum perturbation, we achieve a 35-fold reduction in query complexity.\n\nEfficiency compared to Tu et al + fine tuning:\n\u2014\u2014\u2014\u2014\u2014-\n Tu et al also give a \u201cdistortion fine-tuning\u201d technique that attempts to reduce the mean perturbation after the attack. This fine-tuning takes around 100,000 queries, and in the best case, after using around 100,000 queries, reduces the mean perturbation to 0.4e-4 per-pixel normalized, which works out to just over 10 (see Figure 3a in Tu et al). In Appendix F, we show that running our attack with this lower distortion budget directly gives a similar success rate, using an average of around 900 queries as opposed to 100,000, giving more than a *100-fold* reduction in query complexity.\n\n\u2014\u2014\u2014\u2014\nBound illustration\n\u2014\u2014\u2014\u2014\n- To illustrate, we give an example of our own \\ell_2 threat model, where Theorem 1 gives us a bound on the performance gap between NES and least squares, in Appendix 1 (after the proofs).\n\n\u2014\u2014\u2014\u2014\nEdits to paper\n\u2014\u2014\u2014\u2014\n- We noticed that our image normalization for generating Table 1 was slightly incorrect, so we have fixed it and rerun the experiment\u2014this has not changed the output significantly, and our methods still beat NES by the same margin of normalized queries. However, in the interest of correctness, we have updated the numbers in Table 1 to reflect the experiment run with correct normalization.\n- We have made the pseudocode for the bandits attack clearer, and explicitly noted how the data-dependent prior can be included, as well as justifying the boundary projection step\n- Fixed: \\nabla L \u2014> g^* in Figures\n- Fixed: Section 2.4 sentence (as pointed out by Reviewer 3)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "rke9oWZuam", "original": null, "number": 4, "cdate": 1542095265895, "ddate": null, "tcdate": 1542095265895, "tmdate": 1542095265895, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "B1xIAu-qhm", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the detailed comments on the paper. We address the main points below:\n\n1. Typically black-box adversarial attacks are executed in a multi-step fashion, i.e. by using small numbers of queries per gradient estimates, and taking several gradient estimate steps (Ilyas et al, the NES-based attack, for example, uses 50 queries per gradient estimate). While it may be possible to prove tighter bounds, in the 50-query regime with d=268203, the bound is actually rather tight. (Furthermore, during our own preliminary experimentation, least-squares attacks usually performed identically to NES).\n\n2. Section 2.4 is meant to illustrate that without priors, we have essentially hit the limit of query-efficiency in black-box attacks. In particular, NES, which we found to be the current state-of-the-art, actually turns out to be approximately optimal, even from a theoretical perspective. This motivates us to take a new look on adversarial example generation, breaking through this optimality by introducing new information into the problem.\n\nWithout the proof in Section 2.4, one could reasonably hope that there are simply better gradient estimators that we can use as a drop-in replacement for NES. The theorems we prove there instead motivate our bandit optimization-based view. \n\n3. One iteration constitutes two queries (which are used for a variance-reduced gradient estimate via antithetic sampling). In general, the query count refers to queries of the classifier, whereas iteration counts the number of times that we take an estimated gradient step.\n\nWe hope the above points clarify the reviewer's concerns, and thank the reviewer again for the detailed feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "rJgK3ebOpX", "original": null, "number": 3, "cdate": 1542095025461, "ddate": null, "tcdate": 1542095025461, "tmdate": 1542095025461, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "B1lDhBt52X", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the comments!\n\nWe address the main points below:\n\n> Data dependent prior in pseudocode: Yes it is in fact by choice of d, but we agree this can be made clearer in the pseudocode. We will make sure to describe this more clearly in our final paper.\n\n> Figure 4: We will make sure to update this and be more explicit.\n\n> Figure 4c (low cosine similarity): Remarkably, for black-box attacks, though higher cosine similarity is better, the threshold for a successful adversarial attack (in terms of cosine similarity) is extremely low. In particular, for NES, the cosine similarity (as you mentioned) is almost 0, but the gradient estimates *still* result in a successful attack! We show that using our method leads to significantly better estimates of the gradient, though as one would expect in such a query-deficient domain (100s of queries vs 3*10e5 dimensional images), still pretty poor.\n\nWe will also be sure to address all of the minor comments in our final paper. We thank the reviewer again for the useful comments and suggestions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "B1ead6lupQ", "original": null, "number": 2, "cdate": 1542094196841, "ddate": null, "tcdate": 1542094196841, "tmdate": 1542094196841, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BJx1HlJJpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the detailed comments, we will be sure to make these changes in the final version of the paper. As the reviewer correctly identifies, we consider the theoretical framework of online optimization as a basis for all black-box attacks to be one of our most profound contributions. That said, in order to improve the quality of the experimental results, we have addressed and added each suggested experiment. Specifically:\n\n1) We thank the reviewer for raising this---we initially only used the default NES step size (from Ilyas et al) to evaluate the temporal correlation. To give a fuller picture on how this temporal correlation relates with the step size, we have added a new plot in the appendix, which shows the average correlation on a trajectory as a function of the step size. \n\n2) To address this, we have added a table (in the Appendix) which compares our query-efficiency against that of [1] and [2]. It should also be noted, however, that both [1] and [2] can be integrated as \"priors\" on the gradient; in particular, that the gradient lays in some low-dimensional subspace. Our framework gives us a way to formalize these assumptions, and measure how empirically valid they are in order to find better and better black-box attacks.\n\n3) We have also added results on ResNet-50 and VGG-16 on ImageNet, and have also benchmarked our attack on all three classifiers (Inceptionv3, ResNet-50, VGG-16) on CIFAR as well.\n\nWe will be sure to comment again with a revision when the experiments are complete and integrated into the paper. We thank the reviewer again for the valuable suggestions.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}, {"id": "B1lDhBt52X", "original": null, "number": 2, "cdate": 1541211566529, "ddate": null, "tcdate": 1541211566529, "tmdate": 1541533333593, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "content": {"title": "Good paper, low confidence.", "review": "Paper formalizes the gradient estimation problem in a black-box setting, and provs the equivalence of least Squares with NES. It then improves on state of the art by using priors coupled with a bandit optimization technique.\n\nThe paper is well written. The idea of using priors to improve adversarial gradient attacks is an enticing idea. The results seem convincing.\n\nComments:\n- I missed how data dependent prior is factored into the algorithms 1-3. Is it by the choice of d? I suggest a clearer explanation.\n- In fig 4, I was confused that the loss of the methods is increasing. it took me a minute to realize this is the maximized adversarial loss, and thus higher is better. you may want to spell this out for clarity. I typically associate lower loss with better algorithms.\n- I am confused by Fig 4c. If I am comparing g to g*, I do expect a high cosine similarity. cos = 1 is the best. Why is correlation so small? and why is it 0 for NES? You may also want to offer additional insight in the text explaining 4c. \n\nMinor comments:\n- Is table one misplaced?\n- The symbol for \"boundary of set U\" may be confused with a partial derivative symbol\n- first paragraph of 2.4: \"our estimator a sufficiently\". something missing?\n- \"It is the actions g_t (equal to v_t) which...\" refering to g_t as actions is confusing. Although may be technically correct in bandit setting\n- Further explain the need for the projection of algorithm 3, line 7.\n- Fig 4: refer to true gradient as g*\n\nCaveat: Although I am well versed in bandits, I am not familiar with adversarial training and neural network literature. There is a chance I may have misevaluated central concepts of the paper.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Review", "cdate": 1542234281275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMiWhR5K7", "replyto": "BkMiWhR5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895991, "tmdate": 1552335895991, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgk8e_hFm", "original": null, "number": 1, "cdate": 1538191430906, "ddate": null, "tcdate": 1538191430906, "tmdate": 1538191440771, "tddate": null, "forum": "BkMiWhR5K7", "replyto": "rkg0C6Rjt7", "invitation": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "content": {"title": "Re: Interesting", "comment": "Thank you for the comment!\n\nAs a recent work (e.g. https://arxiv.org/pdf/1804.08598.pdf, https://arxiv.org/pdf/1807.04457.pdf ) has shown, adapting to the label-only setting can be implemented as a modification of the loss function---since our method represents a general framework for gradient estimation through a loss function, the same technique can be used."}, "signatures": ["ICLR.cc/2019/Conference/Paper1206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. The code for reproducing our work is available at https://git.io/fAjOJ.", "keywords": ["adversarial examples", "gradient estimation", "black-box attacks", "model-based optimization", "bandit optimization"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "madry@mit.edu"], "authors": ["Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "TL;DR": "We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.", "pdf": "/pdf/dc922426472f0f1907b6247b8d425abdd084dc2b.pdf", "paperhash": "ilyas|prior_convictions_blackbox_adversarial_attacks_with_bandits_and_priors", "_bibtex": "@inproceedings{\nilyas2018prior,\ntitle={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},\nauthor={Andrew Ilyas and Logan Engstrom and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMiWhR5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605815, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMiWhR5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1206/Authors|ICLR.cc/2019/Conference/Paper1206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1206/Reviewers", "ICLR.cc/2019/Conference/Paper1206/Authors", "ICLR.cc/2019/Conference/Paper1206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605815}}}], "count": 18}