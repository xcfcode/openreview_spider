{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1394815440000, "tcdate": 1394815440000, "number": 10, "id": "ysYmCPhSlgn4_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["Michiel hermans"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Nice paper. Good to see that the deep RNN thread is being continued. A few thoughts, questions and ideas (I\u2019m afraid it became a bit more lengthy than initially anticipated):\r\n\r\n- I only have one true criticism on the content of the paper. The biggest issue I have with the results on the character-wise prediction task (the only task I have a little experience in myself) is that the performance seems to correlate rather strongly with the number of trainable parameters in each network. I fear this might in the end play a more important role than the actual network architecture. What I found when I was working on a highly similar task is that the number of parameters is the single biggest determining factor in model performance, simply because of (much needed) raw storage power. Network architecture itself only provides a modest improvement. I would find the results in this part far more convincing if all architectures are given the same number of trainable parameters.\r\n\r\n- Concerning deep transitions: the authors rightfully state that deep RNNs can be deep both in their transition and in a more spatial sense. When I was working with deep (stacked) RNNs, in the end the reason I didn\u2019t try deep transitions (by feeding top-layer input back into the bottom layer at the next time step) was the fact that it slowed down the simulations too much (harder to exploit parallel computing architectures). I wonder if the authors could give some comments on how fast the simulations are for each model.\r\n\r\n- The discussion at the end of 3.2.4. is interesting. Stacked RNNs could be stated to have a `deep\u2019 transition, but only from lower to higher layers. This means that the higher layers can represent complex features of a summarised history of the input. In this sense, they should also be able to model complicated state transitions, as each time step new information needs to travel through all the layers. What they are *not* able to do is have deep, hierarchical transitions between the high-level representations themselves. The question then arises whether this is *necessary*, as a high level representation should already consist of a conceptualised representation of the input information, in which properties of the input, including complicated transitions, should already be easier to perform.\r\nAfter reading your paper, I believe the best way to systematically investigate deep recurrent topologies is as follows: represent each time step as a vertical hierarchy with a fixed number of layers, and only study different way the layers are connected in time (over a state transition). In this way all topologies discussed in the paper, and more, can be represented and studied in a systematic manner. By keeping the number of layers fixed you also eliminate any performance difference caused by the data running through more or less layers. I hope this is more or less clear, as I cannot post a visual representation of what I mean."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392727380000, "tcdate": 1392727380000, "number": 9, "id": "Jv_Eir6jMQJ9D", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "A new version of the paper is available on arXiv.org, which incorporates most of the comments made by the reviewers. In particular, we have made the following changes:\r\n\r\n* (69d2) In the abstract we say that we 'explore' instead of 'propose a novel way'.\r\n\r\n* (69d2) Changed from 'modeling variable-length sequences' to 'evaluated on two sequence modeling tasks'.\r\n\r\n* (69d2) Removed the normalization by the length of a sequence in Eq. (3)\r\n  \r\n* (69d2) Added information on how many levels of recurrent layers we used with sRNN.\r\n  \r\n* (bfb0) Added the number of parameters for each model in Table 1. However, note that the model sizes were selected based on validation errors, meaning larger models also have been tried.\r\n\r\n* (Bayer) Emphasized the difference between sRNN and DT-RNN. Also, strengthend the intuition behind the reason why the deep\r\ntransition is a useful extension to RNN which is orthogonal to the other variants of deep RNNs.\r\n\r\n* (1f07) Added more details about Pinheiro and Collobert (2013).\r\n\r\n* (1f07) Added the previous results by Mikolov and Tomas in Table 3.\r\n\r\n* (1f07) Added a footnote briefly explaining the method of dynamic evaluation.\r\n\r\nWe would like to thank all the three reviewers for their comments. We are still aiming to improve the paper, particularly by having started more experiments as suggested by the reviewers.\r\n\r\nWe believe that our paper provides a novel perspective on the depth of RNNs and the empirical evidence showing that this direction of research is worth pursuing. We hope the reviewers find this work up to the quality standard of the conference."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392710040000, "tcdate": 1392710040000, "number": 1, "id": "aZX4ZRSflnZoq", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "M-KW-SxV2Q5Nd", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments, Justin! \r\n\r\n'I think this paper is important especially because of the new deep transitions.'\r\n\r\nWe also believe that RNNs will be found to be useful with these deep transitions on many non-trivial sequential data in the future. Thanks for agreeing with us.\r\n\r\n'the amount of layers information travels through scales faster for a DOT-RNN than for an sRNN.'\r\n\r\nYou're correct. Although we have only demonstrated empirically the design of deep transition using a single intermediary layer, it is possible to easily extend it to have multiple intermediary layers without much computational burden. This is in contrast to the sRNN, as the states of the intermediary layers do not need to be carried across time. We will try to make it more clear in a later revision.\r\n\r\n'I applaud the availability of the source code'\r\n\r\nThank you. We hope our code will help researchers to start working on these kinds of deep RNNs. Especially, as you pointed out, we believe more research into learning algorithms for these models is needed.\r\n\r\nThanks again!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392233760000, "tcdate": 1392233760000, "number": 8, "id": "M-KW-SxV2Q5Nd", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The paper proposes ways of extending ordinary RNNs to deep RNNs, where deep operations are used in favor of shallow (=affine with an elementwise nonlinearity) at various stages.\r\n\r\nI think this paper is important especially because of the new deep transitions. It will help practitioners as well as researchers with designing models for sequential data.\r\n\r\nDuring my first read I was not sure what the actual benefit of a DOT-RNN is compared to an sRNN with one additional hidden layer--but if one looks closely, the amount of layers information travels through scales faster for a DOT-RNN than for an sRNN. Maybe the authors would like to explicitly state this (maybe they did and I missed it).\r\n\r\nI also think it is valuable that the optimisation of these models has been identified as one of the major problems, such that the community can focus on trying to improve on this. I applaud the availability of the source code.\r\n\r\nThe experimental evaluation is solid and the results are excellent."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391934420000, "tcdate": 1391934420000, "number": 1, "id": "FF_2O4ik2IFc1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "oz9eoHf8xCFu1", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (1f07),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'Do we achieve additional improvements ? Is it possible to train these networks with back-prop (through time) ?'\r\n\r\nThat is a good point. Our proposed construction does not limit the number of intermediary layers, which should be more thoroughly evaluated with larger, more complex dataset in the future. For the second question, yes, it is possible to train any type of the proposed deep RNNs with BPTT without any modification.\r\n\r\n'In my opinion, you do not consider deep input-to-hidden RNN.'\r\n\r\nYou are correct. This is because our focus was more on the temporal aspect of recurrent neural networks and partly because there has already been some work such as (Chen & Deng, 2013) (cited in the paper) showing benefits of having deep input-to-hidden layer.\r\n\r\n'The input-to-hidden operator could be quite deep since you won't have the problem of vanishing gradients over time.'\r\n\r\nThank you for your suggestion. We will in the future need to experiment with datasets that exhibit strongly non-temporal structure that may benefit from deep input-to-hidden function.\r\n\r\n'How do you initialize the weights of the additional hidden layers ? Is this really comparable to layer-wise pre-training of deep MLPs ? Layer-wise pre-training is unsupervised while you need targets. Maybe we should say that you initialize the weights so that the network is easier to train ? Can you elaborate how important this initialization is ?'\r\n\r\nWe have initialize such that any 'common' parameters between, for instance, DT-RNN and DOT-RNN are pretrained as DT-RNN, however, in a rather supervised fasion. We have mentioned about the potential connection to the layer-wise pretraining, in the sense that the next-step-prediction tasks we considered may also be seen as an unsupervised learning task. \r\n\r\nIn our experiments, when we used logistic sigmoid functions as the activation functions for intermediary layers, the use of pretraining was important. However, a short experiment using L_p units and dropout showed that the importance of pretraining depends on the model structure/design and may not be mandatory for all deep RNNs.\r\n\r\n'please add the results of Graves and Mikolov to Table 3'\r\n\r\nWe will update the paper soon with Graves and Mikolov's results in Table 3.\r\n\r\n'please explain 'dynamic evaluation''\r\n\r\nWe have skipped the explanation of the dynamic evaluation as we have not used it during our experiments. However, we agree that it is better to make it clear by explanaining it. We will update the paper soon.\r\n\r\n'In general, it would be helpful that the authors provide more details on the architectures which do NOT work very well.'\r\n\r\nThanks for your suggestion. Since we have extensively searched for hyperparameters including nonlinearities as well as structures of deep RNNs, we believe we may be able to take advantage of the validated models to take a glimpse at some of the model choices that did not work as well. We will try to briefly summarize our findings (on model choices with negative results) in the future version of the paper.\r\n\r\n'The authors should discuss the relation to other works in more detail, in particular Pinheiro and Collobert (2013)'\r\n\r\nThank you for the suggestion. We will try to draw more detailed connection to other relevant works, including (Pinheiro& Collobert, 2013), in the later versions of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391899080000, "tcdate": 1391899080000, "number": 1, "id": "tPUcVC29cHtdk", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "QEaoQyvnoZJq-", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We have forgotten to list the references we cited in our response. Here is the list of references:\r\n\r\nYoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu. Advances in Optimizing Recurrent Networks. (2012) arXiv:1212.0901 [cs.LG].\r\nBayer, J., Osendorfer, C., Korhammer, D., Chen, N., Urban, S., and van der Smagt, P. (2013). On fast dropout and its applicability to recurrent networks. arXiv:1311.0701 [cs.NE].\r\nChen, J. and Deng, L. (2013). A new method for learning deep recurrent neural networks. arXiv:1311.6091 [cs.LG]."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391876880000, "tcdate": 1391876880000, "number": 7, "id": "oz9eoHf8xCFu1", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["anonymous reviewer 1f07"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of How to Construct Deep Recurrent Neural Networks", "review": "This paper proposes three techniques to make recurrent neural networks (RNN) 'deeper'. In addition to an inherent deepness over time, the authors propose to consider additional hidden layers for all the transition matrices, i.e. input-to-hidden, hidden-to-hidden and hidden-to-output.\r\n\r\nTo implement those ideas, neural operators are introduced:\r\n - predicting the next hidden state in function of the previous one and the input\r\n - predicting the output given the current hidden state\r\n\r\nthose operators are implemented with multi-layer neural networks, in principle of any arbitrary depth.  However, only one-hidden layer operator networks are considered.  I think that it would be really interesting to consider even deeper operators. Do we achieve additional improvements ?  Is it possible to train these networks with back-prop (through time) ?\r\n\r\nIn my opinion, you do not consider deep input-to-hidden RNN. In Figure 2b you propagate h_t-1 and x_t through the SAME hidden layer to h_t. I propose to use separate hidden layers for these tasks, i.e. one deep operator to map the input to the hidden state, and an INDEPENDENT deep operator to map from one hidden to the next hidden state. The input-to-hidden operator could be quite deep since you won't have the problem of vanishing gradients over time.\r\n\r\nYou write that you initialize the sRNN and DOTS-RNN with the weights of a normal RNN to simulate layer-wise pretraining. How do you initialize the weights of the additional hidden layers ?  Is this really comparable to layer-wise pre-training of deep MLPs ? Layer-wise pre-training is unsupervised while you need targets. Maybe we should say that you initialize the weights so that the network is easier to train ? Can you elaborate how important this initialization is ?\r\nDo the deep RNN still converge when starting from random weights ?\r\n\r\nLanguage modeling: \r\n - please add the results of Graves and Mikolov to Table 3\r\n - please explain 'dynamic evaluation'\r\n\r\nIn general, it would be helpful that the authors provide more details on the architectures which do NOT work very well.  This would allow the reader to assess the importance of the different tricks, e.g. short cut connection, initialization scheme, etc.  By these means, one could more easily reproduce the experiments and apply the same ideas to other tasks.\r\n\r\nThe authors should discuss the relation to other works in more detail, in particular Pinheiro and Collobert (2013)\r\n\r\nOverall, this paper presents interesting ideas and opens directions for further research."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391823840000, "tcdate": 1391823840000, "number": 1, "id": "QEaoQyvnoZJq-", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "skPGs-hR1dkQN", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (bfb0),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'There is no best architecture so there's not much one can learn except to try them all which is not very informative'\r\n\r\nWe believe one important lesson from our experiments is that, as was the case with feedforward network, it is beneficial to build a 'deep' recurrent neural network. We expect that the optimal structure of deep RNN depends on dataset, which was apparent from our experiments on both music and language. The choice of deep RNNs among different combinations should consider that each dataset has distinct characteristics (e.g. one data might show highly nonlinear transition, while another data might show simple, linear transition).\r\n\r\n'What happens if you train a standard RNN with the same bells and whistles ? Conversely, which of the 3 techniques made the deep models better than the RNN?'\r\n\r\nThis is a good point. Thank you for pointing this out for us. \r\n\r\nThere have been a few recent research work trying these advanced techniques proposed for deep feedforward neural networks on RNNs.  For instance, Bengio et al. (2012) tried using rectifiers for RNNs with leaky integration units, but with only modest success in terms of the performance on the same music datasets (worse than our result). Justin et al. (2014) which is cited in the paper proposed the method of fast dropout for RNNs, but again the result is slightly worse than the best result we obtained with deep RNNs having L_p units trained with (ordinary) dropout.\r\n\r\nUntil recently, however, not many advances in deep feedforward neural networks have been applied to and tested extensively on recurrent neural networks. We believe in the future it will be important to thoroughly validate these new techniques (e.g., dropout, piece-wise linear units, non-saturating nonlinearities ...) with RNNs.  \r\n\r\n\r\n'The authors describe a deep input to hidden function model (3.2.1) however no experimental evidence of its utility is shown'\r\n\r\nWe focused more on the temporal aspect of recurrent neural networks. There has been already work such as (Chen & Deng, 2013) (which is cited in the paper) showing potential benefits of having a deep input-to-hidden function. \r\n\r\n'Adding columns with the number of parameters to Table 1 would help the reader compare the sizes of the different models'\r\n\r\nThanks for the suggestion. We will add the numbers of parameters to Table 1 later when we revise the paper.\r\n\r\n'What happens if you train a DO-RNN?'\r\n\r\nWe have not tried using a DO-RNN. We will try some additional experiments with DO-RNN and add them to the paper soon."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391738340000, "tcdate": 1391738340000, "number": 1, "id": "00EQUJ_ix30nA", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "M_-3MlaAkti8X", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (69d2),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'Interestingly, the effect of using dropout, maxout and other generic neural network tricks seems to be much greater than the effect of changing the architecture.'\r\n\r\nIt is not trivial to apply some of the successful tricks/methods from the recent research on feedforward neural networks. For instance, rectifiers and maxout which have helped convolutional neural networks achieve impressive recognition performance on computer vision tasks are not suitable to be used for hidden states of conventional shallow recurrent neural networks, due to their non-saturating property. We expect that our approach of having a deeper hidden-to-hidden transition will let many of those recent advances to be explored more actively in the context of recurrent neural networks.\r\n\r\n'the standard shallow RNN with a small hidden layer appears to perform much better than similar networks elsewhere in the literature, e.g. by Tomas Mikolov'\r\n\r\nAs we have described in the paper, we have done an extensive hyperparameter search and also used some algorithms not used by Tomas Mikolov (e.g., weight noise). \r\nThese procedures were equally used with deep RNNs, and the purpose of this extensive search was to ensure that the gain deep RNNs show in our paper is significant and that we do not artificially lower the baseline.\r\n\r\n'A few more experimental details would help to clarify this.'\r\n\r\nWe agree with you and will make available the sets of hyperparameters we used soon. Also, the base code for all the experiments is already available at https://github.com/pascanur/GroundHog/.\r\n\r\n'the paper doesn't really propose 'a novel way' to extend RNNs so much as explore several possible ways'\r\n\r\nAlthough we find some of the proposed approaches (especially, deep hidden-to-hidden transition) novel, we will consider rephrasing some sentences to make it clear that these proposed approaches are some of many possible ways to build deep RNNs.\r\n\r\n''modelling variable-length sequences' is a bit too broad to be a 'task'... maybe 'evaluated on two sequence modelling tasks'?'\r\n\r\nThanks for your suggestion. We will try to rephrase it to be more specific.\r\n\r\n'The 'In Sec. 2' paragraph is more confusing than helpful'\r\n\r\nWe wanted to give an outline of the remaining of the paper, but if the paragraph sounds confusing, we will try to rephrase to be more clear and concise.\r\n\r\n'Eq. (3): do the denominators (1/N and 1/T_n) really belong in the loss? I'd say not... otherwise training would be biased towards shorter sequences and smaller datasets. The dataset factor can be incorporated into the learning rate, but I don't think you want to normalise by sequence length.'\r\n\r\nYou're correct about this. Thanks for pointing this out. In our experiments, we did not divide by the length of sequence. We will fix the equation.\r\n\r\n'I found the repetition of the h_t and y_t terms in eqs (4) and (5) confusing'\r\n\r\nIf you could explain a bit more about what confuses you, we will fix it accordingly.\r\n\r\n'How many hidden layers did the 'sRNN' have?\r\n\r\nWe always used 2 levels of recurrent layers for the sRNN's. We will clarify this in the updated version later.\r\n\r\n''Practical Variational Inference for Neural Networks' would be a better reference for adaptive/fixed weight noise.'\r\n\r\nThank you for pointing this out. We will update the citation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391723820000, "tcdate": 1391723820000, "number": 6, "id": "skPGs-hR1dkQN", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["anonymous reviewer bfb0"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of How to Construct Deep Recurrent Neural Networks", "review": "Solid paper describing a variety of 'deep' RNN architectures constructed by adding additional layers between input and hidden, hidden and hidden and hidden and output. The authors test their models on two types of data (polyphonic music prediction and word and character-based language modeling) and obtain good results (at least on the LM task). While the arguments they make seem compelling, we feel that the experimental results are not very convincing. In particular, on the music task there are a few problems:\r\n- There is no best architecture so there's not much one can learn except to try them all which is not very informative. \r\n- To compete with an RNN with fast dropout, the authors add a lot of bells and whistles to their model (L_p units, maxout units, dropout). What happens if you train a standard RNN with the same bells and whistles ? Conversely, which of the 3 techniques made the deep models better than the RNN? A breakdown of the improvements would be helpful.\r\n- The authors describe a deep input to hidden function model (3.2.1) however no experimental evidence of its utility is shown.\r\n- Adding columns with the number of parameters to Table 1 would help the reader compare the sizes of the different models.\r\n- What happens if you train a DO-RNN?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391640600000, "tcdate": 1391640600000, "number": 5, "id": "M_-3MlaAkti8X", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["anonymous reviewer 69d2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of How to Construct Deep Recurrent Neural Networks", "review": "The paper explores a variety of ways of making RNNs 'deep'. Some of these techniques, such as stacking multiple RNN hidden layers on top of each other, and adding additional feedforward layers between the input and hidden layers, or between hidden and output, have been explored in the literature before. Others, such as adding extra layers between the hidden to hidden transitions, are more unusual. In its simplest form this would be more or less equivalent to inserting a 'null input' at every second step in the input sequences. But as the authors point out, this would exacerbate vanishing gradients, so they add skip connections too.\r\n\r\nAs with all neural networks, the space of RNN topologies is vast. This paper doesn't cover a huge amount of new territory, but it does do a good job of exploring a few promising variants. \r\n\r\nThe experimental results don't suggest a massive benefit from deeper architectures, or a consistent advantage of one over the other, but they do show that adding depth can make a difference. Interestingly, the effect of using dropout, maxout and other generic neural network tricks seems to be much greater than the effect of changing the architecture.\r\n\r\nThe results on the Penn Treebank data are very good. But it isn't entirely clear why they're so good. In particular, the standard shallow RNN with a small hidden layer appears to perform much better than similar networks elsewhere in the literature, e.g. by Tomas Mikolov. Is this a consequence of the training method, the regularisation, or something else? A few more experimental details would help to clarify this.\r\n\r\nSmall comments:\r\n\r\nabstract: - the paper doesn't really propose 'a novel way' to extend RNNs so much as explore several possible ways.\r\n- 'modelling variable-length sequences' is a bit too broad to be a 'task'... maybe 'evaluated on two sequence modelling tasks'?\r\n- The 'In Sec. 2' paragraph is more confusing than helpful\r\n- Eq. (3): do the denominators (1/N and 1/T_n) really belong in the loss? I'd say not... otherwise training would be biased towards shorter sequences and smaller datasets. The dataset factor can be incorporated into the learning rate, but I don't think you want to normalise by sequence length.\r\n- I found the repetition of the h_t and y_t terms in eqs (4) and (5) confusing\r\n- How many hidden layers did the 'sRNN' have? I couldn't find this.\r\n- 'Practical Variational Inference for Neural Networks' would be a better reference for adaptive/fixed weight noise."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391052720000, "tcdate": 1391052720000, "number": 3, "id": "3bFcksTR-gbyq", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have just submitted a new version (v3) of the paper with some minor revisions and additional experimental results. The new version will be available from arXiv on 31 Jan at 1 am GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391052720000, "tcdate": 1391052720000, "number": 4, "id": "RJarRzwV0bJq4", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have just submitted a new version (v3) of the paper with some minor revisions and additional experimental results. The new version will be available from arXiv on 31 Jan at 1 am GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389083160000, "tcdate": 1389083160000, "number": 2, "id": "CQS2C_yz2aQof", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The new version (v2) of the paper is now available at arXiv!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388926740000, "tcdate": 1388926740000, "number": 1, "id": "7FGDPbn3yeFBx", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "L80PLIixPIXTH", "replyto": "-Hns7xQADuKaP", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for the valuable comments! A new version will be available online in a couple of days with all the mistakes you have pointed out fixed and with a revised experiments section. \r\n\r\nI will post a comment here, once the new version is available on arXiv!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388844480000, "tcdate": 1388844480000, "number": 1, "id": "-Hns7xQADuKaP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "L80PLIixPIXTH", "replyto": "L80PLIixPIXTH", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Cool paper!\r\n\r\nsome minor edits:\r\n\r\n2. 1st sentence: discrete -> discrete-time\r\n\r\n3.1 2nd sentence: result -> results.  I would rephrase as: 'A number of recent theoretical results support this hypothesis'\r\n\r\n3.2\r\nFurthermore, there IS a wealth...\r\n\r\n'in this paper we only consider feedforward, intermediate layers' - If I'm reading this right, there should be no comma.\r\n\r\n3.3.1 'approximate f_h instead' In your notation isn't f_h the function that the network performs, not the 'true' transition function you hope to approximate?\r\n\r\n'We call this RNN with a multilayered transition function a deep transition RNN (DT-RNN).' - this was already defined in 3.2\r\n\r\n3.3.3 I'd cite Schmidhuber again in the first sentence\r\n\r\n4. I didn't understand this sentence: 'Note that this is different from (Mikolov et al., 2013a) where the learned embeddings of words were suitable for algebraic operators'\r\n\r\n5. 'on three different tasks' -> 'on two...' same problem next sentence"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387781580000, "tcdate": 1387781580000, "number": 27, "id": "L80PLIixPIXTH", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "L80PLIixPIXTH", "signatures": ["r.pascanu@gmail.com"], "readers": ["everyone"], "content": {"title": "How to Construct Deep Recurrent Neural Networks", "decision": "submitted, no decision", "abstract": "In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.", "pdf": "https://arxiv.org/abs/1312.6026", "paperhash": "pascanu|how_to_construct_deep_recurrent_neural_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@aalto.fi", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 17}