{"notes": [{"id": "Syl38yrFwr", "original": "rJlYddpOPr", "number": 1743, "cdate": 1569439571571, "ddate": null, "tcdate": 1569439571571, "tmdate": 1577168272883, "tddate": null, "forum": "Syl38yrFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SOz567xbqI", "original": null, "number": 1, "cdate": 1576798731376, "ddate": null, "tcdate": 1576798731376, "tmdate": 1576800905092, "tddate": null, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a differentially private mechanism, called Noisy ArgMax, for privately aggregating predictions from several teacher models. There is a consensus in the discussion that the technique of adding a large constant to the largest vote breaks differential privacy. Given this technical flaw, the paper cannot be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724372, "tmdate": 1576800276008, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Decision"}}}, {"id": "B1gTzqlMjB", "original": null, "number": 2, "cdate": 1573157397202, "ddate": null, "tcdate": 1573157397202, "tmdate": 1573246809926, "tddate": null, "forum": "Syl38yrFwr", "replyto": "H1eUlfF_tH", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment", "content": {"title": "Individual Sensitivity Analysis ", "comment": "Thank you for reviewing and comments. The following are the reply to some of your comments.\n\nIn fact, we mainly follow each step as in PATE. In PATE, for each student sample x, it will create an individual mechanism, and then multiple student data samples lead to multiple individual mechanisms. The difference here is that we propose to use each function f_{D, x} which adds a constant to the max, where D is the sensitive dataset. This can potentially make the sensitivity among different mechanisms different. Then, based on different sensitivities of different mechanisms, we add the noise based on each sensitivity of each individual function.\n\nSince both PATE and our work are data-dependent privacy analysis for each mechanism. In this case, what you mentioned \"If c>0, then one sample point can change the count by 1+c\" would not be for all mechanisms. Most mechanisms (with different sample points) can still hold the fact that at most change the count by 1 with adding a constant c, instead of c + 1. \n\nLastly, we estimate the total privacy budget by using the composition theorem. As such, we think our proposed approach is technically correct. Please let us know if you have any further questions and concerns. Thanks a lot again.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl38yrFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1743/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1743/Authors|ICLR.cc/2020/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151533, "tmdate": 1576860538498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment"}}}, {"id": "rylLJjxMoH", "original": null, "number": 3, "cdate": 1573157598336, "ddate": null, "tcdate": 1573157598336, "tmdate": 1573157598336, "tddate": null, "forum": "Syl38yrFwr", "replyto": "HkxrciCwtB", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment", "content": {"title": "Both PATE and our work are data-dependent approach with differential privacy guarantee", "comment": "Thank you for reviewing and comments. The following are the reply to some of your comments.\n\nOur work closely follows PATE uses the sensitive data D and partition it into n sub-dataset. Then individual teachers are trained on each data partition. After a student query sample x, then all teachers can return a voting result with the noise perturbation for privacy concerns. It is clear to see that due to the partition policy and uses the partition sub-dataset to train teachers, PATE is a data-dependent approach. Mathematically, PATE has a sensitive data D as input, and need a voting vector output V, the data transformer function f is f_{D, x}(D) -> V, where f is dependent on both sensitive data and student query, then it can transform the sensitive data D to voting vector V. In the whole process of PATE, for each student sample x, the data transformer function f_{D, x} is different, then PATE uses the composition theorem to estimate the total privacy loss of multiple mechanisms. \n\nWe follow each step as in PATE, and the difference here is that we use a different function f which adds a constant to the max. This can potentially make the sensitivity among different mechanisms different. However, for each mechanism (i.e. given corresponding query x and dataset D), the sensitivity is fixed and will not be changed. Just like PATE need to know the partition information to bound and estimate the sensitivity of each mechanism, we also need to estimate the sensitivity of each of our mechanisms based on the dataset and the output of the data transformer f_{D, x}. Finally, also use the composition theorem to estimate the total privacy loss of multiple mechanisms. As such, we think our proposed approach is technically correct.\n\nPlease let us know if you have any further questions and concerns. Thanks a lot again.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl38yrFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1743/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1743/Authors|ICLR.cc/2020/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151533, "tmdate": 1576860538498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment"}}}, {"id": "Hkli6KxzoH", "original": null, "number": 1, "cdate": 1573157314765, "ddate": null, "tcdate": 1573157314765, "tmdate": 1573157314765, "tddate": null, "forum": "Syl38yrFwr", "replyto": "SyeKgE2JqS", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment", "content": {"title": "The final composition is also differential private.", "comment": "Thank you for reviewing and comments. The following are the reply to some of your comments.\n\nYes, our composition theorem is also differential private (DP). In more detail, our work closely follows PATE uses the sensitive data D and partition it into n sub-dataset. Then individual teachers are then trained on each data partition. After a student query sample x, all teachers can return a voting result with the noise perturbation for privacy concerns. Same as in PATE, for each query sample from the student, there is a particular mechanism, and we use the composition theorem to estimate the total privacy loss of multiple mechanisms. It still satisfies the DP requirements. As such, we think our proposed approach is technically correct. Please let us know if you have any further questions and concerns. Thanks a lot again.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl38yrFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1743/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1743/Authors|ICLR.cc/2020/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151533, "tmdate": 1576860538498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Comment"}}}, {"id": "HkxrciCwtB", "original": null, "number": 1, "cdate": 1571445644889, "ddate": null, "tcdate": 1571445644889, "tmdate": 1572972429269, "tddate": null, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an improvement on the PATE framework for achieving near-zero privacy cost, and showed privacy analyses and experimental evaluations of the proposed method. \n\nThe proposed method can be technically flawed. Adding a consent to the max will not guarantee privacy unless you account for the privacy cost for testing whether the distance of f(D) is larger than 2. This is because the distance of f(D) is data-dependent and revealing it violates privacy. Since the whole privacy analysis of PATE is based on the privacy guarantee of the Noisy ArgMax, the epsilon calculated here is voided."}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575745534843, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Reviewers"], "noninvitees": [], "tcdate": 1570237732935, "tmdate": 1575745534858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review"}}}, {"id": "H1eUlfF_tH", "original": null, "number": 2, "cdate": 1571488238425, "ddate": null, "tcdate": 1571488238425, "tmdate": 1572972429231, "tddate": null, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the teacher ensembles setting for differentially private learning. In this setting, each teacher holds part of the training set and trains a local model. The student uses unlabeled examples to query teacher model. Then the student trains a model from scratch using the examples labeled by teachers.\n\nIn order to make the labeling process differentially private, previous work uses noisy argmax mechanism. Each class of label is assigned with a count number. The student first queries the same example to multiple teachers. To guarantee differential privacy, the counts are perturbed by noise before releasing. Then, because of the post-processing property of differential privacy, the argmax operator on such noisy counts are still differentially private.\n\nThis paper proposes to add a constant c to the largest count before perturbing and releasing the counts. The authors argue this would improve the accuracy of the noisy argmax operator and yield the same privacy loss as previous approach. However, adding a constant c would increase the sensitivity and therefore degenerates the privacy guarantee. The added noise cannot guarantee the privacy if all others are the same as previous work. To see this clearer, for example, if c=0, then one sample point can at most change the count by 1. If c>0, then one sample point can change the count by 1+c. Because of this, the proposed method cannot guarantee the amount of differential privacy as the paper claimed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575745534843, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Reviewers"], "noninvitees": [], "tcdate": 1570237732935, "tmdate": 1575745534858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review"}}}, {"id": "SyeKgE2JqS", "original": null, "number": 3, "cdate": 1571959792557, "ddate": null, "tcdate": 1571959792557, "tmdate": 1572972429196, "tddate": null, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "To improve the privacy-utility tradeoff, this manuscript proposes a voting mechanism used in a teacher-student model, where there is an ensemble of teachers, from which the student can get gradient information for utility improvement. The main idea of the proposed approach is to add a constant C to the maximum count collected from the ensemble, and then noise is furthermore added to the new counts. I can understand that by adding the large constant C, the identity of the maximum count could be preserved with high probability, leading to a better utility on the student side. However, equivalently, this could also be understood as that the noise is not added uniformly across all the counts, but instead a relatively smaller noise is added to the maximum count. Hence it is not clear to me whether the final composition will still be differentially private?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1743/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575745534843, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1743/Reviewers"], "noninvitees": [], "tcdate": 1570237732935, "tmdate": 1575745534858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Official_Review"}}}, {"id": "BJgZqAHoPS", "original": null, "number": 1, "cdate": 1569574537382, "ddate": null, "tcdate": 1569574537382, "tmdate": 1569574537382, "tddate": null, "forum": "Syl38yrFwr", "replyto": "Syl38yrFwr", "invitation": "ICLR.cc/2020/Conference/Paper1743/-/Public_Comment", "content": {"comment": "Hello,\n\nI believe that adding a big constant c depending on who has the largest count does not protect the information of who is the item with largest count, as going from D to D' one would first change the counts (affecting privacy) and only then add the constant. Basically, if on D a given item is the one with the largest count, you're adding the constant to make sure (or with very high prob) such item remains the largest count even after noise addition. That's basically just selecting the item with largest count. And even doing that for a gap > 2 between first/second is not DP.\n\nHaving \"largest count\" minus \"second largest count\" greater than 2 is not sufficient to satisfy DP, as this property itself is data dependent. The way to use this gap usually is to test privately that D is far from a dataset where the item with largest count is not the first anymore. This is normally done using Propose-Test-Release.\nI suggest looking at the papers \"Differentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso\" and \"Model-Agnostic Private Learning\" to see more about this.\n\nSo in essence, the way the mechanism is working is revealing information about the item with largest count on D, so not really differentially private.", "title": "Not really differentially private"}, "signatures": ["~Ricardo_Silva_Carvalho1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ricardo_Silva_Carvalho1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["james.lichao.sun@gmail.com", "yingbo.zhou@salesforce.com", "jia.li@salesforce.com", "rsocher@salesforce.com", "psyu@uic.edu", "cxiong@salesforce.com"], "title": "Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles", "authors": ["Lichao Sun", "Yingbo Zhou", "Jia Li", "Richard Socher", "Philip S. Yu", "Caiming Xiong"], "pdf": "/pdf/d315f5b524580c06a1994893543c610fcffce83f.pdf", "abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).", "keywords": [], "paperhash": "sun|nearzerocost_differentially_private_deep_learning_with_teacher_ensembles", "original_pdf": "/attachment/d315f5b524580c06a1994893543c610fcffce83f.pdf", "_bibtex": "@misc{\nsun2020nearzerocost,\ntitle={Near-Zero-Cost Differentially Private Deep Learning with Teacher Ensembles},\nauthor={Lichao Sun and Yingbo Zhou and Jia Li and Richard Socher and Philip S. Yu and Caiming Xiong},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl38yrFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl38yrFwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504190417, "tmdate": 1576860572179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1743/Authors", "ICLR.cc/2020/Conference/Paper1743/Reviewers", "ICLR.cc/2020/Conference/Paper1743/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1743/-/Public_Comment"}}}], "count": 9}