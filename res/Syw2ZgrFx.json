{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028640789, "tcdate": 1490028640789, "number": 1, "id": "SytDdtpjl", "invitation": "ICLR.cc/2017/workshop/-/paper160/acceptance", "forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinterpreting Importance-Weighted Autoencoders", "abstract": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.  We formally derive this result, and visualize the implicit importance-weighted approximate posterior.", "pdf": "/pdf/7c5234a6b7bb293a449c960892b387308406cea7.pdf", "TL;DR": "IWAE optimizes the standard variational lowerbound, but using a more complex variational distribution", "paperhash": "cremer|reinterpreting_importanceweighted_autoencoders", "keywords": ["Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "harvard.edu", "cam.ac.uk"], "authors": ["Chris Cremer", "Quaid Morris", "David Duvenaud"], "authorids": ["ccremer@cs.toronto.edu", "quaid.morris@utoronto.ca", "duvenaud@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028641356, "id": "ICLR.cc/2017/workshop/-/paper160/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028641356}}}, {"tddate": null, "tmdate": 1489556250633, "tcdate": 1489556250633, "number": 2, "id": "rJ77QL8ix", "invitation": "ICLR.cc/2017/workshop/-/paper160/official/review", "forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "signatures": ["ICLR.cc/2017/workshop/paper160/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper160/AnonReviewer1"], "content": {"title": "IWAE bound derived as VAE bound with particular implicit distribution q_IW", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper introduces a new perspective on IWAE.\nIt is shown that the IWAE bound can be interpreted as a VAE bound with a particular implicit inference model q_IW. This implicit posterior distribution is a function of both the variational parameters, and the generative model parameters.\n\nThe derivation seems novel, and adds an interesting new link to IWAE and VAE objectives.\n\nA potential drawback of the IWAE posterior, in comparison to alternative methods for building complex posteriors, is that it is relatively expensive; you may require a large number of samples to converge to the true distribution, probabily especially so in high dimensional space. However, that's besides the point of this paper, and I think it still proposes a valid and interesting perspective on IWAE.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinterpreting Importance-Weighted Autoencoders", "abstract": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.  We formally derive this result, and visualize the implicit importance-weighted approximate posterior.", "pdf": "/pdf/7c5234a6b7bb293a449c960892b387308406cea7.pdf", "TL;DR": "IWAE optimizes the standard variational lowerbound, but using a more complex variational distribution", "paperhash": "cremer|reinterpreting_importanceweighted_autoencoders", "keywords": ["Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "harvard.edu", "cam.ac.uk"], "authors": ["Chris Cremer", "Quaid Morris", "David Duvenaud"], "authorids": ["ccremer@cs.toronto.edu", "quaid.morris@utoronto.ca", "duvenaud@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489556251580, "id": "ICLR.cc/2017/workshop/-/paper160/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper160/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper160/AnonReviewer2", "ICLR.cc/2017/workshop/paper160/AnonReviewer1"], "reply": {"forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper160/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper160/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489556251580}}}, {"tddate": null, "tmdate": 1489268509028, "tcdate": 1489268509028, "number": 1, "id": "r1Smkgfoe", "invitation": "ICLR.cc/2017/workshop/-/paper160/official/review", "forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "signatures": ["ICLR.cc/2017/workshop/paper160/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper160/AnonReviewer2"], "content": {"title": "Simple and clear result, useful insights", "rating": "7: Good paper, accept", "review": "The authors describe a simple and clear reinterpretation of importance\nweighted autoencoders (Burda et al., 2016). I recommend this paper for\nacceptance. It connects to much recent work on expressive variational\napproximations, especially in those leveraging truncated Markov chains\nas variational approximations. Further, it brings interesting ideas to\nthe table following this simple derivation.\n\nA cool result is that this interpretation relaxes the idea of IWAEs to\nbe more broadly applicable to any divergence measure. Perhaps a key\nexperiment would not be so much comparing IWAEs with itself, but in\nwhat this perspective allows, such as IWAE-based variational families\nwith alpha-divergences or operator variational objectives. Or alternatively,\ncombining the SIR approach with other rich posterior approximations.\n\nWith this perspective in mind, it's not necessarily clear if one\nshould even use IWAEs over other expressive variational\napproximations. From my understanding of the field, most benchmarks\ndisplay IWAEs performing worse (in terms of held-out log-likelihood)\nto others such as the variational Gaussian process (Tran et al.,\n2016) and inverse autoregressive flows (Kingma et al., 2016).\nThis isn't a fault of this paper\u2014it's great that the casting brings\nthese questions to bear\u2014but I think it's something the paper should\ncertainly address if it aims to be more substantial in an extended\npaper in the future.\n\nThe notation is not described in the paper; while experts in the field\ncan understand this, the work would benefit from properly laying out\ndefinitions and properties.\n\nReferences\n+ Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using Real NVP. Presented at the International Conference on Learning Representations.\n+ Kingma, D. P., Salimans, T., & Welling, M. (2016). Improving Variational Inference with Inverse Autoregressive Flow. Presented at the Neural Information Processing Systems.\n+ Li, Y., & Turner, R. E. (2016). R\u00e9nyi Divergence Variational Inference. Presented at the Neural Information Processing Systems.\n+ Ranganath, R., Altosaar, J., Tran, D., & Blei, D. M. (2016). Operator Variational Inference. Presented at the Neural Information Processing Systems.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinterpreting Importance-Weighted Autoencoders", "abstract": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.  We formally derive this result, and visualize the implicit importance-weighted approximate posterior.", "pdf": "/pdf/7c5234a6b7bb293a449c960892b387308406cea7.pdf", "TL;DR": "IWAE optimizes the standard variational lowerbound, but using a more complex variational distribution", "paperhash": "cremer|reinterpreting_importanceweighted_autoencoders", "keywords": ["Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "harvard.edu", "cam.ac.uk"], "authors": ["Chris Cremer", "Quaid Morris", "David Duvenaud"], "authorids": ["ccremer@cs.toronto.edu", "quaid.morris@utoronto.ca", "duvenaud@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489556251580, "id": "ICLR.cc/2017/workshop/-/paper160/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper160/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper160/AnonReviewer2", "ICLR.cc/2017/workshop/paper160/AnonReviewer1"], "reply": {"forum": "Syw2ZgrFx", "replyto": "Syw2ZgrFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper160/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper160/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489556251580}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488231999572, "tcdate": 1487368622893, "number": 160, "id": "Syw2ZgrFx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "Syw2ZgrFx", "signatures": ["~Chris_Cremer1"], "readers": ["everyone"], "content": {"title": "Reinterpreting Importance-Weighted Autoencoders", "abstract": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.  We formally derive this result, and visualize the implicit importance-weighted approximate posterior.", "pdf": "/pdf/7c5234a6b7bb293a449c960892b387308406cea7.pdf", "TL;DR": "IWAE optimizes the standard variational lowerbound, but using a more complex variational distribution", "paperhash": "cremer|reinterpreting_importanceweighted_autoencoders", "keywords": ["Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "harvard.edu", "cam.ac.uk"], "authors": ["Chris Cremer", "Quaid Morris", "David Duvenaud"], "authorids": ["ccremer@cs.toronto.edu", "quaid.morris@utoronto.ca", "duvenaud@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}