{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396442206, "tcdate": 1486396442206, "number": 1, "id": "rkzQ2GLdl", "invitation": "ICLR.cc/2017/conference/-/paper221/acceptance", "forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejecting this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396442758, "id": "ICLR.cc/2017/conference/-/paper221/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396442758}}}, {"tddate": null, "tmdate": 1484959131354, "tcdate": 1484959027526, "number": 5, "id": "Hyo4aXgwx", "invitation": "ICLR.cc/2017/conference/-/paper221/public/comment", "forum": "Hkz6aNqle", "replyto": "B167-Ou4x", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your review!\nFirstly, this paper and the other ICLR submission about MDA are quite different, in the aspect of motivation and experiments.\nSecondly, in this paper, we extend traditional ECOC to a deep model and solve the problem that previous deep learning methods only initialized randomly or using unsupervised pre-training. This is the main contribution of our work. We think the reviewer may pay more attention to the experiments but neglect our main contribution.\nThirdly, although we used CIFAR-10 for comparison, please note that the experimental settings are different from the regular ones. Here, we apply the compared methods on the LBP features of the images.\nFinally, sorry for the typos in Section 3.1. In fact, we use RBF SVMs in all the experiments. We extract the LBP features of images is because some deep learning methods, such as autoencoders, denoising autoencoders and DeepECOC, can only be applied on vector inputs. For the efficient learning of SVMs, it's not in the scope of this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287678117, "id": "ICLR.cc/2017/conference/-/paper221/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkz6aNqle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper221/reviewers", "ICLR.cc/2017/conference/paper221/areachairs"], "cdate": 1485287678117}}}, {"tddate": null, "tmdate": 1484958036534, "tcdate": 1484958036534, "number": 4, "id": "rJn8Y7xwg", "invitation": "ICLR.cc/2017/conference/-/paper221/public/comment", "forum": "Hkz6aNqle", "replyto": "r14_-IONg", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your review!\nFirstly, thank you for pointing out the writing problems. We will improve our paper accordingly.\nSecondly, please just consider ECOC as a building block in DeepECOC, like RBMs in deep autoencoders. The advantages using ECOC are: 1) its learning is supervised (we proposed a supervised layer-wise pre-training method); 2) for the pre-training of some weights, according to the ECOC coding, some classes of data may not be used; 3) we can use random dense or random sparse ECOC coding design, which is equivalent to random initialization of the network structure (the weights are still learned based on the binary classsifiers in the ECOC module). So sorry for the misleading! It's really difficult to clarify so many concepts in one paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287678117, "id": "ICLR.cc/2017/conference/-/paper221/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkz6aNqle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper221/reviewers", "ICLR.cc/2017/conference/paper221/areachairs"], "cdate": 1485287678117}}}, {"tddate": null, "tmdate": 1484957141546, "tcdate": 1484957141546, "number": 3, "id": "HkTAH7lPg", "invitation": "ICLR.cc/2017/conference/-/paper221/public/comment", "forum": "Hkz6aNqle", "replyto": "HkhDsHr4l", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your comments!\nFirstly, there are very few papers to consider supervised layer-wise pre-traing and deep ensemble learning. Hence, from the perspective of machine learning, this paper is not only a preliminary work.\nSecondly, thanks for pointing out the writing problems. We'd like to revise this paper accordingly.\nThirdly, we have compared the proposed method, DeepECOC, with deep autoencoder (AE), stacked denoising autoencoder (DAE), LeNet and PCANet. Since DeepECOC is generally applied to inputs of the vector form, we think the comparison results are sufficient to show its advantages.\nFourthly, the experimental conditions are applied to all the compared methods. \nFinally, thanks for the suggested NIPS paper. In fact, we have more interests to learn end-to-end deep architectures."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287678117, "id": "ICLR.cc/2017/conference/-/paper221/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkz6aNqle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper221/reviewers", "ICLR.cc/2017/conference/paper221/areachairs"], "cdate": 1485287678117}}}, {"tddate": null, "tmdate": 1482354981260, "tcdate": 1482354981260, "number": 3, "id": "B167-Ou4x", "invitation": "ICLR.cc/2017/conference/-/paper221/official/review", "forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "signatures": ["ICLR.cc/2017/conference/paper221/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper221/AnonReviewer3"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout.\n\nNote that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author\u2019s other ICLR submission: \u00ab\u00a0Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications\u00a0\u00bb. The two works differ in the supervised initialization strategy employed. \n\nWhile layer-wise initialization strategies are worthy of further exploration, the paper doesn\u2019t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art.\n\nExplanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. \nOne important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (\u00ab\u00a0In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers\u00a0\u00bb) \nor kernel SVMs as mentioned later \u00ab\u00a0For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function\u00bb. In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?).\n\nFinally for image datasets, a visual comparison of learned filters could help provide some qualitative insight.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512658916, "id": "ICLR.cc/2017/conference/-/paper221/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper221/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper221/AnonReviewer2", "ICLR.cc/2017/conference/paper221/AnonReviewer1", "ICLR.cc/2017/conference/paper221/AnonReviewer3"], "reply": {"forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512658916}}}, {"tddate": null, "tmdate": 1482346860538, "tcdate": 1482346860538, "number": 2, "id": "r14_-IONg", "invitation": "ICLR.cc/2017/conference/-/paper221/official/review", "forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "signatures": ["ICLR.cc/2017/conference/paper221/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper221/AnonReviewer1"], "content": {"title": "review", "rating": "3: Clear rejection", "review": "Error correcting output coding is well established supervised learning approach. Stacking several layers of ECOC seems a natural way of extending the framework to deep learning. \nThe main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision. The main idea of this work is interesting. However, the presentation can be improved and several sections (about SVMs for example) can be shortened. \n\nI am not convinced by the advantage of this approach compared to end-to-end training of neural networks of other layer-wise training of deep architecture. In particular, when the codes at every layer can are randomly generated the induced binary problems can be arbitrarily difficult and this could lead to learnability issues hence affecting the quality of the features. While this problem is counterbalanced by the decoding scheme (and error-correction) at the prediction layer, it is not clear how it should be dealt with in the intermediate layers. In addition, other approaches such as stacked RBMs or Autoencoders learn regularities capturing information about the distribution of the data at every layer. In comparison, it is not clear what the learned representation by an ECOC layer is unless it is supervised (One versus Rest).\n\nOverall, though interesting, I am not convinced by the proposed approach and the experimental section does not help mitigate the impression. \nMore insights on the learned internal representations, and experiments using standard datasets could help.\n\n ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512658916, "id": "ICLR.cc/2017/conference/-/paper221/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper221/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper221/AnonReviewer2", "ICLR.cc/2017/conference/paper221/AnonReviewer1", "ICLR.cc/2017/conference/paper221/AnonReviewer3"], "reply": {"forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512658916}}}, {"tddate": null, "tmdate": 1482148707591, "tcdate": 1482148707591, "number": 1, "id": "HkhDsHr4l", "invitation": "ICLR.cc/2017/conference/-/paper221/official/review", "forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "signatures": ["ICLR.cc/2017/conference/paper221/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper221/AnonReviewer2"], "content": {"title": "", "rating": "3: Clear rejection", "review": "The paper proposes a strategy to pre-train the successive layers of a multi-layer perceptron for classification tasks. It consists in training successively each layer to predict an ECOC corresponding to the classification problem. The first layer predicts this ECOC from the input data and each successive layer takes as output its preceding layer to predict another ECOC. Different regularization strategies are used during training (input noise, Dropout). The MLP is then fine-tuned using SGD. Comparisons are performed with baselines on different datasets.\n\nThis is a preliminary work. The idea might be valuable but it should be pushed further. Concerning the writing, sections concerning well known notions (e.g. SVM) could be suppressed. Since ECOC are central to this contribution, a more detailed description of the different ECOC strategies would be helpful. The experiments do not compare the proposed model with state of the art classifiers. The experimental conditions should be made more precise, e.g., it is not clear whether the regularization strategies have been used for all the NN architectures or only for the ECOC based ones.\n\nFinally note that there are several papers that try to learn intermediate representations for classification problems, see e.g. Samy Bengio, Jason Weston, David Grangier: Label Embedding Trees for Large Multi-Class Tasks. NIPS 2010: 163-171\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512658916, "id": "ICLR.cc/2017/conference/-/paper221/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper221/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper221/AnonReviewer2", "ICLR.cc/2017/conference/paper221/AnonReviewer1", "ICLR.cc/2017/conference/paper221/AnonReviewer3"], "reply": {"forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512658916}}}, {"tddate": null, "tmdate": 1481187773186, "tcdate": 1481187773179, "number": 2, "id": "r1Sp-iI7g", "invitation": "ICLR.cc/2017/conference/-/paper221/public/comment", "forum": "Hkz6aNqle", "replyto": "B1KaImbmx", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your review! \n1. We used same pre-training+finetuning procedure for the training of both DeepECOC and DAE. For the dropout regularization, we used it in DeepECOC but not DAE. However, we have compared the results of DAE with dropout and without dropout. Since denoising can be considered as a regularization method (dropout is also), DAE with dropout and without dropout perform similarly. \n2. SVMs are widely used in traditional ECOC methods. However, (regularized) logistic regression is used much less. So, we used SVMs in the proposed DeepECOC model. Here, even we use (regularized) logistic regression as binary classifiers of the ECOC modules, the sigmoid activation functions are still necessary. For the C parameter in SVMs, we used the default value set in the LIBSVM toolbox.\n3. Fig. 3 shows the results obtained by different nets with the same structure. We can change it into a table in next version of this paper. In Fig. 3(a), we first design the deep architecture using stacked ECOC modules, and adopt this architecture for other compared deep networks. We can see that DeepECOC performed much better than the compared methods. In Fig 3(b), we use a state-of-the-art architecture for all the methods. We can see that DeepECOC performed similarly with other deep learning methods, and significantly better than single layer ECOC. Here, we focus on comparing related deep architectures but not achieving the best results on MNIST. The results may be a little lower than the highest reported in the literature. For Sparse, Dense and Single, they refer to DeepECOC initialized with random sparse coding, DeepECOC initialized with random dense coding and single layer ECOC (traditional ECOC), respectively."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287678117, "id": "ICLR.cc/2017/conference/-/paper221/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkz6aNqle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper221/reviewers", "ICLR.cc/2017/conference/paper221/areachairs"], "cdate": 1485287678117}}}, {"tddate": null, "tmdate": 1480828609063, "tcdate": 1480828609058, "number": 1, "id": "B1KaImbmx", "invitation": "ICLR.cc/2017/conference/-/paper221/pre-review/question", "forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "signatures": ["ICLR.cc/2017/conference/paper221/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper221/AnonReviewer3"], "content": {"title": "Clarifications", "question": "1. In your experiments, can you clarify that you use the same training procedure for DeepECOC and DAE, do you first pre-train and then fine tune the network with dropout for both methods ?\n2. Why do you use SVM as the binary classifier and not another method, such as (regularized) logistic regression as this would matches your subsequent use of a sigmoid? How was the C parameter of SVM set? \n3. Clarifications for MNIST results: Figure 3 is difficult to read a table would be much better. Also accuracies for the non-convolutional architectures seem much worse than what is typically reported with stacked DAE for instance. (>98.7%). Howcome? Also what are Sparse, Dense, Single? \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959397524, "id": "ICLR.cc/2017/conference/-/paper221/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper221/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper221/AnonReviewer3"], "reply": {"forum": "Hkz6aNqle", "replyto": "Hkz6aNqle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper221/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959397524}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478278586371, "tcdate": 1478278586363, "number": 221, "id": "Hkz6aNqle", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hkz6aNqle", "signatures": ["~Yuchen_Zheng1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Deep Error-Correcting Output Codes", "abstract": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases. ", "pdf": "/pdf/b3b4aa3c2bf2ce70e7887b228cc985f8f21e44e1.pdf", "paperhash": "zhong|deep_errorcorrecting_output_codes", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Peng Zhang", "Mengqi Li", "Junyu Dong"], "authorids": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 10}