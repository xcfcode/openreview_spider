{"notes": [{"id": "H1ep5TNKwr", "original": "Bkeb2VCvwB", "number": 725, "cdate": 1569439125486, "ddate": null, "tcdate": 1569439125486, "tmdate": 1577168226238, "tddate": null, "forum": "H1ep5TNKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VtMf73VgVd", "original": null, "number": 1, "cdate": 1576798704327, "ddate": null, "tcdate": 1576798704327, "tmdate": 1576800931724, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Decision", "content": {"decision": "Reject", "comment": "The paper learns an embedding on the nodes of the graph, iteratively aligning the vector associated to a node with that of its neighbor nodes (based on the Hebbian rule). \n\nThe reviews state that the approach is interesting though very natural/straightforward, and that it might go too far to call it \"Hebbian\" (Rev#2) - you might want also to see it as a Self-Organizing Map for graphs. \n\nA main criticism was about the comparison with the state of the art (all reviewers). The authors did add empirical comparisons with the suggested VGAE and SEAL, and phrase it nicely as \"our algorithm outperforms SEAL on one out of four data sets\". Looking at the revised paper, this is true: the approach is outperformed by SEAL on 3 out of 4 datasets.\n\nAnother criticism regards the insufficient analysis of the results (e.g. through visualization, studying the clusters obtained along different runs, etc). \nThis aspect is not addressed in the revised version.\n\nAn excellent point is the scalability of the approach, which is worth emphasizing.\n\nI thus encourage the authors to rewrite and polish the paper, improving the positioning of the proposed approach w.r.t. the state of the art, and providing a more thorough analysis of the results.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722159, "tmdate": 1576800273398, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper725/-/Decision"}}}, {"id": "rJeTak3J5r", "original": null, "number": 3, "cdate": 1571958725027, "ddate": null, "tcdate": 1571958725027, "tmdate": 1574359385917, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Thanks to the authors for their response. There are still significant issues with motivation, writing, and baseline comparisons (the latter noted by R3). I would encourage the authors to continue to polish and investigate their method and submit to a future conference. \n\n=====\n\nThis paper proposes an approach to learning embeddings associated with nodes in a graph. Inspired by Hebbian learning, the representations of a node are iteratively updated to be similar to representations of its neighbors. The update is performed by adding a scaled vector sampled from a Gaussian centered on a neighbor's representation to the current node's vector. After learning, the embeddings may be used for tasks such as graph reconstruction, link prediction, or product recommendation.\n\nContributions include:\n* Proposal of an approach to learn embeddings of nodes in a graph in which representations of a node are updated by scaled and Gaussian-perturbed versions of its neighbors' representations.\n* Experiments demonstrating reconstruction and link prediction performance of the proposed approach on several datasets, as well as product recommendation perfomance on a large retail dataset.\n  \nThere are several significant concerns with the paper as it currently stands. The three most pressing issues are as follows:\n1. The proposed algorithm is not situated relative to related work.\n2. The paper does not provide the reader with enough details or precision to be able to replicate the work.\n3. Experiments do not compare to any baselines other than random embeddings.\n  \nThe paper suggests that the proposed algorithm is a form of Hebbian learning because the representation of nearby nodes in the graph are encouraged to be similar. However, this idea has long been used for learning node embeddings (for example, LLE encourages representations of a node to be predicted as a linear combination of neighboring nodes). The connection seems loose other than a superficial similarity in the update rule and naming the algorithm after Hebbian learning is somewhat misleading.\n\nThe algorithm is reminiscent of message-passing inference in a continuous Markov random field with pairwise potentials encouraging nearby nodes to have similar representations. I am not an expert in graph embedding approaches, but I would be surprised if the approach could not be easily related to classical approaches such as MDS or LLE.\n\nThere are several notational/clarity issues:\n* j is used for both the node index and the embedding of the node itself (equation 1-2). Replacing the j on the left hand side with w_j would resolve ambiguity and bring the equations in line with Algorithm 1.\n* Equation 2 is inconsistent with equation 1 because they both specify different distributions over the same embedding. Framing equation 1 as an initialization and equation 2 as providing the conditional distribution p(w_j | w_i) may make the situation clearer.\n* Equation 3 is a mixture of Gaussian distributions, yet \\delta_j is a vector added to the current node embedding. Instead, first write \\tilde{w}_i as a sample from the Gaussian and then let \\delta_j be the weighted sum of the samples.\n* Equation 3 is not consistent with equations 4-5. Equation 3 suggests that a node is updated by summing over neighbors and then applying the update. But Algorithm 1 suggests that nodes are updated based on only a single neighbor at a time.\n* What does it mean when the negative embedding is propagated with a small transition probability? This should be described mathematically.\n* It is misleading to call the graph a Gaussian hierarchy, since a hierarchy implies that certain nodes are higher than others.\n* How are the \"transition probabilities\" set for an unweighted graph? Specifically, the GrQc dataset doesn't appear to have edge weights.\n* What values of the variance and \\tau hyperparameters were used?\n* How are reconstructions and link predictions computed?\n  \nExperimentally, the proposed approach is not compared to any baselines other than random embeddings. The claim made in the paper that the method compares favorably is thus not backed up by results. The results in section 3.2 should be described in greater detail. If the items are nodes, then how are edges and weights determined?\n\n Other specific comments:\n* What is the connection between the current work and hyperbolic geometry of Nickel & Kiela (2017)? The proposed algorithm does not rely on hyperbolic geometry so this seems like a non sequitur.\n* Algorithm 1: Rather than describing the algorithm in terms of the intended application (products), it would be useful to describe it in general terms and then use retail products as specific application.\n* Figures 2 and 3 are not particularly useful. The most important information for the reader or practitioner is how various methods compare on the same dataset, not how a single method performs across different datasets.\n  \nQuestions for the authors:\n* How is the proposed algorithm similar/different to related approaches for learning node embeddings?\n* What are baseline results for related algorithms on the datasets experimented upon?\n* What is the role of the variance scaling? How do the results change if the variance is reduced to 0 immediately after random initialization?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575329028720, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper725/Reviewers"], "noninvitees": [], "tcdate": 1570237747996, "tmdate": 1575329028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Review"}}}, {"id": "SJgTZcZIjH", "original": null, "number": 5, "cdate": 1573423621281, "ddate": null, "tcdate": 1573423621281, "tmdate": 1573423621281, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1ehHMbriH", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment", "content": {"title": "Comparison with SEAL and VGAE", "comment": "Thank you for the response!\n\nWe ran our algorithm for comparison with SEAL and VGAE.\nOur algorithm outperforms VGAE on all four data-sets.\nAnd, our algorithm outperforms SEAL on one out of four data sets.\nPlease see tables 6 and 7.\nAlso, note that our algorithm is run on Apache Spark, so there is some initial time spent on initialization and resource allocation.\nThe larger the graph, the more noticeable is the difference in the run-time. Please see the power data-set.\nIt might also be infeasible to run SEAL and VGAE on our recommender system graph with 200,000 nodes and 13.1 billion edges.\nPlease review the modified paper. Thanks again."}, "signatures": ["ICLR.cc/2020/Conference/Paper725/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ep5TNKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper725/Authors|ICLR.cc/2020/Conference/Paper725/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167164, "tmdate": 1576860556760, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment"}}}, {"id": "H1ehHMbriH", "original": null, "number": 4, "cdate": 1573356099763, "ddate": null, "tcdate": 1573356099763, "tmdate": 1573356099763, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1e6HBTgsB", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment", "content": {"title": "Thank you for your response.", "comment": "I appreciate the authors' efforts.\n\nHowever, [1] was published in May 2017. For 2.5 years, many advanced models for graph representation learning have been proposed. Therefore, I recommended comparing the proposed method with recent works such as [2] for the link prediction. Also, VGAE [3] was introduced as related work in [1], but [1] did not provide the comparison results with [3]. Considering that the main task of this work is graph reconstruction, at least, the proposed method should be compared with these methods and their competitiveness. \n\nAlso, it seems that the proposed method has an advantage in its computation time. So, it is important to compare real computation times for training and inference with those of other models. \n\nConsidering the characteristics of ICLR (learning representation), if the proposed method makes a good representation, the authors need to investigate how the method can do that with intensive analysis including representation visualization.\n\n[1] Goyal and Ferrara, Graph embedding techniques, applications, and performance: A survey, ArXiv 2017.\n[2] M. Zhang and Y Chen, Link Prediction Based on Graph Neural Networks, NIPS 2018.\n[3] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ep5TNKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper725/Authors|ICLR.cc/2020/Conference/Paper725/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167164, "tmdate": 1576860556760, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment"}}}, {"id": "ryxXOSTliH", "original": null, "number": 3, "cdate": 1573078379328, "ddate": null, "tcdate": 1573078379328, "tmdate": 1573078379328, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "B1eMMi63FB", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment", "content": {"title": "Revised paper", "comment": "Thank you for the comments!\nWe have added results from other state of the art algorithms like mentioned in [1].\nWe find that our algorithm outperforms others.\nPlease see table 1 and table 5 in the modified paper.\nThe notation has been corrected.\n\n[1] Graph embedding techniques, applications, and performance: A survey\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper725/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper725/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ep5TNKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper725/Authors|ICLR.cc/2020/Conference/Paper725/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167164, "tmdate": 1576860556760, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment"}}}, {"id": "H1e6HBTgsB", "original": null, "number": 2, "cdate": 1573078340635, "ddate": null, "tcdate": 1573078340635, "tmdate": 1573078340635, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "BJg_uTZ15B", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment", "content": {"title": "Revised paper", "comment": "Thank you for the comments!\nWe have updated the introduction.\nWe have added results from other state of the art algorithms like mentioned in [1].\nWe find that our algorithm is significantly better.\nPlease see table 1 and table 5 in the modified paper.\n\n[1] Graph embedding techniques, applications, and performance: A survey"}, "signatures": ["ICLR.cc/2020/Conference/Paper725/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper725/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ep5TNKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper725/Authors|ICLR.cc/2020/Conference/Paper725/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167164, "tmdate": 1576860556760, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment"}}}, {"id": "rylmfSpgoB", "original": null, "number": 1, "cdate": 1573078283025, "ddate": null, "tcdate": 1573078283025, "tmdate": 1573078283025, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "rJeTak3J5r", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment", "content": {"title": "Revised paper with comparison to state of the art", "comment": "Thank you for the comments!\nWe have added results from other state of the art algorithms like mentioned in [1].\nWe find that our algorithm outperforms all others in [1].\nPlease see table 1 and table 5 in the modified paper.\nThe key contribution of our algorithm is the inspiration from annealing and errorless learning.\nWe were not aware of LLE. We have added a reference and some notes on how our paper is different.\nNote that LLE uses a loss function while our method is error-free or Hebbian; this is an important difference.\nFor this reason, our method is embarrassingly parallelizable on platforms like Apache Spark.\nAll other comments have been noted and we have made changes to the paper respectively.\n\n[1] Graph embedding techniques, applications, and performance: A survey\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper725/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper725/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ep5TNKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper725/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper725/Authors|ICLR.cc/2020/Conference/Paper725/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167164, "tmdate": 1576860556760, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper725/Authors", "ICLR.cc/2020/Conference/Paper725/Reviewers", "ICLR.cc/2020/Conference/Paper725/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Comment"}}}, {"id": "B1eMMi63FB", "original": null, "number": 1, "cdate": 1571769097793, "ddate": null, "tcdate": 1571769097793, "tmdate": 1572972560159, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed a simple but effective node embedding method for large-scale graphs. \nThe proposed method is based on Hebbian learning, enhancing the connections between neighbor nodes iteratively. \nThe idea is very straightforward and suitable for large-scale applications. \nThe authors tested the proposed method on multiple real-world datasets under different configurations.\n\nThe strategy of the work makes sense to me, but the work itself is incomplete. \nAs mentioned in the conclusion, there are some potential competitors of the proposed method, which should be considered as baselines in the experiments. \nAdditionally, for conceptual proof, the authors can consider a synthetic/real-world dataset with relatively small size and compare their method with state-of-the-art methods.\n\nAdditionally, the notations in Eqs. (1-3) are inconsistent with those in Algorithm 1. Especially Eq.(3), delta_j should be an incremental vector of node j\u2019s embedding, while N(I, sigma^2 I) is a distribution. The authors should write Eqs. (1-3) as Eqs. (4-6)."}, "signatures": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575329028720, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper725/Reviewers"], "noninvitees": [], "tcdate": 1570237747996, "tmdate": 1575329028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Review"}}}, {"id": "BJg_uTZ15B", "original": null, "number": 2, "cdate": 1571917167889, "ddate": null, "tcdate": 1571917167889, "tmdate": 1572972560114, "tddate": null, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "invitation": "ICLR.cc/2020/Conference/Paper725/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "[Summary]\nThis paper proposes an error-free rule update method for graph embedding. The authors employ the Hebbian learning concept iteratively using the pre-calculated transition probability. They evaluate their model on six benchmark datasets.\n\n[Pros]\n- Very simple and fast by using an error-free update rule.\n\n[Cons]\n- The introduction is not organized. What are the motivation, related work, and contribution?\n- No comparison with conventional methods such as PageRank and NN-based models such as SEAL [1] and VGAE [2]. Even if this method is not learning-based, the proposed model should be compared.\n- The authors claimed they applied their model to real recommendation systems. But there is no specific information. At least, the author describes what is recommended, the data size, how large performance is improved, etc.\n- It is required to be evaluated on conventional datasets.\n- Ablation on sigma\n\n[1] M. Zhang and Y Chen, Link Prediction Based on Graph Neural Networks, NIPS 2018.\n[2] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\n\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper725/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hebbian Graph Embeddings", "authors": ["Shalin Shah", "Venkataramana Kini"], "authorids": ["shalin.shah@target.com", "venkataramana.kini@target.com"], "keywords": ["graph embeddings", "hebbian learning", "simulated annealing"], "TL;DR": "Graph embeddings for link prediction, reconstruction and for a recommender system", "abstract": "Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window.  In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "pdf": "/pdf/09ac4ce2cdccca6e5954e3869793211362ecc33e.pdf", "paperhash": "shah|hebbian_graph_embeddings", "original_pdf": "/attachment/c6e3836d853ec45399216ca666d80ac506e5886b.pdf", "_bibtex": "@misc{\nshah2020hebbian,\ntitle={Hebbian Graph Embeddings},\nauthor={Shalin Shah and Venkataramana Kini},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ep5TNKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ep5TNKwr", "replyto": "H1ep5TNKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575329028720, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper725/Reviewers"], "noninvitees": [], "tcdate": 1570237747996, "tmdate": 1575329028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper725/-/Official_Review"}}}], "count": 10}