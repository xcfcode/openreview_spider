{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458070725921, "tcdate": 1458070725921, "id": "GvVGgXNrJU1WDOmRiM08", "invitation": "ICLR.cc/2016/workshop/-/paper/165/review/10", "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "signatures": ["ICLR.cc/2016/workshop/paper/165/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/165/reviewer/10"], "content": {"title": "This is a very badly written paper and there are some mistakes. The idea presented in the paper are not really suprising. This paper requires a serious and much better write-up.", "rating": "4: Ok but not good enough - rejection", "review": "This is a very badly written paper. There are several typos and mistakes. I will not list them, I recommend the authors to proof-read the paper a few more times. On the other hand, those mistakes make the paper difficult to follow, although the ideas presented there are not very complicated. I think this paper requires a better write-up.\n\nSection 2 is very confusing and ambiguous. Where does w^{(l)} come from? It is not define in the text. \nI really can not see how you go from Eqn 3 to Eqn 4.\n\nTaylor approximations in Eqns 7,8 and 9 are done around 0. You should explicitly state that in the text. Eqn 7 is wrong.\nEqn 10 is basically 2*tanh(x).\nThere is a discontinuity between the ideas presented in Section 2 and Section 3. \n\nWhy the results of Sigmoid on Table 1 are N/A. If you are not going to put the results there, why did you put this there?\nIt seems like still leaky-tanh performs slightly worse than leaky-ReLU. Why would one prefer leaky-tanh over leaky-ReLU for feedforward networks? The conclusion provided in the paper is obvious and really not surprising. These ideas could be better analyzed and investigated.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580091124, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580091124, "id": "ICLR.cc/2016/workshop/-/paper/165/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/165/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457483296999, "tcdate": 1457483296999, "id": "MwVLGlnxNCqxwkg1t7Jp", "invitation": "ICLR.cc/2016/workshop/-/paper/165/comment", "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "p8jJ1gJvOcnQVOGWfpYL", "signatures": ["~Bing_Xu1"], "readers": ["everyone"], "writers": ["~Bing_Xu1"], "content": {"title": "RE: Dmytro", "comment": "Thanks for reviewing.\na> \nI have tested 8 layers and there is no problem. Recall IEEE-754 standard (https://en.wikipedia.org/wiki/IEEE_floating_point), large/small number representation is not a problem, but may not be accurate when doing multiplication for a very small number and a very large number.\n\nAlso, a heuristic learning rate factor \\alpha = ||w||_f / ||g||_f is helpful to train 8 layers sigmoid network. \n\nb> \n\"Sigmioid*\" is simply a name for convenient scale each layer\u2019s learning rate and initialization for Sigmoid activation without multiply a verge large number, but multiply 4 layer by layer.\nThe key idea is: We need to preserve variance of output feature and gradient. The Sigmoid* fix sigmoid\u2019s feature/gradient vanishing problem by rescaling. Again we can definitely use standard Sigmoid activation function but change initialization & learning rate according to Equ(4) and Equ(6) as it is equivalent. (commutative property of multiplication)\n\n\nc> \nYes empirically run a lot of experiments are helpful. However I think current experiment is able to demonstrate the problem.  \n\nI have RRTanh, PTanh\u2019s result, which is similar to ReLU case in http://arxiv.org/abs/1505.00853.\n\nI think we need to focus more on theoretical understanding in the question \u201cHow does the positive part (on [0, +\u221e)) and the negative part (on (\u2212\u221e, 0]) of the activation function affect the performance of the network?\u201d\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831130944, "ddate": null, "super": null, "final": null, "tcdate": 1455831130944, "id": "ICLR.cc/2016/workshop/-/paper/165/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDjyJjXF5jEJ1zfE53", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/165/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457481105739, "tcdate": 1457481105739, "id": "p8jJ1gJvOcnQVOGWfpYL", "invitation": "ICLR.cc/2016/workshop/-/paper/165/unofficial_review", "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "signatures": ["~Dmytro_Mishkin2"], "readers": ["everyone"], "writers": ["~Dmytro_Mishkin2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The authors study weaknesses of the saturated activation functions (TanH and Sigmoid) and propose ways of improving it. \nIn first part of paper authors propose to fix difficulties of training sigmoid networks by one of two ways:\n\na) Simply apply separate learning rates to the different layers: starting with 1 for linear classifier, multiply each previous lr by 4 and correct the weight matrices the opposite way. This could be seen as analogy of (He et al., 2015) correction of (Glorot et.al, 2010) formula but for Sigmoid networks instead of ReLU.\nThe idea is simple - to compensate vanishing gradient by proportionally increasing learning rate. As far as I checked on 4-layer network, it works, but seems rather impractical for deep network, i.e. correction coefficient for 11-layer network would be 4^11 = 4194304.\nHowever, the idea of layer-wise adjustment of learning rate potentially could help is other situations, not necessary with sigmoid. \n\nb)rescale and shift sigmoid to so called sigmoid*(x) = 4*sigmoid(x) - 2.\nThis solution is much simpler, but I believe, such function cannot be called sigmoid, rather tanh. Reason:\n sigmoid*(x) = 4*sigmoid(x) - 2 = 4/(e^-x +1) - 2 = (4 - 2e^(-x) - 2 ) / (e^-x + 1)= 2 (1 - e^-x)/(e^-x + 1) = 2*tanh(x/2).\n\nIn the second part of the paper authors propose \"leaky\" version of TanH, showing that it could compete with ReLU activation.\nIt could be interesting to see how other ReLU-family inspired variants of TanH would perform:\na)rectified TanH = 0, if x < 0\nb)Randomized Leaky TanH in RReLU fashion\nc)Parametric Leaky TanH.\n\nI believe that paper with such evaluation will help future research in saturated activation functions. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831131866, "ddate": null, "super": null, "final": null, "tcdate": 1455831131866, "id": "ICLR.cc/2016/workshop/-/paper/165/unofficial_review", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"pdf": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["antinucleon@gmail.com", "muli@cs.cmu.edu", "rtonghuang@gmail.com"]}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457480146992, "tcdate": 1457480146992, "id": "yovVNL4Opur682gwszQj", "invitation": "ICLR.cc/2016/workshop/-/paper/165/comment", "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "XL9ZAq22LsXB8D1RUGDZ", "signatures": ["~Bing_Xu1"], "readers": ["everyone"], "writers": ["~Bing_Xu1"], "content": {"title": "RE: Reviewer 11", "comment": "Thanks for reviewing.\n \n1>   \u201cI do not understand the statement \"Clearly, when x is around 0 Sigmoid will make gradient vanishing if we use same learning rate in each layer.\" \u201c\n \nBy doing z-score normalization on input data, which is commonly used in training neural networks, we normalize the data to  have mean 0 and variance 1. Assume x is near 0, according to Equ(6), each sigmoid layer will reduce the variance to (1/4)^2 that of the previous upper layer, which is 1/16. For a network with $l$ sigmoid layers, the bottom gradient variance is (1/16)^l, e.g. for a network with 33 sigmoid layers in our experiment, if the variance of the top layer gradient is 1, the bottom layer gradient variance will be: 1.8367099231598242e-40 which is very close to 0. \n\n2> I believe gradient vanishing occurs when the activation function is pushed into saturated regime. The proposed version of sigmoid (sigmoid*) can be seen as a version of tanh function with larger non-saturated regime and steeper gradient. Similar activations were proposed in past (sorry for not being able to provide a reference at this moment)\n \nIt is widely accepted that when the neuron is pushed into a saturated regime, the gradient will suffer vanishing problem. In our paper, we propose another possible reason for this vanishing problem. \nNote that for all these variance based initialization methods, each neuron is initialized independently around 0. Sigmoid networks fail to converge at the very beginning of the training procedure. Hardly all the neuron can be pushed into their saturated regime in this case. \n\nIndeed, compared to tanh, sigmoid* has a larger non-saturated regime, but with an higher order of x^3 which is neglectable when x is near 0. (see the Taylor expansion. It remains interesting how one can construct an better activation function incorporate such property.) The conclusion we try to make in this paper is that under these variance based initialization methods, the gradient when x near 0 being not 1 is the problem causing failure of the sigmoid function. We also test the activation function 2 * sigmoid*, and it also fails to converge. Note that this function has a even larger non-saturated regime, but violate the precondition that gradient equals 1 when x is near 0.\n\nWe are looking forward to the references about using activation function with larger non-saturated regime and steeper gradient.\n\n\n3> I encourage authors to look at the hidden responses of the network trained using sigmoid*, they might be operating in non-saturated regime. \n\nThe main contribution of sigmoid* is that point to gradient vanishing reason and solution joint in initialization and optimization (learning rate in each layer). We can definitely use sigmoid activation function but change initialization & learning rate according to Equ(4) and Equ(6) as it is equivalent.\n \n4>   I also don't see how the proposed activation is equivalent to using different learning rates for different layers?, unless combined with some specific initialization scheme.\n \nBasically, this comes from the commutative property of multiplication.\n\nForward pass: Recall Equ(4), for each layer $l$, $Var[y^l] = n_l Var[w^l] diag(f\u2019(y^{l-1})) Var[y^{(l-1)}] ] diag(f\u2019(y^{l-1}))$. If we use sigmoid, the top output variance will be $1/16$ of previous layer. In Sigmoid^* activation function, it is:\n \n$Var[y^l] = 16 * n_l Var[w^l] diag(f\u2019(y^{l-1})) Var[y^{(l-1)}] ] diag(f\u2019(y^{l-1}))$. \n \nRecall the Xavier initialization method, it is equivalent to initialize w as sqrt(16 * original term), which is multiply 4 for each weight layer\u2019s initialization. According to Equ(6),  for $l$th sigmoid layer, this activation is equivalent to multiply 4^l to the weight term before sigmoid activation.\n\nBackward pass: Recall Equ(6). Similarly, we can derive the scale 4^l in each gradient term. This scale can be communicated to learning rate term, which make learning rate multiply by 4^l.\n \n5>   RectifiedTanh function, which is a specific case of proposed leaky tanh, was shown to perform as good as Relu in http://arxiv.org/pdf/1506.08700v1.pdf. but the main jump in performance might be due to the rectification nature of the function.\n\nI have tested the Rectified Tanh function, the performance is slightly worse than ReLU, and far worse than Leaky ReLU.  Rectified Tanh also breaks the symmetry of Tanh. We don\u2019t make a conclusion that this kind of breaking is the reason, but our empirical experiments show the result. We raise an interesting question in the end of this paper: \u201cHow does the positive part (on [0, +\u221e)) and the negative part (on (\u2212\u221e, 0]) of the activation function affect the performance of the network?\u201d We believe this question requires some more theoretical explanation but not empirical conclusion.   \n\n \n \n\n \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831130944, "ddate": null, "super": null, "final": null, "tcdate": 1455831130944, "id": "ICLR.cc/2016/workshop/-/paper/165/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDjyJjXF5jEJ1zfE53", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/165/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457437036269, "tcdate": 1457437036269, "id": "XL9ZAq22LsXB8D1RUGDZ", "invitation": "ICLR.cc/2016/workshop/-/paper/165/review/11", "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "signatures": ["ICLR.cc/2016/workshop/paper/165/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/165/reviewer/11"], "content": {"title": "The authors try to understand why deep networks using sigmoid non-linearity are very hard to train and often never converge. They propose a scaled version of sigmoid based on their reasoning for the failure of sigmoid and experimentaly show that proposed activation function can be used to train deep networks. The authors also show that even saturating activation functions being \"Leaky\" improves the performance of networks to same level as that of non-saturating activation functions.", "rating": "5: Marginally below acceptance threshold", "review": "The authors claim that under certain assumptions on the variance of pre-nonlinear output and the gradient, the initialization method from [Glorot&Bengio,2010] can be recovered when the derivative of the activation equals to one. Via Taylor expansion of the activation functions they show that sigmoid violates this condition. \n\nI do not understand the statement \"Clearly, when x is around 0 Sigmoid will make gradient vanishing if we use same learning rate in each layer.\" I believe the derivative of sigmoid is non-zero especially around zero and close to zero else where. I believe gradient vanishing occurs when the activation function is pushed into saturated regime. \n\nThe proposed version of sigmoid (sigmoid*) can be seen as a version of tanh function with larger non-saturated regime and steeper gradient. Similar activations were proposed in past (sorry for not being able to provide a reference at this moment). I encourage authors to look at the hidden responses of the network trained using sigmoid*, they might be operating in non-saturated regime. I also don't see how the proposed activation is equivalent to using different learning rates for different layers?, unless combined with some specific initialization scheme.\n\nRectifiedTanh function, which is a specific case of proposed leaky tanh, was shown to perform as good as Relu in http://arxiv.org/pdf/1506.08700v1.pdf. I believe that the leaky nature of proposed activation helps, but the main jump in performance might be due to the rectification nature of the function.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580090686, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580090686, "id": "ICLR.cc/2016/workshop/-/paper/165/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "D1VDjyJjXF5jEJ1zfE53", "replyto": "D1VDjyJjXF5jEJ1zfE53", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/165/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455831126590, "tcdate": 1455831126590, "id": "D1VDjyJjXF5jEJ1zfE53", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "D1VDjyJjXF5jEJ1zfE53", "signatures": ["~Bing_Xu1"], "readers": ["everyone"], "writers": ["~Bing_Xu1"], "content": {"CMT_id": "", "title": "Revise Saturated Activation Functions", "abstract": "  In this paper, we revise two commonly used saturated functions, the\n  logistic sigmoid and the hyperbolic tangent (tanh).\n\n  We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible\n  reason making training deep networks with the logistic function\n  difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid\n  achieves comparable results with tanh.\n\n  Then following the same argument, we improve tahn by penalizing in the negative part. \n  We show\n  that ``penalized tanh'' is comparable and even outperforms the state-of-the-art\n  non-saturated functions including ReLU and leaky ReLU on deep convolution\n  neural networks.\n\n  Our results contradict to the conclusion of previous works that the saturation\n  property causes the slow convergence. It suggests further investigation is necessary to\n  better understand activation functions in deep architectures.", "pdf": "/pdf/D1VDjyJjXF5jEJ1zfE53.pdf", "paperhash": "xu|revise_saturated_activation_functions", "authors": ["Bing Xu", "Ruitong Huang", "Mu Li"], "conflicts": ["ualberta.ca", "cs.cmu.edu"], "authorids": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 6}