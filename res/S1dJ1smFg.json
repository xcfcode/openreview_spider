{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1492230229430, "tcdate": 1487281888430, "number": 48, "id": "S1dJ1smFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1dJ1smFg", "original": "HyecJGP5ge", "signatures": ["~Sahil_Garg1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484596062575, "tcdate": 1478287842302, "number": 365, "id": "HyecJGP5ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyecJGP5ge", "signatures": ["~Sahil_Garg1"], "readers": ["everyone"], "content": {"title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model\u2019s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the \u201cneuronal birth\u201d is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. \u201cNeuronal death\u201d is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.", "pdf": "/pdf/fb6912d6259de45abff8efb092d6c30cc681125c.pdf", "TL;DR": "An online dictionary learning incorporates dynamic model adaptation, adding/deleting its elements in response to nonstationary data.", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Unsupervised Learning", "Computer vision", "Transfer Learning", "Optimization", "Applications"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028570189, "tcdate": 1490028570189, "number": 1, "id": "HJMm_FTog", "invitation": "ICLR.cc/2017/workshop/-/paper48/acceptance", "forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028570751, "id": "ICLR.cc/2017/workshop/-/paper48/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028570751}}}, {"tddate": null, "tmdate": 1489700367468, "tcdate": 1489700367468, "number": 2, "id": "BJDzUK_sx", "invitation": "ICLR.cc/2017/workshop/-/paper48/public/comment", "forum": "S1dJ1smFg", "replyto": "S1OIl5Wol", "signatures": ["~Irina_Rish1"], "readers": ["everyone"], "writers": ["~Irina_Rish1"], "content": {"title": "reply to AnonReviewer2", "comment": "Thank you for valuable comments!\nRegarding the parameter tuning, there is clearly some room for improvement, such as a more automated procedure for adapting the parameters  to the changing datasets, which is the topic of our ongoing work. Interestingly, however,  our results were quite  robust to the parameter variations, as discussed in detail in section E.9 of the Appendix and shown in Figures 22-27. \n\nRegarding full images vs small patches, one of our goals was actually to test the approach on high-dimensional inputs, with further applicability to other types of data beyond just images. Also, rather than  explicitly representing an image as a set of patches and then learning a dictionary  of dense elements for accurate representation of such patches, a dictionary of full-image-size, but sparse dictionary elements can be used to implicitly represents an image as a linear combination of those elements, with possible overlap of non-zero pixels between elements; the non-zero pixels in a sparse element of a dictionary are learned automatically. However, we agree that exploring the patch-based setting should be added to our evaluation.\n \nFinally, when mentioning an extension of this approach to nonlinear autoencoders, such as neural nets, we planned to build upon alternating minimization approaches naturally extending the current dictionary learning method, similar to the work of  Carreira-Perpinan (AISTATS 2014) and similar methods, but the details are indeed to be worked out."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487281889080, "tcdate": 1487281889080, "id": "ICLR.cc/2017/workshop/-/paper48/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper48/reviewers"], "reply": {"forum": "S1dJ1smFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487281889080}}}, {"tddate": null, "tmdate": 1489698380955, "tcdate": 1489698380955, "number": 1, "id": "rkrLROuil", "invitation": "ICLR.cc/2017/workshop/-/paper48/public/comment", "forum": "S1dJ1smFg", "replyto": "SkX9sc7sx", "signatures": ["~Irina_Rish1"], "readers": ["everyone"], "writers": ["~Irina_Rish1"], "content": {"title": "reply to AnonReviewer1", "comment": "Thank you! We updated the paper trying to improve the clarity as you suggested (though the 3-page constraint made the clarity vs brevity trade-off a bit more challenging); in Fig 1 caption, we now mention that the 'final' dictionary size corresponds to the size of the dictionary learned by our adaptive method, as opposed to the 'initial' dictionary size the method started with; in Fig 1a, we plotted 'learned', or 'final' size, vs the 'initial' size; in Fig 1b-e, x-axis represents the 'learned' ('final') dictionary size for NODL, and the corresponding \u00a0fixed size used by ODL, while y-axis represents the reconstruction accuracy.\n\nRe: why does the correlation coefficient decline for larger k? Indeed, we observed that the accuracy is not necessarily monotonic in dictionary size, e.g., this effect is even more pronounced with dense dictionaries and very sparse codes, e.g. in Fig 9 in the Appendix. The reason can be that our sparse coding ratio (i.e. the number of non-zeros in a code) is defined w.r.t. the input dimension rather than w.r.t. dictionary size. In other words, for a fixed number of non-zeros in a code, the 'effective' code sparsity w.r.t. dictionary size actually increasing with the size of the dictionary, which could at some point \u00a0cause the performance decline. In Fig. 1d and e, however, codes are fully dense, so the reconstruction accuracy improves monotonically with dictionary size.\n\nRe: sparse dictionaries - indeed, the advantages of adaptive vs nonadaptive scheme are most pronounced in case of sparse dictionaries, while with dense dictionaries the performance is similar (see paragraph 3 in Evaluation section, and Fig. 9 and Fig 26 in the Appendix). Sparse dictionaries are interesting, however, since (1) they resulted in better classification accuracy (paragraph 3 in Evaluation section) and (2) they are more biologically plausible (correspond to sparse connectivity in network representation of sparse coding models). For a detailed discussions on the rationale behind sparse dictionary elements, see also section B.1 of the Appendix."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487281889080, "tcdate": 1487281889080, "id": "ICLR.cc/2017/workshop/-/paper48/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper48/reviewers"], "reply": {"forum": "S1dJ1smFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487281889080}}}, {"tddate": null, "tmdate": 1489378187256, "tcdate": 1489378187256, "number": 2, "id": "SkX9sc7sx", "invitation": "ICLR.cc/2017/workshop/-/paper48/official/review", "forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "signatures": ["ICLR.cc/2017/workshop/paper48/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper48/AnonReviewer1"], "content": {"title": "nice idea!", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a very nice idea for introducing neurogenesis into sparse coding, which should help in training on non-stationary datasets.  However the presentation of the results is not very clear.  The main results are shown in Figure 1 and the caption is short and cryptic.  In panels b and c for example, why does the correlation coefficient decline for larger k?  and what is meant by \"final\" dictionary size k?   Also it appears extreme sparseness was imposed on the dictionary elements themselves, allowing only 5 nonzero elements out of 1024 dimensions.  These seems strange and introduces an additional modification of standard sparse coding, making it difficult to appreciate the results.\n\nOverall impression is that this paper presents a very nice idea with great potential, but the presentation of results is a bit strange.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489378188140, "id": "ICLR.cc/2017/workshop/-/paper48/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper48/AnonReviewer2", "ICLR.cc/2017/workshop/paper48/AnonReviewer1"], "reply": {"forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper48/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489378188140}}}, {"tddate": null, "tmdate": 1489244240417, "tcdate": 1489244240417, "number": 1, "id": "S1OIl5Wol", "invitation": "ICLR.cc/2017/workshop/-/paper48/official/review", "forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "signatures": ["ICLR.cc/2017/workshop/paper48/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper48/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper addresses the problem of online sparse dictionary learning with non-stationary streams. The data is presented in episodic way, and the challenge is to adapt the model without forgetting what was learned previously. The main contributions are to propose a method with for adding or removing atoms of the dictionary based on the current performance of the model at representing incoming data. These techniques are inspired by adult neurogenesis which is a process that occurs in the hippocampus (which is involved in handling episodic memory).\n\nPros:\n- The problem is very relevant, and the paper is very well executed.\n- The conceptual ideas and their connections with adult neurogenesis are very interesting\n\nCons:\n- The proposed ideas, while never used before together, are very related to published work (as stated by the authors)\n- Experimental evaluation, while very extensive, are rather simplistic. A stronger application would improve the paper significantly.\n\nThe problem of adapting the capacity of the model with the complexity of the task is very relevant. The ideas of neural genesis and death seem very natural (and they have been explored before). I appreciate the connection with adult neurogenesis. \n\nWhile these ideas are very intuitive and seem very interesting, their implementation seems a bit heuristic (which would be OK with very strong applications). In the neural genesis, atoms are added proportional to the Pearson correlation between the reconstruction obtained by the system and the data itself. How easy is to set these parameters? This seems to be something that needs to be cross-validated for each dataset. \n\nThe experimental results show that the method performs better than the non-adaptive online method by Mairal et al. It is probably not the best to use dictionary learning methods with very high dimensional inputs, such as  full images. An interesting application would be learning patch-based dictionary where the patches come from individual images (thus patches will be very correlated), having a natural episodic setting.\n\nThe authors mention that this method could be extended to use other approaches (such as neural network based auto-encoders), this is not entirely clear, as they lack inference stage (it's just a feed-forward process), hence, adding new random weights could lead to interference of the previous model. I'm not saying it's impossible to divise a method including these ideas (which would certainly be very interesting), but I don't see it as straight forward.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse.\n", "pdf": "/pdf/8116288dd762594c4557552b2c6ada516550dd55.pdf", "paperhash": "garg|neurogenesisinspired_dictionary_learning_online_model_adaption_in_a_changing_world", "keywords": ["Computer vision", "Unsupervised Learning", "Transfer Learning", "Applications", "Optimization"], "conflicts": ["usc.edu", "us.ibm.com", "isi.edu"], "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489378188140, "id": "ICLR.cc/2017/workshop/-/paper48/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper48/AnonReviewer2", "ICLR.cc/2017/workshop/paper48/AnonReviewer1"], "reply": {"forum": "S1dJ1smFg", "replyto": "S1dJ1smFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper48/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489378188140}}}], "count": 6}