{"notes": [{"id": "Oecm1tBcguW", "original": "W_0Wcvd6W1m", "number": 626, "cdate": 1601308074701, "ddate": null, "tcdate": 1601308074701, "tmdate": 1614985774910, "tddate": null, "forum": "Oecm1tBcguW", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NJgZhWBVafv", "original": null, "number": 1, "cdate": 1610040356309, "ddate": null, "tcdate": 1610040356309, "tmdate": 1610473945921, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper addresses the problem of prior selection in Bayesian neural networks by proposing a meta-learning framework based on PAC-Bayesian theory. The authors optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. The method does not rely on nested optimization schemes, instead, they directly minimize PAC bound via a variational algorithm called PACOH-NN which is based on SVGD and the reparameterization trick.  The method is evaluated on experiments with both synthetic and real-world data showing improvements in both predictive accuracy and uncertainty estimates.\n\nInitially many reviewers were positive about the paper. However, it was noticed by one reviewer that the submitted paper presents a very significant overlap with\n\nJonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. arXiv, 2020.\n\nAnother reviewer mentioned that they were actually reviewing for AISTATS the above manuscript by Rothfuss et al. The ICLR program chairs were contacted for a possible violation of the dual submission policy for ICLR: \n\n\"Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences or journals, are not allowed and violate our dual submission policy.\" \n\nThe ICLR program chairs decided that the similarities between the two papers are not enough to issue a desk-rejection. However, in the discussion period,  three reviewers out of 4 pointed out that, even though the authors did revise Sections 4 and 5 in the current version, these modifications do not seem to be strong enough to make up for the really strong overlaps between the two papers. The reviewers agreed on rejection and stated that this paper should either be merged with the Rothfuss et. al. one (assuming the authors are the same), or its content should be developed to the point of making both of them clearly distinct. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040356294, "tmdate": 1610473945901, "id": "ICLR.cc/2021/Conference/Paper626/-/Decision"}}}, {"id": "Ie1WzXJCzB", "original": null, "number": 12, "cdate": 1606304210778, "ddate": null, "tcdate": 1606304210778, "tmdate": 1606304210778, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "8iIskaRhQhy", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Experiment results for GP-based method of Rothfuss et al. (2020) have been added to revised paper", "comment": "In response to the suggestion of AnonReviewer2, we conducted experiments with the GP-based method of Rothfuss et al. (2020) on the five regression environments. The respective results have been added to Table 1 & 2 and the experiment description / discussion has been modified accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "UxRQDNN2edq", "original": null, "number": 11, "cdate": 1606303721691, "ddate": null, "tcdate": 1606303721691, "tmdate": 1606303721691, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "kAyPKctFfo", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "answer to the remaining questions", "comment": "> I believe it is fine to have additional content in addition to the main results but I do appreciate the efforts if the authors can clearly distinguish the new results from the previous efforts.\n\nIn the revised version of the paper, especially in Section 4, we try to stress out more clearly when we rely on the methodology of previous work and when new results are contributed. We hope that this resolves your concerns.\n\n> Also, is there a specific reason the authors did not compare PACOH-NN with the GP-based method in Rothfuss et al. (2020)? It would definitely help better clarify the significance of this paper compared to Rothfuss et al. (2020) if the authors can do so.\n\nOriginally we did not include the GP-based method of Rothfuss et al. (2020) in the benchmark experiments due to space constraints and with the reasoning to restrict the comparison to NN-based methods so that the performance differences reflect upon the meta-learning algorithm rather than on the prediction model. However, we agree with the reviewer that a comparison to Rothfuss et al. (2020) would be helpful. Hence, the last two days, we ran experiments with PACOH-GP (Rothfuss et al., 2020) under similar conditions like PACOH-NN (e.g. $K=5$ SVGD particles) and added them to Table 1 and 2 of the revised paper. We hope that this helps towards further clarifying the significance of the paper.\n\n> Regarding the question on neural process, in Section 6.1 (page 7) of the current revision, it still reads \"Second, we compare our proposed approach against various popular meta-learning algorithms, including neural processes (NPs) ...\" If there is no straightforward to implement it for classification, the authors may need to revise accordingly.\n\nThanks for pointing that out. We have revised the respective paragraph in Section 6.1 accordingly.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "kAyPKctFfo", "original": null, "number": 10, "cdate": 1606170274317, "ddate": null, "tcdate": 1606170274317, "tmdate": 1606170274317, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "JHOCuQKc8uA", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "a couple of remaining questions", "comment": "This reviewer truly appreciates the detailed response from the authors, especially clarification of the new developments compared to Rothfuss et al. (2020).\n\nRegarding \"Would you prefer if we remove large parts of Section 4 and just state the end-result, e.g., Proposition 1?\"\n\nI believe it is fine to have additional content in addition to the main results but I do appreciate the efforts if the authors can clearly distinguish the new results from the previous efforts. \n\nAlso, is there a specific reason the authors did not compare PACOH-NN with the GP-based method in Rothfuss et al. (2020)? It would definitely help better clarify the significance of this paper compared to Rothfuss et al. (2020) if the authors can do so. \n\n\n\n\nRegarding the question on neural process, in Section 6.1 (page 7) of the current revision, it still reads \"Second, we compare our proposed approach against various popular meta-learning algorithms, including neural processes (NPs) ...\" If there is no straightforward to implement it for classification, the authors may need to revise accordingly. "}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "4BNKK-Llspc", "original": null, "number": 9, "cdate": 1606162115008, "ddate": null, "tcdate": 1606162115008, "tmdate": 1606162115008, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "8iIskaRhQhy", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "BMAML experiment results for classification  have been added", "comment": "The experiment results BMAML on the classification tasks have been added to Table 3 in the revised version of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "JHOCuQKc8uA", "original": null, "number": 8, "cdate": 1606058001025, "ddate": null, "tcdate": 1606058001025, "tmdate": 1606058001025, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "mbx-2Ozp4x", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Dear reviewer,\n\nThank you for your detailed feedback. In what follows, we will address your concerns in order:\n\n---\n1.\nWhile our approach indeed builds on the methodology developed in Rothfuss et al. (2020), the present submission goes substantially beyond it, as elaborated below. Crucially, in terms of algorithms, previous work was restricted to tractable base learners (e.g. GPs).  We theoretically show how to lift these restrictions, and present extensive empirical demonstrations on (intractable) Bayesian neural network base learners. \n\nTo ensure our paper is self-contained, Section 4 indeed has some overlap as we build on the methodology of Rothfuss et al. (2020). It provides necessary background to understand the PAC-Bayesian setup and how to arrive at the PAC-optimal hyper-posterior. Would you prefer if we remove large parts of Section 4 and just state the end-result, e.g., Proposition 1?\n\nMoreover, we want to emphasize that, thanks to our improved proof methodology, our bounds are tighter than the ones in Rothfuss et al. (2020). We extended the corresponding discussion under Theorem 2 in the reviser paper to point this out more clearly.\n\nWe acknowledge that in the submitted version of the paper, the PACOH-NN algorithm is presented only very quickly and is discussed insufficiently. Aiming to address this, we have significantly extended Section 5 and Appendix B. In particular, we have added \n*   a detailed discussion of the properties / approximation quality of our proposed generalized marginal log-likelihood estimator\n*   a description of the proposed PACOH-NN algorithm\n*   a discussion of PACOH-NN\u2019s advantages over previous methods\nto the revised paper.\n\nOverall, the key algorithmic contribution of our paper over Rothfuss et al. (2020) is the numerically stable score estimator for the generalized marginal log-likelihood (MLL) and our analysis which shows that despite the approximation error, we still minimize a valid upper bound on the transfer error. This makes PAC-Bayesian meta-learning both tractable and scalable for a much larger array of models, including BNNs. In contrast, Rothfuss et al. is restricted to models with closed-form MLL which severely restricts its use. PACOH-NN can also be used for tasks such as image classification or high-dimensional Bayesian optimization (c.f. Sec. 6.2.) to which the method of Rothfuss et al. (2020) is not applicable. Hence, we rather consider PACOH-NN complementary to the GP based method of Rothfuss et al. (2020) than as a replacement.\n\nOverall, we believe that making PAC-Bayesian meta-learning scale to large-scale settings and work on challenging real-world problems is a relevant contribution and certainly of interest to the ICLR community.\n\n---\n2.\n Importantly, the (Gibbs) hyper-posterior in Prop. 1 is not equivalent to the posterior given hyper-prior and likelihood of the union of all datasets. For instance, this can be shown by the convexity of the log-Laplace transform in $\\ln Z(S_i, P_\\phi)$. In fact, the log-Laplace transform makes sure that the boundaries between tasks are implicitly taken into account which is crucial if we want to successfully meta-learn. Only in the edge case $L=1$ of our (generalized) marginal log-likelihood estimator, the log-sum-exp is no longer strictly convex and becomes the (linear) identity function. In this edge case, we effectively fall back on the likelihood of the union of all datasets which ignores the task boundaries. This is discussed in Sec. 5 and Appendix B of the revised paper.\n\n---\n\n3.\nFigure 1 is meant as a brief qualitative inspection that compares a standard Gaussian BNN prior with a meta-learned one. For the purpose of benchmarking against other meta-learners, we provide extensive quantitative results in Table 1, 2 and 3. \n\nWe did not find a neural process implementation for classification. The authors only provide one for regression. If you can point us to a NP for classification implementation, we are happy to include it into our experiments.\n\nWe agree that the wording \u201cmaximum regret\u201d is misleading and renamed it into \u201csimple regret\u201d, referring to $\\max_{a} r(a) - \\max_{t=1,...,T}r(a_t)$. We added details on the bandit experiment and regret definitions to Sec. C.4 of the revised paper. Table S2 reports the number of samples per tasks (103 - 3088), which substantially varies across the 5 alleles.\n\n---\n 4.\n We have fixed typos and inconsistencies in the mathematical notation.\n\n---\n\n5.\n Thanks for noticing the ambiguity in the complexity analysis. To address this issue, we have added a memory and computation complexity analysis of the algorithm to Section 5 and added the pseudocode for a batched version of Algorithm 1 in Appendix B. Now the computational properties of PACOH-NN are discussed in detail and compared to previous methods such as MLAP.\n---\n\nFinally, we want to thank you once again for the provided feedback, which allowed us to provide a better emphasis on the strengths and relevance of our approach.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "_YbIz8hGsoq", "original": null, "number": 7, "cdate": 1606056332533, "ddate": null, "tcdate": 1606056332533, "tmdate": 1606056332533, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "BmQha9LMPcS", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Dear reviewer,\n\nThank you for your thorough review and for the encouraging feedback.\n\nSince MLAP relies on closed form KL-divergences and we want to keep the benchmark comparison fair, we have restricted our experiments to Gaussian priors with diagonal covariance matrix and a spherical, zero-centered Gaussian hyper-prior. This is the setup that was used in the MLAP paper (Amit & Meir, 2018). However, we believe that having more expressive priors that account for the interactions between the NN weights would be greatly beneficial, not only for the accuracy, but also for the quality of the uncertainty estimates. To this end, Matrix Variate Gaussian distributions (Louizos & Welling, 2016) or Normalizing flows (Rezende & Mohamed, 2015) can be considered. In addition, a less simplistic hyper-prior, such as the horse-shoe prior could be employed to enable a data driven model selection (Ghosh & Doshi-Velez 2017). Compared to MLAP, our proposed method offers the flexibility and computational efficiency to enable the use of more expressive priors and hyper-priors. This is why, in conclusion of the paper, we suggest exploring such avenues in future work.\n\nFor the regression experiments we have used a NN with 4 hidden layers of size 32. Since we run experiments with 8 different meta-learning environments and include various baseline methods, the computational burden is very high. To not further increase the computational burden, we haven\u2019t experimented with different model sizes.\n\nHowever, the fact that we can successfully deploy PACOH-NN on a much larger CNN for the classification tasks seems promising. So far we have not experienced any issues, i.e. that the system is not able to properly learn the data. As we argue in the introduction, a key issue with BNNs is the arbitrary choice of the prior, i.e., zero-centered, spherical Gaussian which may lead to such problems/artifacts. By acquiring the prior in a principled data-driven way, we can most likely avoid such issues with the prior. Nonetheless, we do see the possible adaptation/extension of our framework to function space priors and functional posterior inference as an interesting direction. \n\nThanks for catching the typo. A reference that provides more details about the set-up in Figure 1 has been included. Also, we have added qualitative descriptions to e.g. $\\phi(\\sqrt{m})$ to make the writing more accessible.\n\n---\n\nAmit, Ron, and Ron Meir. \"Meta-learning by adjusting priors based on extended PAC-Bayes theory.\" In: ICML, pp. 205-214. PMLR, 2018.\n\nDanilo Jimenez Rezende and Shakir Mohamed. \u201cVariational inference with normalizing flows\u201d. In: ICML. 2016.\n\nChristos Louizos and Max Welling. \u201cStructured and efficient variational deep learning with matrix gaussian posteriors\u201d. In: ICML. 2016, pp. 1708\u20131716.\n\nSoumya Ghosh and Finale Doshi-Velez. \u201cModel selection in Bayesian neural networks via horseshoe priors\u201d. In: arXiv preprint arXiv:1705.10388 (2017)."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "o87y8Guq5a1", "original": null, "number": 6, "cdate": 1606056162593, "ddate": null, "tcdate": 1606056162593, "tmdate": 1606056162593, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "kwcSlote-9", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Dear reviewer,\n\nThanks a lot for taking the time to write a thorough review and giving detailed tips for improvement. In the following, we try to answer your questions and respond to your comments:\n\n* How tight the bounds and the corresponding performance guarantees depends strongly on setting. For instance in a Bayesian linear regression similar to the toy setup in Section 6 of Catoni et al. (2016), we expect the bounds to be quite tight and useful. However, for large over-parametrized models such as neural networks, as all other PAC bounds for NNs, we do not expect the bounds to be particularly tight. Why neural networks generalize despite their over-capacity is an open problem which our paper does not resolve.\nIn general, we see the PAC-Bayesian meta-learning bounds mainly as a guidance for developing a principled algorithm that works well in practice, rather than an artefact that is useful in itself for real-world problems.\n\n* Hoeffding\u2019s lemma: We have checked Case I in Appendix A1 and could not find any mistakes. Note that in equation 18, we have a product with $n$ and $n m$ factors respectively which cancels out with the $n^2$ and $(nm)^2$ in the denominator of the exponent which we get from Hoeffding's lemma and the sub-gamma assumption respectively. This leaves us with $\\frac{\\beta}{8n}$ and $\\frac{\\lambda}{nm}$. Does this resolve your question?\n\n* Hyper-prior: We use a hyper-prior over $\\phi:= (\\mu_P, \\log \\sigma_P)$, i.e., both the prior mean and log variances (see Section 5.2.). As suggested by you, this allows to selectively \u201cfreeze\u201d any weights that can be stable across all tasks.\n\n * Sinusoids in Figure 1: The details for the sinusoid environment can be found in Appendix C.1.1. A visual representation is also available in Figure S2a. Indeed, the tasks do share a lot of similarities which makes meta-learning possible, i.e. they all slope upward and have a sinusoidal shape. However, the tasks differ in four parameters (slope, amplitude and translation in x/y direction) and contain an additive gaussian observation noise. This makes task identification non-trivial and thus reasoning about epistemic uncertainty crucial.\n\n* BMAML: Upon your request, we have implemented Bayesian MAML and are currently running the experiments using it \u2013 The results for BMAML on the regression tasks have already been incorporated into the revised version of the paper. The BMAML experiments on the classification tasks are still running. We will add the corresponding results for the paper by Tuesday, Nov 24.\n\n* In fact, to ensure a fair comparison, for GP-UCB and GP-TS, we meta-learn the GP prior parameters by minimizing the marginal log-likelihood on the five meta-training tasks (see Appendix C4). Did you have any specific meta-bandit algorithm in mind that goes beyond that?\n\n   \nMinor comments:\n\n* The classification (labelling) noise is implicit in the softmax-categorical class probabilities which are determined by the logits which the neural network outputs. Technically, one could add a temperature/scaling parameter to the softmax/logits. However, the neural network can scale the logits itself - thus no need for an extra parameter. If the logits values are small in magnitude, the categorical distribution has higher entropy corresponding to more labelling noise / aleatoric uncertainty. If the logit values are large in magnitude the softmax tends towards a max operation and thus little labelling noise.\n\n* Regarding Theorem 1, the reference should be correct. Theorem 1 in our paper corresponds to Theorem 4.1 in Alquier et al 2016.\n\n* We have added the chapter and pages to the Bucheron et al. reference.\n\n* Thank you for noticing the ambiguity regarding the complexity analysis. Since the favorable scaling properties are a key advantage of PACOH-NN over MLAP, we have added a memory and computation complexity analysis to Section 5 of the revised paper. In the mini-batched version of the algorithm, both the memory and computational complexity are constant w.r.t. the number of tasks n.\n\n* Thanks for catching the typos and the undefined abbreviations/variables \u2013 they have been fixed.\n\nThanks again for your constructive feedback. We hope that our corresponding changes have improved the paper.\n\n---\n\nGermain, Pascal, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. \"PAC-Bayesian theory meets Bayesian inference.\" In NIPS, pp. 1884-1892. 2016."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "CfyXrvzLIW6", "original": null, "number": 5, "cdate": 1606055720213, "ddate": null, "tcdate": 1606055720213, "tmdate": 1606055720213, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "I8-dlxiWKO6", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Dear reviewer,\n\nThank you for your review \u2013 we are happy to hear that you enjoyed reading our paper and consider it of interest to the ICLR community.\n\nWhile our approach indeed builds on the PAC-Bayesian meta-learning methodology developed in Rothfuss et al. (2020), the present submission goes substantially beyond it, as elaborated below.  Crucially, in terms of algorithms, previous work was restricted to tractable base learners (such as Gaussian processes).  We theoretically show how to lift these restrictions, and present extensive empirical demonstrations on (intractable) Bayesian neural network base learners. \n\nWe agree though that in the submitted version of the paper, the PACOH-NN algorithm is presented very quickly and discussed insufficiently. In order to address these shortcomings, we have significantly extended Section 5 and its counterpart in Appendix B. In particular, we have added:\n\n\n\n*   a detailed discussion of the properties / approximation quality of our proposed generalized marginal log-likelihood estimator\n*   a description of the proposed PACOH-NN algorithm\n*   a memory and computation complexity analysis thereof\n*   a discussion of PACOH-NN\u2019s advantages over previous methods\n\nto the revised version of the paper.\n\nMoreover, we want to emphasize that, thanks to our improved proof methodology, our proposed  bounds are tighter than the ones in Rothfuss et al. (2020). We extended the corresponding discussion under Theorem 2 in the reviser paper.\n\nOverall, the key algorithmic contribution of our paper over Rothfuss et al. (2020) is the numerically stable score estimator for the generalized marginal log-likelihood and our analysis which shows that despite the approximation error, we still minimize a valid upper bound on the transfer error. This makes PAC-Bayesian meta-learning both tractable and scalable for a much larger array of models, including BNNs. In contrast, Rothfuss et al. is restricted to models with closed-form marginal log-likelihood which severely restricts its use. In contrast, PACOH-NN can also be used for tasks such as image classification or high-dimensional Bayesian optimization (e.g. the experiment in Section 6.2.) to which the method of Rothfuss et al. (2020) is not applicable. \n\nThus, we consider PACOH-NN as an algorithm of high practical relevance which we try to showcase in our extensive experiments on real-world meta-learning problems.\n\nFinally, we want to thank the reviewer for the provided feedback, which allowed us to provide a better emphasis on the strengths and novelty of our approach."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "8iIskaRhQhy", "original": null, "number": 4, "cdate": 1606055534038, "ddate": null, "tcdate": 1606055534038, "tmdate": 1606055552271, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Changes in the revised version of the paper", "comment": "Addressing the reviewers feedback, we have made the changes to the paper and ran further experiments. In summary, the most significant changes were the following:\n\n\n\n*   In Section 4, we have extended the discussion below Theorem 2, detailing how our bound improves upon previous bounds for meta-learning.\n*   In Section 5 we have added :\n    *   a detailed discussion of the properties / approximation quality of our proposed generalized marginal log-likelihood estimator\n    *   a description of the proposed PACOH-NN algorithm\n    *   a memory and computation complexity analysis thereof\n    *   a discussion of PACOH-NN\u2019s advantages over previous methods\n*   Section 6: Upon the reviewers request, we ran experiments with BMAML and have included it into our empirical benchmark study. Corresponding results on our regression benchmark tasks have been added to Table 1 & 2. The experiment results for the classification tasks will be added soon to Table 3.\n*   Appendix: \n    *   We have added a mini-batched version of the PACOH-NN algorithm to Appendix B.1.\n    *   We have extended Appendix B.3, deriving and discussing the statistical properties of the general marginal log-likelihood estimator.\n    *   We have added Section C.4. which provides further details on the bandit experiment in Section 6.2.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "JPtRJzQlJ27", "original": null, "number": 3, "cdate": 1605730451834, "ddate": null, "tcdate": 1605730451834, "tmdate": 1605730451834, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "kwcSlote-9", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment", "content": {"title": "Didn't finish a sentence", "comment": "Above was: \n\"One of the main motivations for the paper is that the method comes with performance guarantees. E.g. in the related work section ...\"\n\nThis should say:\n\nOne of the main motivations for the paper is that the method comes with performance guarantees. E.g. in the related work section other methods criticised as they \"fall short of performance guarantees\". How strong are the guarantees in practice? Generalisation error bounds tend to be very loose (often vacuous), so while they're a \"nice to have\" they often don't provide any practical guarantees."}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oecm1tBcguW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper626/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper626/Authors|ICLR.cc/2021/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Comment"}}}, {"id": "BmQha9LMPcS", "original": null, "number": 2, "cdate": 1603818562898, "ddate": null, "tcdate": 1603818562898, "tmdate": 1605024644437, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Review", "content": {"title": "Review of the PACOH-NN algorithm for learning BNN priors ", "review": "### Summary:\n\nOne of the main issues that BNNs have to face is the choice of good informative priors in order to provide precise information about the uncertainty of predictions. The present work connects BNNs and PAC theory to formulate a new system to obtain a general-purpose approach for obtaining significative priors for Bayesian NNs. This is done by employing the closed-form solution for the PAC-Bayesian meta-learning problem. The meta learner here learns a hyper-posterior over the priors of the parameters of the BNN by using the closed-form expression PACOH  (PAC Optimal Hyper-Posterior). This is applied in the context of NNs, where the priors are to be defined over the NN parameters. Extensive experiments are carried out to show the performance both in regression and classification datasets, as well as its scalability. In all of these regards, the system seems to be competitive, improving on the results of previous state-of-the-art methods and producing promising results in real-world problems. \n\n##### Pros:\n\n* The method does not employ nested optimization problems, therefore avoiding all the issues related to these approaches altogether.\n* The usage of Bayesian seems to point in the right direction since the Bayesian framework allows for an easy formulation of the different levels needed here to formulate the system. \n* The construction of the PACOH closed-form expression seems innovative and relevant \n* The final system improves on the previous state-of-the-art in most of the cases here shown, and in some experiments, the improvement is very clear. Thanks to the fact that it is agnostic to the inference method in use, it presents itself as a very general-purposed approach, able to improve the predictive qualities of previous methods. \n* The article is very complete and detailed, both in the main body and in the appendix. The appendix is particularly extensive, providing insight on many of the main points of the main text. \n* The experiments conducted are exhaustive and complete, providing a very wide scope of the capabilities of the method. Detailed results can be found for every experiment and task. \n* The text is well written and comprehensible\n\n##### Cons:\n\n* The choice in section 5 of using Gaussian priors with diagonal covariance matrices is motivated by the convenience in the computations. Moreover, the hyper-prior is also modeled by a zero-centered spherical Gaussian. How does choosing these distributions affect the final results? Are there been any experiments on which the parametric distributions chosen here are different from the ones presented? Please, describe how do you think these choices may affect the results and bias the final distributions obtained. \n* The calibration error is all the information provided to quantify the quality of the predictive intervals for regression. It would be helpful to include some other quantification of this, such as the usage of CRPS or other strictly proper scoring rules. In the same line, using metrics such as the Brier score for classification tasks would help getting a more complete picture. \n* How does the complexity of the BNNs employed affect the final results? Does using more layers and/or more units improve the results? There have been recent works (e.g. Functional Variational BNNs) regarding artifacts that arise when using large BNNs which make the system not able to properly learn the data. Could these problems be solved as well with this approach since the final prior is constructed using the data? \n\n###### Other comments: \n\n* As an optional suggestion, the article is well written but can be difficult to comprehend at some points. To that end, I would suggest trying to provide qualitative descriptions of some of the quantities in the expressions that later prove to be of importance. As an example, providing some intuition on $\\psi(\\sqrt{m})$ would help to understand the expression (1). In general, I think the article would benefit from extending a bit the explanations of some parts, especially section 4. \n* Typo: section 5, paragraph 2 - \"categorigal\" \n* To have a clearer explanation of figure 1 I would try to include the description of the sinusoidal functions that is already present in the appendix section C.1, since it seems relevant to the text in section 6.1 as well.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138921, "tmdate": 1606915760168, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Review"}}}, {"id": "mbx-2Ozp4x", "original": null, "number": 1, "cdate": 1603771349922, "ddate": null, "tcdate": 1603771349922, "tmdate": 1605024644375, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Review", "content": {"title": "PAC-Bayesian meta learning with Bayesian neural nets", "review": "This submission has significant overlap with the following reference: \n\nJonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-Optimal Meta-Learning with\nPAC-Guarantees. arXiv, 2020.\n\nAssumably, the submission is from the same authors as theoretical analyses and experiment setups are almost identical. The main difference I can see is the Bayesian baseline model is Bayesian neural nets while the reference used Gaussian processes. The rating I gave is because of this main concern on overlapping content. \n\nThe analysis of PAC-Bayesian bounds led to the conclusion that Gibbs posterior can help minimize the bounds in the meta-learning settings. With Gibbs base learner, the bound expressed with the corresponding partition function can be done more efficiently compared to existing PAC-Bayesian meta-learners, including MLAP. \n\nMy other concerns are as follows: \n\n1. The authors may need to make sure about the main contributions of this submission with most of theoretical results already presented in the referred paper. If Bayesian neural nets are indeed showing much better empirical performances compared to GPs, the authors may need to emphasize that and discuss thoroughly with performance comparison. \n\n2. The final solution to derive Gibbs posterior is just deriving the model posterior given the hyper-prior using the data from all training tasks. If that is the case, the authors may present it clearly and discuss the connections as well as pros and cons to other existing meta-learners, for example, the existing metric-based methods that reweighting the loss across training tasks. \n\n3. The experiments may need to be done more consistently. For example, it is not clear why figure 1 only compares PACOHNN with a vanilla BNN using a simply prior instead of, for example, comparing with MLAP or PACOH with GPs? For classification examples, the authors did not compare with neural processes as claimed at the beginning of section 6.1. In section 6.2, why max regrets are smaller than average regrets in figure 3? How many training samples were used for the five tasks to get the reported results? \n\n4. The writing of theoretical analyses (main text, but especially appendix) can be improved. There are typos and confusing math notations. For example, in section 5,  above equation 7, \"\\epsilon_l j \", where j appears to be a typo? In A.1, \"to proof the theorem\" should be \"to prove the theorem...\"; in step 1 and 2 of the proof, X_k was used to denote the data point and then task, respectively, which is indeed confusing. It is not clear either what the subscript k was used for. Cleaner math notations, better organization, and careful rewriting may significantly help the readability of the paper. \n\n5. The authors may want to add complexity analysis of PACOH-NN. It is in fact quite confusing to me why it \"maintains a nearly constant memory and compute load as the number of tasks grow.\" as stated since the number of tasks does factor in based on Algorithm 1 in Appendix B. Clearer setup descriptions with complexity analysis and discussion are needed. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138921, "tmdate": 1606915760168, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Review"}}}, {"id": "kwcSlote-9", "original": null, "number": 3, "cdate": 1603904994375, "ddate": null, "tcdate": 1603904994375, "tmdate": 1605024644313, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Review", "content": {"title": "Principled approach to meta-learning with strong empirical results", "review": "The paper proposes a method for learning BNN priors based on optimising a PAC-Bayes bound for meta-learning, which they call PACOH-NN. This extends previous work on optimising PAC-Bayes bounds (PACOH), and is attractive for being principled in construction and effective in practice. The main claims are that fewer tasks are needed to achieve good performance in terms of both utility and calibration, and that the algorithm comes with performance guarantees.\n\nIn the empirical analysis, they firstly show that the method is competitive in terms of utility (for both classification and regression) with baseline methods, but with superior calibration properties. Meanwhile the method is also superior in terms of computational scaling. They also demonstrate the effectiveness on a bandit learning task for vaccine development. Along with proofs, extensive details are given for the experimental setup and training algorithms.\n\n\nStrengths:\n- Principled approach to meta-learning for BNNs\n- Efficient algorithm with good scaling properties\n\nWeaknesses:\n- A little hard to read in places\n- Performance guarantees never shown\n\nSpecific comments:\n- One of the main motivations for the paper is that the method comes with performance guarantees. E.g. in the related work section\n- The use of Hoeffding\u2019s lemma could be very loose, particularly in the cases where values close to 0 or 1 are unlikely\n- In the use of the Hoeffding\u2019s lemma, it looks like there should be an extra $m^2$ term in the numerator. Where has this term gone? Similarly, I would expect a factor of $m$ for the sub-Gamma bound in the numerator. This would carry over to the derivations for Thm2 as well, and would also affect the convergence properties\n- It seems that you have hyper-priors for the means of the BNN priors only. Are the variances always set to a default value for new tasks? Did you consider putting a hyperprior on the variances/precisions as well? This would feel more in keeping with the Bayesian flavour, and might give the ability to selectively \u201cfreeze\u201d (softly) any weights that are stable across all tasks.\n- In Figure 1, with only a single training example, I don\u2019t understand how PACOH-NN is able to transfer the knowledge of sinusoidal functions. Since the transfer is only at the hyperprior level, it seems to me that the only possibility is that the tasks are similar enough that the hyperpriors are actually capturing the entire distribution over tasks, so that there is almost no learning required at all on the target task. Can you comment on this?\n- You cite the Bayesian MAML paper by Yin et al. However, in the experiments, you only use the regular MAML and first order version, both of which give point estimates. A comparison to BMAML seems to be essential here\n- The experiment in 6.2 is a nice addition, and the results are quite impressive on first glance. However, since none of the baselines are meta-learners, we can\u2019t be sure here if the difference is due to the effect of transfer learning or the superior uncertainty quantification. Another meta-learner should be included. There are several meta-bandit algorithms that are available.\n- Figure S1 in the appendix is really helpful! If possible, I would prefer to see this in the main paper\n\n\nMinor comments/Typos:\n- P1 variation -> variational\n- P2 in the BNN description, the regression model have a noise parameter but the classification model does not. Wouldn\u2019t that imply that the classes would have to be separable (i.e. no mislabelled examples)?\n- P3 $\\mathcal{H}$ is undefined on first use. Also, in the description the prior is defined as $P \\in \\mathcal{M}(\\mathcal{H})$, but in (1) you have $P \\in \\mathcal{\\Theta}(\\mathcal{H})$ and $\\mathbb{E}_{\\theta \\sim P}$. On page 4, you then have $P \\in \\mathcal{M}(\\mathcal{\\Theta})$\n- Where does Theorem 1 appear in Alquier et al 2016? Is this the correct reference?\n- For the sub-Gamma case, you reference Boucheron et al , which is a book. Please make the reference more specific\n- Tasks are defined as $\\tau_i := (\\mathcal{D}_i, S_i)$, but it would seem more accurate to define them as $\\tau_i := (\\mathcal{D}_i, m_i)$. Not that later you have an expectation where you draw $(\\mathcal{D}, m) \\sim \\tau$ which is also consistent with the latter\n- In (7) you use LSE - mention this is LogSumExp\n- P8 you say \u201cnearly constant memory and compute\u201d. I\u2019m not sure what \u201cnearly constant\u201d means - is it really linear, but with better scaling constants? Or is it truly constant?\n- P12 $p$ undefined in lemma 1\n- P18 (53) second line should be $\\theta_l$ rather than $\\theta_L$\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138921, "tmdate": 1606915760168, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Review"}}}, {"id": "I8-dlxiWKO6", "original": null, "number": 4, "cdate": 1604177207835, "ddate": null, "tcdate": 1604177207835, "tmdate": 1605024644249, "tddate": null, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "invitation": "ICLR.cc/2021/Conference/Paper626/-/Official_Review", "content": {"title": "Paper is well-written but its contribution is marginal", "review": "The paper addresses the problem of learning data-driven priors for Bayesian neural networks. Assuming zero-centered Gaussian priors for BNNs often results in poor generalization and uncertainty quantification, whereas choosing informative priors is challenging due to limited interpretability of network weights. To be able to overcome these issues, the authors propose a meta-learning framework based on PAC-Bayesian theory, in which they optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. Unlike the previous approaches in the literature, their approach doesn\u2019t rely on nested optimization schemes, instead they directly minimize PAC bound via a variational algorithm called PACOH-NN which is based on SVGD and reparameterization trick.\n\nThe main paper and its comprehensive appendix are well written mostly, especially the PAC-Bayes review is very concise and elaborate for a conference paper. After the introduction of related concepts about PAC-Bayes bounds and meta-learning, their application to the derivation of PACOH bound for finding a hyper-posterior expressed so naturally. Proposed methodology sounds, and claims of the authors seem correct. Experimental results both on synthetic and real-world data seem fine, and there seems to be no fallacy in empirical evaluations. Also, experimental methodology is discussed thoroughly both in the main paper and appendix. Empirical results from various experimental setups are sufficient to show that their framework improves both predictive accuracy and the uncertainty estimates compared to several popular meta-learning approaches with additional computational advantages.\n\nAs a researcher with superficial knowledge of the field, I didn't have a major difficulty in following the paper, despite the use of heavy mathematics in methodology. Mathematical notation is clear and consistent (except the usage of $M(\\Theta)$ and $M(H)$ interchangeably). Derivations in both the main paper and the appendix are easy to follow. Also, relation to prior work is clearly addressed, relative advantages and disadvantages of the proposed methodology are discussed.\n\nIn my opinion, the problem of meta-learning informative priors for Bayesian neural networks will certainly be of interest to the ICLR community. On the other hand, paper\u2019s contribution to the work of Rothfuss et al. seems limited. Authors state that the main contribution of the paper is their practical variational algorithm for PACOH bound. In order to tractably optimize PACOH for a BNN, they employ a Stein gradient variational descent algorithm by utilizing reparameterization trick. However, the specifics of their algorithm are presented very quickly, which makes it difficult to grasp the actual novelty of PACOH-NN. For instance, neither the advantages of this particular optimization scheme over the previous methods, nor the theoretical guarantees regarding convergence and approximation quality for PACOH-NN aren\u2019t discussed.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper626/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper626/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Bayesian Neural Network Priors Based on PAC-Bayesian Theory", "authorids": ["~Jonas_Rothfuss1", "martin.josifoski@epfl.ch", "~Andreas_Krause1"], "authors": ["Jonas Rothfuss", "Martin Josifoski", "Andreas Krause"], "keywords": ["meta-learning", "life-long learning", "transfer", "bayesian neural networks", "prior", "few-shot learning", "pac-bayes", "generalization bound"], "abstract": "Bayesian deep learning is a promising approach towards improved uncertainty quantification and sample efficiency.\nDue to their complex parameter space, choosing informative priors for Bayesian Neural Networks (BNNs) is challenging. Thus, often a naive, zero-centered Gaussian is used, resulting both in bad generalization and poor uncertainty estimates when training data is scarce. In contrast, meta-learning aims to extract such prior knowledge from a set of related learning tasks. We propose a principled and scalable algorithm for meta-learning BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require optimizing the prior and multiple variational posteriors in an interdependent manner, our method does not rely on difficult nested optimization problems and is agnostic to the variational inference method in use. Our experiments show that the proposed method is not only computationally more efficient but also yields better predictions and uncertainty estimates when compared to previous meta-learning methods and BNNs with standard priors.", "one-sentence_summary": "A principled and scalable meta-learning algorithm for Bayesian neural network priors based on PAC-Bayesian learning thory.", "pdf": "/pdf/18bf717848e1698a886630abb287488155efae61.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rothfuss|metalearning_bayesian_neural_network_priors_based_on_pacbayesian_theory", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HvY9o0Hf0t", "_bibtex": "@misc{\nrothfuss2021metalearning,\ntitle={Meta-Learning Bayesian Neural Network Priors Based on {\\{}PAC{\\}}-Bayesian Theory},\nauthor={Jonas Rothfuss and Martin Josifoski and Andreas Krause},\nyear={2021},\nurl={https://openreview.net/forum?id=Oecm1tBcguW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oecm1tBcguW", "replyto": "Oecm1tBcguW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper626/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138921, "tmdate": 1606915760168, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper626/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper626/-/Official_Review"}}}], "count": 16}