{"notes": [{"id": "0MCY0JklOeJ", "original": null, "number": 12, "cdate": 1593465705654, "ddate": null, "tcdate": 1593465705654, "tmdate": 1593466053606, "tddate": null, "forum": "rJeB36NKvB", "replyto": "Mb-oPQ1aMx", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Thomas Brox", "comment": "Hi Thomas Brox, thanks for your interesting question. I would like to discuss about this question, but can I ask if you are asking the input of the PEN module is a constant value? The input of the PEN is directly extracted from a pre-trained CNN model, so are you asking if given a natural image X, the multi-level feature from a VGG could be a constant? I can better answer this question if understand it correctly, thanks Sen."}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "Mb-oPQ1aMx", "original": null, "number": 4, "cdate": 1593019807850, "ddate": null, "tcdate": 1593019807850, "tmdate": 1593019807850, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment", "content": {"title": "positional encoding vs. constant features", "comment": "Are you sure that the results tell something about positional encoding in the feature embedding? All the PEN needs is a feature channel that is constant for all inputs (like a bias). Then the PEN can normalize this constant to 1 and multiply it with the desired constant output. "}, "signatures": ["~Thomas_Brox1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Thomas_Brox1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204261, "tmdate": 1576860562874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment"}}}, {"id": "rJeB36NKvB", "original": "SygwRklODr", "number": 781, "cdate": 1569439149441, "ddate": null, "tcdate": 1569439149441, "tmdate": 1583912032170, "tddate": null, "forum": "rJeB36NKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "PaaqsTbMD", "original": null, "number": 11, "cdate": 1582294267107, "ddate": null, "tcdate": 1582294267107, "tmdate": 1582294267107, "tddate": null, "forum": "rJeB36NKvB", "replyto": "KLN8PkPA6T", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Alberto Bernacchia", "comment": "Addition to my answer, our work was trying to analyze and understand the backbone by using PEM, instead of designing a PEM to output position information. \n\nCheers\nSen"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "KLN8PkPA6T", "original": null, "number": 10, "cdate": 1582293970705, "ddate": null, "tcdate": 1582293970705, "tmdate": 1582293970705, "tddate": null, "forum": "rJeB36NKvB", "replyto": "Vt9BTvCtgv", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Alberto Bernacchia", "comment": "Dear Alberto Bernacchia:\n\nThanks for your new questions.\n1) \"analyze the information inside a CNN, do you agree that the choice of the architecture of the PEG is arbitrary?\".\nA: Yes, we analyzed the information inside a CNN about the position information. The choice of PEM could be arbitrary but note that this PEM is only a read-out module. You can replace it with one or more conv layers and your result should be valid.\n\n2)  \"then I choose a PEG that ignores the input, and always gives the pre-defined output. The fact that my input is the activity taken from a CNN does not matter, it can be anything.\"\nA: I assume you are trying to put a PEM on top of the input data directly. Note that we mentioned the source of the position information is from zero-padding, you can have the position information if you put the \"source\" there.\n\nIn a way you can consider the backbone CNN(resnet or vgg) does not matter. On the contrary, however, we were trying to explore if a widely used backbone (resnet or vgg) contains this position information and where does it come from, we \"interpret\" our work as CNN understanding. Because this long-ignored problem is important in computer vision tasks, given that the absolute position can be converted to relative position in theory.\n\nThanks\nSen\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "Vt9BTvCtgv", "original": null, "number": 3, "cdate": 1582292349313, "ddate": null, "tcdate": 1582292349313, "tmdate": 1582292349313, "tddate": null, "forum": "rJeB36NKvB", "replyto": "2EjvkJCzrZ", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment", "content": {"title": "Answers to Paper781 Authors", "comment": "Thank you for your response, i understand that the PEM is unable to output the pre-defined pattern for w=0.\n\nI have another two questions, which I mentioned already in comments above.\n1) Given that the point of the paper is to analyze the information inside a CNN, do you agree that the choice of the architecture of the PEG is arbitrary? In other words, if I repeat your study, but I use a different architecture for the PEG, then my results are equally valid.\n2) If you agree with 1, then I choose a PEG that ignores the input, and always gives the pre-defined output. The fact that my input is the activity taken from a CNN does not matter, it can be anything. Then, according to your logic, I conclude that everything (every possible input) has information about the pre-defined output, which is obviously wrong. In fact, in that case the mutual information between the input and the output of the PEG is zero."}, "signatures": ["~Alberto_Bernacchia1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alberto_Bernacchia1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204261, "tmdate": 1576860562874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment"}}}, {"id": "2EjvkJCzrZ", "original": null, "number": 9, "cdate": 1580303840253, "ddate": null, "tcdate": 1580303840253, "tmdate": 1580303840253, "tddate": null, "forum": "rJeB36NKvB", "replyto": "FogcyJZsY", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Alberto Bernacchia", "comment": "In addition, the scores on the synthetic images, white, black and noise in Table 1 could be a proof to question the hypothesis \"w = z * m / (s^2 + m^2)\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "FogcyJZsY", "original": null, "number": 8, "cdate": 1580303512721, "ddate": null, "tcdate": 1580303512721, "tmdate": 1580303512721, "tddate": null, "forum": "rJeB36NKvB", "replyto": "FXNRLA98Z", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Alberto Bernacchia", "comment": "Thanks for your further question. We thought about when the model is input-agnostic, w=0. In that case, the model is not able output the pre-define pattern. We did not experience this problem of zero vector in our experiment as well. You might want to check the answer to Reviewer 1, in which we showed the effect of circular padding. If somehow the model was adjusted to \"z * m / (s^2 + m^2)\", the circular padding should improve the performance. Please correct me if I am wrong and help me recall other cases that the model can produce this output without knowing the position information.\n\nRegards\nSen"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "FXNRLA98Z", "original": null, "number": 2, "cdate": 1580299982993, "ddate": null, "tcdate": 1580299982993, "tmdate": 1580299982993, "tddate": null, "forum": "rJeB36NKvB", "replyto": "KFqvROX01_", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment", "content": {"title": "Answer to author 781", "comment": "Thank you for your answers.\nSorry I've just seen your response now! I thought I would get a notification by email but I didn't, so I thought you did not answer.\nConcerning your point 2:\n- I agree with you that a model with enough complexity can output whatever fixed image. However, it will not do so by memorizing the input, instead it will do so by ignoring the input. Therefore, it will work just fine on the test set.\n- I understand that, in your experiment, the performance of PEN depends on zero padding. However, my opinion is that this dependency is a consequence of the architecture of PEM, which is completely arbitrary. My prediction is that, if you change the architecture of PEM, then you will find that its performance will not depend on zero padding any more, instead it will depend on some other parameter of the CNN. Since the choice of PEM is arbitrary, the dependency of its accuracy on whatever parameter of the CNN is meaningless.\n- Let me give you a very simple example to prove my point. \nLet say you have a scalar linear model:\ny = w*x\nwhere x is the input, y is the output and w is the weight, each one is a scalar number.\nThe goal is to output always the same number, let say this number is equal to z, regardless of the input, and we use a square loss function L = (y - z)^2.\nThe solution to this problem is\nw = z * m / (s^2 + m^2)\nwhere m and s^2 are the sample mean and variance of the input x.\nThe loss at the optimum is\nL = z^2 * (1 - m^2 / (s^2 + m^2))\nSo the solution depends on the input (through its mean and variance), as it is the case in your experiments.\nHowever, I can change the model by constraining W in some arbitrary way. \nFor example, let say z>0 and m>0, and  I constrain the weight to be w<0.\nThen the solution is always w=0, and the loss is L = z^2, , regardless of the input.\nYou can explore other arbitrary ways of constraining w, and you will find different dependencies of the loss on the input. Since your choice of the constraints are arbitrary, so are those dependencies.\n\n\n\n"}, "signatures": ["~Alberto_Bernacchia1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alberto_Bernacchia1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204261, "tmdate": 1576860562874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment"}}}, {"id": "KFqvROX01_", "original": null, "number": 7, "cdate": 1578530552860, "ddate": null, "tcdate": 1578530552860, "tmdate": 1578530552860, "tddate": null, "forum": "rJeB36NKvB", "replyto": "TI-r_hwtYv", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Answers to Alberto Bernacchia", "comment": "Dear Alberto,\n\nThanks for this interesting question and we will further explain our work in this post.\n\n1. I think your understanding about the network architecture is correct, PEM is designed to output a fixed image(the gradient groundtruth) based on the output of a pre-trained model, e.g., VGG or ResNet.\n\n2. Can this model output a fixed image(say a dog) regardless the input? The short answer is it is possible to generate a fixed output, in this case, the learning process should be independent of position information(zero-padding). However, our experiment (Table 5) shows the output is indeed based on the position information delivered by zero-padding. An addition to your question, I personlly believe a model(given enough complexity) could blindly memorize each of the training image to output the desired prediction(say a dog), but I don't think this can give you the output you want on the test set.(https://arxiv.org/abs/1611.03530, ICLR2017).\n\nRegards\nAuthors of paper 781"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "TI-r_hwtYv", "original": null, "number": 1, "cdate": 1578476575178, "ddate": null, "tcdate": 1578476575178, "tmdate": 1578478764015, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment", "content": {"title": "Clarification", "comment": "Hello, this is my first comment on OpenReview so please let me know if this not the right place for asking clarifications about a manuscript. I read this paper and I feel there is something fundamental I do not understand about it.\n \nIf I understood correctly, the input to the Position Encoding Module (PEM) is the combined outputs of a few layers of a pre-trained CNN, and the target output of the PEM is a fixed image (the value of x coordinates, for example). The target output image is always the same, fixed, irrespective of what's the input image. Then, I do not understand how we can draw any conclusions from this model.\n\nThe mutual information between the input and the output is zero by construction, and I can always set up a toy model that gives me the target output, irrespective of the input. For example, let say that my target output is a picture of my dog, always the same picture, irrespective of the input. I set up a toy model that discards the input and gives me always that output picture of my dog. Can I conclude that the input, whatever that was, has information about my dog? "}, "signatures": ["~Alberto_Bernacchia1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alberto_Bernacchia1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204261, "tmdate": 1576860562874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Public_Comment"}}}, {"id": "GdrBW7YY1g", "original": null, "number": 1, "cdate": 1576798705882, "ddate": null, "tcdate": 1576798705882, "tmdate": 1576800930262, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (i.e. near the edges of the image).  A detailed discussion and analysis is performed, which shows where this positional information comes from.  \n\nThe reviewers were happy with your paper and found it to be quite interesting.  The reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in CNNs.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728524, "tmdate": 1576800280955, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper781/-/Decision"}}}, {"id": "SklQXVNkcB", "original": null, "number": 2, "cdate": 1571927067504, "ddate": null, "tcdate": 1571927067504, "tmdate": 1574676420395, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper studied the problem of the encoded position information in convolution neural networks. The hypothesis is that CNN can implicitly learn to encode the position information. The author tests the hypothesis with lots of experiments to show how and where the position information is encoded.\n\nClarity:\nThis paper is interesting for me. It tries to understand the encoded position information that is easily ignored by researchers. I like adequate experiments with learned position information and position illustrations.\n\nExperiments:\n1. The paper mainly discussed the zero-padding and found it is the source of position information. How about other padding modes like constant-padding, reflection-padding, and replication-padding?\n\n2. The partial convolution-based padding method [1] (padded regions are masked out) shows that its recognition accuracy is higher than the traditional zero-padding approach. Can you help investigate where the position information comes from for this case?\n\n[1] Partial Convolution based Padding, https://arxiv.org/pdf/1811.11718.pdf.\n\n\nSome of my concerns are well addressed by the author thus I upgrade my score.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575606653808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper781/Reviewers"], "noninvitees": [], "tcdate": 1570237747172, "tmdate": 1575606653820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Review"}}}, {"id": "ryxckH_joB", "original": null, "number": 3, "cdate": 1573778658228, "ddate": null, "tcdate": 1573778658228, "tmdate": 1573778658228, "tddate": null, "forum": "rJeB36NKvB", "replyto": "r1xDPX_yqH", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We thank Reviewer 3 for the detailed feedback and we will further explain the question raised in the comment. \n\nWe think the first question is about initialization, (cold or hot start). We also thought about a longer training procedure for the PosENet because it was trained from scratch. But we found that the training loss does not decrease after the first several iterations, the weight becomes saturated quickly. The training loss of the PosENet converges at 0.084 after the first epoch. Also, all the test losses (on natural images PASCAL-S or synthetic images BLACK, WHITE or NOISE) are the same as the training loss. This suggests that the prediction may be completely independent of the content of images.\n\nWe believe the reason behind this is that zero-padding delivers obvious boundary information, the transition between the zeros padded and the content. As discussed in the answer to Reviewer 1, we believe not all padding strategies can deliver this position information.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "r1gWFEOosS", "original": null, "number": 2, "cdate": 1573778552989, "ddate": null, "tcdate": 1573778552989, "tmdate": 1573778552989, "tddate": null, "forum": "rJeB36NKvB", "replyto": "SklQXVNkcB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "Many thanks for your review and we appreciate your insightful feedback.\n\nIn our paper we discussed the implicit effect of the widely used zero-padding mechanism in CNNs. We believe the strong position information is encoded by the value transition near the boundary, zero to non-zero values. Intuitively, we believe other padding strategies, e.g. reflection or replication padding, are not able to deliver this clear position information.\n\nWe compared the effect of Circular padding implemented in Pytorch with the commonly used zero-padding on the Horizontal (H) setting using VGG16, First row of Table 1 (VGG). The training loss of zero-padding starts from 0.045 and drops to 0.03 in the end. While the loss for circular-padding begins at 0.065 and ends at 0.056, much higher than zero-padding. The results of circular-padding on the PASCAL-S dataset are (SPC 0.381, MAE 0.224). Note that this result is similar to the setting of VGG w/o padding, Table 4 (VGG w/o padding on H). This further validates our hypothesis that the position information is delivered by the value transition of zero-padding.\n\nFor the conv-padding paper, according to Equations (4) and (5), their method essentially still applies zero-padding, which means the position information should be encoded. Their method is actually weighing the output of the convolution based on how many zeros are padded, r(i, j).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "rJlN7fOioB", "original": null, "number": 1, "cdate": 1573777947811, "ddate": null, "tcdate": 1573777947811, "tmdate": 1573777947811, "tddate": null, "forum": "rJeB36NKvB", "replyto": "BklLFSzRtB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "We really appreciate your review and  we\u2019re glad to hear you are pleased with the paper!\n\nPlease let us further clarify the implementation details. We did not remove any pooling layers except the last average pooling layer in the ResNet, which was designed to compress the output in order to feed to a Fully Connected (FC) layer. The pooling layers within each network (convolutional part, sometimes called backbone) have been retained because the weight was trained based on that structure design. It is commonplace to replace the FC layers with conv layers as in most dense labeling tasks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeB36NKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper781/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper781/Authors|ICLR.cc/2020/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166316, "tmdate": 1576860529108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper781/Authors", "ICLR.cc/2020/Conference/Paper781/Reviewers", "ICLR.cc/2020/Conference/Paper781/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Comment"}}}, {"id": "BklLFSzRtB", "original": null, "number": 1, "cdate": 1571853693894, "ddate": null, "tcdate": 1571853693894, "tmdate": 1572972553162, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper investigates to what degree Convolutional Neural Networks (CNNs) learn to encode positional information.\nRather interesting finding is the not only they do encode this information, but that it is to a large degree function of the padding commonly used in the CNN architectures.\n\nThe problem the paper is looking at is well motivated, the experiments are nicely designed and it includes comprehensive ablation study.\nPrevious and related work seems to be well referenced.\nThe main idea of introducing the PosENet to predict the gradient map is neat, and allows for interesting experiments (e.g. what layers most strongly encode the positional information).\n\nI really enjoyed the paper, the overall quality is high and does not seem to be rushed (no obvious typos or mistakes in the figures/tables).\nI believe this should be an accept.\n\nQ:\nI can understand why you removed the pooling layers, but did you try to run some of your experiments with these as well? How were the numbers effected?"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575606653808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper781/Reviewers"], "noninvitees": [], "tcdate": 1570237747172, "tmdate": 1575606653820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Review"}}}, {"id": "r1xDPX_yqH", "original": null, "number": 3, "cdate": 1571943263458, "ddate": null, "tcdate": 1571943263458, "tmdate": 1572972553075, "tddate": null, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "invitation": "ICLR.cc/2020/Conference/Paper781/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies whether and how position information is encoded in CNNs. On top of VGG and ResNet, it constructs an additional PosENet to recover position information. By analyzing how well PosENet recovers position information, this paper provides several interesting findings: CNNs indeeds encode position information and zero-padding is surprisingly important here.\n\n[Pros]\n\n1. I enjoy reading this paper: probing CNNs is not easy, but it designs experiments in an intuitive way and rigorously performs ablation studies and analysis.\n2. The observations and findings are interesting and helpful to the community.\n\n[Cons]\n\n1. A weakness of this paper is that it ignores the impact of training process while probing PosENet: In Table 1, VGG/ResNet perform much better than PosENet, but it could be because VGG/ResNet is easier to train (kind of fine-tuning PosENet only) than PosENet. Would be nice to show the training curve and train PosENet longer.\n2. Zero-padding seems to play a surprisingly important role in encoding position information (Table 5), but it is still unclear why it is so important and how it helps.\n\nOverall, I think this is a good paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper781/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "keywords": ["network understanding", "absolute position information"], "TL;DR": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "pdf": "/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf", "paperhash": "islam|how_much_position_information_do_convolutional_neural_networks_encode", "_bibtex": "@inproceedings{\nIslam*2020How,\ntitle={How much Position Information Do Convolutional Neural Networks Encode?},\nauthor={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeB36NKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2584bac4c5038bef361ca3b8a96152ac8e05c7db.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeB36NKvB", "replyto": "rJeB36NKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper781/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575606653808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper781/Reviewers"], "noninvitees": [], "tcdate": 1570237747172, "tmdate": 1575606653820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper781/-/Official_Review"}}}], "count": 18}