{"notes": [{"id": "PO9AR1F-sP", "original": null, "number": 4, "cdate": 1587116382306, "ddate": null, "tcdate": 1587116382306, "tmdate": 1587116382306, "tddate": null, "forum": "S1lvm305YQ", "replyto": "H1eiwWnxRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "content": {"comment": "> As for the repo, we are currently working on cleaning up the code and will release once we finish.\n\nThough it's been almost a year and a half since you wrote in the rebuttal, the source code has still not uploaded yet in the repository specified in the paper.\nConsidering that the content of the rebuttal can be a factor to be accepted, I think you must be responsible for what you wrote in your rebuttal as a member of the scientific community.", "title": "When will you release the code?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613005, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1lvm305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613005}}}, {"id": "S1lvm305YQ", "original": "S1x6TbRcFX", "number": 1370, "cdate": 1538087967507, "ddate": null, "tcdate": 1538087967507, "tmdate": 1550884212904, "tddate": null, "forum": "S1lvm305YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJlTzPwIZV", "original": null, "number": 17, "cdate": 1546184469438, "ddate": null, "tcdate": 1546184469438, "tmdate": 1546184469438, "tddate": null, "forum": "S1lvm305YQ", "replyto": "HJe2eAlSZN", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Thanks for your interest and the comment!", "comment": "Hi Rahul, \n\nThanks for your interest in our work and thanks for the comment! \n\nTheoretically there is no size constraint in the generator network as it\u2019s fully convolutional. However practically because the generator was initially written in TensorFlow, we initially specified the input placeholder with size [1, 257, 251] (Note that it\u2019s no longer 256 as in the image case) and that was the \u201csize constraint\u201d. We removed it (so that it can take in arbitrary size during test time) to address the volume jump issue. This will be further clarified in the appendix C.6 of the camera-ready version. \n\nAnd yes it was due to limited computation resources. We did not experiment with larger batch size. \n\nPlease let us know if you have any further question, thanks! \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "HJe2eAlSZN", "original": null, "number": 3, "cdate": 1546092020345, "ddate": null, "tcdate": 1546092020345, "tmdate": 1546092020345, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "content": {"comment": "Congratulations for good research work! But it misses some technical details.\n\nFirstly in appendix C.6, there is no information given about how the size constraint in generator network was removed for processing arbitrary length inputs. Preserving the musical length to at most 2 minutes (due to GPU memory constraint, as written in paper) will be a long 7680 x 256 image whereas the generator is designed to process 256 x 256 image inputs (in accordance to original CycleGAN paper details). The explicit details about how the 7680 x 256 image is fed to the generator in one-shot must be provided. \n\nSecondly there is no discussion given about setting up the batch size equal to 1 i.e.:\n1. Is it due to limited computation resources like CycleGAN research https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/198\n2. Does higher batches not result in better quality of timbre style transfer?\n3. Is it just a matter of choice?", "title": "Incomplete information about one-shot generation of long musical pieces"}, "signatures": ["~Rahul_Bhalley1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rahul_Bhalley1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613005, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1lvm305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613005}}}, {"id": "B1eVK21xxV", "original": null, "number": 1, "cdate": 1544711291878, "ddate": null, "tcdate": 1544711291878, "tmdate": 1545354507668, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Meta_Review", "content": {"metareview": "Strengths: This paper is \"thorough and well written\", exploring the timbre transfer problem in a novel way. There is a video accompanying the work and some reviewers assessed the quality of the results as being good relative to other approaches. Two of the reviewers were quite positive about the work.\n\nWeaknesses: Reviewer 2 (the lowest scoring reviewer) felt that the paper was a little too far from solving the problem to be of high significance and that there was:\n - too much focus on STFT vs. CQT\n - too little focus on getting WaveNet synthesis right\n - too limited experimental validation (too restricted choice of instruments)\n - poor resulting audio quality\n - feels too much of combining black boxes\n\nAMT listening tests were performed, but better baselines could have been used.\nThe author response addressed some of these points.\n\nContention: \nAn anonymous commenter noted that the revised manuscript added some names in the acknowledgements, thereby violating double blind review guidelines. However, the aggregated initial scores for this work were past the threshold for acceptance. Reviewer 2 was the most critical of the work but did not engage in dialog or comment on the author response. \n\nConsensus:\nThe two positive reviewers felt that this work is worth of presentation at ICLR. The AC recommends accept as poster unless the PC feel the issue of names in the Acknowledgements in an updated draft is too serious of an issue.\n ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting paper, has some issues, but would be of interest to the community"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1370/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352863780, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352863780}}}, {"id": "HJg6f75xe4", "original": null, "number": 16, "cdate": 1544753940655, "ddate": null, "tcdate": 1544753940655, "tmdate": 1544754304299, "tddate": null, "forum": "S1lvm305YQ", "replyto": "HyxuHufle4", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Response to R2. Thanks for the comments! ", "comment": "Thanks for your comments! \n\n- Sorry about the broken YouTube link, that link stopped working and please use this link instead: https://youtu.be/2ypcAZRYZJg  We checked that this one is working. \n\n- We agree with your point that, at least in certain contexts, a research system should be better than commercially available systems, or at least be on par. However our work is about Timbre transfer, which doesn\u2019t have an existing commercially available system yet. The pitch shift and time stretch experiment were just included to demonstrate that we can get those side benefits for free by training TimbreTron on CQT representation; it was not our primary goal, and was not allocated the majority of space in the paper as well. We agree that if the sole purpose of our system had been to do pitch shifting or time stretching for music, then existing commercial tools would have been an important baseline to consider.\n\n- As for the audio quality, one way we chose to address the subjective nature of the question was by conducting AMT human study. Based on our human study results there was strong evidence that TimbreTron is indeed able to transfer Timbre recognizably while preserving other musical content. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "HyxuHufle4", "original": null, "number": 14, "cdate": 1544722495746, "ddate": null, "tcdate": 1544722495746, "tmdate": 1544722495746, "tddate": null, "forum": "S1lvm305YQ", "replyto": "HylL4LaKCX", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "the new examples confirm to me that the approach is not ready", "comment": "Thanks for the detailed response and extra effort to address some of my points. However, listening to the new examples, I have come to an even stronger opinion that the proposed method is not ready. From the samples, it is clear that WaveNet and the mapping method both introduce artefacts that cause unacceptable loss of quality. Yes, when mapping flute to piano it definitely does have characteristics of piano, but at the cost of extreme distortions that are far away from being useful for any purpose at this point in time.\n\nI do believe that this is an interesting idea and an interesting line of research, but I also continue to believe that the results are not good enough at this point in time.\n\nSome details:\n\n> \"We demonstrate that the aforementioned translation does indeed result in a perceivable pitch shift when fed to our conditional WaveNet.\" I believe my original comment is still correct, \"The real question here is whether the reconstructed signal is of the same quality as, for example, a simple PSOLA-based pitch change would be.\"\n\nI believe that a research system should be better than commercially available systems, or at least be on par. Please check out this Youtube video that uses Audacity to time-stretch an audio clip: https://www.youtube.com/watch?v=SjVY2Fs8-24. Although this is time-stretching and not pitch-changing, obviously pitch-shifting can be implemented by time-stretching followed by playing back at a different sampling rate. Their example time-stretches a pop song by 30%. Please play back from 3:50. With concentration, I can hear some minor artefacts, but by and large, it sounds good and not unpleasant at all.\n\nCompare this to the pitch change examples \"Mozart up/down3.wav\", which has strong distortions which are very unpleasant. Using WaveNet to reconstruct *originals* seems to work, by and large, except for quite some additional noise, but even the basic task of pitch-shifting already shows strong artefacts.\n\nThis shows me that WaveNet synthesis itself is inadequate in its current form.\n\n> \"We did some PSOLA experiments for baseline to address your concern\"\n\nThe shifted_bumble.wav example in https://onedrive.live.com/?authkey=%21ACqUS3QXHSdcSW8&id=9AD8937254DEBD90%2120331&cid=9AD8937254DEBD90\n\nis full of crackling artefacts. I have never heard such artefacts for any re-pitching algorithm. I think you did not do the windowing right.\n\n> \"as well as our final demo video(https://www.youtube.com/watch?v=aT4D4mTITko)\"\n\nYoutube says \"This video is unavailable.\" Did Youtube kill it due to copyright claims? Then please post the video on the OneDrive as well.\n\n> \"we also included some samples of Violin -> Piano(Sample 19) and Harpsichord -> Piano(Samples 16 and 18) and Piano -> Flute(sample 17)\"\n\nThis example shows strong artefacts in that it synthesizers frequencies below the original pitch. It sounds like a base guitar playing unisono with the main melody (and sometimes a different note). Does this come from WaveNet or the mapping method? Either way, my takeaway is, again, that the method is not ready.\n\n> \"For the poor audio quality in the source material, could you point us to some specific samples?\"\n\nThis is an example which is very noisy, has some strange volume dip at the very beginning that is impossible to produce with a piano, and a drop-out at 3.5 seconds: https://onedrive.live.com/?authkey=%21ACqUS3QXHSdcSW8&id=9AD8937254DEBD90%2120329&cid=9AD8937254DEBD90\n\nIt is, however, possible that the poor quality I perceived is an byproduct of OneDrive's audio-playback interface, which may send compressed audio when playing uncompressed WAV files. So I concede this point."}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "S1gy7d6pA7", "original": null, "number": 12, "cdate": 1543522326646, "ddate": null, "tcdate": 1543522326646, "tmdate": 1543522326646, "tddate": null, "forum": "S1lvm305YQ", "replyto": "BJlCISfsAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "I agree with R3", "comment": "I'm standing by my initial assessment.  This paper is thorough and well written, and as R3 says, it makes significant progress on this problem even if it's not \"solved\"."}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "HylL4LaKCX", "original": null, "number": 8, "cdate": 1543259694369, "ddate": null, "tcdate": 1543259694369, "tmdate": 1543259694369, "tddate": null, "forum": "S1lvm305YQ", "replyto": "H1x8JZ2lA7", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "WaveNet ablation study added", "comment": "We added in Appendix E the ablation study focusing on the WaveNet component of TimbreTron. It showed the impact of each of our modification (reverse generation, data augmentation and beam search) on the output quality of WaveNet. The corresponding audio samples can be found here: https://1drv.ms/f/s!ApC93lRyk9iagZ8qP0IlxLXZkbO-iA"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "Bkx0mGhxRX", "original": null, "number": 7, "cdate": 1542664742513, "ddate": null, "tcdate": 1542664742513, "tmdate": 1543259649120, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "General updates to reviewers", "comment": "\n1.\tWe included more samples that were generated by TimbreTron but without any beam search here: https://1drv.ms/f/s!ApC93lRyk9iagZ5M91jko9nSAiYMiA\n\n2.\tWe included samples reconstructed by WaveNet that were not transferred by CycleGAN in order to show that without phase information, our WaveNet can reconstruct waveform from CQT pretty well (i.e, WaveNet(CQT(source audio))). The samples can be found here:https://1drv.ms/f/s!ApC93lRyk9iagZ5NBNV_ERqOJFK_3w\n\n3.\tWe are in progress of making a project page so that all audio samples will be more organized in the camera-ready version.\n\n4.\tWe added the original user interface of our AMT experiments and they can be found here:https://1drv.ms/f/s!ApC93lRyk9iagZ0ndQIYdJBAqYDlDA\n\n5.     We added in Appendix E the ablation study focusing on the WaveNet component of TimbreTron. It showed the impact of each of our modification (reverse generation, data augmentation and beam search) on the output quality of WaveNet. The corresponding audio samples can be found here: https://1drv.ms/f/s!ApC93lRyk9iagZ8qP0IlxLXZkbO-iA\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "HyeAAbiq57", "original": null, "number": 1, "cdate": 1539121621564, "ddate": null, "tcdate": 1539121621564, "tmdate": 1542770950923, "tddate": null, "forum": "S1lvm305YQ", "replyto": "HkgRLhKU97", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Thanks for the comment!", "comment": "Hi Keunwoo, thanks for the comment! \n\nFor WaveNet, we used kernel size of 3 for all the dilated convolution layers and the initial causal convolution. The residual connections and the skip connections all have width of 256 for all the residual blocks. The initial causal convolution maps from a channel size of 1 to 256. The dilated convolutions map from a channel size of 256 to 512 before going through the gated activation unit. In addition, we also added a constant shift to the spectrogram before feeding it into the WaveNet as the local conditioning signal; this shift of +2 was chosen to achieve a mean of approximately zero. These details will be updated in the paper.\n\nIn terms of the discriminator, we adopted the original discriminator architecture from the original CycleGAN paper except that we ran it on the full signal rather than random patches, as was discussed in section 4. \n\nThe dataset we used for training both the CycleGAN and the wavenet consists of real world recordings that contain only a single timbre, collected from YouTube. We have not filtered the samples based on whether they are polyphonic or not - for instruments that support polyphony (such as piano and harpsichord), the majority of the recordings are polyphonic. You can see the full list of the recordings we used in our experiments in Appendix C.1 in the updated paper.\nAs for the OneDrive link, there seems to be an issue with the OpenReview redirection. However if you copy the link and paste it in your browser then it should work. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "H1x8JZ2lA7", "original": null, "number": 4, "cdate": 1542664413987, "ddate": null, "tcdate": 1542664413987, "tmdate": 1542666977460, "tddate": null, "forum": "S1lvm305YQ", "replyto": "Skx38gheCm", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Continued Response to R2", "comment": "\n...\n\n6.\tFor the poor audio quality in the source material, could you point us to some specific samples? We used 16kHz as our raw source audio and we chose that because even in 2018, the significant majority of audio-related ML research is done with 16 kHz audio, (For example, in recent research on audio superresolution, https://kuleshov.github.io/audio-super-res/ , the authors describe their 16kHz samples as their high-quality dataset) and perceptually it was clear enough to tell details of timbre. Furthermore, the focus of this research is to get timbre transferred correctly while preserving other musical content. Audio quality is part of musical content and is not within the problem we\u2019re trying to solve. For example, we are not trying to do super resolution on audio, and I think it\u2019s fair to say 16 kHz is the standard resolution, corresponding to the 256x256 resolution used in preprocessed ImageNet in CV research. So far we haven\u2019t found any sample that\u2019s 11kHz as you suggested, but we\u2019ll make sure all source audio in the updated paper will be the standard 16kHz resolution. \n\n7.\tDue to potential copyright issue, we didn\u2019t find enough samples in other instrument online that we are absolutely certain won\u2019t have any copyright issue, so we recorded our own samples, thus didn\u2019t have a lot of them. \n\n8.\tAs for the beam search, we are aware that beam search should not be a key component on which the entire model relies in order to work (and if that were the case, then we agree that that would be problematic). However, the system does not rely on it: the improvement is only subtle and our results are still acceptable without it. As shown in the original paper, the AMT results we\u2019ve shown are actually done without beam search. We only included it in the paper because it can still marginally improve the generated audio quality. We added some samples generated by TimbreTron without any beam search and they can be found here: https://1drv.ms/f/s!ApC93lRyk9iagZ5M91jko9nSAiYMiA\n\n9.\tDue to the commercial success of WaveNet, well-resourced company labs are already working hard on improving it, and we look forward to being able to integrate such improvements into our pipeline. Hence, improving WaveNet itself wasn\u2019t a major focus of our work. Instead, we gave two simple methods which adapted WaveNet to our task and pipeline -- beam search and reverse generation -- and we expect these tricks would apply equally well to future improved WaveNet-like models. We added some samples that were reconstructed by WaveNet from source audio with and without beam search and they can be found here: https://1drv.ms/f/s!ApC93lRyk9iagZ5NBNV_ERqOJFK_3w.  We are in progress of adding more detailed ablation study on the wavenet component that, hopefully, will show the impact of reverse generation, data augmentation and beam search. \n\n10.\tWe are in progress of making a project page so that all audio samples can be more organized in the camera ready version.\n\nAgain, we sincerely appreciate such a detailed and insightful critique of our work. Please let us know of any other changes that would improve our work. Thanks!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "Skx38gheCm", "original": null, "number": 3, "cdate": 1542664275816, "ddate": null, "tcdate": 1542664275816, "tmdate": 1542665075325, "tddate": null, "forum": "S1lvm305YQ", "replyto": "H1g0p_-ChX", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Response to R2. Thank you for your review, which allowed us to improve our work", "comment": "We would like to thank Reviewer2 for the insightful, high quality and clear review! It has allowed us to greatly improve our work and our paper. We hope the revised draft of the paper with the clarifications and improvements made below serve to increase your rating of our work.\n \n1.\tIn choosing a CQT rather than STFT representation, we agree that, devoid of any other context, such a choice might indeed not be contentious, especially from a signal processing perspective. But the context here is that we are not just using this for discrimination or analysis purposes, but optimizing reconstruction is also essential: since Griffin-Lim is not possible for CQT, then in this case, switching from STFT to CQT means that we need to fundamentally change the pipeline. Furthermore, STFT and Grifflin-Lim is still widely used in state of the art speech and music research, for example in Tacotron 2 by Shen et al.(2018) and Deep Voice 3 by Ping et al.(2018).  Hence, we needed to demonstrate that the improvement from CQT in this context is significant enough to justify the added complexity of the WaveNet synthesizer.\n\n2.\tIn advance of our experiments, it would certainly be a reasonable conjecture that the spectrogram-shifting trick would work for pitch shifting. But we don\u2019t think this was so \u201cobvious\u201d as to not require experimental confirmation.\n\n3.\tThank you for suggesting the comparison of our pitch shifted samples with PSOLA-based pitch change. We did some PSOLA experiments for baseline to address your concern. The results can be found here: https://1drv.ms/f/s!ApC93lRyk9iagZ5pKpRLdBcdJ1xJbw As you can see, PSOLA performs well on simple samples, like notes_shifted.wav However, it completely fails on more complicated monophonic samples. We think that\u2019s due to the fact that for PSOLA to work well, fundamental frequency needs to be precisely detected, which itself is not a trivial task for a complex monophonic musical piece. We haven\u2019t put this in the paper yet because we are not sure this is a proper baseline and we are still working on better baseline comparison, and will add the comparison on to paper later.  \n\n4.\tWe did in fact run more than two instrument pairs, and we encourage you to take a look at the example outputs we provided in the OneDrive folder, as well as our final demo video(https://www.youtube.com/watch?v=aT4D4mTITko). In the OneDrive folder Section 7 (which corresponds to the Section 7. Conclusion of the paper), we also included Violin to Flute and Piano to Violin in our original submission. (The main obstacle to reporting more instrument pairs was simply the computational cost of training additional CycleGANs; our computational resources were much more limited than some prominent groups publishing on deep learning for music.) The idea of this work is not to develop a commercial level product for artists, but rather we are only aiming at empirically verifying that our methodology is a practical approach to the problem of Timbre Transfer, a proof of concept. But we agree that the more instrument pairs the better, thus in this folder: https://1drv.ms/f/s!ApC93lRyk9iagZ5M91jko9nSAiYMiA, we also included some samples of Violin -> Piano(Sample 19) and Harpsichord -> Piano(Samples 16 and 18) and Piano -> Flute(sample 17). The latter is a particularly interesting failure case where TimbreTron dreamed up all the \u201cpiano vibrato\u201d from the long notes in the source flute sample.  We will gradually add more instruments and translation directions, including both successes and other failure cases as well.\n\n5.\tThank you for bringing up ADSR and vibrato. These are indeed interesting cases where it\u2019s ambiguous what the \u201ccorrect\u201d mapping is. Because TimbreTron does not simulate the physics of the instruments, it sometimes transfers effects that would be impossible or unusual for the target instrument. We highlighted two such examples in our demo video: a crescendo on a sustained piano note, and a string ensemble pausing to breathe. In the context of TimbreTron, we consider these to be interesting artifacts. If one wants to build a commercial system to produce convincing instrument samples, one would want to somehow remove these artifacts. (On the other hand, they might be beneficial, insofar as they enable a broader range of expression in the target instrument, as was the case for our generated harpsichord for the Moonlight Sonata.) \n\n(due to characters limit, more response will follow in the next post)\n..."}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "H1eiwWnxRm", "original": null, "number": 5, "cdate": 1542664546861, "ddate": null, "tcdate": 1542664546861, "tmdate": 1542664668834, "tddate": null, "forum": "S1lvm305YQ", "replyto": "Skx4CYgj3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Response to R3. Thank you for your review, which allowed us to improve our work", "comment": "We would like to thank Reviewer3 for the helpful and high quality review! It has enabled us to add some missing information to our paper and improve the writing quality. \n\nAs for the repo, we are currently working on cleaning up the code and will release once we finish. As for the baseline comparison of our work, we did include a detailed ablation study comparing our final model with various baseline and \u201cpartial\u201d models(subtracting a modification each time)\n\nThanks for bringing up three papers that are related to ours. We indeed cited work by Verma. et al in our original paper. We now cite work done by Dai. et al and Bitton. et al in the updated paper. Thanks!  \n\nAs for the detailed information on the number of AMT workers (30 workers per questionnaire, but number of questions per questionnaire varies, for example, we asked 8 question per worker for comparing STFT and CQT experiment, thus in total we have 240 data points. See more details in the paper), the size of the CQT frame/hop over which they are summarized(16ms frame hop (256 time steps under 16kHz)), and the set of instruments(Piano, Violin, Flute, Harpsichord) that are being used in the experiments, they are updated in the main body of the paper.\n\nSection 6.3, sentence 2 was reworked.\n\nAttack is now explained in section 3.2 as the onset characteristics of an instrument in which it reaches a large amplitude quickly.  \n\nThanks for catching the \u201cthanks\u201d typo. It\u2019s now fixed.\n\nThanks for bringing up the percentage being meaningless, it\u2019s now removed.  \n\nAgain, thanks for your very constructive comments which allowed us to improve our paper! \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "rJeIRWhgAX", "original": null, "number": 6, "cdate": 1542664654264, "ddate": null, "tcdate": 1542664654264, "tmdate": 1542664654264, "tddate": null, "forum": "S1lvm305YQ", "replyto": "r1gf_Og53X", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Response to R1. Thank you for your review, which allowed us to improve our work", "comment": "Thanks for catching the typos! They are now fixed. GP means Gradient Penalty; sorry for not writing down the abbreviation in the first place, it is now fixed. \n\nAs for quantitative measurement of phase retrieval of the decoder component, we will include qualitative comparison of rainbowgram(which encodes phase information with color) of the source audio and the wavenet reconstruction (wavenet(CQT(source audio)) and you can find the samples here:https://1drv.ms/f/s!ApC93lRyk9iagZ5NBNV_ERqOJFK_3w. The reason why we don\u2019t think in our particular case that quantitative measurement can provide much insight in this regard is because phase retrieval is neither the focus nor a sub-goal of our work: so if, for example, our system produces an output that's shifted by some number of time steps, it would be a perfectly good output, even though the phases are all completely wrong.  \n\nWe apologize for the complexity of listening to samples via OneDrive\u2026 In testing it anonymously in advance, we did not have any of these errors, so we did not anticipate such a problem. We are glad you were able to access the youtube link, and we are in the progress of making a project page for our camera ready version, and by then it should be easier to go through all the samples.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "H1g0p_-ChX", "original": null, "number": 3, "cdate": 1541441734073, "ddate": null, "tcdate": 1541441734073, "tmdate": 1541533190260, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "content": {"title": "interesting idea, but weak experimental validation, and too much of combining black-boxes", "review": "The paper proposes a method for converting recordings of a specific musical instrument to another. The proposed approach is apply CycleGAN, which was developed for image style transfer, to transfer spectrograms. The synthesis is done using WaveNet.\n\nThe paper is interesting in the core idea. It demonstrates that this combination of building blocks can indeed map recordings while achieving certain characteristics of the target instrument.\n\nThe paper correctly describes \"timbre\" as a catch-all term for characterizing instruments besides pitch and volume. The success of the method should be judged along two dimensions, which are both very subjective:\n - Does the method transfer \"enough\" of the target instrument's characteristics?\n - Is the resulting audio quality sufficient?\n\nThe paper is easy to follow for someone with background in signal processing. I believe it is sufficiently easy to follow for readers with general computer-science and machine-learning background.\n\nThe paper focusses a lot on the choice of spectral representation. It compares short-term Fourier transform (STFT), which is the generic standard, and Constant-Q Transforms (CQT), a variant of STFT that uses a logarithmic frequency axis. To someone with signal-processing background, the choice of CQT seems logical and not something that would be challenged strongly as long as a simple comparison to STFT confirms that it works a bit better. I find the comparison between the two too dominant in the paper, and distracting from the other issues that I feel are more important (see below). For example, Section 6.2 states \"We demonstrate that the aforementioned translation does indeed result in a perceivable pitch shift when fed to our conditional WaveNet.\" But that is trivial: Since changing the playback sample rate by a few half steps does not fundamentally alter the perceived timbre of an instrument, and such a change will, by construction of CQT, shift the CQT representation, and since the WaveNet has seen examples of the source instrument for all notes, shifting the CQT respresentation must necessarily result in a perceived pitch change in the re-synthesized wveform that does not fundamentally change its timbre. The real question here is whether the reconstructed signal is of the same quality as, for example, a simple PSOLA-based pitch change would be.\n\nA larger problem of the paper is that the result section seems to only test two instrument mappings, violin to flute, and piano to harpsichord. One notes that these instrument pairs mostly differ in spectral envelope, while they are rather similar in longer-term temporal variations, such as what is sometimes characterized as the ADSR curve (attack-decay-sustain-release) and vibrato. These, in my view, are very important aspects of a musical instrument's characteristics, which are not addressed by the paper. (Whether they are considered part of \"timbre\" is not clear, but without mapping these, one cannot meaningfully speak of mapping instruments, which is the end goal of this paper.)\n\nAnother big problem in my view is that the audio quality is just not good. I hear a lot of musical-noise artifacts and local timbre modulations. Also it is not clear why the source material is of poor quality (sounds quite noisy, most likely in part due to mu-law 8-bit encoding, and they sound like 11 kHz recordings), for which there is no justification in 2018.\n\nLastly, I am not happy with the \"beam-search\" approach. That approach is used to post-correct imperfections in the WaveNet synthesis. It samples multiple generation hypotheses, and re-weights hypotheses by how well they match the original CQT when converted back. The need for this indicates a fundamental flaw in the WaveNet synthesizer. The authors explicitly say they did not want to fix the WaveNet algorithm itself. In my view, this is what should have been done.\n\nThe authors should focus much more on how to achieve sufficient WaveNet synthesis quality. This should be the main bulk of the paper, and would be a requirement for me to accept the paper.\n\nSo overall, the paper feels a little too much of combining black boxes.\n\nIn terms of significance, I would not think that this paper is getting near solving this problem, hence I rate it of less significance in the current state of results.\n\nPros:\n - interesting idea\n - reasonable approach by combining existing building blocks\n\nCons:\n - too much focus on STFT vs. CQT\n - too little focus on getting WaveNet synthesis right\n - too limited experimental validation (too restricted choice of instruments)\n - poor resulting audio quality\n - feels too much of combining black boxes\n\nAs a result I rate the paper \"not good enough\" in its current form.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "cdate": 1542234244338, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932246, "tmdate": 1552335932246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Skx4CYgj3X", "original": null, "number": 2, "cdate": 1541241292503, "ddate": null, "tcdate": 1541241292503, "tmdate": 1541533190005, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "content": {"title": "Timbre can be tranferred pretty well using a constant-Q transform for features, followed by a CycleGAN to do the transfer, followed by a Wavenet to resynthesize it to audio.", "review": "Main Idea: The authors use multiple techniques/tools to enable neural timbre transfer (converting music from one instrument to another, ex: violin to flute) without paired training examples. The authors are inspired by the success of CycleGANs for image style transfer, and by the success of Wavenet for generating realistic audio waveforms. Even without the CycleGAN, the use of CQT->WaveNet for time stretching and pitch shifting of a single piece is an interesting and valuable contribution.\n\nMethodology: Figure 1 captures the overall timbre-conversion methodology concisely. In general the details of the methodology look sound. The lengthy appendices offer additional implementation details, but without access to a source code repository, it is hard to say if the results are perfectly reproducible.\n\nExperiment and Results: Measuring the quality of generated audio is challenging.  To do so, subjective listening tests are conducted on Amazon mechanical turk, but without a comparison to a baseline system except for another performance of the target piece. Note that there are few published timbre-transfer methods (see Similar Work).\n\nOne issue with the AMT survey is that the total number of workers is not reported, and as such the significance of the results can be questioned.\n\nSignificance: In my mind, the paper offers validation of the three techniques used. CycleGANs, originally designed for images,  are shown to work for style transfers on audio spectrograms. Wavenet's claim to be a generic technique for audio generation is tested and validated for this domain (CQT spectrogram to audio). That CQT outperforms STFT on musical data seems to be a well established result already, but this offers further validation.\n\nThis paper also offers practical advice for adapting the techniques/tools (Wavenet, CycleGAN, CQT) to the timbre-transfer task.\n\n\nSimilar Work:\n\nI have only found 2 papers dedicated to timbre transfer in the field of Learning Representations.\n\nBitton, Adrien, Philippe Esling, and Axel Chemla-Romeu-Santos. \"Modulated Variational auto-Encoders for many-to-many musical timbre transfer.\" arXiv preprint arXiv:1810.00222 (2018).\n\nwhich was published on sept 29th 2018, so less than 30 days ago, which is fine according to the reviewer guidelines.\n\n\nVerma, Prateek, and Julius O. Smith. \"Neural style transfer for audio spectograms.\" arXiv preprint arXiv:1801.01589 (2018).\n\nwhich is a short 2 page exploratory paper.\n\n \nIt could be useful to cite:\n\nShuqi Dai, Zheng Zhang, Gus G. Xia.  \"Music Style Transfer: A Position Paper.\" arXiv preprint arXiv:1803.06841 (2018)\n\n\nWriting Quality\n\nOverall the paper is written well with clear sentences.\n\nCertain key information would be useful to move from the appendices to the main body of the paper.  This includes the number of AMT workers, the size of the CQT frame/hop over which they are summarized, and the set of instruments that are being used in the experiments.\n\n\nSome minor nitpicks: \n\nsection 6.3, sentence 2 needs to be reworked. ('After moving on to real world data, we noticed that real world data is harder to learn because compared to MIDI data it\u2019s more irregular and more noisy, thus makes it a more challenging task.') \n\nsection 3.2 sub-section 'Reverse Generation', sentence 1 uses the word 'attacks' for the first time. Please explain this for those not familiar.\n\nsection 3.1, sentence 3 has a typo, 'Thanks' is wrongly capitalized.\n\ntable 1 (and other tables in appendix), 'Percentage' (top left) does not add anything to the table.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "cdate": 1542234244338, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932246, "tmdate": 1552335932246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gf_Og53X", "original": null, "number": 1, "cdate": 1541175402362, "ddate": null, "tcdate": 1541175402362, "tmdate": 1541533189734, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "content": {"title": "Compelling results on timbre transfer, backed up by human evaluation", "review": "Summary\n-------\nThis paper describes a model for musical timbre transfer.\nThe proposed method uses constant-Q transform magnitudes as the input representation, transfers between domains (timbres) by a CycleGAN-like architecture, and resynthesizes the generated CQT representation by a modified WaveNET-like decoder. The system is evaluated by human (mechanical turk) listening studies, and the results indicate that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.\n\n\nHigh-level comments\n-------------------\n\nThis paper is extremely well written, and the authors clearly have a great attention to detail in both the audio processing and machine learning domains.  Each of the modifications to prior work was well motivated, and the ablation study at the end, while briefly presented, provides a good sense of the contributions of each piece.\n\nI was unable to listen to the examples provided by the link in section 6, which requires a Microsoft OneDrive login to access.  However, the youtube link provided in the ICLR comments gave a reasonable sample of the results of the system.  Overall, the outputs sound compelling, and match my expectations given the reported results of the listening studies.\n\nOn the quantitative side, it would have been nice to see a measurement of phase retrieval of the decoder component, which could be done in isolation from the transfer components by feeding in original CQT magnitudes.  This might help give a sense of how well the model can be expected to perform, particular as it breaks down along target timbres.  I would expect some timbres to be easier to model than others, and having a quantitative handle on that could help put the listener study in a bit more perspective.\n\nDetailed comments\n-----------------\n\nThe paper contains numerous typos and grammatical quirks, e.g.:\n    - page 5: \"GP can stable GAN training\"\n    - page 7: \"CQT is equivalent to pitch\"\n\n\nThe reverse-generation trick in section 3.2 was clever!\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Review", "cdate": 1542234244338, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932246, "tmdate": 1552335932246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgsgqXK2X", "original": null, "number": 2, "cdate": 1541122546586, "ddate": null, "tcdate": 1541122546586, "tmdate": 1541122546586, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "content": {"title": "Demo Video", "comment": "We made a demo video for this work, and we strongly encourage you to watch it as it will give you a general and intuitive idea about this work. You can find it here:  https://youtu.be/2ypcAZRYZJg"}, "signatures": ["ICLR.cc/2019/Conference/Paper1370/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609715, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lvm305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1370/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1370/Authors|ICLR.cc/2019/Conference/Paper1370/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609715}}}, {"id": "HkgRLhKU97", "original": null, "number": 1, "cdate": 1538853974014, "ddate": null, "tcdate": 1538853974014, "tmdate": 1538853974014, "tddate": null, "forum": "S1lvm305YQ", "replyto": "S1lvm305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "content": {"comment": "Thanks for the work. I'd expect more details of the system. For example, what's the kernel sizes and number of channels of the WaveNet? With only the number of layers and their dilation rates it's not possible to understand/reproduce the proposed system. Details of the discriminator should be provided as well.\n\nI'd also appreciate more details of the datasets, e.g. who are the composers, what kind of music it is (more than simply 'classical music'), etc. One important information would be if they are polyphonic. \n\nFinally, the OneDrive link does not work at the moment. ", "title": "Some details of the system are missing"}, "signatures": ["~Keunwoo_Choi1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1370/Reviewers/Unsubmitted"], "writers": ["~Keunwoo_Choi1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \u201cimage\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper.", "keywords": ["Generative models", "Timbre Transfer", "Wavenet", "CycleGAN"], "authorids": ["huang@cs.toronto.edu", "colinli@cs.toronto.edu", "anilcem@cs.toronto.edu", "jennybao@cs.toronto.edu", "sageev@dal.ca", "rgrosse@cs.toronto.edu"], "authors": ["Sicong Huang", "Qiyang Li", "Cem Anil", "Xuchan Bao", "Sageev Oore", "Roger B. Grosse"], "TL;DR": "We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.", "pdf": "/pdf/d0e4711ace87b36b9beece56f56322e083d1463c.pdf", "paperhash": "huang|timbretron_a_wavenetcyclegancqtaudio_pipeline_for_musical_timbre_transfer", "_bibtex": "@inproceedings{\nhuang2018timbretron,\ntitle={TimbreTron: A WaveNet(Cycle{GAN}({CQT}(Audio))) Pipeline for Musical Timbre Transfer},\nauthor={Sicong Huang and Qiyang Li and Cem Anil and Xuchan Bao and Sageev Oore and Roger B. Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lvm305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1370/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311613005, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1lvm305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1370/Authors", "ICLR.cc/2019/Conference/Paper1370/Reviewers", "ICLR.cc/2019/Conference/Paper1370/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311613005}}}], "count": 20}