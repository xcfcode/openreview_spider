{"notes": [{"id": "EQtwFlmq7mx", "original": "mH1mujXmpJ0", "number": 3407, "cdate": 1601308378240, "ddate": null, "tcdate": 1601308378240, "tmdate": 1614985648776, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LSLYD2hYRJI", "original": null, "number": 1, "cdate": 1610040513914, "ddate": null, "tcdate": 1610040513914, "tmdate": 1610474121990, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "invitation": "ICLR.cc/2021/Conference/Paper3407/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers recommend rejection: concerns were raised in terms of technical correctness, quality of presentation and the quality of experiments. There was no rebuttal. The AC agrees with the reviewers and recommends rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040513901, "tmdate": 1610474121975, "id": "ICLR.cc/2021/Conference/Paper3407/-/Decision"}}}, {"id": "xYX4ikYSEuM", "original": null, "number": 1, "cdate": 1603519046063, "ddate": null, "tcdate": 1603519046063, "tmdate": 1605024006019, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "invitation": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review", "content": {"title": "Analysis seems not rigorous", "review": "In this paper the authors study stochastic proximal point algorithm for nonconvex optimization, where the model is iteratively updated by solving a proximal optimization problem based on a randomly selected loss function. The authors develop efficient implementation for solving the proximal optimization problem: first for nonlinear least squares and then for general losses. Then the authors study the convergence rates for the developed algorithm. Upper bounds on the expected average squared gradients are developed for both constant step sizes and diminishing step sizes. Experimental results are also reported to support the algorithm in practical implementations.\n\nComments.\n\n1. The authors show that the proximal optimization problem can be efficiently solved. This is nice. However, the idea in the development of the algorithm seems standard. It seems a bit surprising that this algorithm has not been developed before.\n\n2. The convergence rates are a bit surprising. For example if we set $\\lambda=0$ in Thm 2, then eq (9) shows that the averaged gradient converges to zero, which should not happen since in this case the algorithm makes no progress.\n\n3. In Appendix A, the authors make two assumptions in (13) and (14). However, it remains unclear whether these two assumptions can be satisfied simultaneously. In particular, does the stationary point in (14) satisfies the sufficient decrease in (13)?\n\n4. I think eq (20) is not correct. The term $\\sqrt{\\lambda_t}c$ should be $c/\\lambda_t$. As the theoretical results depend on this inequality, the results are not correct.\n\n5. In Assumption 3, the authors assume the updates lie in a compact set. This can be only guaranteed if you impose a constraint on the space. However, the constraint would make the stationarity in (14) no longer hold. \n\n6. In Theorem 1, the equation is not complete.\n\n7. In eq (10), $\\alpha$ should be $\\alpha_t$", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3407/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3407/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3407/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076364, "tmdate": 1606915804026, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3407/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review"}}}, {"id": "buj5oICT9QC", "original": null, "number": 2, "cdate": 1603890930842, "ddate": null, "tcdate": 1603890930842, "tmdate": 1605024005953, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "invitation": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review", "content": {"title": "A badly-written paper", "review": "This paper considers the stochastic proximal point algorithm for solving nonconvex nonlinear least squares optimization problems. A linearization strategy is used to accelerate the procedure and in each iteration the algorithm works by solving a linear system. Some convergence analysis for the proposed is present. Some experiments have been conducted.\n\nI have the following comments.\n1. In the proposed algorithm, the authors only take one example instead of a batch of training examples to construct the gradient. This strategy often results in much large variance and slow convergence in practice.\n\n2. The results in Proposition 1 does not imply the convergence of the algorithm. The theoretical analysis is incremental. \n\n3. The numerical comparisons are not sufficient.  The authors should include the comparisons with state-of-the-art second-order optimization solver such as K-FAC.\n ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3407/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3407/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3407/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076364, "tmdate": 1606915804026, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3407/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review"}}}, {"id": "ygqk6REafp", "original": null, "number": 3, "cdate": 1603894100264, "ddate": null, "tcdate": 1603894100264, "tmdate": 1605024005887, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "invitation": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review", "content": {"title": "Insufficient theoretical and experimental results", "review": "This paper studies the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. The authors propose to use Gauss-Newton to perform the proximal update in nonlinear least squares and L-BFGS or accelerated gradient for generic problems. The authors derive the convergence of SPPA to a stationary point in expectation for nonconvex problems, and perform numerical experiments to showcase the effectiveness of the proposed method compared to SGD and its variants. \n\nThe paper is generally clear, yet the convergence analysis is mainly based on adapting Bottou et al. (2018). While the proposed methods could be significant additions to stochastic optimizers in deep learning, I found the study of the current paper is insufficient; see the cons below. \n\nPros: \n- It is well-known that the proximal point algorithm (PPA) converges faster than gradient descent (GD), and the same holds for their stochastic counterparts. One advantage is that the step sizes in PPA and SPPA can be larger than those in GD and SGD, which can speed up convergence. The proximal steps in PPA and SPPA are however hard to perform for generic (nonconvex) problems since the proximity operator of the objective functions usually do not have closed forms. The proposal of the authors to perform efficient inner-loop optimization schemes like Gauss-Newton and L-BFGS allows approximation of such proximal steps, without much computational burden added. \n\nCons: \n- Theory:\n    * I found that Assumption 3 is too strong and do not think it is a standard assumption. Otherwise the constant $ c $ can be very large. This also leads to a question of why using the upper bound $ c $ is the second part of the RHS of (20) but not the first part?\n    * Also why $ \\sqrt{\\lambda_t} $ instead of $ \\lambda_t $ in (20)? If I did not misunderstand, it is derived from (14).\n    * As $ c $ and hence $ C $ can be very large, the bounds (9) and (10) in Theorems 2 and 3 can well be vacuous.\n    * Also the quantifier in Assumption is missing (for all $ i\\in \\lbrace 1, \\ldots, n \\rbrace $?) \n    * Another pitfall of the theoretical results of this work is that the convergence analyses of the proposed SPPA-LBFGS, SPPA-AGD and SPPA-GN are all missing, especially since this work considers nonconvex problems. \n\n- Experiments:\n    * To showcase the proposed methods are really comparable to or outperform methods like SGD or Adam, numerical experiments should be performed on data sets of larger scales and much deeper networks. In particular, the regression data sets in the paper are so small that the gain of the proposed method over other baselines are so marginal. \n\nTypos: \n- Theorem 1: do you mean the limit is equal to 0 or is finite? I guess something is missing. \n- Proof of Theorem 2: $ L(\\theta_1) - \\mathbb{E}[L(\\theta_{T+1})] $ instead of $ L(\\theta_0) - \\mathbb{E}[L(\\theta_{T})] $\n- (10): the LHS of the inequality should be $ \\alpha_t $ instead of $ \\alpha $", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3407/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3407/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3407/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076364, "tmdate": 1606915804026, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3407/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review"}}}, {"id": "1VbaktTPQzJ", "original": null, "number": 4, "cdate": 1604459931095, "ddate": null, "tcdate": 1604459931095, "tmdate": 1605024005823, "tddate": null, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "invitation": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review", "content": {"title": "the novelty is not enough", "review": "This paper revisits the stochastic proximal point algorithm (SPPA) and apply SPPA to solve nonconvex optimization problems with efficient subproblem solvers.\n\nFirstly, there is a work [Chen et. al 2020] which also provides the convergence result of SPPA on manifold problem and it is not the weakly convex setting.\n\nSecondly, the convergence rate of SPPA is 1/epsilon^2, which is the same as SGD. Regarding the convergence result, there is no advantage of SPPA against SGD and the author should have a discussion about this. The convergence rate is asymptotic and this paper does not point out which iteration we should use as the final output.\n\nThirdly, the convergence analysis is rather standard and lack of novelty. Moreover, the convergence result of Gauss-Newton and L-BFGS to solve the proximal subproblem should also be provided. Since the main concern for PPA-type method lies on the convergence behavior and efficiency to solve the proximal subproblem. Furthermore, in the experiment part, the comparison of running time between SPPA and SGD, ADAM, Adagrad should also be provided. \n\nLastly, I wonder whether the assumption 1 is reasonable, since I have not seen this assumption in other nonconvex stochastic programming papers. Authors should remark on this assumption and it would be better to put some references on this.\n\n\nConfidence level: 5, abusolutely certain.\n\nRating: reject\n\nreferences:\n[1] Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning.\nShixiang Chen, Zengde Deng, Shiqian Ma, Anthony Man-Cho So.  arXiv preprint arXiv:2005.02356, 2020.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3407/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3407/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks", "authorids": ["aysegul.bumin@ufl.edu", "~Kejun_Huang1"], "authors": ["Aysegul Bumin", "Kejun Huang"], "keywords": [], "abstract": "We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. SPPA has been shown to converge faster and more stable than the celebrated stochastic gradient descent (SGD) algorithm, and its many variations, for convex problems. However, the per-iteration update of SPPA is defined abstractly and has long been considered expensive. In this paper, we show that efficient implementation of SPPA can be achieved. If the problem is a nonlinear least squares, each iteration of SPPA can be efficiently implemented by Gauss-Newton; with some linear algebra trick the resulting complexity is in the same order of SGD. For more generic problems, SPPA can still be implemented with L-BFGS or accelerated gradient with high efficiency. Another contribution of this work is the convergence of SPPA to a stationary point in expectation for nonconvex problems. The result is encouraging that it admits more flexible choices of the step sizes under similar assumptions. The proposed algorithm is elaborated for both regression and classification problems using different neural network structures. Real data experiments showcase its effectiveness in terms of convergence and accuracy compared to SGD and its variants.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bumin|stochastic_proximal_point_algorithm_for_largescale_nonconvex_optimization_convergence_implementation_and_application_to_neural_networks", "supplementary_material": "/attachment/6c5aaabeb6d8e0f18702c84edf2a9d799b5d56dd.zip", "pdf": "/pdf/4d3b299ce69bb905d9b0e8114eb34601b869d132.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pYgtCR8UVR", "_bibtex": "@misc{\nbumin2021stochastic,\ntitle={Stochastic Proximal Point Algorithm for Large-scale Nonconvex Optimization: Convergence, Implementation, and Application to Neural Networks},\nauthor={Aysegul Bumin and Kejun Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=EQtwFlmq7mx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EQtwFlmq7mx", "replyto": "EQtwFlmq7mx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3407/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076364, "tmdate": 1606915804026, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3407/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3407/-/Official_Review"}}}], "count": 6}