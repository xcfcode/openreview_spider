{"notes": [{"id": "K9bw7vqp_s", "original": "a9XZdz3fRzE", "number": 255, "cdate": 1601308036821, "ddate": null, "tcdate": 1601308036821, "tmdate": 1616080311308, "tddate": null, "forum": "K9bw7vqp_s", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7baJCnTEMPF", "original": null, "number": 1, "cdate": 1610040398128, "ddate": null, "tcdate": 1610040398128, "tmdate": 1610473993571, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Reviewers like the simplicity of the approach and the fact that it works well.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040398114, "tmdate": 1610473993553, "id": "ICLR.cc/2021/Conference/Paper255/-/Decision"}}}, {"id": "ECxq4_ZBfV", "original": null, "number": 4, "cdate": 1606028862790, "ddate": null, "tcdate": 1606028862790, "tmdate": 1606120885194, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "IM0fcQlyhhM", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "We sincerely thank you for your comments as well as the appreciation of our work. Our responses to your concerns are demonstrated below:\n\n**Q1**: The proposed SAD metric seems to work just on the same network during its own training process, as it does not consider the fact that at different training runs the hidden neurons may develop differently.\n\n**A1**: Yes, SAD only works on the same network, since we use SAD here only to analyze how the topology of the **same** neural network evolves in the training process with STE or SR-STE. The research of effective metrics to compare topology differences between individually trained networks will be left to future work.\n\n\n\n**Q2**: The related work is not complete\n\n**A2**: Due to the page limit of our initial ICLR submission, we did not add some related work in our first version. We totally agree with you and have added more related discussion in the revised version (2nd Paragraph Page 3).\n\n**Q3**: The paper has a number of sloppy written passages and inconsistencies in the mathematical formalism.\n\n**A3**: We have reviewed our paper regarding your proposed problems and have updated the draft.\n\n**Q4**: Comparisons with NNSTD (https://arxiv.org/pdf/2006.14085 )\n\n**A4**: Thanks for pointing out the interesting and related paper, we have added discussions about NNSTD at the end of 2nd Paragraph on Page 6.\n\n**Q5**: The last phrase of the above-mentioned paragraph (the 4th paragraph on page 2) and the next one are, up to my knowledge, accurate just for CNNs. It has been shown already that for other types of neural networks (e.g. MLPs), sparse networks can easily and constantly outperform dense networks (e.g. https://arxiv.org/abs/1905.09397). Please make this discussion more accurate and informative.\n\n**A5**: Thank you for your suggestions. We have added proper phrases to avoid readers leaving the discussion scope, which is about training sparse CNN only. We also added discussions on MLPs where sparse networks' performances may constantly surpass those of the dense ones.\n\n*Hope our above responses are helpful to address your concerns. If you have further questions, please let us know. Thanks!*\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper255/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "yCbZ7lNln0D", "original": null, "number": 11, "cdate": 1606079041248, "ddate": null, "tcdate": 1606079041248, "tmdate": 1606100945212, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "Vo5CVedPWZ-", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 (Part 1/2) ", "comment": "We sincerely appreciate your valuable comments on our work. To address your concerns, our responses are demonstrated below:\n\n**Q1**: It seems like a mitigation measure, which cannot completely eliminate the negative impact caused by the approximate measures taken in the STE.\n\n**A1**: Yes, the proposed SR-STE is a mitigation that cannot completely eliminate the negative impacts. However, we justify SR-STE's contributions from two aspects.\n\n(i) Our proposed SR-STE can significantly close the performance gap between N:M sparse networks and dense ones, compared with other methods such as STE and two-stage methods proposed by Nvidia [6]. For example, we can mitigate the performance drop of 2:4 sparse ResNet-18 brought by training with STE by 92.9% (refer to Q4 in our response to AnonReviewer4). Although we do not fully eliminate the negative impact brought by STE, we do regard SR-STE as an effective and versatile strategy for N:M sparsity training. We sincerely hope our work can contribute to the hardware-aided deep model acceleration and shed light on further research and investigations in N:M sparsity training.\n\n(ii) On the other hand, in a larger scope, we are the first to attempt to analyze and mitigate the negative impact brought by STE when it is applied to N:M sparsity training. Our experimental results show that 2:4 structured sparse model trained with SR-STE can achieve **comparable or even better** results with **negligible extra training cost** and **only a single easy-to-tune hyperparameter $\\lambda_w$** than original dense models. SR-STE consistently performs well on **different tasks** (classification, detection, machine translation), **different model** architectures (ResNet18/50, RegNetXs, Faster RCNN, Mask RCNN, RAFT, Transformer), and **different sparsity types** (unstructured and N:M structured sparsity).\n\n\n**Q2**: The novelty of the proposed SR-STE is not enough.\n\n**A2**: We propose the first method to tackle the problem of how to train an N:M structured sparsity network. This is a novel problem that has never been investigated before. We also show that simply adopting STE cannot effectively train an N:M structured sparsity network. The proposed SR-STE has the advantages of being **simple** and **easy** when being applied to **different large-scale tasks**(classification, detection, segmentation, machine translation, optical flow) with different types of sparsity (structured, unstructured).\n\n\n\nComparisons of top-1 accuracy between **SOTA** coarse-grained structured and N:M structured ResNet50 networks on ImageNet are shown below:\n\n|                           |                               |                                    |\n|-|-|-|\n| Coarse structured | 76.1 (compression ratio=1) [3] | 74.3 (compression ratio = 1.26) [3] |\n| N:M structured            | **77.4** (compression ratio = 1)   | **76.4** (compression ratio = **4**)       |\n\nFurthermore, in the paper, we demonstrate the rationality of SR-STE with careful discussions and adequate experiments, which makes SR-STE a universal and reliable recipe in training N:M sparse DNNs. Other than SR-STE, we also have two other major contributions: \n\n(i) We propose a more general form of structured fine-grained sparse DNNs that can be accelerated on designated hardware. It has huge advantages over the other two forms of sparse DNNs that have been studied, namely, unstructured sparse DNNs and coarse-grained sparse DNNs. The efficacy of unstructured sparsity is highly limited and should not be further studied according to [1]. As for the coarse-grained sparsity (channel/filter pruning), it has a much lower performance compared with sparse neural networks (76.1 (coarse-grained) vs 77.4 (N:M structured) with 50% sparsity, in the above Table) according to [3], and even more performance drop with increasing compression ratios. N:M sparsity's outstanding performance compared to all other existing sparsity methods proves its huge potential in deep model acceleration.\n\n(ii) We propose SAD, an important metric that is closely related to model performance during training sparse neural networks according to our investigations, to foster future research on measuring topological changes of sparse DNNs in the training stage.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "Zz6_A71snjO", "original": null, "number": 12, "cdate": 1606079118903, "ddate": null, "tcdate": 1606079118903, "tmdate": 1606079118903, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "UIRFFFDj0HK", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4 (Part 1/2)", "comment": "We highly appreciate your elaborate comments. We hope our answers below can address your concerns.\n\n**Q1**: Explanation of \u201cN:M\u201d in the introduction part.\n\n**A1**: We have given the explanation of \"N:M\" in the introduction (last line on page1)\n\n**Q2**: Codes cannot run.\n\n**A2**: We did include our implementations of SR-STE in the code. However, we didn't update train_imagenet.py after reconstructing our source codes, You can refer to our implementations of SR-STE from the 19th/20th/22nd line of https://github.com/anonymous-NM-sparsity/NM-sparsity/tree/main/devkit/sparse_ops. We also have updated train_imagenet.py correspondingly.\n\n**Q3**: Explanations of R and C under Fig. 1.\n\n**A3**: Thank you for your careful review. We have added explanations of these two notations in the figure description.\n\n**Q4**: The original STE is used for unstructured sparsity, while you improved STE for N:M structured sparsity. Why does your improvement encourage structured sparsity?\n\n\n**A4**: Before our proposal of N:M fine-grained structured sparsity, there are mainly two kinds of sparsity studied: fine-grained unstructured sparsity and coarse-grained structured sparsity sparsity. We agree with you that STE is mainly used for unstructured sparsity before our work but we think it is not because unstrucutred sparsity is \"unstructured\", instead, it is because the pruning metric in unstructured sparsity is **fine-grained**, so that STE can be applied to. And, in our work, although N:M sparsity is of the type of structured sparsity, it is also fine-grained, which means STE is a natural and suitable option in the process of training an N:M sparse DNN.\n\nOn the other hand, although we mainly focus on N:M fine-grained structured sparsity here, we also perform some experiments of SR-STE on unstructured sparse DNNs for ablation study. In our results, SR-STE consistently outperforms STE in training both structured and **unstructured sparse** DNNs, which was not included in our first ICLR version since we think it deviates from our targeted problem. However, the question raised by you made us aware of the importance of this ablation study, so we re-added the results to our updated draft (In Table 3), detail as follows: \n\n|unstrucutred sparse DNNs  |        | Top-1 Acc (%) | Sparsity (%) | Improvement (%) |\n|-|:-|:-:|:-:|:-:|\n| ResNet18 | STE    |     66.3     |      90     |                |\n| ResNet18 | SR-STE |     69.1     |      90     |      **+2.8**      |\n| ResNet18 | STE    |     63.1     |      95     |                |\n| ResNet18 | SR-STE |     66.3     |      95     |      **+3.2**      |\n\n| unstrucutred sparse DNNs |        | Top-1 Acc (%) | Sparsity (%) | Improvement (%) |\n|-|:-|:-:|:-:|:-:|\n| ResNet50 | STE    |     73.8     |      90     |                |\n| ResNet50 | SR-STE |     74.9     |      90     |      **+1.1**      |\n| ResNet50 | STE    |     68.8     |      95     |                |\n| ResNet50 | SR-STE |     72.4     |      95     |      **+3.6**      |\n\n\nSpecifically, in the results, when training a 90% sparse (90% of all weights are zeros) **unstructured sparse** ResNet18 and ResNet50 on ImageNet, the network trained with STE has a top-1 accuracy of 66.3%, which is far lower than that of SR-STE (69.1%). Here are the experimental results when STE and SR-STE are utilized in unstructured sparse DNNs.\n\nTable 3 shows that SR-STE outperforms **SOTA unstructured sparsity** method STE by about 2.2% with 95% sparsity, which verifies that our method can improve the unstructured sparsity rather than only for N:M structured.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "cVpmogR1E0D", "original": null, "number": 10, "cdate": 1606078963897, "ddate": null, "tcdate": 1606078963897, "tmdate": 1606078963897, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Paper update", "comment": "Thanks to all the reviewers for their constructive suggestions and comments. We really appreciate all your inputs. In this updated version, our paper has been refined after we carefully considered all reviewers\u2019 suggestions.\n\n1. Related work is added in the 2nd paragraph on Page 3. (AnonReviewer1)\n2. Discussions about MLPs are added in the 4th paragraph on Page 2. (AnonReviewer1)\n3. The inconsistencies in Tables 1, 2, 3 are fixed. (AnonReviewer1)\n4. Explanations of SAD are refined in 1st paragraph on Page 5. (AnonReviewer1)\n5. Comparisons and discussions between SAD and NNSTD are added in the 2nd paragraph on Page 6. (AnonReviewer1)\n\n\n6. The layout of our paper is refined, and the figures are rearranged so that readers can easily find corresponding information. (AnonReviewer2)\n7. Typos in our paper are fixed in the last paragraph on Page 2 and 1st paragraph on Page 6. (AnonReviewer2)\n8. Explanations of SAD and comparisons between SAD with other relevant methods are demonstrated in the 2nd paragraph on Page 6. (AnonReviewer2)\n\n\n\n9. Typos in the demonstration of SAD are fixed in 1st paragraph on Page 6. (AnonReviewer3)\n10. Typos in Tables 1 and 3 are fixed. (AnonReviewer3)\n\n11. The explanation of N:M is added in the last line on Page 1. (AnonReviewer4)\n12. Explanations of R and C are added in the caption of Fig. 1 (AnonReviewer4)\n13. Experimental results on unstructured sparsity are added in Table 3. (AnonReviewer4)\n14. Experimental results on more tasks are added in Tables 4, 5, 6, 7. (AnonReviewer4)\n\nApart from the issues addressed above, we also revised our paper with regard to consistency and readability.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "ps5dP4qQ7I", "original": null, "number": 6, "cdate": 1606029191735, "ddate": null, "tcdate": 1606029191735, "tmdate": 1606074015243, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "Vo5CVedPWZ-", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 (Part 2/2) ", "comment": "**Q3**: And in my opinion, the analysis of SAD lacks relevant theoretical support and is not very convincing. I expect more explanations for SAD.\n\n**A3**: Yes, SAD is an empirical measurement. Although it does not have theoretical support yet, we have empricially shown its relevance to the sparse network's performance in our experiments in Fig. 3(a) and Fig. 6(b). Furthermore, we justify the proposed SAD in two folds:\n\n(i) SAD has been implicitly adopted in existing published research papers. Two examples are demonstrated here:\n\n1. In RigL [4], the authors consider the case of  **$SAD_t = 2 f_{decay}(t;\\alpha,T_{end})(1-s^{l})N^l$**, RigL can achieve SOTA results on training unsturctured sparse networks from scratch. \n2. If we prune the network at initialization and don\u2019t update the connections, then according to the recent work [2], the performance drops significantly [2]. This can be regarded as setting **$SAD_t=0$** during the whole training phase.\n\n\n(ii) To the best of our knowledge, we are the first to explicitly define and investigate SAD on how it relates to the sparse model's performance during sparse training. Our study about the relevance between SAD and the sparse model performance is shown in Fig. 3(a) of our paper. In Fig. 3(a), the curve that corresponds to a higher accuracy tends to consistently have a larger SAD value during the training process.\n\n\n[1] Ma, Xiaolong, et al, \"Non-Structured DNN Weight Pruning \u2013 Is It Beneficial in Any Platform?,\"\" IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2020.\n\n[2] Frankle, Jonathan, et al. \"Pruning Neural Networks at Initialization: Why are We Missing the Mark?.\" arXiv preprint arXiv:2009.08576 (2020).\n\n[3] Renda, Alex, Jonathan Frankle, and Michael Carbin. \u201cComparing rewinding and fine-tuning in neural network pruning.\u201d arXiv preprint arXiv:2003.02389 (2020).\n\n[4] Evci, Utku, et al. \"Rigging the lottery: Making all tickets winners.\" arXiv preprint arXiv:1911.11134 (2019).\n\n[5] Dettmers, Tim, and Luke Zettlemoyer. \"Sparse networks from scratch: Faster training without losing performance.\" arXiv preprint arXiv:1907.04840 (2019).\n\n[6] Nvidia. \"Nvidia  a100  tensor  core  GPU  architecture.\" https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf, 2020.\n\n\n**Q4**: There are still several issues that need to be addressed.\n\n**A4**: Following your suggestions, we tried our best on revising and proof-reading of our manuscript. \n\n*Hope our above responses are helpful to address your concerns. If you have further questions, please let us know. Thanks!*\n    \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper255/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "QiTGoXdHVQH", "original": null, "number": 9, "cdate": 1606029688251, "ddate": null, "tcdate": 1606029688251, "tmdate": 1606063619836, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "UIRFFFDj0HK", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4 (Part 2/2) ", "comment": "**Q5**: According to Table 2, the classification performance gap between STE and SR-STE is minor. So I think SR-STE is not a significant improvement compared to the original STE. I am not 100% sure about my opinion on performance, since a comparison between STE and SR-STE is not provided for the experiments on other applications (detection, segmentation, optical flow, and machine translation.\n\n\n**A5**: Regarding your concern, we explain the proposed SR-STE's performance in three parts:\n\n(i) For heavy models such as ResNet-50, the absolute performance gain is indeed \"minor\" but it is also significant if we consider it relatively. Specifically, if we compare the performance gain ($ACC_{SR-STE}-ACC_{STE}$) of SR-STE ($ACC_{dense}-ACC_{STE}$), it can be noticed that SR-STE significantly mitigates the performance drop of DNNs trained with STE from -0.9% to -0.3%. SR-STE can significantly mitigate 66.7% of the performance drop brought by STE! We believe the redundancy of ResNet-50 causes the *minor* performance drop of ResNet-50-STE and leaves SR-STE little space to improve over it.\n\n\n(ii) As for smaller-scaled models such as ResNet-18 and RegNet, our proposed SR-STE consistently improves the performance both relatively and absolutely, compared with STE. ResNet-18-SR-STE outperforms ResNet-18-STE by **1.3%** and at the same time mitigates the ResNet-18-STE's performance drop by 92.9% (from -1.4% to -0.1%). On the other hand, RegNetX002-SR-STE outperforms RegNetX002-STE by **3.7%**(see Table 8 in Appendix A.4).\n\nAll the results in i) and ii) are demonstrated in the following table. Here, absolute improvement indicates $ACC_{SR-STE}-ACC_{STE}$ and relative improvement means $\\frac{ACC_{SR-STE}-ACC_{STE}}{ACC_{dense}-ACC_{STE}}$. \n\n|  | Dense Top-1 Acc (%) | STE Top-1 Acc (%) | SR-STE Top-1 Acc (%) | Absolute Improvement (%) | Relative Improvement |\n|-|-|-|-|-|-|\n| RegNetX-002 | 68.3 | 63.0 | 66.7 | **3.7** | 69.8% |\n| ResNet-18 | 71.3 | 69.9 | 71.2 | 1.3 | 92.9% |\n| ResNet-50 | 77.3 | 76.4 | 77.0 | 0.6 | 66.7% |\n\n\n\n(iii) Following your suggestions, we have added **detection, segmentation, transformer, optical flow** performance comparisons in Tables 4, 5, 6, 7. From these results, **SR-STE** can outperform **STE** in all tasks.\n\n\n*Hope our above responses are helpful to address your concerns. If you have further questions, please let us know. Thanks!*\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper255/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "gYv1h1uH_bz", "original": null, "number": 7, "cdate": 1606029338976, "ddate": null, "tcdate": 1606029338976, "tmdate": 1606063450529, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "jtt8MEeeY-Z", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "Thank you for your comments and appreciation of our work. To relieve some of your concerns, we have posted our responses below:\n\n\n**Q1**: However, weight-sorting process which occurs in every training step can decrease the training speed compared to other threshold-based pruning method. Could you compare the actual consuming time to train sparse network for each training method?\n\n**A1**: The weight-sorting process of N:M structured sparsity is very efficient, compared with other fine-grained pruning methods, such as the weight sorting method [1] and threshold-based pruning method [2]. Implementation details and efficiency benchmarks are shown as follows:\n\n|weight_sorting(N:M structured)(ours)   |    weight_sorting(unstructured)[1]    | threshold-based(unstructured)[2] |\n|:----------:|:-------:|:--------------:|\n| 0.00045s | 0.0021s    |     0.00041s     |\n\n(i) Our weight-sorting implementation of achieving N:M sparsity is shown below:\n\n```python\ndef forward(ctx, weight, N=4, M=2):\n    output = weight.clone()\n    length = weight.numel()\n    group = int(length/M)\n    weight_temp = weight.detach().abs().reshape(group, M)\n    index = torch.argsort(weight_temp, dim=1)[:, :int(M-N)]\n    \n    # compute the mask ($epsilon_t$ in the paper)\n    mask = torch.ones(weight_temp.shape, device=weight_temp.device)\n    mask = mask.scatter_(dim=1, index=index, value=0).reshape(weight.shape)\n\n    return output*mask, mask\n```\n\nThe weight-sorting technique in our implementation is very computationally efficient during the training process. It only takes **0.00045s** (averaged over 1000 iterations with batch size 256), which takes only 0.37% of 0.122s (the average time cost of one iteration).\n\n(ii) Recent SOTA unstructured sparsity methods also have to sort the weight tensor, which takes approximately **0.0021s** in one iteration, over 3.6 times more than SR-STE. Compared to N:M sparsity, when computing argsort(), unstructured sparsity methods need to sort the weights in one layer as a whole, which is more time-consuming than N:M sparsity. Because in N:M sparsity, weights are sorted inside its N-sized groups, which can be paralleled in popular deep learning libraries. The forward implementation of unstrucutred sparsity with STE is shown as follows:\n\n```python\ndef forward(ctx, weight, pruning_ratio=0.5):\n\n    output = weight.clone()\n    length = weight.numel()\n    weight_temp = weight.detach().abs().reshape(length)\n    index = torch.argsort(weight_temp, dim=0)[:int(pruning_ratio*length)]\n    mask = torch.ones(weight_temp.shape, device=weight_temp.device)\n    mask = mask.scatter_(dim=0, index=index, value=0).reshape(weight.shape)\n\n    return output*mask, mask\n```\n\n(iii) We also implement the threshold method [2], which takes **0.00041s** (comparable with SR-STE) in one iteration, the implementation details are shown as follows:\n\n```python\ndef sparse_thres(W, crate=1.0):\n    W_abs = W.detach().abs()\n    # compute the threshold according to [2]\n    threshold = 0.9*(torch.mean(W_abs)+crate*torch.std(W))\n    mask = (W_abs<threshold)\n\n    return W*mask\n```\n\nBesides, we also provide complete training time. Compared to 20h 46m 28s, which is the time cost to train a dense ResNet-18 network from scratch, training a sparse one with SR-STE takes 21h 20m 15s, which only adds 2.7% extra cost.\n\n[1]. Renda, Alex, Jonathan Frankle, and Michael Carbin. \"Comparing rewinding and fine-tuning in neural network pruning.\" arXiv preprint arXiv:2003.02389 (2020).\n\n[2]. Guo, Yiwen, Anbang Yao, and Yurong Chen. \"Dynamic network surgery for efficient DNNs.\" Advances in neural information processing systems. 2016.\n\n**Q2**: Comment: Typos.\n\n**A2**: We have gone through the paper and fixed the typos in the paper.\n\n\n*Hope our above responses are helpful to address your concerns. If you have any questions, please let us know. Thanks!*\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper255/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K9bw7vqp_s", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper255/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper255/Authors|ICLR.cc/2021/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872990, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Comment"}}}, {"id": "UIRFFFDj0HK", "original": null, "number": 1, "cdate": 1603811082173, "ddate": null, "tcdate": 1603811082173, "tmdate": 1605024730141, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Review", "content": {"title": "Clear motivation and detailed explanations on SAD, but I have concerns on structured sparsity and performance improvement", "review": "To maintain advantages of both unstructured and structured sparsity, the paper improves previous STE method based on the proposed SAD. The experiments on five types of applications show its outstanding performance.\n\npros.\n1. The motivation of this paper, maintaining advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity, is clear and persuasive.\n2. To validate outstanding performance of the proposed method, the authors did experiments on five different types of applications: image classification, object detection, instance segmentation, optical flow, and machine translation.\n3. The authors provide detailed explanations on physical meaning of their proposed SAD.\n\ncons.\n1. In Section Introduction, I think the authors should formally explain meaning of \"N:M\" when \"N:M\" is mentioned for the first time.\n2. In your github code train_imagenet.py, implementation of \"devkit\" package on 19th/20th/22nd line can not be found. I think your proposed method is mainly implemented in devkit.sparse_optimizer, while you do not include the most important code.\n3. In Figure 1, physical meaning of variables R and C is not introduced.\n4. The original STE is used for unstructured sparsity, while you improved STE for N:M structured sparsity. Why does your improvement encourage structured sparsity? \n5. According to Table 2, the classification performance gap between STE and SR-STE is minor. So I think SR-STE is not a significant improvement compared to the original STE.  I am not 100% sure about my opinion on performance, since comparison between STE and SR-STE is not provided for the experiments on other applications (detection, segmentation, optical flow, and machine translation).", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147086, "tmdate": 1606915772051, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper255/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Review"}}}, {"id": "Vo5CVedPWZ-", "original": null, "number": 2, "cdate": 1603867460129, "ddate": null, "tcdate": 1603867460129, "tmdate": 1605024730081, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Review", "content": {"title": "In this paper, the authors propose to train N:M structured sparse neural networks from scratch.  To improve the effectiveness of the training process, a sparse-refined straight through estimator (SR-STE) is proposed based on vanilla STE. Finally, the authors define Sparse Architecture Divergence (SAD) to indicate the topology change of the sparse network. The novelty of the proposed SR-STE is not enough, and the defined SAD lacks relevant theoretical support. ", "review": "In this paper, the authors propose to train N:M structured sparse neural networks from scratch.  To improve the effectiveness of the training process, a sparse-refined straight through estimator (SR-STE) is proposed based on vanilla STE. Finally, the authors define Sparse Architecture Divergence (SAD) to indicate the topology change of the sparse network. The novelty of the proposed SR-STE is not enough, and the defined SAD lacks relevant theoretical support. Despite the good performance, I am concerned about the technical novelty of this paper. \n(1) The main contribution of the proposed SR-STE lies in a sparse-refined regularization term. It seems like a mitigation measure, which cannot completely eliminate the negative impact caused by the approximate measures taken in the STE. \n(2) And in my opinion, the analysis of SAD lacks relevant theoretical support and is not very convincing. I expect more explanations for SAD.\n\nThere are still several issues that need to be addressed. Firstly, the layout of this paper is not very suitable, and it is not very convenient in the process of finding the corresponding figures. Secondly, there are several narrative errors in this papere, which need to be checked carefully, e.g., in the third-to-last line of Section 1, I think it should be 'we propose a sparse refined term to enhance the effectiveness on training the sparse neural networks from scratch'; in the second-to-last line of Section 3.2, it should be '\\left | ^{S}\\textrm{SAD}_{0:t}^{l}- ^{D}\\textrm{SAD}_{0:t}^{l}\\right |'.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147086, "tmdate": 1606915772051, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper255/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Review"}}}, {"id": "jtt8MEeeY-Z", "original": null, "number": 3, "cdate": 1603894819882, "ddate": null, "tcdate": 1603894819882, "tmdate": 1605024730016, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Review", "content": {"title": "Simple technique, but nice results ", "review": "The authors proposed a new method for training N:M fine-grained structured sparse networks from scratch. The authors found that the SAD metric, which measures the number of weights whose pruning state is changed, became higher if the existing STE is used to train sparse networks and this metric had the positive relationship with accuracy drop. To reduce the SAD, SR-STE which gives higher weight decay coefficient to the pruned weights is applied to train sparse networks, improving the accuracy of sparse networks trained from scratch.\n\nSR-STE which is penalizing the pruned weights is somewhat simple, but the results in the paper are strong. The proposed method supports to increase the accuracy of N:M fine-grained structured sparse networks, and it can be applied in many diverse tasks regardless of pruning method like general unstructured pruning. I think it is a good technique to easily apply when pruning states of each weight are continuously changed.\n\nHowever, weight-sorting process which occurs in every training step can decrease the training speed compared to other threshold-based pruning method. Could you compare the actual consuming time to train sparse network for each training method? \n\nComment: Below are some typos I think.\n\n1. 3.2. Analysis: I think |^{S}{SAD}^{l}_{0:t}-^{D}{SAD}^{l}_{0:t}| instead of |^{D}{SAD}^{l}_{0:t}-^{D}{SAD}^{l}_{0:t}| is likely to be the right expression in context.\n2. 3.2. Analysis: The sentence \"This formula measures the number of neurons that are pruned~\" is unfamiliar to me, because \"neuron\" is usually used when depicting activation, not weight of artificial neural network. I think \"weight\" is more matched expression.\n3. The stated GFLOPs of ResNet-50 (2:8 sparse pattern, SR-STE method) are different in Table 1 (1.02) and Table 3 (0.1). I think the former is right. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147086, "tmdate": 1606915772051, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper255/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Review"}}}, {"id": "IM0fcQlyhhM", "original": null, "number": 4, "cdate": 1603966934825, "ddate": null, "tcdate": 1603966934825, "tmdate": 1605024729955, "tddate": null, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "invitation": "ICLR.cc/2021/Conference/Paper255/-/Official_Review", "content": {"title": "SR-STE and happy SAD", "review": "Summary\n\nThe paper introduces a new sparse training algorithm (SR-STE) based on the straight through estimator which is specially designed for the hardware constraints of Nvidia A100 GPU. Auxiliary, in order to study better this algorithm, the paper introduces also a metric (SAD) to measure the changes in the sparse network topology during training. The contributions have real added value as they show that sparse neural networks can actually benefit of hardware designed to consider sparsity. The experiments on CNNs and Transformers support the claims.\n\nStrong Points\n\n\u2022\tSR-STE can take real advantage of A100 sparse capabilities.\n\n\u2022\tThe algorithm is designed for the general form of n:m sparsity and, likely, can be used also for the next generation of GPUs with sparse networks support.\n\n\u2022\tIt is the first study which, up to my knowledge, shows consistently that sparse networks can outperform dense networks also for convolutional layers.\n\n\u2022\tThe paper is written in a clear manner and well structured.\n\nWeak points\n\n\u2022\tThe proposed SAD metric seems to work just on the same network during its own training process, as it does not consider the fact that at different training runs the hidden neurons may develop differently.\n\n\u2022\tThe related work is not complete\n\n\u2022\tThe paper has a number of sloppy written passages and inconsistencies in the mathematical formalism.\n\nDuring the rebuttal, I would suggest to the authors to consider the following comments:\n\n1) Improve the mathematical formalism and perform a careful proof-read to make sure that all notations are consistent. For instance page 5,  2nd paragraph, E_f is actually \\epsilon_f?;  page 5, third paragraph, the last equation has two equal terms, and so on.\n\n2) I believe that it would be informative to discuss the SAD metric limitations and advantages in comparison with other state-of-the-art metrics suitable to measure the distance between two sparse topologies, e.g. NNSTD https://arxiv.org/abs/2006.14085 .\n\n3)  Related work. the 4th paragraph on page 2 about the one-stage scheme to obtain efficient sparse networks is missing the first two references from 2017 which introduced sparse training with dynamic topology: https://arxiv.org/abs/1707.04780, https://arxiv.org/abs/1711.05136 . Auxiliary, I believe that also this work has to be cited https://arxiv.org/abs/1907.04840 . \n\n4) The last phrase of the above mentioned paragraph and the next one are, up to my knowledge, accurate just for CNNs. It has been shown already that for other types of neural networks (e.g. MLPs), sparse networks can easily and constantly outperform dense networks (e.g. https://arxiv.org/abs/1905.09397). Please make this discussion more accurate and informative.\n\n5)  Please read the paper thoroughly to address all typos and small inconsistencies. E.g., In Tables 1 and 2 \u201cours\u201d appear just in some cases of SR-STE; In Table 3, STR appears twice with exactly the same sparsity level, but different number of parameters (more clarifications would make easier the reader job).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper255/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper255/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch", "authorids": ["~Aojun_Zhou2", "~Yukun_Ma2", "junnan.zhu@nlpr.ia.ac.cn", "~Jianbo_Liu3", "~Zhijie_Zhang1", "~Kun_Yuan1", "~Wenxiu_Sun1", "~Hongsheng_Li3"], "authors": ["Aojun Zhou", "Yukun Ma", "Junnan Zhu", "Jianbo Liu", "Zhijie Zhang", "Kun Yuan", "Wenxiu Sun", "Hongsheng Li"], "keywords": ["sparsity", "efficient training and inference."], "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and\ndecent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing\ncomprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|learning_nm_finegrained_structured_sparse_neural_networks_from_scratch", "one-sentence_summary": "a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch", "pdf": "/pdf/6b4a1e280d0d8aacd0298d472f2c255fbad8498f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021learning,\ntitle={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},\nauthor={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=K9bw7vqp_s}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K9bw7vqp_s", "replyto": "K9bw7vqp_s", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147086, "tmdate": 1606915772051, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper255/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper255/-/Official_Review"}}}], "count": 13}