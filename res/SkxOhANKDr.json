{"notes": [{"id": "SkxOhANKDr", "original": "rJe7uYF_wS", "number": 1361, "cdate": 1569439407601, "ddate": null, "tcdate": 1569439407601, "tmdate": 1577168252014, "tddate": null, "forum": "SkxOhANKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1mjEPEjums", "original": null, "number": 1, "cdate": 1576798721469, "ddate": null, "tcdate": 1576798721469, "tmdate": 1576800915117, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image. \nAfter the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719612, "tmdate": 1576800270292, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Decision"}}}, {"id": "B1erP1osoS", "original": null, "number": 16, "cdate": 1573789532776, "ddate": null, "tcdate": 1573789532776, "tmdate": 1573789532776, "tddate": null, "forum": "SkxOhANKDr", "replyto": "rJlzPUN9jS", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Responses to Reviewer 3 comment", "comment": "\nWe really appreciate your valuable comment! \n\nBPDA is an attack method. In the original paper of BPDA, they provided results on MINST, CIFAR-10, and ImageNet. Here is the reason that we only had BPDA defense results on the CIFAR-10. (1) We found that only one defense paper had results on MINST, so we did not provide comparisons on MINST. (2) For the ImageNet, it is too huge and extremely time consuming for us to run all of these experiments. (3) In recently defense papers, only the STL method from CVPR 2019 provided results on BPDA, which we have included it in the paper and our algorithm significantly outperforms it. \u00a0\nWe used the standard attack-defense evaluation package called AdverTorch which provides implementations of major attack methods, including BPDA.\n\nWe set 10 attack iterations as the BPDA baseline setting. In addition, the large iterations and large epsilon evaluation of BPDA attacks and are also presented in Figure 3 and Figure 4, which show the proposed method is able to effective defend BPDA attacks and outperform other methods. \n\nWe did used the adversarial samples for training the generator and discriminator. But we did not re-train the target classifier and made no modification to the target classifier."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "rJlzPUN9jS", "original": null, "number": 15, "cdate": 1573697113670, "ddate": null, "tcdate": 1573697113670, "tmdate": 1573697113670, "tddate": null, "forum": "SkxOhANKDr", "replyto": "BylE_J6vsr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Thanks + question about BPDA comparisons", "comment": "Thanks. It appears then that BPDA at least is needed for accurately evaluating adversarial robustness of your model. My understanding is that Table 2 is the main place where you compare performance against BPDA to other models. Can you clarify how many BPDA iterations was used for this table? Am I missing any other place where the BPDA comparison is performed? In general more info about the implementation of BPDA would be helpful since it is the main relevant metric in the paper.\n\nI also had a question regarding the training procedure; a contrast you make with other methods is that they perform adversarial training whereas your method does not, but it seems that the generator-discriminator algorithm used at training time is essentially doing adversarial training (but on the generator rather than the final classifier). Is this correct? Or if this is wrong could you explain why?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "BylE_J6vsr", "original": null, "number": 14, "cdate": 1573535596278, "ddate": null, "tcdate": 1573535596278, "tmdate": 1573535596278, "tddate": null, "forum": "SkxOhANKDr", "replyto": "ryea03dwoB", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Responses to Reviewer 3 comment ", "comment": "We really appreciate your insightful comment! \n\nWe forgot to plot more points on the curve. Yes, even with epsilon = 0.2, the accuracy with BPDA attack becomes 1.35%, far below 10%, according to our evaluation. However, with PGD at epsilon=0.5, our accuracy is 12.63%, which is slightly above 10% as you mentioned. This is because our nonlinear transform layer has disrupted the gradient propagation behavior of PGD. Because of this, the attacked image by PGD is messed up, instead of being totally random. When we set the epsilon to 0.6, the accuracy drops to near 10%. This shows BPDA is more effective than PGD with our defense. Hope this has addressed your concern. We will update the figure in the paper shortly to include these additional points on the curve, plus the large-epsilon BPDA curve. Thank you very much!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "ryea03dwoB", "original": null, "number": 13, "cdate": 1573518548926, "ddate": null, "tcdate": 1573518548926, "tmdate": 1573518548926, "tddate": null, "forum": "SkxOhANKDr", "replyto": "S1eBQkXmjr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Thanks + comments on figure 4", "comment": "Thank you for including these figures. If I understand figure 4 (right panel) correctly, the purported accuracy of your method at epsilon=0.4 is above 15%, and extrapolating to epsilon=0.5 it looks like it should be above 10%. However, at epsilon=0.5 an adversary could map every single image to be uniformly gray (all pixels = 0.5), which gives an upper bound on possible performance of 10% since there are 10 classes. (This is likely a very loose upper bound.) So it is concerning that the proposed method appears to overcome this fundamental limit, and calls into question the reliability of the evaluation. It would be helpful if you could comment on this."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "S1eBQkXmjr", "original": null, "number": 11, "cdate": 1573232413451, "ddate": null, "tcdate": 1573232413451, "tmdate": 1573232413451, "tddate": null, "forum": "SkxOhANKDr", "replyto": "HJgwJxIzsB", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Responses to Reviewer 3 comment", "comment": "Dear Reviewer 3, as suggested by you, we have updated the paper to include the large-iterations and large-epsilon PGD attacks of our method. We also compare our algorithm against PNI and vanilla adv. training method. Please see Figure 4. We can see that our method is able to surve the large-iteration PGD attacks and significantly outperform the other two methods. Also, our method performs much better than the other two methods during large-epsilon attacks with epsilon going to 1, as you suggested. Please review. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "B1xYZtPzoB", "original": null, "number": 10, "cdate": 1573185793025, "ddate": null, "tcdate": 1573185793025, "tmdate": 1573185793025, "tddate": null, "forum": "SkxOhANKDr", "replyto": "HJgwJxIzsB", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Responses to Reviewer 3 comment", "comment": "Dear Reviewer, Thank you so much for the quick response and appreciate your valueable suggestion! We are updating the paper to include these two figures, as you suggested. We will upload the paper soon and let you know once it is updated. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "HJgwJxIzsB", "original": null, "number": 9, "cdate": 1573179358590, "ddate": null, "tcdate": 1573179358590, "tmdate": 1573179358590, "tddate": null, "forum": "SkxOhANKDr", "replyto": "ryeM4izzsB", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Please clarify re: best pracitces", "comment": "Hello,\n\nI do not understand your response:\n\n\"As we can see from our paper, we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer.\"\n\nEither I am confused or this is false, as I brought up two explicit best practices you did not follow: a large enough number of PGD iterations, and checking that accuracy goes to zero for large epsilon. These do not seem to be in the paper, if I have missed them please tell me where they are. If you are claiming that you performed them but did not include them in the paper, then please update the paper to include them so that I can look at the numbers and re-assess.\n\n\"Figure 3 shows the large number of BPDA iterations. Due to the page limitation, we did not include the figure for larger number of PGD iterations, since BPDA is a more powerful attack method than PGD. \"\n\nBPDA is not uniformly stronger than PGD; it is more finicky, and so while it sometimes works when PGD fails it is important to also use PGD. Moreover it is not clear that 100 iterations is enough for BPDA due to its more finicky nature. In any case, I would like to see PGD vs. number of iterations with number of iterations going at least up to 100 and preferably higher.\n\n\"We could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations. \"\n\nIf you are pressed for space I recommend removing the black-box evaluation (since black-box accuracy is mostly meaningless) to make room for more careful checks of the white-box evaluation. But regardless of space limitations, the large epsilon sanity check is essential for assessing the method."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "ryeM4izzsB", "original": null, "number": 8, "cdate": 1573165866445, "ddate": null, "tcdate": 1573165866445, "tmdate": 1573165866445, "tddate": null, "forum": "SkxOhANKDr", "replyto": "BJgs00VTKS", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Responses to Reviewer 3 comment", "comment": "As we can see from our paper, we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer. Following existing papers recently published in ICLR/CVPR/ICCV/ECCV 2017-2019, we used the advTorch standard package to generate all attacks on our method.\n\nFigure 3 shows the large number of BPDA iterations. Due to the page limitation, we did not include the figure for larger number of PGD iterations, since BPDA is a more powerful attack method than PGD. \n\nWe could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations. \n\nFollowing recently published paper, we have included the most comprehensive performance comparison results in the paper. We have demonstrated  that our method significantly outperforms existing state-of-the-art methods."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "rklNHwMGsS", "original": null, "number": 6, "cdate": 1573164860068, "ddate": null, "tcdate": 1573164860068, "tmdate": 1573164860068, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SkxWHoIRYH", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We sincerely thank the Reviewer for the positive feedback and high recommendation of our paper!\n\nThanks for pointing out this! In this paper, we use the benchmark datasets and compare our results against the results published in existing papers. \n\nUnfortunately, not all published papers provided complete results for both of these datasets. For example, some papers did not have results on SVHN. In  this case, we left them empty. \n\nWe chose not to re-implement existing methods since it will be very hard to re-produce their exact results, which might lead to unfair performance comparisons.  \n\nDuring our experiments, we tried our best to compare with as many methods as possible if they have published results on the dataset."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "rJgRrNMfoB", "original": null, "number": 4, "cdate": 1573164102076, "ddate": null, "tcdate": 1573164102076, "tmdate": 1573164102076, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SygPDfaXcr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to Reviewer 1 Comments", "comment": "We sincerely thank Reviewer 1 for the positive and encouraging comments!\n\nThe reviewer suggests that we need to provide theoretical analysis or proof why the proposed deep neural network defense method is working. We must admit that this is really hard. The deep learning research community is still working very hard to establish theoretical analysis or proof for deep neural networks, which is however is very challenging. \n\nHowever, our proposed method is based on data-driven deep learning. All the defense networks are trained based on the loss functions. So, if the loss function converges, the proposed method is achieving the target performance. \n\nFollowing existing state-of-the-art methods published in recent ICLR/CVPR/ICCV/ECCV papers, we use the benchmark their datasets and standard evaluation protocols to demonstrate that our proposed method is significantly outperforming existing methods.  \n\nFor the experiments, we have results on two datasets, CIFAR-10 and SVHN with two different attack modes: white-box attack and black-box attacks. Sorry for the confusion. We will better organize these experimental results.\n\nFor the attacks, we follow the standard procedure used in all existing methods. Specifically, we use advTorch evaluation package to generate all attacks. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxOhANKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1361/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1361/Authors|ICLR.cc/2020/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157162, "tmdate": 1576860546544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Authors", "ICLR.cc/2020/Conference/Paper1361/Reviewers", "ICLR.cc/2020/Conference/Paper1361/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Comment"}}}, {"id": "BJgs00VTKS", "original": null, "number": 1, "cdate": 1571798739180, "ddate": null, "tcdate": 1571798739180, "tmdate": 1572972478919, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method for adversarial defense based on generative cleaning.\n\nThe paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers:\n\"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705\n\"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420\n\nFor instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes.\n\nIn the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected."}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575707280453, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Reviewers"], "noninvitees": [], "tcdate": 1570237738496, "tmdate": 1575707280466, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review"}}}, {"id": "SkxWHoIRYH", "original": null, "number": 2, "cdate": 1571871545458, "ddate": null, "tcdate": 1571871545458, "tmdate": 1572972478877, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new method to defend a neural network agains adversarial attacks (both white-box and black-box attacks). By jointly training a Generative Cleaning Network with quantized nonlinear transform, and a Detector Network, the proposed cleans the incoming attacked image and correctly classifies its true label. The authors use state-of-the-art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods.\n\nComment:\nIs there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR-10?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575707280453, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Reviewers"], "noninvitees": [], "tcdate": 1570237738496, "tmdate": 1575707280466, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review"}}}, {"id": "SygPDfaXcr", "original": null, "number": 3, "cdate": 1572225631254, "ddate": null, "tcdate": 1572225631254, "tmdate": 1572972478827, "tddate": null, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "invitation": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. \n\nA few comments: \n\n1. It does not provide theoretical reasons why the prosed method can defend against those attacks. \n\n2. The experiments are a bit messy and the attacks' setup need to improve. \n\n3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1361/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "pdf": "/pdf/bd73331d720dfaa329bb398f5396dd007e3cb09c.pdf", "TL;DR": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"], "paperhash": "yuan|generative_cleaning_networks_with_quantized_nonlinear_transform_for_deep_neural_network_defense", "original_pdf": "/attachment/9cf5e693939ca0240784a26f6da325d436b92ca5.pdf", "_bibtex": "@misc{\nyuan2020generative,\ntitle={Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense},\nauthor={Jianhe Yuan and Zhihai He},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxOhANKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxOhANKDr", "replyto": "SkxOhANKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575707280453, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1361/Reviewers"], "noninvitees": [], "tcdate": 1570237738496, "tmdate": 1575707280466, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1361/-/Official_Review"}}}], "count": 15}