{"notes": [{"id": "S1gDCiCqtQ", "original": "Skgcrnp5FQ", "number": 904, "cdate": 1538087887203, "ddate": null, "tcdate": 1538087887203, "tmdate": 1545355385130, "tddate": null, "forum": "S1gDCiCqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning", "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma's Revenge.\n", "keywords": ["Reinforcement Learning", "Model-Free Hierarchical Reinforcement Learning", "Subgoal Discovery", "Unsupervised Learning", "Temporal Difference", "Temporal Abstraction", "Intrinsic Motivation", "Markov Decision Processes", "Deep Reinforcement Learning", "Optimization"], "authorids": ["jrafatiheravi@ucmerced.edu", "dnoelle@ucmerced.edu"], "authors": ["Jacob Rafati", "David Noelle"], "TL;DR": "We offer an original approach to model-free deep hierarchical reinforcement learning, including unsupervised subgoal discovery and unified temporal abstraction and intrinsic motivation learning. ", "pdf": "/pdf/e4b525030cb6bdfaf77fc3492532d0ba948049a4.pdf", "paperhash": "rafati|learning_representations_in_modelfree_hierarchical_reinforcement_learning", "_bibtex": "@misc{\nrafati2019learning,\ntitle={Learning Representations in Model-Free Hierarchical Reinforcement Learning},\nauthor={Jacob Rafati and David Noelle},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gDCiCqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJlpWINWgV", "original": null, "number": 1, "cdate": 1544795653199, "ddate": null, "tcdate": 1544795653199, "tmdate": 1545354524994, "tddate": null, "forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper904/Meta_Review", "content": {"metareview": "Pros:\n- good results on Montezuma\n\nCons:\n- moderate novelty\n- questionable generalization\n- lack of ablations and analysis\n- lack of stronger baselines\n- no rebuttal\n\n\nThe reviewers agree that the paper should be rejected in its current form, and the authors have not bothered revising it to take into account the detailed reviews.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper904/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper904/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning", "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma's Revenge.\n", "keywords": ["Reinforcement Learning", "Model-Free Hierarchical Reinforcement Learning", "Subgoal Discovery", "Unsupervised Learning", "Temporal Difference", "Temporal Abstraction", "Intrinsic Motivation", "Markov Decision Processes", "Deep Reinforcement Learning", "Optimization"], "authorids": ["jrafatiheravi@ucmerced.edu", "dnoelle@ucmerced.edu"], "authors": ["Jacob Rafati", "David Noelle"], "TL;DR": "We offer an original approach to model-free deep hierarchical reinforcement learning, including unsupervised subgoal discovery and unified temporal abstraction and intrinsic motivation learning. ", "pdf": "/pdf/e4b525030cb6bdfaf77fc3492532d0ba948049a4.pdf", "paperhash": "rafati|learning_representations_in_modelfree_hierarchical_reinforcement_learning", "_bibtex": "@misc{\nrafati2019learning,\ntitle={Learning Representations in Model-Free Hierarchical Reinforcement Learning},\nauthor={Jacob Rafati and David Noelle},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gDCiCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper904/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353042268, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper904/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper904/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper904/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353042268}}}, {"id": "BJexixtqhX", "original": null, "number": 3, "cdate": 1541210264072, "ddate": null, "tcdate": 1541210264072, "tmdate": 1541533591884, "tddate": null, "forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "content": {"title": "A simple ideas works well in a challenging RL domain. The generalizability and significance can be improved if more domains can be tested", "review": "This paper proposed a model-free HRL method, which is combined with unsupervised learning methods, including abnormality discovery and clustering for subgoal discovery. In all, this paper studies a very important problem in RL and is easy to follow. The technique is sound. Although the novelty is not that significant (combining existing techniques), it showed good results on Montezuma\u2019 revenge, which is considered as a very challenging  problem for primitive action based RL.\n\nAlthough the results are impressive, I still have some doubt about the generalizability of the method. It might be helpful to improve its significance if more diversified domains can be tested.\n\nThe paper can be strengthen by providing some ablation test, for example, is performance under different K for Kmeans? \n\nAlso some important details seems missing, for example, the data used for kmeans, it is mentioned that the input to the controller is four consecutive frame of size 84x84, so the input data dimension is more than 10k, I guess some further dimensionality reduction technique has to be applied in order to run kmeans effectively.\n\nRegarding the comparisons, the proposed method is only compared with one primitive action based method. It might be better to include results from other HRL methods, such as Kulkarni et al.\n\nIs the curve based on the mean of different runs? It might be useful to include an errorbar to show the statistical significance.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper904/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning", "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma's Revenge.\n", "keywords": ["Reinforcement Learning", "Model-Free Hierarchical Reinforcement Learning", "Subgoal Discovery", "Unsupervised Learning", "Temporal Difference", "Temporal Abstraction", "Intrinsic Motivation", "Markov Decision Processes", "Deep Reinforcement Learning", "Optimization"], "authorids": ["jrafatiheravi@ucmerced.edu", "dnoelle@ucmerced.edu"], "authors": ["Jacob Rafati", "David Noelle"], "TL;DR": "We offer an original approach to model-free deep hierarchical reinforcement learning, including unsupervised subgoal discovery and unified temporal abstraction and intrinsic motivation learning. ", "pdf": "/pdf/e4b525030cb6bdfaf77fc3492532d0ba948049a4.pdf", "paperhash": "rafati|learning_representations_in_modelfree_hierarchical_reinforcement_learning", "_bibtex": "@misc{\nrafati2019learning,\ntitle={Learning Representations in Model-Free Hierarchical Reinforcement Learning},\nauthor={Jacob Rafati and David Noelle},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gDCiCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "cdate": 1542234350239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper904/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829610, "tmdate": 1552335829610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper904/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxOFb1937", "original": null, "number": 2, "cdate": 1541169536308, "ddate": null, "tcdate": 1541169536308, "tmdate": 1541533591676, "tddate": null, "forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "content": {"title": "The methods seem somewhat tailored for the tasks and the results on the harder problem are not that convincing.", "review": "Summary:\nThe authors propose an HRL system which learns subgoals based on unsupervised analysis of recent trajectories. The subgoals are found via anomaly/outlier detection (in this case states with a very high reward) and the clustering together of states that are very similar. The system is evaluated on the 4-rooms task and on the atari game Montezuma\u2019s Revenge.\n\nThe paper cites relevant work and provides a nice explanation of subgoal-based HRL. The paper is for the most part well-written and easy to follow. \n\nThe experiments are unfortunately not making a very convincing case for the general applicability of the the methods. While the system does not employ a model of the environment, k-means clustering based on distances seems to be particularly well-suited for the two environments investigated in the paper. It is known that the 4-rooms experiment is much easier to solve with subgoals that correspond to the rooms themselves. I can only conclude from this experiment that k-means can find those subgoals given the right number (4) of clusters and injecting the knowledge that distances in grid-worlds correlate well with transition probabilities. Similarly, the use of distance-based clustering seems well-suited for games with different rooms like Montezuma\u2019s Revenge but that might not generalize to many other games. \n\nThe anomaly detection subgoal discovery is interesting as a method to speed-up learning but it still requires these (potentially sparse) high reward states to be found first. For tasks with sparse rewards it does make sense to set high reward states as potential subgoals instead of waiting for value to propagate. That said, the reward for the lower level policy is only less sparse in the sense that wasting time gets punished with a negative reward. Subgoal discovery based on rewards should probably also take the ability of the current policy to obtain those rewards into account like some other methods for subgoal discovery do (see for example Florensa et al., 2018). The authors mention that the subgoals were manually chosen by Kulkarni et al. (2016) instead of learned in an unsupervised way but I don\u2019t think that the visual object detection method employed there is that much more problem specific. \n\nLike Kulkarni et al. (2016), the authors compare their method with DQN (Mnih et al. 2015) but it was already known that that baseline cannot solve the task at all and a lot more results on Montezuma\u2019s Revenge have been published since then. A more insightful baseline would have been to compare with at least some other HRL methods that are able to learn the task to some extend like perhaps Feudal Networks (Vezhnevets et al., 2017). Looking at the graph in the Feudal Networks paper for comparison, the results in this paper seem to be on par with the LSTM baseline there but it is hard to compare this on the basis of the number of episodes. Did the reward go up further after running the experiment longer? \n\nSince the results are not that spectacular and a comparison with prior work is lacking, the main contributions of the paper are more conceptual. I think that it is interesting to think more carefully about how sparse reward states and state similarities can be used more efficiently but the ideas in the paper are not original or theoretically founded enough to have a lot of impact without the company of stronger empirical results.\n\nExtra reference:\nCarlos Florensa, David Held, Xinyang Geng, Pieter Abbeel. (2017). Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper904/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning", "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma's Revenge.\n", "keywords": ["Reinforcement Learning", "Model-Free Hierarchical Reinforcement Learning", "Subgoal Discovery", "Unsupervised Learning", "Temporal Difference", "Temporal Abstraction", "Intrinsic Motivation", "Markov Decision Processes", "Deep Reinforcement Learning", "Optimization"], "authorids": ["jrafatiheravi@ucmerced.edu", "dnoelle@ucmerced.edu"], "authors": ["Jacob Rafati", "David Noelle"], "TL;DR": "We offer an original approach to model-free deep hierarchical reinforcement learning, including unsupervised subgoal discovery and unified temporal abstraction and intrinsic motivation learning. ", "pdf": "/pdf/e4b525030cb6bdfaf77fc3492532d0ba948049a4.pdf", "paperhash": "rafati|learning_representations_in_modelfree_hierarchical_reinforcement_learning", "_bibtex": "@misc{\nrafati2019learning,\ntitle={Learning Representations in Model-Free Hierarchical Reinforcement Learning},\nauthor={Jacob Rafati and David Noelle},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gDCiCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "cdate": 1542234350239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper904/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829610, "tmdate": 1552335829610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper904/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eU-a3K3X", "original": null, "number": 1, "cdate": 1541160190366, "ddate": null, "tcdate": 1541160190366, "tmdate": 1541533591432, "tddate": null, "forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "content": {"title": "Review", "review": "This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of \u201cinteresting\u201d states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.\n\nClarity:\nI found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? It\u2019s not clear from the algorithm box. Some important details are also left out. The section on Montezuma\u2019s Revenge mentioned that the goal set was initialized using a \u201ccustom edge detection algorithm\u201d. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I can\u2019t find it anywhere in the paper. Omissions like this make the method completely unreproducible. \n\nNovelty:\nThe idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of \u201cDynamic Abstraction in Reinforcement Learning via Clustering\u201d by Mannor et al. or of \u201cLearning Options in Reinforcement Learning\u201d by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.\n\nSignificance:\nI was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezuma\u2019s Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, it\u2019s not clear how this method could address scalability issues. Repeated clustering could become expensive and it\u2019s not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.\n\nOverall quality:\nThe proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) I\u2019m not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.\n\nMinor comments:\n- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.\n- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I don\u2019t understand what it means for a problem to have scaling issues.\n- Please check your usage of \\cite and \\citep. Some citations are in the wrong format.\n- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\\gamma max Q) are held fixed in Q-learning.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper904/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning", "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma's Revenge.\n", "keywords": ["Reinforcement Learning", "Model-Free Hierarchical Reinforcement Learning", "Subgoal Discovery", "Unsupervised Learning", "Temporal Difference", "Temporal Abstraction", "Intrinsic Motivation", "Markov Decision Processes", "Deep Reinforcement Learning", "Optimization"], "authorids": ["jrafatiheravi@ucmerced.edu", "dnoelle@ucmerced.edu"], "authors": ["Jacob Rafati", "David Noelle"], "TL;DR": "We offer an original approach to model-free deep hierarchical reinforcement learning, including unsupervised subgoal discovery and unified temporal abstraction and intrinsic motivation learning. ", "pdf": "/pdf/e4b525030cb6bdfaf77fc3492532d0ba948049a4.pdf", "paperhash": "rafati|learning_representations_in_modelfree_hierarchical_reinforcement_learning", "_bibtex": "@misc{\nrafati2019learning,\ntitle={Learning Representations in Model-Free Hierarchical Reinforcement Learning},\nauthor={Jacob Rafati and David Noelle},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gDCiCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper904/Official_Review", "cdate": 1542234350239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gDCiCqtQ", "replyto": "S1gDCiCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper904/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829610, "tmdate": 1552335829610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper904/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}