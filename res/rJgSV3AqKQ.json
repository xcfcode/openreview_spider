{"notes": [{"id": "rJgSV3AqKQ", "original": "rJx187qYYX", "number": 1447, "cdate": 1538087980956, "ddate": null, "tcdate": 1538087980956, "tmdate": 1545355409857, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rylaDK7klE", "original": null, "number": 1, "cdate": 1544661349280, "ddate": null, "tcdate": 1544661349280, "tmdate": 1545354504278, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Meta_Review", "content": {"metareview": "The paper is a premature submission that needs significant improvement in terms of conceptual, theoretical, and empirical aspects.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1447/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352834496, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1447/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1447/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352834496}}}, {"id": "Hkxi1iUDpX", "original": null, "number": 3, "cdate": 1542052579085, "ddate": null, "tcdate": 1542052579085, "tmdate": 1542226126456, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rygZgpNBpm", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "content": {"title": "Thanks for the constructive feedback", "comment": "First, the authors would like to thank you for the time given to reviewing this paper and the constructive comments you are offering. We will take them into account for future submission.\n\n1. , 4. , 5.  -  These are relevant suggestions and will be followed for the future version.\n\n2. By \"true benefit\", we meant the gain in performance of Hypergradient when its hyperparameters are tuned more carefully (as we proved in the paper, hypergradient can significantly benefit from such tuning). However, we do agree this claim could be formulated in a more consistent way w.r.t to our results.\n\n3. As in 2., the sensitivity analysis was only empirical. We will investigate a large set of experimental settings to support our observations.\n\n6. Robust Implicit Backpropagation (Fagan & Iyengar, 2018)  is offering ideas that can perfectly fit in the landscape of this study. In fact, Implicit Backpropagation (IB) approximates the update of Implicit Stochastic Gradient Descent which is known to be stable and robust to learning rate. This makes it a good candidate to consider in an investigation like the one we are conducting, in order to check how IB compares to adaptive gradient methods and the various learning rate schedules we are considering. More specifically, IB seems to be very efficient for recurrent models. Since, we are planning to extend our investigation to tasks that correspond to recurrent models (e.g. language modelling), IB would definitely be a good method to compare to. Thank you again for sharing this reference. We will be considering it in a future version eventually.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626435, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgSV3AqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1447/Authors|ICLR.cc/2019/Conference/Paper1447/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626435}}}, {"id": "rygZgpNBpm", "original": null, "number": 1, "cdate": 1541913832595, "ddate": null, "tcdate": 1541913832595, "tmdate": 1541913832595, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Public_Comment", "content": {"comment": "On the positive side this paper performs several interesting experiments comparing various learning rate tuning algorithms. \nThe paper also spends time on sensitivity/robustness, which has not received adequate attention in the literature.\nHowever, I am afraid there is no technical or methodological contribution from this paper that meets the ICLR standards.\n\nSome feedback which will hopefully help in future submission:\n\n1. Use clear definitions and notation to introduce methods. Currently, all methods are only described in words, and this creates confusion, especially for the \"hypergradient method\" that is new.\n\n2. Be consistent about claims and results presented in the paper. For example, in the Abstract the claim is \"We analyze the true benefit of these hypergradient methods...\"  but not such analysis is presented. If your goal is experimentation and not analysis it is better to make that clear early.\n\n3. As above, there is no \"sensitivity analysis\" offered in this paper. I do think this is an important subject and I applaud the authors for focusing on that. However, currently in the paper there are only experimental results and simulations. \nThere are limitations with the experiments as well. In Figures 4-5-6 we only get some plots on how the train error depends on the learning rate on some particular datasets. A deeper investigation would be helpful here as to why we see the results we see, so as to substantiate claims such as \" Figure 4 shows the performance of SGD and SGDN worsens faster when increasing\nthe learning rate than when decreasing it.\" (page 6) It would be nice to get such general results, but this requires a deeper and more thorough investigation, whereas currently the evidence may be circumstantial. \n\n4. Section 2.1 is a good place to start introducing notation. Although the referenced methods are known, it helps to lay out some notation so that readers have a clear idea what the authors have in mind.\n\n5. Similar to point #1: p2 \"but this method seems to work better in practice.\" Blanket statements are hard to accept without solid arguments. What does \"better\" mean here and what does \"in practice\" mean? Overall, the authors should avoid such statements without presenting solid evidence. Another example in p5: \"It is interesting to see that, by using the optimization dynamics in an online fashion, one can recover the training performance of a carefully tuned decay schedule.\"\n\n6. About sensitivity analysis the authors could also look into (Robust Implicit Backpropagation, Fagan & Iyengar, 2018) where the authors use implicit methods to stabilize fitting algorithms for neural networks. Could the ideas in that paper apply here?", "title": "Review"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311595208, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJgSV3AqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311595208}}}, {"id": "B1O9N17MTQ", "original": null, "number": 2, "cdate": 1541709618020, "ddate": null, "tcdate": 1541709618020, "tmdate": 1541709618020, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "content": {"title": "Thank you for your valuable comments", "comment": "We would like to thank the reviewers for their time. We will take their comments into account for a future version of this work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626435, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgSV3AqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1447/Authors|ICLR.cc/2019/Conference/Paper1447/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626435}}}, {"id": "SkgupY1-p7", "original": null, "number": 3, "cdate": 1541630400008, "ddate": null, "tcdate": 1541630400008, "tmdate": 1541630400008, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "content": {"title": "A technical report rather than a research paper", "review": "General:\nIn general, this looks like a technical report rather than a research paper to me. Most parts of the paper are about the empirical analysis of adaptive algorithms and hyper-gradient methods. The contribution of the paper itself is not sufficient to be accepted.\n\nPossible Improvements:\n1. The study of such optimization problem should consider incorporating mathematics analysis with necessary proof. e.g. show the convergence rate under specific constraints. Even the paper is based on others' work, the author(s) could have extended their work by giving stronger theory analysis or experiment results.\n2. Since this is an experimental-based paper, besides CIFAR10 and MNIST data sets, the result would be more convincing if the experiments were also done on ImageNet(probably should also try deeper neural networks).\n3. The sensitivity study is interesting but the experiment results are not very meaningful to me. It would be better if the author(s) gave a more detailed analysis.\n4. The paper could be more consistent. i.e. emphasize the contribution of your own work and be more logical. I might miss something, but I feel quite confused about what is the main idea after reading the paper. \n\nConclusion:\nI believe the paper has not reached the standard of ICLR. Although we need such paper to provide analysis towards existing methods, the paper itself is not strong enough.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "cdate": 1542234227676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335948805, "tmdate": 1552335948805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkgyx6Cg67", "original": null, "number": 1, "cdate": 1541627110786, "ddate": null, "tcdate": 1541627110786, "tmdate": 1541627110786, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "SyeuS3Ae6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "content": {"title": "my apologies", "comment": "Sorry, I  meant to erase the comment \"which is self-apparently important\", which isn't appropriate and doesn't make sense."}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1447/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626435, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgSV3AqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1447/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1447/Authors|ICLR.cc/2019/Conference/Paper1447/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers", "ICLR.cc/2019/Conference/Paper1447/Authors", "ICLR.cc/2019/Conference/Paper1447/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626435}}}, {"id": "SyeuS3Ae6X", "original": null, "number": 2, "cdate": 1541626944176, "ddate": null, "tcdate": 1541626944176, "tmdate": 1541626944176, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "content": {"title": "incremental empirical contribution", "review": "Clarity: Below average\n- The introduction would be easier to follow if you named Baydin's approach and your own approach, because in the 2-4 bullet points you say \"this online scheme\", and \"the learning rate schedule\", without being perfectly clear what you are talking about\n- The last sentence of the introduction is meant to clearly state your hypothesis, so I was expecting \"emphasize the value of *\", i.e. either adaptive or non-adaptive methods, rather than just general 'tuning', which is self-apparently important.\n\nQuality: Below average\nThis is a purely empirical study that does not go too deep. It is not quite a review paper, but only compares previous methods.\n\nPros:\nI especially appreciate the sensitivity analysis, ie Fig 6. If only all ML papers had something like this to suggest the difficulty of setting hyperparameters for their proposed methods.\n\nCons:\n- You should use mathematics to describe what you are talking about with adaptive stepsize in Sec 2.1. \"these methods multiply the gradient with a matrix\". Just giving one equation would be extremely helpful.\n- If I understand correctly, you are interpreting the inverse-Hessian as used in Newton's method and other non-diagonal 'gradient conditioners' as types of stepsize. This is definitely interesting, but again it would be very simple to see what you are saying with an equation instead of starting with the phrase \"stepsize\" which is generally understood to be a scalar multiple on the gradient.\n- I'm surprised you jump right into experiements after your background settings. It's apparent that this paper fundamentally relies on the Wilson (2017) hypergradient paper. Your paper should be more self-contained: 'hypergradient' is not even defined in this paper, is it?...\n\nEspecially:\nHow do you know that if you change the model architecture, data, and loss, that a similar result will occur? I imagine that it heavily relies on the data and model-- in other words, that the sensitivity is dependent on \"how an algorithm reacts to a certain data/loss/model landscape\". I'm trying to say that I'm not convinced these results generalize to any other situation than the one presented here (so does it really say anything about the different stepsize selection rules?)\n\nRandom side note:\nSince your appendix is only a few lines, you could consider succinctly listing learning rates with set notation, for example {1e-n,5e-n : -5<n<1}.", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "cdate": 1542234227676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335948805, "tmdate": 1552335948805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxfqdZq3Q", "original": null, "number": 1, "cdate": 1541179530057, "ddate": null, "tcdate": 1541179530057, "tmdate": 1541533125050, "tddate": null, "forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "content": {"title": "An emperical study on several methods for adjusting learning rate ", "review": "The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). \n\nThough it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. \n  \nOn page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?\n\nThe URL in References looks out of bound. \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1447/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining adaptive algorithms and hypergradient method: a performance and robustness study", "abstract": "Wilson et al. (2017) showed that, when the stepsize schedule is properly designed, stochastic gradient generalizes better than ADAM (Kingma & Ba, 2014). In light of recent work on hypergradient methods (Baydin et al., 2018), we revisit these claims to see if such methods close the gap between the most popular optimizers. As a byproduct, we analyze the true benefit of these hypergradient methods compared to more classical schedules, such as the fixed decay of Wilson et al. (2017). In particular, we observe they are of marginal help since their performance varies significantly when tuning their hyperparameters. Finally, as robustness is a critical quality of an optimizer, we provide a sensitivity analysis of these gradient based optimizers to assess how challenging their tuning is.", "keywords": ["optimization", "adaptive methods", "learning rate decay"], "authorids": ["akram.er-raqabi@umontreal.ca", "nicolas@le-roux.name"], "authors": ["Akram Erraqabi", "Nicolas Le Roux"], "TL;DR": "We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis.", "pdf": "/pdf/8ecf4971fff5e08d16f8b3a45e73f7ad52932a84.pdf", "paperhash": "erraqabi|combining_adaptive_algorithms_and_hypergradient_method_a_performance_and_robustness_study", "_bibtex": "@misc{\nerraqabi2019combining,\ntitle={Combining adaptive algorithms and hypergradient method: a performance and robustness study},\nauthor={Akram Erraqabi and Nicolas Le Roux},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgSV3AqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1447/Official_Review", "cdate": 1542234227676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgSV3AqKQ", "replyto": "rJgSV3AqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1447/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335948805, "tmdate": 1552335948805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1447/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}