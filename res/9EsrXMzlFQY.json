{"notes": [{"id": "9EsrXMzlFQY", "original": "znAhoSS3v9j", "number": 874, "cdate": 1601308100433, "ddate": null, "tcdate": 1601308100433, "tmdate": 1613012624173, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DIqTWYDHcpw", "original": null, "number": 1, "cdate": 1610040419813, "ddate": null, "tcdate": 1610040419813, "tmdate": 1610474018440, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040419798, "tmdate": 1610474018422, "id": "ICLR.cc/2021/Conference/Paper874/-/Decision"}}}, {"id": "VzUVDbqkawR", "original": null, "number": 3, "cdate": 1605721017781, "ddate": null, "tcdate": 1605721017781, "tmdate": 1606256121061, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers for their time and valuable feedback. We provide individual responses addressing the points of each review below, highlighting the changes we made in the revision to reflect these points. **We include an extra pdf file (additionl_results.pdf) in our supplement (.zip) for showing additional figures and tables.**"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "pcwu96p0-10", "original": null, "number": 5, "cdate": 1605723417753, "ddate": null, "tcdate": 1605723417753, "tmdate": 1606189816377, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "nZvQrR9p6VE", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "**1. In p.1, is there any condition on the comparison, e.g., m \u226a n, required in the introduction?**\n* Async-RED and its convergence analysis are independent of the specifics in the relationship between m and n. Compressive sensing (where indeed m \u226a n) is one of many possible applications of Async-RED. However, the method is also applicable to problems where m \u2265 n, which has been mentioned in the updated manuscript.\n\n**2. The denoised version $D_\\sigma(x^\\ast)$ by some image denoiser essentially provides a more accurate estimate of $x^\\ast$. Can this be replaced by other similar operators? Also, in the compressive sensing, the recovered image $x^\\ast$ at the first few iterations may not be good enough, will the application of this operator make it worse? Does the parameter \u03c3 need to tune or update dynamically in the iterations?**\n* The revised manuscript mentions that Async-RED and its theoretical analysis are not restricted to denoising operators $D_\\sigma$. Our theoretical analysis holds for any nonexpansive operator, which includes the traditional proximal operators (e.g., soft-thresholding) and the more recent artifact-removal operators [1,2].\n* The usage of the denoiser within iterations of our algorithm is similar to the usage of the soft-thresholding operator for l1-minimization in compressive sensing [3]. Thus, by using the operator in every iteration, Async-RED improves the solution compared to the scenario where such an operator is not used.\n* In the experiment section, we explicitly state that the parameter $\\sigma$ is fixed across all iterations. Its value is fine-tuned for the best SNR performance (see details in Section 5), similarly to the way the regularization parameter is fine-tuned in traditional sparse recovery and compressive sensing.\n\n**3. There seems to be some mixup between BC-RED and GM-RED throughout. Are these different methods?**\n* Thanks for catching this. Indeed, BC-RED and GM-RED are different methods. This has been fixed in the updated manuscript.\n\n**4. In Alg.1-2, the two operators read(\u00b7) and minibatch(\u00b7) should be explicitly defined.**\n* We have explicitly defined the two operators in the updated manuscript.\n\n**5. In the numerical experiments, discussion on the influence of the block/minibatch size on the performance could be strengthened. In the Async-RED-SG, how many trials were conducted to get an average performance? Robustness to the noise could be analyzed.**\n* We performed additional experiments that have been included in the supplement. Table 1 (see additional_results.pdf in the supplement) summarizes the SNR values obtained for three minibatch sizes {60, 80, 120}. Async-RED achieves almost the same SNR values under these settings. Table 2 (see additional_results.pdf in the supplement) summarizes the SNR values for three minibatch sizes {1120, 2240, 3360}, which corresponds to 1/4, 1/2, and 3/4 of the full batch. As minibatch size increases, the final SNR performance improves, which is consistent with our theory. The revised manuscript explicitly mentions that the average performance in the manuscript is obtained by running a single trial for each image. We do not expect that running multiple trials would influence our results. We would like to highlight that our theoretical results stay valid for any amount of measurement noise. This is the reason we did not perform specific experiments on noise robustness, as they would not directly relate to our contributions. We include this discussion in the supplement.\n\n**Reference:**\n\n[1] K. Zhang, W. Zuo, and L. Zhang. Deep plug-and-play super-resolution for arbitrary blur kernels. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pages 1671\u20131681, Long Beach, CA, USA, June 2019.\n\n[2] J. Liu, Y. Sun, C. Eldeniz, W. Gan, H. An, and U. S. Kamilov. Rare: Image reconstruction using deep priors learned without groundtruth. IEEE Journal of Selected Topics in Signal Processing, 14(6):1088\u20131099, 2020.\n\n[3] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. Pure Appl. Math., 57(11):1413\u20131457, November 2004."}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "bCwDBH_jYR-", "original": null, "number": 8, "cdate": 1605725112283, "ddate": null, "tcdate": 1605725112283, "tmdate": 1606189781060, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "lPb5BfYp0JQ", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "**1. After Assumption 4: when mentioning the literature, I think you should include the paper \u201cBUILDING FIRMLY NONEXPANSIVE CONVOLUTIONAL NEURAL NETWORKS\u201d by Terris et al. and shortly discuss the relationship.**\n* We cite the paper in the updated manuscript.\n\n**2. In particular, nonexpansiveness is sufficient in your setting, whereas it is usually not sufficient in\noptimization, with averagedness/firm nonexpansiveness assumed. Why is it so?**\n* This is the direct consequence of the way we formulate our operators. If denoiser $D_\\sigma$ is nonexpansive, then according to Lemma 1 in Supplement B.1, we know that the composite operator $G := I \u2212 D_\\sigma$ is $1/(L + 2\\tau)$-cocoercive. Consider the inequality in Eq. 20 in Supplement B.1,  where the cross term $\\dagger$ can be bounded by using the cocoercivity of $G$. The expression of the final bound can be found in Eq. 26 in Supplement B.1. Once we obtain the bound for the expectation, we then can take the total expectation and apply the telescopic-sum over 1,...,t iterations to establish the convergence (Eq. 27-Eq. 39). Similar formulations can be found in the analysis of BC-RED.\n\n**3. About asynchronous optimization, Franck Iutzeler has his PhD thesis and several papers on the topic. You might have a look and cite some of them, which are relevant in your setting.**\n* We have reviewed several papers by Franck Lutzeler and cite them in the updated manuscript.\n\n**4. Theorem 2 shows that the \u201cconvergence\u201d is not variance-reduced. I would appreciate a discussion on whether this is unavoidable or if this is an open question for future work, if there is relevant literature on this matter, and why it is difficult to derive a variance-reduced approach with similar features.**\n* We have added the discussion on p.6 of the revised manuscript.\n\n**5. The DnCNN architecture is used in the experiments: does it satisfy Assumption 4? More generally, are all assumptions met so that the theorems apply in the experiments? You should discuss the match between your theoretical results and the conditions of the practical experiments more closely.**\n* In Supplement D.1, we provide more technical details on the DnCNN architecture and its training strategy. We adopt the spectral normalization technique from [1] to control the Lipschitz constant (LC) of our DnCNN prior. In the training, we constrain the residual network $R_\\sigma$ (see Fig. 5 in the Supplement) such that its LC is smaller than 2. Since the non-expansiveness of $D_\\sigma$ implies that R\u03c3 has LC \u2264 2, this provides a necessary condition for $D_\\sigma$ to satisfy Assumption 4. The effectiveness of this strategy was also discussed in BC-RED paper, where it was shown that without the constraint on LC of DnCNN the algorithm can diverge. Finally, note that all the other assumptions in our analysis are also satisfied in our experiments, which has been explicitly stated in the revision.\n\n**Reference**\n\n[1] H. Sedghi, V. Gupta, and P. M. Long. The singular values of convolutional layers. In International Conference on Learning Representations, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "rz4C_UTOGal", "original": null, "number": 6, "cdate": 1605724395577, "ddate": null, "tcdate": 1605724395577, "tmdate": 1606189704651, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "_TNlO25Dqw0", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "**1. The proposed Async-RED methods apply the denoiser on blocks of the image, while the original RED applies the denoiser on the whole image. Intuitively, such a scheme may be suboptimal and somewhat compromise the recovery performance, since state-of-the-art DNN denoisers utilize non-local similarity across the image. Although the experiments in this work do not demonstrate such deficit, the reviewer suspects that, for some images, the Async-RED may not do well on the pixels near the edges of the blocks. The reviewer is a bit dubious about whether such type of block decomposition of denoiser in this line of work can be universally reliable.**\n* This is an excellent remark worth a discussion in the revised manuscript. Let us start by mentioning that the block-wise nature of Async-RED is by no means universally applicable; however, the algorithm and its analysis are compatible with a variety of block decomposition strategies (e.g. by rows or columns of the image) that might be useful for different applications. This feature is consistent with the traditional block coordinate methods [1]. We would also like to point out that a simple strategy for mitigating artifacts at the block edges is to include additional neighboring pixels at the input of the DNN (but use the exact block size at the output). Table 3 (see additional_results.pdf in the supplement) reports our results of experimenting with this idea of input padding. The results indicate that by including a small number of pixels around each block at the input of DNN, one can match the performance of using the full image at the input of DNN. This is included in the supplement in the revision.\n\n**2. The comparative results shown in Fig 3 regarding compressed-sensing reconstruction appear to be somewhat unconvincing: Firstly, the authors use a suboptimal Gm-RED/Sync-RED for baseline, which is gradient descent RED without Nesterov acceleration, while it is well-known that the accelerated gradient descent RED (AGD-RED) converges at a much better rate and the reviewer believes that this is a more sensible baseline. Maybe an important advantage of original RED and synchronous RED is that they are easier to be accelerated by momentum tricks?**\n* Nesterov acceleration is an excellent strategy for improving the convergence rate in optimization; yet, we would like to highlight that it is complementary to our key contributions since it does not directly address the scalability of an algorithm on multicore systems. A great direction for future research would be to equip Async-RED with Nesterov acceleration (as was done in [2] for traditional asynchronous block-coordinate algorithms) to better understand the tradeoffs between momentum and scalability in multicore systems. On the other hand, the direct comparison between AGD-RED and Async-RED would be misleading since they correspond to different classes of algorithms (a close analogy would be the comparison between the vanilla SGD and vanilla AGD, with the former being faster on large-enough problems and the latter being faster on small-enough problems). We have added the discussion on Nesterov acceleration in the revision.\n\n**3. Secondly, Async-RED-SG seems to yield very limited acceleration in terms of convergence rate over the deterministic counterpart Async-RED-BG in CS example \u2013 what is the minibatch size used in this experiment? Would reducing the minibatch size yield a faster convergence rate? Meanwhile, the recovery result of Async-RED-SG is 1dB worse than the other methods, which seems a bit problematic (would a shrinking step-size help to fix this issue?).**\n* This observation highlights the tradeoff between the convergence rate and accuracy in stochastic optimization [3]. Our experiment runs Async-RED-SG with the minibatch 1120. A smaller minibatch would certainly reduce the per-iteration complexity; however, it would also reduce the accuracy of Async-RED-SG relative to GM-RED (see Theorem 2 and Fig. 3 (Left)). This trend is also illustrated in Table 2 (see additional_results.pad in the supplement) in terms of SNR. Note also how the error term due to stochastic processing in Theorem 2 is also proportional to the step size $\\gamma$, which means that by using smaller $\\gamma$, Async-RED-SG can approximate GM-RED as accurately as desired. However, a reduction in \u03b3 would also lead to slower convergence. One thus needs to tradeoff the desired accuracy against the desired speed to select a suitable configuration for Async-RED-SG. We have added a discussion of this in the revision.\n\n**4. It is highly suggested that the authors should also plot the convergence curves for the CT experiments as in the Fig.3.**\n* Prompted by your remark, we draw Figure 1 in additional_results.pdf. The figure plots SNR against the iteration number for Async-RED-BG/SG, Async-RED, and Gm-RED. Due to the lower per-iteration complexity, Async-RED-SG achieves the highest SNR value within the time budget of 1 hour.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "9N8ETrVDk-", "original": null, "number": 4, "cdate": 1605721905513, "ddate": null, "tcdate": 1605721905513, "tmdate": 1606189257121, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "RTlKkpSsLJ4", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "**1. The main weak point of the paper is motivating the problem. In other words, it is not clear why a multicore method is needed, although the numerical results demonstrate this later on. For example, Section 3 starts by stating that \u201cASYNC-RED addresses the computational bottleneck. . . \u201d, but what that computational bottleneck consists of is never explained. Figure 1 helps in explaining this, but much is left unexplained at this stage. Some discussion of the regime where multicore processing makes sense would also be in order.**\n* We have clarified the importance of using a multicore system for imaging and rephrase the first sentence in Section 3 in the updated manuscript.\n\n**2. On p.2, $H(x)$ is not defined.**\n* The updated manuscript has defined $H(x)$ on p.2.\n\n**3. On p.2, $G(x)$ is defined twice: once in eq. (2) and one in eq. (4). Presumably these refer to the same non-linear mapping.**\n* Indeed, since they refer to the same mapping, we have deleted the definition of $G$ in eq. (4).\n\n**4. There seems to be some mixup between BC-RED and GM-RED throughout. Are these different\nmethods?**\n* Thanks for catching this. Indeed, BC-RED and GM-RED are different methods. This has been fixed in the updated manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "_TNlO25Dqw0", "original": null, "number": 2, "cdate": 1603557707757, "ddate": null, "tcdate": 1603557707757, "tmdate": 1606080975832, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Review", "content": {"title": "First asynchronous regularization-by-denoising algorithms for imaging with deep denoising priors", "review": "This paper proposed for the first time the asynchronous variants of deterministic and stochastic regularization-by-denoising (RED) algorithms which have become popular recently in image recovery and reconstruction applications since they leverage the power of  pretrained deep denoising neural networks into the traditional model-based schemes, and often achieve state-of-the-art recovery results. These new variants are aimed to fully utilize the multi-cores structure of computational devices and to maximize the practicality of PnP/RED methods in large-scale inverse problems. The authors provide gradient-norm convergence analysis for the proposed algorithms under standard assumptions, along with detailed numerical studies demonstrating practical advantageous of proposed methods. \n\nOverall the paper is well-written and has the potential to be a nice contribution to the community. The reviewer has several concerns about the current version which hopefully the authors would address and clarify:\n\n(1) The proposed Async-RED methods apply the denoiser on blocks of the image, while the original RED applies the denoiser on the whole image. Intuitively, such a scheme may be suboptimal and somewhat compromise the recovery performance, since state-of-the-art DNN denoisers utilize non-local similarity across the image. Although the experiments in this work do not demonstrate such deficit, the reviewer suspects that, for some images, the Async-RED may not do well on the pixels near the edges of the blocks. The reviewer is a bit dubious about whether such type of block decomposition of denoiser in this line of work can be universally reliable.\n\n(2) The comparative results shown in Fig 3 regarding compressed-sensing reconstruction appear to be somewhat unconvincing:\n\nFirstly, the authors use a suboptimal GM-RED/Sync-RED for baseline, which is gradient descent RED without Nesterov acceleration, while it is well-known that the accelerated gradient descent  RED (AGD-RED) converges at a much better rate and the reviewer believes that this is a more sensible baseline. Maybe an important advantage of original RED and synchronous RED is that they are easier to be accelerated by momentum tricks? \n\nSecondly, Async-RED-SG seems to yield very limited acceleration in terms of convergence rate over the deterministic counterpart Async-RED-BG in CS example -- what is the minibatch size used in this experiment? Would reducing the minibatch size yield a faster convergence rate? Meanwhile the recovery result of Async-RED-SG is 1dB worse than the other methods, which seems a bit problematic (would a shrinking step-size help to fix this issue?).\n\nIt is highly suggested that the authors should also plot the convergence curves for the CT experiments as in the Fig.3.\n\n******************** after rebuttal*********************\n\nThe authors have provided a responsive feedback and addressed the comments to satisfactory, therefore the reviewer votes for acceptance.\n\n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132889, "tmdate": 1606915778138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Review"}}}, {"id": "h4Hli_XERbZ", "original": null, "number": 7, "cdate": 1605724410905, "ddate": null, "tcdate": 1605724410905, "tmdate": 1605724410905, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "rz4C_UTOGal", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment", "content": {"title": "Reference", "comment": "**Reference:**\n\n[1] Z. Peng, T. Wu, Y. Xu, M. Yan, and W. Yin. Coordinate-friendly structures, algorithms and applications. Adv. Math. Sci. Appl., 1(1):57\u2013119, April 2016.\n\n[2] R. Hannah, F. Feng, and W. Yin. A2BCD: Asynchronous acceleration with optimal complexity. In International Conference on Learning Representations, 2019.\n\n[3] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Proc. Advances in Neural Information Processing Systems 20, pages 161\u2013168, Vancouver, BC, Canada, December 3-6, 2007."}, "signatures": ["ICLR.cc/2021/Conference/Paper874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9EsrXMzlFQY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper874/Authors|ICLR.cc/2021/Conference/Paper874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866247, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Comment"}}}, {"id": "lPb5BfYp0JQ", "original": null, "number": 1, "cdate": 1603275821635, "ddate": null, "tcdate": 1603275821635, "tmdate": 1605024585411, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Review", "content": {"title": "A good paper on accelerating distributed computations by asynchronous and stochastic activation, with applications to regularized imaging problems", "review": "The main contribution is a combination of asynchronous processing and stochastic activation of blocks in a distributed computing environment. The framework is general, since the regularization/denoising operator is any nonexpansive operator; but focusing on the PnP/RED framework is a very relevant choice.\nTo my knowledge (I am not an expert of this particular area of optimization), the contribution is new and solid. \nI have the following questions/remarks:\n* After Assumption 4: when mentioning the literature, I think you should include the paper \"BUILDING FIRMLY NONEXPANSIVE CONVOLUTIONAL NEURAL NETWORKS\" by Terris et al. and shortly discuss the relationship. In particular, nonexpansiveness is sufficient in your setting, whereas it is usually not sufficient in optimization, with averagedness/firm nonexpansiveness assumed. Why is it so?\n* About asynchronous optimization, Franck Iutzeler has his PhD thesis and several papers on the topic. You might have a look and cite some of them, which are relevant in your setting.\n* Theorem 2 shows that the \"convergence\" is not variance-reduced. I would appreciate a discussion on whether this is unavoidable or if this is an open question for future work, if there is relevant literature on this matter, and why it is difficult to derive a variance-reduced approach with similar features.\n* the DnCNN architecture is used in the experiments: does it satisfy Assumption 4? More generally, are all assumptions met so that the theorems apply in the experiments? You should discuss the match between your theoretical results and the conditions of the practical experiments more closely.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132889, "tmdate": 1606915778138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Review"}}}, {"id": "nZvQrR9p6VE", "original": null, "number": 3, "cdate": 1603859523555, "ddate": null, "tcdate": 1603859523555, "tmdate": 1605024585340, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Review", "content": {"title": "Review of \"Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors\"", "review": "Due to the growth of data sets in a lot of applications, it is important to develop algorithms to achieve great performance but with significantly reduced computational cost. The paper proposes asynchronous type of parallel algorithms by combining the pre-trained deep denoisers. In particular, batch gradient and stochastic gradient are applied to the proposed algorithm framework by taking advantage of the coordinate separable structures of the problem. Convergence of the algorithms are guaranteed under the four specified assumptions. Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in terms of running time. The importance and contribution of this work in compressive sensing algorithms stand. However, the novelty of the methods look incremental. There are some other issues listed as follows. \n\n1. In p.1, is there any condition on the comparison, e.g., m<<n, required in the introduction?\n2. The denoised version $D_\\sigma(x^*)$ by some image denoiser essentially provides a more accurate estimate of $x^*$. Can this be replaced by other similar operators? Also, in the compressive sensing, the recovered image $x^*$ at the first few iterations may not be good enough, will the application of this operator make it worse? Does the parameter $\\sigma$ need to tune or update dynamically in the iterations?\n3. In Alg.1-2, the two operators read() and minibatch() should be explicitly defined. \n4. In the numerical experiments, discussion on the influence of the block/minibatch size on the performance could be strengthened. In the ASYNC-RED-SG, how many trials were conducted to get an average performance? Robustness to the noise could be analyzed. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132889, "tmdate": 1606915778138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Review"}}}, {"id": "RTlKkpSsLJ4", "original": null, "number": 4, "cdate": 1604101934314, "ddate": null, "tcdate": 1604101934314, "tmdate": 1605024585200, "tddate": null, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "invitation": "ICLR.cc/2021/Conference/Paper874/-/Official_Review", "content": {"title": "Well explained method and interesting results", "review": "The paper describes a novel implementation of RED, regularization by denoising, which better leverages multicore architectures to achieve a significant speedup. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy. The crucial result is two sets of convergence guarantees showing that this delayed update will not cause too much error, even if the updates from different cores arrive at different times. The speedups achieved range from 6\u00d7 to 8\u00d7 on two tasks (compressive sensing and computed tomography reconstruction).\n\nThe paper is well organized and the details are explained well. The numerical results are convincing and the analysis is adequate. The main weak point of the paper is motivating the problem. In other words, it is not clear why a multicore method is needed, although the numerical results demonstrate this later on. For example, Section 3 starts by stating that \u201cASYNC-RED addresses the computational bottleneck\u2026\u201d, but what that computational bottleneck consists of is never explained. Figure 1 helps in explaining this, but much is left unexplained at this stage. Some discussion of the regime where multicore processing makes sense would also be in order. That being said, I think the results are interesting enough and the description of the method compelling enough that I recommend this work be published as part of the proceedings.\n\nThere are some small issues:\n\u2013 On p. 2, H(x) is never defined.\n\u2013 On p. 2, G is defined twice: once in eq. (2) and one in eq. (4). Presumably these refer to the same non-linear mapping.\n\u2013 There seems to be some mixup between BC-RED and GM-RED throughout. Are these different methods?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper874/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper874/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "authorids": ["~Yu_Sun11", "jiaming.liu@wustl.edu", "yiran.s@wustl.edu", "~Brendt_Wohlberg2", "~Ulugbek_Kamilov1"], "authors": ["Yu Sun", "Jiaming Liu", "Yiran Sun", "Brendt Wohlberg", "Ulugbek Kamilov"], "keywords": ["Regularization by denoising", "Computational imaging", "asynchronous parallel algorithm", "Deep denoising priors"], "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.", "one-sentence_summary": "Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|asyncred_a_provably_convergent_asynchronous_block_parallel_stochastic_method_using_deep_denoising_priors", "supplementary_material": "/attachment/5c966ab57a6bc5503214725599856a145f5437f6.zip", "pdf": "/pdf/42abafb63caa1b6ddc6bda1b8e8337b1c2a9db91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsun2021asyncred,\ntitle={Async-{\\{}RED{\\}}: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors},\nauthor={Yu Sun and Jiaming Liu and Yiran Sun and Brendt Wohlberg and Ulugbek Kamilov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9EsrXMzlFQY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9EsrXMzlFQY", "replyto": "9EsrXMzlFQY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132889, "tmdate": 1606915778138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper874/-/Official_Review"}}}], "count": 12}