{"notes": [{"id": "ryggIs0cYQ", "original": "HJei3rRFY7", "number": 143, "cdate": 1538087751822, "ddate": null, "tcdate": 1538087751822, "tmdate": 1556012088245, "tddate": null, "forum": "ryggIs0cYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkxVsCjblE", "original": null, "number": 1, "cdate": 1544826524099, "ddate": null, "tcdate": 1544826524099, "tmdate": 1545354529917, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Meta_Review", "content": {"metareview": "This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Well motivated simple idea and solution that work well in practice"}, "signatures": ["ICLR.cc/2019/Conference/Paper143/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper143/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353323135, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper143/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper143/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper143/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353323135}}}, {"id": "rkeynlndxE", "original": null, "number": 2, "cdate": 1545285799177, "ddate": null, "tcdate": 1545285799177, "tmdate": 1545286502585, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Public_Comment", "content": {"comment": "the idea to uniform some of the best normalization techniques is quite interesting. \nQuick question: does it make sense to use separate control parameters for the means and stds?\nadditional experiment request: Could you please provide us with a benchmark on CIFAR-10/100, which is less resource demanding so that people with limited computational resources can also have a quick look at the efficacy of the method? thanks", "title": "interesting idea. Could you provide some results on CIFAR-10/100?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311908679, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryggIs0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311908679}}}, {"id": "B1gLdh5uh7", "original": null, "number": 1, "cdate": 1541086317749, "ddate": null, "tcdate": 1541086317749, "tmdate": 1543243353780, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "content": {"title": "Neat motivation and very extensive experiments", "review": "In this work, the authors propose Switchable Normalization (SN), which *learns* to switch / select different normalization algorithms (including batch normalization (BN), Instance Normalization (IN), Layer Normalization (LN)) in layers of the networks and in different applications. The idea is motivated by observations (shown in Fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. BN) are fragile to very small batch size.\n\nThe authors propose a general form for different normalization methods, which is a Gaussian normalization and then scale and shift by scalars. Different normalization methods utilize different statistics as the mean and the variance of the Gaussian normalization. The authors further propose to learn the combination weights on mean and variance, which is w_k and w'_k in Eqn (3). To avoid duplicate computation, the authors also do some careful simplification on computing mean and variance with all of the three normalization methods.\n\nIn the experiment part, the authors demonstrate the effectiveness of the proposed SN method on various kinds of tasks, including ImageNet classification, object detection and instance segmentation in COCO, semantic image parsing and video recognition. In all of the tasks tested, which also cover the common application in computer vision, SN shows superior and robust performance.\n\nPros:\n+ Neat motivation;\n+ Extensive experiments;\n+ Clear illustration;\n\nCons\n- There are still some experiment results missing, as the authors themselves mentioned in the Kinetics section (but the reviewer thinks it would be ready); \n- In Page 3 the training section and Page 4, the first paragraph, it mentioned \u0398 and \u03a6 (which are the weights for different normalization methods) are jointly trained and different from the previous iterative meta-learning style method. The authors attribute \"In contrast, SN essentially prevents overfitting by choosing normalizers to improve both learning and generalization ability as discussed below\". The reviewer does not see it is well justified and the reviewer thinks optimizing them jointly could lead to instability in the training (but it did not happen in the experiments). The authors should justify the jointly training part better.\n- Page 5 the final paragraph, the reviewer does not see the point there. \"We would also like to acknowledge the contributions of previous work that explored spatial region (Ren et al., 2016) and conditional normalization (Perez et al., 2017). \"  Please make it a bit more clear how these works are related. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper143/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "cdate": 1542234528732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper143/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335658358, "tmdate": 1552335658358, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper143/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJg4K43k07", "original": null, "number": 4, "cdate": 1542599804103, "ddate": null, "tcdate": 1542599804103, "tmdate": 1542599892929, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "B1gLdh5uh7", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "Thanks for your reviews. We would like to provide feedback as below.\n\n1. In the Kinetics section, SN achieves top1/top5 accuracy of 73.5/91.2 that outperforms BN and GN by 0.2/0.5 and 0.5/0.6 respectively.\n\n2. We will add more discussions of the joint training process. In fact, SN optimizes both network parameters (\\Theta) and control parameters (\\Phi) in the same training set, because \\Phi in SN controls the ratios between different normalizers where each one has certain regularization capacity. Their combination provides stronger regularization to prevent over-fitting in training.\n\n3. In the related work section, we would like to acknowledge more normalization methods in the literature. We will discuss more details of them.\n\nBesides the above feedback, we would also like to highlight that SN would inspire future work of both applications and theories, as discussed in the \"response to AnonReviewer4\" in https://openreview.net/forum?id=ryggIs0cYQ&noteId=B1x5xwU-a7&noteId=B1x5xwU-a7 "}, "signatures": ["ICLR.cc/2019/Conference/Paper143/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper143/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622270, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryggIs0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper143/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper143/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper143/Authors|ICLR.cc/2019/Conference/Paper143/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622270}}}, {"id": "BygLp1wrpm", "original": null, "number": 3, "cdate": 1541922749947, "ddate": null, "tcdate": 1541922749947, "tmdate": 1541922749947, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "content": {"title": "Simple approach that works", "review": "This paper considers normalization by learning to average across well-known normalization techniques.\nThis meta-procedure almost by definition can yield better results, and this is demonstrated nicely by the authors.\nThe idea is simple and easy to implement, and easy works in this case.\n\nI would like to hear more about the connections to other meta-learning procedures, by expanding the discussion on page 3.\nThat discussion is very interesting but quite short, and I am afraid I can't see how SN avoids overfitting compared to other approaches. \nAlso, the section on Inference in page 4 is unclear to me. What parameters are being inferred and why is the discussion focused on convergence instead?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper143/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "cdate": 1542234528732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper143/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335658358, "tmdate": 1552335658358, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper143/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1x5xwU-a7", "original": null, "number": 1, "cdate": 1541658353879, "ddate": null, "tcdate": 1541658353879, "tmdate": 1541658353879, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "r1xiHRoe6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Official_Comment", "content": {"title": "Responds to AnonReviewer4", "comment": "Dear AnonReviewer4,\n\nThe authors appreciate you're voting to accept and you're definitely a responsible reviewer. While we still have two missing reviews from R1 and R2.\n\nThanks for your good question for us to consider. In fact, the authors have been working on it as SNv2, which will be released very soon. However, the details of SNv2 might be out of the scope of this paper.\n\nThe authors would like to respond to your comment \"the proposed method itself is not significant, given many existing efforts/algorithms; it is almost straightforward to do so, without any challenges\". We believe you're saying that the technical challenge of SN is not significant because SN combines existing normalizers in a straightforward way.\n\nTo be honest, the authors might not agree with the above comment. (1) SN is not only widely applicable, but also a thought provoking method because it's the first time to demonstrate different convolutional layers should have different normalizers. This viewpoint may undoubtedly inspire many areas in deep learning as pointed out in the paper.\n\n(2) The authors would also highlight that SN is designed to be as concise and effective as possible without any hyper-parameter and easy to implement and use. We believe that the current formulation of SN is important and valuable, and it should be presented to the community. \n\nAlthough SN has a straightforward form that uses softmax to combine normalizers, its theoretical analysis (such as learning dynamics and generalization) is already a big challenge because theory of BN, IN, and LN should be built up before analyzing SN. \n\nWhile analyzing any one of these normalizers is still an open problem, not to mention to investigate the interactions between them, despite their interactions are captured by using a \"simple\" softmax function.\n\nWe also might not agree that the papers submitted to this top-tier venue would be \"easy to do in practice\" especially for those demonstrated new perspective and inspiration for deep learning, otherwise they would be already present in any where else.\n\nFrankly, the authors believe the SN paper deserves a better score. Do the above feedback persuade you to raise your rating?"}, "signatures": ["ICLR.cc/2019/Conference/Paper143/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper143/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622270, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryggIs0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper143/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper143/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper143/Authors|ICLR.cc/2019/Conference/Paper143/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper143/Reviewers", "ICLR.cc/2019/Conference/Paper143/Authors", "ICLR.cc/2019/Conference/Paper143/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622270}}}, {"id": "r1xiHRoe6Q", "original": null, "number": 2, "cdate": 1541615170856, "ddate": null, "tcdate": 1541615170856, "tmdate": 1541615170856, "tddate": null, "forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "content": {"title": "A clear paper with clear contributions", "review": "Summary: \n(1) This paper proposes the concept of Switchable Normalization (SN), which learns a weighted combination of three popular/existing normalization techniques, Instance Normalization (IN) for channel-wise, Layer Normalization (LN) for layer-wise, and Batch Normalization (BN) for minibatch-wise.\n(2) Some interesting technical details: a) A softmax is learned to automatically determine the importance of each normalization; b) Reuse of the computation to accelerate.  c) Geometric view of different normalization methods.\n(3) Extensive experimental results to show the performance improvement. Investigation on the learned importance on different parts of networks and tasks.\n\n\nComments:\n\nThe writing of this paper is excellent, and contributions are well presented and demonstrated.\nIt is good for the community to know SN is an option to consider. Therefore, I vote to accept the paper. \n\nHowever, the proposed method itself is not significant, given many existing efforts/algorithms; it is almost straightforward to do so, without any challenges. \n\nHere is a more challenging question for the authors to consider: Given the general formulation of normalization methods in (2), it sees more interesting to directly learn the pixel set I_k. The proposed SN can be considered as a weak version to learn the pixel set: SN employs the three candidates set pre-defined by the existing methods, and learns a weighted combination over the  \u201ctemplate\u201d sets. This is easy to do in practice, and it has shown promising results. A natural idea to learn more flexible pixel set, and see the advantages. Any thoughts?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper143/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Learning-to-Normalize via Switchable Normalization", "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released.", "keywords": ["normalization", "deep learning", "CNN", "computer vision"], "authorids": ["pluo@ie.cuhk.edu.hk", "renjiamin@sensetime.com", "pengzhanglin@sensetime.com", "zhangruimao@sensetime.com", "lijingyu@sensetime.com"], "authors": ["Ping Luo", "Jiamin Ren", "Zhanglin Peng", "Ruimao Zhang", "Jingyu Li"], "pdf": "/pdf/5170b99841d34d19e04153419f4002c4234b0c57.pdf", "paperhash": "luo|differentiable_learningtonormalize_via_switchable_normalization", "_bibtex": "@inproceedings{\nluo2018differentiable,\ntitle={Differentiable Learning-to-Normalize via Switchable Normalization},\nauthor={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryggIs0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper143/Official_Review", "cdate": 1542234528732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryggIs0cYQ", "replyto": "ryggIs0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper143/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335658358, "tmdate": 1552335658358, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper143/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}