{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392802980000, "tcdate": 1392802980000, "number": 1, "id": "bjdXjLV2FZjye", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "nny0nGJmvYs2b", "replyto": "bpRoWiBVJ8WUG", "signatures": ["Yann Dauphin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments. You can see an updated version of the paper at http://arxiv.org/pdf/1401.0509.pdf.\r\n\r\n1. 'The main proposed method is to learn from query click logs a representation of words into an embedding space.  The paper has one huge problem which is its lack of comparison to the most obvious baselines which are other word embedding models.'\r\n\r\nThe main proposed method is a zero-shot learning framework for utterance classification, not the embeddings. What's more, the learned embeddings are not word-level embeddings, but sentence-level embeddings. We've clarified this in the paper.\r\n\r\n2. 'Given that the model is very similar to NNLMs like those of Bengio, Collobert and Weston, Huang et al., Mikolov et al., etc it would seem crucial to compare to these existing methods to know whether this model improves over existing literature.'\r\n\r\nThe embeddings cannot be compared directly to these approaches because they are word-level embeddings. Our zero-shot learning framework requires sentence-level embeddings.\r\n\r\n3. 'It also seems like, if the click log data includes words like restaurant that are similar to the SUC class names, the task becomes essentially supervised. Even if not, the tasks are related and I would call this approach more of a distant supervision type approach and not zero shot learning since users clicking on semantically important URLs is some type of supervision (albeit one impossible to obtain for anybody except the big search engines). '\r\n\r\nWe clarified in the paper the link between the embeddings and the zero-shot framework. The embeddings are learned using a supervised task that involves the click logs. That task and its labels are different from the SUC task. The framework we propose is zero-shot because we can obtain a classifier for any SUC task without labelled data for that task using the trained embeddings.\r\n\r\n4. 'Frome et al. and Socher et al. both had a NIPS paper last year'\r\n\r\nWe will include these works in Section 7.\r\n\r\n5. 'The terms/ideas of zero-shot clustering seem confusing. Clustering is, by its usual definition, always unsupervised and hence 'zero-shot'. In fact, the point of zero shot learning is that one can do a usually supervised task but without supervision of the classes that are to be predicted. When I assign an element to a cluster in k-means, am I doing zero-shot clustering in the author's view?'\r\n\r\nWe've renamed this method 'zero-shot discriminative embedding' for clarity. The paper also better explains the motivation behind the method (see the new Figure 3). By clustering, we had meant that the classes are clustered, not just clustering in the density estimation sense. The goal of the method is to make sure that the embeddings are discriminative for the SUC task, without labelled data of the SUC task. That is a novel algorithm.\r\n\r\n6. 'The improvement to a simple existing kernel based method in table 3 is tiny (0.2%) improvement.'\r\n\r\nWe use a linear SVM, for which the state-of-the art feature set gives 6.36%. Our method reduce the error of the linear SVM to 5.75%. The main goal of this experiments is to compare the feature sets which is why we use the linear SVM instead of the kernel SVM.\r\n\r\n7. '- it would be better if citations mentioned the names of the authors so the reader familiar with the field doesnt have to go back and forth between the text and references to know what paper is being cited. '\r\n\r\nThanks, our next update will implement this."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392802800000, "tcdate": 1392802800000, "number": 1, "id": "Y5rm5PFJ-tls1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "nny0nGJmvYs2b", "replyto": "2PRbPFT6fcz9-", "signatures": ["Yann Dauphin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks, your comments were helpful and we've taken them into account in the updated paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392800640000, "tcdate": 1392800640000, "number": 1, "id": "LG-cGAgNIiZnJ", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "nny0nGJmvYs2b", "replyto": "6w8d6uJUDW63r", "signatures": ["Yann Dauphin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments. We've significantly updated the paper to give it more polish and to make the explanations less idiosyncratic (see https://arxiv.org/submit/914938/view). We've also added more experiments. One of which shows a performance boost by using the ZSC embeddings in the zero-shot setting.\r\n\r\nDetailed answers follow:\r\n1. 'I had to get back to the ' Zero-Shot Learning with Semantic Output Codes' paper to understand the idea in its full generality'\r\n\r\nThe paper now contains an overview of zero-shot learning (Section 3). \r\n\r\n2. 'In particular, it would be useful to formalize the 'intuition' behind equation (2), which corresponds to the 'knowledge base'. '\r\n\r\nWe've also added Figure 1 to give an intuition behind Equation 2.\r\n\r\n3. 'It is the most interesting: it proposes an excellent, and to my knowledge, novel idea, that I understood as soon as I saw equation (3). However, the 3 paragraphs of explanation that precede are so confusing that I nearly suspected deliberate obfuscation of a good idea.'\r\n\r\nWe've rewritten this section to make it more streamlined and clear. In particular, we've added a visualization that illustrates what the method does.\r\n\r\n4. 'Experiments '\r\n\r\nWe have clarified our experimental setup in the paper. The raw features are bag-of-words. We used SVMs because they better show the difference between the feature extractors. DNNs could be used also, but we initially wanted to focus on better features extraction. However, we will experiment with more powerful classifiers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392799020000, "tcdate": 1392799020000, "number": 1, "id": "tV-7Q180huV5c", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "nny0nGJmvYs2b", "replyto": "Jvo1fmyuaV-Y3", "signatures": ["Yann Dauphin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your helpful comments. We have updated to the paper to address them (see https://arxiv.org/submit/914938/view). The update clarifies many sections and adds new experiments.\r\n\r\nMore detailed answers follow:\r\n1) 'The type of regularisation used is similar to entropy minimization'\r\n\r\nWe have added a discussion of this work in Section 7. The key difference with our approach is that entropy minimization is a semi-supervised method. Our approach addresses the problem where no task-specific labels are available.\r\n\r\n2) 'The dataset used for testing query classification does not seem to be very standard and actually consists of utterances by users of a spoken dialog system rather than search queries.'\r\n\r\nWe chose this dataset because we are interested in solving the problem of classification for natural language queries. This type of problem arises when a user interact with a system through free-form speech for example.\r\n\r\n4) 'Section 5: 'the best class name would have a meaning P(H | C_r)'\r\n\r\nWe have corrected that statement. A good category name is one that is close to utterances of the class.\r\n\r\n5) 'The authors limit themselves to using only 1,000 most frequent URLs'\r\n\r\nThis is an improvement that we have considered. It is not clear if that will be very helpful because the URLs follow a Pareto distribution.\r\n\r\n6) 'The paper would also benefit from some polishing'.\r\n\r\nWe have carefully edited the paper in the new version.\r\n\r\n7) 'I have a feeling that the authors tend to over-generalize'.\r\n\r\nWe have re-written this section to be more specific and clear. In particular, we illustrate the idea with a concrete example (see Figure 3)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392531240000, "tcdate": 1392531240000, "number": 4, "id": "bpRoWiBVJ8WUG", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "nny0nGJmvYs2b", "replyto": "nny0nGJmvYs2b", "signatures": ["anonymous reviewer e900"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Zero-Shot Learning and Clustering for Semantic Utterance Classification", "review": "This paper introduces a method to classify search queries into a set of classes in an utterance frame classification task.\r\n\r\nThe main proposed method is to learn from query click logs a representation of words into an embedding space.\r\n\r\nThe paper has one huge problem which is its lack of comparison to the most obvious baselines which are other word embedding models.\r\n\r\nFor example, the table 2 of the embedding nearest neighbors looks very similar to what neural network language models would learn. In fact, the paper mentions that the proposed model is also very similar to such neural network language models (NNLM).\r\n\r\nGiven that the model is very similar to NNLMs like those of Bengio, Collobert and Weston, Huang et al., Mikolov et al., etc it would seem crucial to compare to these existing methods to know whether this model improves over existing literature.\r\n\r\nUnlike NNLMs which are truly unsupervised and can be trained on easily accessible abundant large text corpora like wikipedia, the paper instead proposes to use a proprietary dataset that is not and will not be available to anybody to learn essentially the same type of embeddings.\r\n\r\nIt also seems like, if the click log data includes words like restaurant that are similar to the SUC class names, the task becomes essentially supervised. Even if not, the tasks are related and I would call this approach more of a distant supervision type approach and not zero shot learning since users clicking on semantically important URLs is some type of supervision (albeit one impossible to obtain for anybody except the big search engines).\r\n\r\nFrome et al. and Socher et al. both had a NIPS paper last year that also used deep methods for zero shot learning, using embeddings that were learned in an unsupervised way, mention or comparison to these projects would be reasonable.\r\n\r\nThe terms/ideas of zero-shot clustering seem confusing. Clustering is, by its usual definition, always unsupervised and hence 'zero-shot'. In fact, the point of zero shot learning is that one can do a usually supervised task but without supervision of the classes that are to be predicted.\r\nWhen I assign an element to a cluster in k-means, am I doing zero-shot clustering in the author's view?\r\n\r\nThe improvement to a simple existing kernel based method in table 3 is tiny (0.2%) improvement.\r\n\r\nMinor comments:\r\n- 'models [15] who learn' -> models which learn\r\n- Table 1 is in an odd place that is way too early since it's referenced only a few pages later.\r\n- it would be better if citations mentioned the names of the authors so the reader familiar with the field doesnt have to go back and forth between the text and references to know what paper is being cited.\r\n- citation 25 just mentions ICML and has no title or authors\r\n- fig.3 is not readable in a black and white printout\r\n- 'to the the 1000 most popular' \r\n\r\n\r\nConclusion:\r\nThe merit of the paper is highly questionable without a comparison to similar models that learned word embeddings with just raw text. Several word embeddings are available for download and I encourage the authors to pick one and update their paper on arxiv with an added comparison."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392016320000, "tcdate": 1392016320000, "number": 3, "id": "6w8d6uJUDW63r", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "nny0nGJmvYs2b", "replyto": "nny0nGJmvYs2b", "signatures": ["anonymous reviewer 8b78"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Zero-Shot Learning and Clustering for Semantic Utterance Classification", "review": "This paper offers 2 contributions: one confirms that zero-shot learning has some practical use for semantic classification. The second one, about zero-shot clustering is much more original, but unfortunately less mature.\r\nWhen using deep learning for sentence or document-level classification, it has been observed that discriminantly tuning the word embedding significantly improved performance.\r\nThis paper does such discriminant tuning *without* labeled data, by assuming that the classifier has been obtained through zero-shot learning. They call the method 'zero-shot clustering', and I find it very neat and original.\r\n\r\nUnfortunately, this paper seems to have been hastily written . Explanations  are thought-provoking but very idiosyncratic, thus very hard to follow. Experiments  are limited and poorly explained, especially about zero-shot clustering.\r\n\r\nSection 5: \r\nZero-shot learning is introduced in section 5, but I had to get back to the ' Zero-Shot Learning with Semantic Output Codes' paper to understand the idea in its full generality. In particular, it would be useful to formalize the 'intuition' behind equation (2), which corresponds to the 'knowledge base'. \r\n\r\nSection 6:\r\nIt  is the most interesting: it proposes an excellent, and to my knowledge, novel idea, that I understood as soon as I saw equation (3). However, the 3 paragraphs of explanation that precede are so confusing that I nearly suspected deliberate obfuscation of a good idea. The discussion starts by assuming we want to build density  model of the data P(X) like in auto-encoder, and then show this is a bad idea: why bother? What is proposed here has nothing to do with density estimation. This proxy framework is completely cryptic to me: proxy of what? P(C|X)?\r\n\r\nWhat has this to do with the choice of the entropy (excellent by the way)?\r\nThe link seems to be in the sentence:\r\n\u201cThe better the proxy function hat{f} the better this measure (H(f(X)) - H( hat{f}(X))^2<= K*(f(X) - hat{f}(X))^2 by Lipschitz continuity).\u201d\r\nWhat this sentence tells us is that we should get P(C|X) as close as possible to the true posterior to get its entropy close to the true entropy? But we are doing the opposite here: minimizing the estimated entropy, which does not even have to be close to the true entropy.\r\n\r\nSection 7: experiments\r\nThe part about how zero shot clustering improves SVM classification is very frustrating to read: results are so promising, but very few details are shared (table 3).\r\n-\tWhat are the raw features? N-grams?\r\n-\tWhy only SVMs are tried on the DNN and ZSC embeddings? It would make sense to try DNNs or DCNs.\r\n-\tAn interesting further experiment would be if discriminant fine tuning of the embedding further improves performance over ZSC. In this case, ZSC training would be comparable to semi-supervised training, with a mixture of labelled and unlabeled examples."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391630100000, "tcdate": 1391630100000, "number": 2, "id": "Jvo1fmyuaV-Y3", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "nny0nGJmvYs2b", "replyto": "nny0nGJmvYs2b", "signatures": ["anonymous reviewer 9b82"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Zero-Shot Learning and Clustering for Semantic Utterance Classification", "review": "The paper considers the task of categorizing queries into classes revealing user intent (e.g., weather vs. flight booking). In doing this, the authors exploit query log data (specifically, queries paired with URLs clicked after sending the query to a search engine), however they use no or little supervised data. Instead,  they learn query representations predictive of URLs, and use proximity between a query representation and a category name representation to make the classification decision (word(s) in the category name are also treated as a query).  Additionally they consider a regularizer which favors low entropy predictions on unlabelled queries (with a distance-based probability model). Finally, they also use their representation in supervised learning. \r\n\r\nI find the ideas and their implementation interesting and the experiments seem fairly convincing to me as well (however, see (2)).\r\n\r\nComments:\r\n1) The type of regularisation used is similar to entropy minimization  proposed in Grandvalet and Bengio (2004), and can also be regarded as a form of posterior regularization (Ganchev et al., 2010) or generalized expectation criteria (Druck et al., 2008). I think the authors should briefly discuss the relation.\r\n2) The dataset used for testing query classification does not seem to be very standard and actually consists of utterances by users of a spoken dialog system rather than search queries. I am wondering why a more standard dataset is not selected (e.g., KDDCUP).\r\n3) Not being an expert in query classification, I would appreciate some discussion of related approaches, as I assume this is not the first method which considers a semi-supervised approach to this task. \r\n4) Section 5: 'the best class name would have a meaning P(H | C_r) that is the mean of the meaning of all its utterances  E_{X_r | C_r}{ P(H | X_r)} '. I am not sure in what sense it would be the best. If in terms of classification accuracy then this is not entirely correct -- e.g., a single datapoint can move the mean to an arbitrary position and dramatically affect the error rate. I understand what the authors are trying to say here but I think it is a bit vague.\r\n5) The authors limit themselves to using only 1,000 most frequent  URLs when learning the representations, perhaps because they use the sofmax error function. They might consider using 'standard' techniques which avoid summation over the categories (i.e. URLs)  in training: some form of binarization (e.g., Huffman trees) or a ranking error function with sub-sampled negative examples. \r\n\r\nThe paper would also benefit from some polishing, a couple of points:\r\n-- Section 6 (page 5, 1st par): 'However, it is not clear how of a proxy that is for a given task.' ?? \r\n-- caption Fig 3: sentence 'ZSL achieves \u2026' does not seem grammatical\r\n\r\nOverall, I have a feeling that the authors tend to over-generalize -- it is a nice paper about query classification, and this over-generalization makes it less readable (e.g, see section 6 where 'proxy functions' are discussed). \r\n\r\nPro:\r\n-- a clever application of distributed representation learning for an important task\r\n-- the approach may have applications in other domains (e.g., in opinion summarization -- learning to categorize online review sentences according to product features)\r\nCon:\r\n-- the dataset may not be entirely appropriate \r\n-- discussion of related work does not seem quite adequate \r\n-- writing (minor)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389226500000, "tcdate": 1389226500000, "number": 1, "id": "2PRbPFT6fcz9-", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "nny0nGJmvYs2b", "replyto": "nny0nGJmvYs2b", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I thought this was a very intersting and well written paper. \r\nSome comments:\r\nI'm ashamed to admit that I didn't what the two methods mentioned were until I read the outro... I thought maybe there would be a fundamentally different approach to both learning AND clustering, or something.  Just to be very clear, I would add this to the 1st sentence of the abstract: '...using deep learning: zero-shot learning and zero-shot clustering'.  I'd also change the 2nd to last sentence of the intro to 'zero-shot learning and zero-shot clustering'.  \r\n\r\nWhile I agree with your choice to distinguish your approach from 'Traditional SUC systems' that 'rely on a large set of labelled examples', I think you should emphasize your use of Query Click Logs more.  I would describe them as analagous with labels in their role in your system (although maybe there is a significant disanalogy I'm overlooking).  This would make things clearer, I think.  \r\n\r\nWith this in mind (actually, in any case, if I understood this part properly), I think Section 5, paragraph 3, 2nd sentence should say '...using only [...] X, [...] C [...], and Q = query click logs' or something to that effect.  \r\n\r\nAlso, in Table 1, I believe the first three other ZSL methods you are comparing with do NOT make use of QCLs, and I would emphasize that as well.  I would suggest putting a horizontal dividing line between those three and the next two methods.\r\n\r\n\r\nsome little edits:\r\n6. first paragraph 'However, it is not clear how of a proxy' -> 'However, it is not clear how GOOD of a proxy' (I think that's the word you want?)\r\n\r\nnext paragraph: 'Is is easy to see THAT' (missing that) \r\n\r\n'If f: X -> Y then we must have...' says the same thing as the next sentence, so I would remove one of them, or at an 'i.e.' or something equivalent.\r\n\r\nlast sentence 'In general, finding a function satisfying these restrictions is hard,' - this is not technically true, since f itself is such a function and there would generally be other 'trivial' examples.  You should say 'finding a GOOD PROXY function satisfying these restrictions...' or something to that effect.  \r\n\r\nnext paragraph:\r\nI would rephrase 'We would like these embeddings to cluster...' as 'We would like to cluster these embeddings...' I think this makes it more clear, especially in the context of the next sentence. \r\n\r\nafter the equation display:\r\n'The entropy tell us about the uncertainty we have over the class' -> 'The entropy tell us about the uncertainty we have over the CLASSES' (I think this is what you mean.)\r\n\r\nlast paragraph, you say that P(C | X) is predicted by a DNN, but isn't it predicted by equation (2)?  This uses P(H | X), which IS predicted by a DNN.  A minimal edit I think would be acceptable is changing 'predicted by' to 'predicted using'.  But you could also be more explicit.  \r\n\r\n\r\nExperiments section 7:\r\nI think you should specify how the ZSL (bag of words) model works; what is it's prediction function?\r\n\r\nIn that same paragraph, I think the description of the Representative URL Heuristic could be clearer.  I would make these edits:\r\n\r\n'We associate each domain with...' -> 'FOR THIS HEURISTIC, we associate each CLASS with...' (I think you mean class, not domain, isn't a domain just a website name?)\r\n\r\nnext sentence: '...associated with the website for that semantic class' -> '...associated with that class's website'\r\n\r\nNext paragraph, 2nd sentence: 'The task is identify...' -> 'The task is TO identify...'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1388771760000, "tcdate": 1388771760000, "number": 64, "id": "nny0nGJmvYs2b", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "nny0nGJmvYs2b", "signatures": ["dhaemon@gmail.com"], "readers": ["everyone"], "content": {"title": "Zero-Shot Learning and Clustering for Semantic Utterance Classification", "decision": "submitted, no decision", "abstract": "We propose two novel zero-shot learning methods for semantic utterance classification (SUC) using deep learning. Both approaches rely on learning deep semantic embeddings from a large amount of Query Click Log data obtained from a search engine. Traditional semantic utterance classification systems require large amounts of labelled data, whereas our proposed methods make use of the structure of the task to allow classification without labeled data. We also develop a zero-shot semantic clustering algorithm for extracting discriminative features for supervised semantic utterance classification systems. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by cite{DCN}. Furthermore, we show that extracting features using zero-shot semantic clustering for a linear SVM reaches state-of-the-art result on that dataset.", "pdf": "https://arxiv.org/abs/1401.0509", "paperhash": "dauphin|zeroshot_learning_and_clustering_for_semantic_utterance_classification", "keywords": [], "conflicts": [], "authors": ["Yann N. Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "authorids": ["dhaemon@gmail.com", "gokhan.tur@microsoft.com", "dilek.hakkani-tur@microsoft.com", "larry.heck@microsoft.com"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 9}