{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457552302912, "tcdate": 1457552302912, "id": "5QzBR8G84FZgXpo7i324", "invitation": "ICLR.cc/2016/workshop/-/paper/122/review/11", "forum": "k80kn82ywfOYKX7ji42O", "replyto": "k80kn82ywfOYKX7ji42O", "signatures": ["ICLR.cc/2016/workshop/paper/122/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/122/reviewer/11"], "content": {"title": "This paper brings attention to the fact that even-number filter sizes can maximize the efficacy of CNN accelerators.", "rating": "6: Marginally above acceptance threshold", "review": "The authors bring attention to the fact that odd-number filter sizes waste computational resources; even-number filter sizes can maximize the efficacy of CNN accelerators. They are able to reduce the complexity of LeNet and VGG11-Nagadomi network with comparable performance in accuracy.\n\nFigure 1 is very good to understand what the paper is about. \n\nFigure 2, on the other hand, is hard to understand; caption doesn't provide enough information. \nFor Figure 2a, why are there two sizes for each test error and normalized complexity bars? If they are the size of the first and second layer filters, why do  8x8, 4x4 filters have less complexity compared to 4x4, 4x4 filters?\nIn Figure 2b there are two bar sets for 2x2 filters, later in the text it appears that one uses more feature maps, this information should be at least in the caption if not in the chart.\n\nOverall, the idea is useful and good to keep in mind.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "HARDWARE-FRIENDLY CONVOLUTIONAL NEURAL NETWORK WITH EVEN-NUMBER FILTER SIZE", "abstract": "Convolutional Neural Network (CNN) has led to great advances in computer vision. Various customized CNN accelerators on embedded FPGA or ASIC platforms have been designed to accelerate CNN and improve energy efficiency. However, the odd-number filter size in existing CNN models prevents hardware accelerators from having optimal efficiency. In this paper, we analyze the influences of filter size on CNN accelerator performance and show that even-number filter size is much more hardware-friendly that can ensure high bandwidth and resource utilization. Experimental results on MNIST and CIFAR-10 demonstrate that hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4x to 2x with comparable accuracy; With same FLOPs, even kernel can have even higher accuracy than odd size kernel. \n", "pdf": "/pdf/k80kn82ywfOYKX7ji42O.pdf", "paperhash": "yao|hardwarefriendly_convolutional_neural_network_with_evennumber_filter_size", "conflicts": ["huawei.com"], "authors": ["Song Yao", "Song Han", "Kaiyuan Guo", "Jianqiao Wangni", "Yu Wang"], "authorids": ["songyao@mail.tsinghua.edu.cn", "songhan@stanford.edu", "gky15@mails.tsinghua.edu.cn", "wnjq11@mails.tsinghua.edu.cn", "yu-wang@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580013952, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580013952, "id": "ICLR.cc/2016/workshop/-/paper/122/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "k80kn82ywfOYKX7ji42O", "replyto": "k80kn82ywfOYKX7ji42O", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/122/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1456589526138, "tcdate": 1456589526138, "id": "nx937z6nMu7lP3z2ioNm", "invitation": "ICLR.cc/2016/workshop/-/paper/122/review/10", "forum": "k80kn82ywfOYKX7ji42O", "replyto": "k80kn82ywfOYKX7ji42O", "signatures": ["~Lingxi_Xie1"], "readers": ["everyone"], "writers": ["~Lingxi_Xie1"], "content": {"title": "This paper provides an interesting and instructive discussion to the industrial community", "rating": "7: Good paper, accept", "review": "In this paper, the authors present a fact that neural networks may become less efficient when odd-sized convolution kernels (like 3x3, 5x5 kernels) are used. The main consideration is from the implementation of the inner-product operation in hardware.\n\nFigure 1 is quite intuitive: one can catch the main idea by taking a glance at it.\n\nExperimental results are acceptable: with smaller kernels, the recognition performance is comparable while the FLOPs are effectively reduced. It would be better if this idea is verified on some larger experiments such as SVHN and ImageNet.\n\nMinor things. (1) The mathematical notations can be more formal: in representing the network structure (20Conv5 ...), please use \\rm{Conv} or \\mathrm{Conv}, also please replace all the 'x' to '\\times' (in 'Conclusion'). (2) Please magnify the font size in both figures, until one can read it clearly on a printed version of the paper. (3) The fonts of digits in Figure 2(a) and Figure 2(b) are different, which is weird.\n\nIn conclusion, this is a good workshop paper that tells the community a simple yet useful fact.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "HARDWARE-FRIENDLY CONVOLUTIONAL NEURAL NETWORK WITH EVEN-NUMBER FILTER SIZE", "abstract": "Convolutional Neural Network (CNN) has led to great advances in computer vision. Various customized CNN accelerators on embedded FPGA or ASIC platforms have been designed to accelerate CNN and improve energy efficiency. However, the odd-number filter size in existing CNN models prevents hardware accelerators from having optimal efficiency. In this paper, we analyze the influences of filter size on CNN accelerator performance and show that even-number filter size is much more hardware-friendly that can ensure high bandwidth and resource utilization. Experimental results on MNIST and CIFAR-10 demonstrate that hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4x to 2x with comparable accuracy; With same FLOPs, even kernel can have even higher accuracy than odd size kernel. \n", "pdf": "/pdf/k80kn82ywfOYKX7ji42O.pdf", "paperhash": "yao|hardwarefriendly_convolutional_neural_network_with_evennumber_filter_size", "conflicts": ["huawei.com"], "authors": ["Song Yao", "Song Han", "Kaiyuan Guo", "Jianqiao Wangni", "Yu Wang"], "authorids": ["songyao@mail.tsinghua.edu.cn", "songhan@stanford.edu", "gky15@mails.tsinghua.edu.cn", "wnjq11@mails.tsinghua.edu.cn", "yu-wang@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580014171, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580014171, "id": "ICLR.cc/2016/workshop/-/paper/122/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "k80kn82ywfOYKX7ji42O", "replyto": "k80kn82ywfOYKX7ji42O", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/122/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455820808122, "tcdate": 1455820808122, "id": "k80kn82ywfOYKX7ji42O", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "k80kn82ywfOYKX7ji42O", "signatures": ["~Song_Yao1"], "readers": ["everyone"], "writers": ["~Song_Yao1"], "content": {"CMT_id": "", "title": "HARDWARE-FRIENDLY CONVOLUTIONAL NEURAL NETWORK WITH EVEN-NUMBER FILTER SIZE", "abstract": "Convolutional Neural Network (CNN) has led to great advances in computer vision. Various customized CNN accelerators on embedded FPGA or ASIC platforms have been designed to accelerate CNN and improve energy efficiency. However, the odd-number filter size in existing CNN models prevents hardware accelerators from having optimal efficiency. In this paper, we analyze the influences of filter size on CNN accelerator performance and show that even-number filter size is much more hardware-friendly that can ensure high bandwidth and resource utilization. Experimental results on MNIST and CIFAR-10 demonstrate that hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4x to 2x with comparable accuracy; With same FLOPs, even kernel can have even higher accuracy than odd size kernel. \n", "pdf": "/pdf/k80kn82ywfOYKX7ji42O.pdf", "paperhash": "yao|hardwarefriendly_convolutional_neural_network_with_evennumber_filter_size", "conflicts": ["huawei.com"], "authors": ["Song Yao", "Song Han", "Kaiyuan Guo", "Jianqiao Wangni", "Yu Wang"], "authorids": ["songyao@mail.tsinghua.edu.cn", "songhan@stanford.edu", "gky15@mails.tsinghua.edu.cn", "wnjq11@mails.tsinghua.edu.cn", "yu-wang@mail.tsinghua.edu.cn"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}