{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396574210, "tcdate": 1486396574210, "number": 1, "id": "SJLj3ML_e", "invitation": "ICLR.cc/2017/conference/-/paper422/acceptance", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Modeling nonlinear interactions between variables by imposing Tensor-train structure on parameter matrices is certainly an elegant and novel idea. All reviewers acknowledge this to be the case. But they are also in agreement that the experimental section of this paper is somewhat weak relative to the rest of the paper, making this paper borderline. A revised version of this paper that takes reviewer feedback into account is invited to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396574725, "id": "ICLR.cc/2017/conference/-/paper422/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396574725}}}, {"tddate": null, "tmdate": 1484680016054, "tcdate": 1484680016054, "number": 12, "id": "rJdIo1nIe", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rJjjlvkVl", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Update", "comment": "> On a related note, it'd be interesting to know what capacity your model has as a function of the TT-rank: does the generalization error depend in a nice way on the TT-rank, or does it blow up?\n\nI wanted to add that in the new version of the paper we added a TT-rank vs. test AUC plot for the MovieLens dataset.\n\nThanks for your interest and all the best,\nAlexander"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679920588, "tcdate": 1484679920588, "number": 11, "id": "Skdlsy38g", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "HJod5BH4l", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Author reply", "comment": "Thank you for your time and thorough evaluation of our work.\n\nNote that it\u2019s fairly easy to make a fast inference algorithm for sparse data by building a segment tree on the TT-cores (but we believe it is out of the scope of this already pretty packed paper) and one can readily adapt the SGD learning algorithm to it. However, we don\u2019t know if it\u2019s possible to adapt the Riemannian approach to the sparse setting.\n\nWe agree with your (and some other reviewer\u2019s) criticism of the experimental evaluation and significantly revised the experiments section.\n1) We added a more detailed description of the datasets and preprocessing steps\n2) We added the test set performance for all the datasets\n3) We did our best to evaluate each of the characteristics of the proposed methods on each of the datasets.\n4) We found that after a slight change to the learning procedure dropout becomes unnecessary, so we decided to remove it whatsoever from all the experiments (see the description of the updates made to the paper).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679758345, "tcdate": 1484679758345, "number": 10, "id": "By88q13Ig", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "BJMAByP4l", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Author reply", "comment": "Thank you for your review, I\u2019m glad you liked our work.\nThe computational complexity is indeed important, we updated the paper to add it (it\u2019s O(d r^2 (r + M)) per step of the Riemannian optimization where d is the number of features, r is the TT-rank, and M is the size of the mini-batch)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679693605, "tcdate": 1484679693605, "number": 9, "id": "B1Iz91nUx", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "B12cSftVl", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Answer", "comment": "Hi Mathieu,\n\nThanks for your interest in our work!\n\nI agree that comparing the model sizes would be interesting, but I\u2019m not sure if there is a fair way to do it. Since the models are different, it seems impossible to compare the number of parameters while fixing the flexibility of the models to some level. And comparing the model sizes as is may be misleading (EMs have O(d r^2) params while m-way FMs have O(d m r), but the ranks r have completely different nature in the two models).\nAlso, according to our experiments, m-way FMs for m > 30 are hard to train, so extremely high-order FMs may not be that practical even with a reasonable model size.\n\nGood point about the Riemannian optimization, we\u2019ve updated this statement to be more accurate: we wanted to say that FMs do not allow for Riemannian optimization for m >= 3.\n\nWe added all three papers you mentioned into the related works section, thanks for suggesting them.\n\nAll the best,\nAlexander"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679565943, "tcdate": 1484679565943, "number": 8, "id": "B1IqtJ2Ig", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "BylWr2YHg", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Author reply", "comment": "We would like to thank the reviewer for a thorough review and valuable suggestions.\n\nRegarding the comparison with FMs optimized over the manifold of positive definite matrices, we believe this comparison is out of the scope of our work since we mainly aimed at modeling high-order interactions, while the Riemannian optimization is possible only for the second-order FMs.\n\nYou are very right to question the random initialization results. We further investigated this effect and found out that it is not that important to initialize the learning from the linear solution built for the dataset at hand, but we may as well generate a random linear model and initialize the learning from it.\nSo there is nothing special about the initialization we had chosen, but it\u2019s important to initialize the model from a proper distribution.\nTwo reasons that may cause this effect:\na) The Tensor Train is a product of a large number of factors (especially in the UCI experiment with 160 TT-cores), and it may raise the problem of vanishing and exploding gradients. It may be interesting to explore how our proposed initialization (a random linear model) is connected to the Xavier initialization from the Neural Networks community.\nb) If we initialize the model to put too much weight on a large number of high-order interactions, it may be hard to recover by gradient optimization: each gradient step would largely affect the high-order interactions and slightly affect the low-order interactions. On the other hand, it seems important to firstly approximate the linear part of the model, and only then try to model the residuals with interactions of a higher order.\n\nAs you suggested, we tried a vanilla feedforward neural network baseline on the synthetic dataset and ran into severe overfitting: it\u2019s easy to optimize the training loss to zero, while the test set performance was 0.5 AUC in all our experiments.\n\n> A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.\nThank you for pointing out the typos, we fixed them in the new version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679423414, "tcdate": 1484679423414, "number": 7, "id": "r1wWKynUx", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "Sy9wO0tSl", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Author reply", "comment": "Thank you for this very thorough review and for the numerous comments, suggestions, and insightful questions. We\u2019ve updated the paper to address them.\n\nRegarding the experimental evaluation, we added more baselines, restructured the text and added experiments to assess different aspects of the model on all the datasets considered, and investigated the influence of the rank on the proposed model performance. \n\n\n> -formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n\nThanks, we clarified this part.\n\n> -the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n\nThat\u2019s right, we clarified this in the new version.\n\n>-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n\nGood point, I haven\u2019t explained it properly in the original submission (fixed in the new version).\nThe reason why it\u2019s O(d r^2 (r+N)) is because the projection consists of two parts: 1) preprocessing, which cost O(d r^3); 2) and the actual projection, which cost O(d r^2 TT_rank(Z)^2).\nSince we are projecting all the gradients on the same tangent space defined by W, we have to do the preprocessing step just once.\n\n>-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n\nThat\u2019s an excellent question, and we believe that we found an answer (and added it to the new version of the paper). We observed that it is not that important to initialize the learning from the linear solution built for the dataset at hand, but we may as well generate a random linear model and initialize the learning from it.\nSo there is nothing special about the initialization we had chosen, but it\u2019s important to initialize the model from a proper distribution.\nTwo reasons that may cause this effect:\na) The Tensor Train is a product of a large number of factors (especially in the UCI experiment with 160 TT-cores), and it may raise the problem of vanishing and exploding gradients. It may be interesting to explore how the initialization from a random linear model is connected to the Xavier initialization from the Neural Networks community.\nb) If we initialize the model to put too much weight on a large number of high-order interactions, it may be hard to recover by gradient optimization: each gradient step would largely affect the high-order interactions and slightly affect the low-order interactions. On the other hand, it seems important to firstly approximate the linear part of the model, and only then try to model the residuals with interactions of a higher order.\n\n> -section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n> -section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n\nWe found that after a slight change to the learning procedure dropout becomes unnecessary, so we decided to remove it whatsoever from all the experiments (see the description of the updates made to the paper).\n\n> -how do you choose r_0 in you experiments? with a validation set?\n\nWe tried a few reasonable options by hand and chose the best one according to the test error.\nI agree, using the validation set would be fairer, but the overfitting is unlikely to occur in this situations because of the small number of variants we tried and because of the robustness of the model to the choice of the TT-rank (see the experiment we added to investigate this).\n\n> -in section 7: why you don't have x_1 x_2 among the variables?\n\nActually, we have all terms from the previous model (\\hat{y}(x)) plus some new terms which involve logarithms. Thank you for spotting this clarity issue, we made this point more clear in the new version.\n\n> -section 8: there is a typo in \"experiments\"\n> -section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n\nThanks, we fixed these typos.\n\n> -section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?\n\nWe added the training and inference time and also the performance of the logistic regression."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1484679058630, "tcdate": 1484679058630, "number": 6, "id": "ryscP1n8e", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Update", "comment": "We would like to thank all the reviewers and commenters for their time, feedback, and ultimately for making our paper better.\n\nWe updated the paper to address the questions raised in the reviews.\n\nWe found out that the backtracking algorithm is beneficial only for the small-scale setting when the mini-batch size is of the order of the full dataset (e.g. UCI experiments). Otherwise, backtracking is (locally) tuning the learning rate too well and overfits to each mini-batch, which stagnates the convergence process.\nFor this reason, we removed the backtracking from the paper text and from all the experiments.\nWe also found out that after removing the backtracking, the dropout becomes unnecessary, and we removed it as well.\n\nOther changes:\n1) We added the complexity of each step of the Riemannian gradient descent.\n2) Investigated the effect of the proper initialization on the model and proposed a random initialization that works on par with the initialization from the solution of the linear model.\n3) Added the validation loss for the comparison of optimizers on the UCI datasets.\n4) Added the convergence plots to compare Riemannian optimization vs SGD baseline on the synthetic dataset.\n5) Added 3 suggested papers to the related work section.\n6) Plotted the TT-rank vs. accuracy graph on the MovieLens 100K dataset.\n7) Restructured the experiments section to make it easier to assess different aspects of the model on different datasets.\n8) Added a feedforward neural network baseline for the synthetic dataset.\n9) Fixed typos and made a few clarifications. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484679002622, "tcdate": 1478290208758, "number": 422, "id": "rkYmiD9lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkYmiD9lg", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "content": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": ["rkm1sE4tg"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483495521766, "tcdate": 1483495521766, "number": 4, "id": "Sy9wO0tSl", "invitation": "ICLR.cc/2017/conference/-/paper422/official/review", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/conference/paper422/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper422/AnonReviewer2"], "content": {"title": "Nice idea, but experiments are very preliminary", "rating": "5: Marginally below acceptance threshold", "review": "The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension. The factorization employed is based on the TT format, first proposed by Oseledests (2011). The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.\n\nThe paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.\n\nOn the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets. Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments. In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.\n\nI think the paper is close to a weak acceptance / weak rejection, I don't rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting. In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.\n\nSome minor comments:\n-formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n-the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n-section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n-how do you choose r_0 in you experiments? with a validation set?\n-in section 7: why you don't have x_1 x_2 among the variables?\n-section 8: there is a typo in \"experiments\"\n-section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n-section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n-section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483495522304, "id": "ICLR.cc/2017/conference/-/paper422/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper422/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper422/AnonReviewer3", "ICLR.cc/2017/conference/paper422/AnonReviewer1", "ICLR.cc/2017/conference/paper422/AnonReviewer4", "ICLR.cc/2017/conference/paper422/AnonReviewer2"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483495522304}}}, {"tddate": null, "tmdate": 1483486456518, "tcdate": 1483486456518, "number": 3, "id": "BylWr2YHg", "invitation": "ICLR.cc/2017/conference/-/paper422/official/review", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/conference/paper422/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper422/AnonReviewer4"], "content": {"title": "Interesting idea, analysis could be improved", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.\n\nFrom a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.\n\nI think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.\n\nA few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.\n\nOverall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483495522304, "id": "ICLR.cc/2017/conference/-/paper422/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper422/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper422/AnonReviewer3", "ICLR.cc/2017/conference/paper422/AnonReviewer1", "ICLR.cc/2017/conference/paper422/AnonReviewer4", "ICLR.cc/2017/conference/paper422/AnonReviewer2"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483495522304}}}, {"tddate": null, "tmdate": 1482397154558, "tcdate": 1482397076322, "number": 5, "id": "B12cSftVl", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["~Mathieu_Blondel1"], "readers": ["everyone"], "writers": ["~Mathieu_Blondel1"], "content": {"title": "Some comments and related works", "comment": "Hi Alexander,\n\nYour paper is an interesting addition to the literature on low-rank polynomial models. Good work!\n\nI agree with a previous comment that it would be worth adding more comments on the difference between FMs and EMs in Section 9. For instance, EMs model all d-combinations while FMs model combinations up to some degree. It would also be informative to compare the total model size of both models.\n\nI am not sure I agree that \"TT-format allows for Riemannian optimization\" is an advantage of tensor trains. For example, it should be possible to train FMs over the positive definite matrix manifold.\n\nAdmittedly, the literature is fairly recent but here are a few relevant prior works:\n\n- \"On the Computational Efficiency of Training Neural Networks\" by Livni et al. https://arxiv.org/abs/1410.1141\n\n-  \"Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms\" by my colleagues and me https://arxiv.org/abs/1607.08810\n\n- \"Supervised Learning with Quantum-Inspired Tensor Networks\" by Stoudenmire and Schwab https://arxiv.org/abs/1605.05775\n\nBest regards"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1482253770537, "tcdate": 1482253770537, "number": 2, "id": "BJMAByP4l", "invitation": "ICLR.cc/2017/conference/-/paper422/official/review", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/conference/paper422/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper422/AnonReviewer1"], "content": {"title": "A good paper ", "rating": "7: Good paper, accept", "review": "This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.\n\nThe proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483495522304, "id": "ICLR.cc/2017/conference/-/paper422/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper422/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper422/AnonReviewer3", "ICLR.cc/2017/conference/paper422/AnonReviewer1", "ICLR.cc/2017/conference/paper422/AnonReviewer4", "ICLR.cc/2017/conference/paper422/AnonReviewer2"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483495522304}}}, {"tddate": null, "tmdate": 1482148466639, "tcdate": 1482148466639, "number": 1, "id": "HJod5BH4l", "invitation": "ICLR.cc/2017/conference/-/paper422/official/review", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/conference/paper422/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper422/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper describes how to use a tensor factorization method called Tensor Train for modeling the interactions between features for supervised classification tasks. Tensor Train approximates tensors of any dimensions using low rank products of matrices. The rank is used as a parameter for controlling the complexity of the approximation. Experiments are performed on different datasets for binary classification problems.\n\nThe core of the paper consists in demonstrating how the TT formalism developed by one of the authors could be adapted for modeling interactions between features. Another contribution is a gradient algorithm that exploits the geometrical structure of the factorization. These ideas are probably new in Machine learning. The algorithm itself is of reasonable complexity for the inference and could probably be adapted to large size problems although this is not the case for the experiments here.\n\nThe experimental section is not well structured, it is incomplete and could be improved. We miss a description of the datasets characteristics. The performance on the different datasets are not provided. Each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments. The experiments on the 2 UCI datasets show optimization performance on the training set, you could provide the same curves on test sets to show how the algorithm generalizes. The comparison to other approaches (section 8.4) is only performed on artificial data, which are designed with interacting features and are not representative of diverse situations. The same holds for the role of Dropout. The comparison on the Movielens dataset is incomplete. Besides, all the tests are performed on small size problems.\n\nOverall, there are original contributions which could be worth a publication. The experiments are incomplete and not conclusive. A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483495522304, "id": "ICLR.cc/2017/conference/-/paper422/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper422/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper422/AnonReviewer3", "ICLR.cc/2017/conference/paper422/AnonReviewer1", "ICLR.cc/2017/conference/paper422/AnonReviewer4", "ICLR.cc/2017/conference/paper422/AnonReviewer2"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483495522304}}}, {"tddate": null, "tmdate": 1481963209091, "tcdate": 1481963209091, "number": 4, "id": "B1ZAI_GVl", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Revision", "comment": "Yesterday I updated the pdf. The only thing that changed is that I added a paragraph at the end of the related works section to credit the papers suggested by the comment below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1481903817843, "tcdate": 1481903817843, "number": 3, "id": "r1MCRKbEx", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rJjjlvkVl", "signatures": ["~Alexander_Novikov1"], "readers": ["everyone"], "writers": ["~Alexander_Novikov1"], "content": {"title": "Thanks for the comments", "comment": "Thank you for your comment.\nIndeed, we didn't outperform the High-Order Factorization Machines in the experiments. However, we obtained the same results as they did (up to the noise in the experiment runs), and this may serve as a proof-of-the-concept that our approach - to model very high-degree polynomials - may work in principle.\nRegarding the computational time, note that the High Order Factorization Machines are implemented in Tensor Flow in a highly parallel manner, while we use a pure Python+Jit implementation.\n\nThanks a lot for pointing out these very relevant works. Indeed, one can generalize the CP bounds to our case by using the fact that any CP tensor of rank r can be represented as a TT tensor of rank r^2. The bound obtained this way would, however, be rather weak (of order r^2(1+BBx)^d d^3 sqrt(ln(d))/sqrt(n) ), since Tensor Machines assumes that the order of interactions that we model `q` is small. On the other hand, we observed in the experiments that modeling all interactions of every order can give reasonable test accuracy.\nHowever, I do agree with you that careful adaptations of the proof of the CP bound can provide a much tighter generalization bound and it\u2019s an interesting future research direction. \nI\u2019ll add the papers you mentioned to the related works in the following few hours.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1481760931477, "tcdate": 1481760931468, "number": 2, "id": "rJjjlvkVl", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "questions and related works", "comment": "From the reported experiments on MovieLens and synthetic data, it seems that higher-order FMs give better accuracy at lower cost than ExMs. Is there experimental evidence that potentially modeling all the feature interactions gives better performances on some problems (on real data)?\n\nOn a related note, it'd be interesting to know what capacity your model has as a function of the TT-rank: does the generalization error depend in a nice way on the TT-rank, or does it blow up?\n\nTwo related works are:\n- \"Tensor machines for learning target-specific polynomial features\" which learns a CP factorization for polynomial regression and proves generalization error bounds; the proof is probably adaptable to your setting.\n- \"Learning multidimensional Fourier series with tensor trains\", which uses TT-decomposition to learn 'random' Fourier features."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1480769213932, "tcdate": 1480769213927, "number": 1, "id": "SkLT0VlXg", "invitation": "ICLR.cc/2017/conference/-/paper422/public/comment", "forum": "rkYmiD9lg", "replyto": "S1HA2symx", "signatures": ["~Ivan_Oseledets1"], "readers": ["everyone"], "writers": ["~Ivan_Oseledets1"], "content": {"title": "It is tensor-train decomposition", "comment": "Neither of both; it is a tensor-train decomposition.  You can read more about it here: http://epubs.siam.org/doi/abs/10.1137/090752286\nShortly, it can be computed using SVD (PARAFAC can not), and does not have intrinsic curse of dimensionality (as Tucker) for really high dimensions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287583964, "id": "ICLR.cc/2017/conference/-/paper422/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkYmiD9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper422/reviewers", "ICLR.cc/2017/conference/paper422/areachairs"], "cdate": 1485287583964}}}, {"tddate": null, "tmdate": 1480731852851, "tcdate": 1480731852846, "number": 1, "id": "S1HA2symx", "invitation": "ICLR.cc/2017/conference/-/paper422/pre-review/question", "forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "signatures": ["ICLR.cc/2017/conference/paper422/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper422/AnonReviewer1"], "content": {"title": "tensor decomposition", "question": "Is the tensor decomposition in figure 1 Tucker decomposition or Paraface decomposition?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "pdf": "/pdf/49df2ac890dcb6377613b286f748e49f6eb70824.pdf", "TL;DR": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "paperhash": "novikov|exponential_machines", "conflicts": ["phystech.edu", "skoltech.ru", "bayesgroup.ru", "msu.ru", "hse.ru"], "keywords": ["Supervised Learning", "Optimization"], "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959290634, "id": "ICLR.cc/2017/conference/-/paper422/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper422/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper422/AnonReviewer1"], "reply": {"forum": "rkYmiD9lg", "replyto": "rkYmiD9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959290634}}}], "count": 19}