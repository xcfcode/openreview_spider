{"notes": [{"id": "rkl6As0cF7", "original": "SkgpPa69Y7", "number": 938, "cdate": 1538087893149, "ddate": null, "tcdate": 1538087893149, "tmdate": 1548269563269, "tddate": null, "forum": "rkl6As0cF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkxYnG7-g4", "original": null, "number": 1, "cdate": 1544790704850, "ddate": null, "tcdate": 1544790704850, "tmdate": 1545354524708, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Meta_Review", "content": {"metareview": "Pros:\n- novel idea of endowing RL agents with recursive reasoning\n- clear, well presented paper\n- thorough rebuttal and revision with new results\n\nCons:\n- small-scale experiments\n\nThe reviewers agree that the paper should be accepted.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper938/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352772868, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352772868}}}, {"id": "BJltf7tYk4", "original": null, "number": 14, "cdate": 1544291089398, "ddate": null, "tcdate": 1544291089398, "tmdate": 1544291089398, "tddate": null, "forum": "rkl6As0cF7", "replyto": "BygPe4xt67", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Thanks for clarifications.", "comment": "Thanks for the comments and more thorough comparison. This addresses most of my concerns.\n\nAs noted by someone in the comment earlier, LOLA does seem to work with opponent modeling. So I would avoid claiming that as an advantage of PR2."}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "Syee-B352X", "original": null, "number": 2, "cdate": 1541223672112, "ddate": null, "tcdate": 1541223672112, "tmdate": 1544290650813, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "content": {"title": "Interesting and sound ideas and algorithms, but experimental validation is weak", "review": "\n# Summary:\nThe paper proposes a new approach for fully decentralized training in multi-agent reinforcement learning, termed probabilistic recursive reasoning (PR2). The key idea is to build agent policies that take into account opponent best responses to each of the agent's potential actions, in a probabilistic sense. The authors show that such policies can be seen as recursive reasoning, prove convergence of the proposed method in self-play, a demonstrate it in a couple of iterated normal form games with non-trivial Nash equilibria where baselines fail to converge.\n\nI believe the community will find intuitions, methods, and theory developed by the authors interesting. However, I find some parts of the argument somewhat questionable as well as experimental verification insufficient (see comments below).\n\n\n# Comments and questions:\n\n## Weaknesses in the experimental evaluation:\nI find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradient-based policy search methods).\n\nIn the introduction (paragraph 2), the authors point out potential limitations of previous opponent modeling algorithms, but never compare with them in experiments. If the claim is that other methods \"tend to work only under limited scenarios\" while PR2 is more general, then it would be fair to ask for a comprehensive comparison of PR2 vs alternatives in at least 1 such scenario. I would be interested to see how the classical family of \"Win or Learn Fast\" (WoLF) algorithms (Bowling and Veloso, 2002) and the recent LOLA (Foerster et al, 2018) compare with PR2 on the iterated matrix game (section 5.1).\n\nAlso, out of curiosity, it would be interesting to see how PR2 works on simple iterated matrix games, eg iterated Prisoner's dilemma.\n\n## Regarding the probabilistic formulation (section 4.3)\nEq. 8 borrows a probabilistic formulation of optimality in RL from Levine (2018). The expression given in Eq. 8 is proportional to the probability of a trajectory conditional on that each step is optimal wrt the agent's reward r^i, i.e., not for p(\\tau) but for p(\\tau | O=1).\n\nIf I understand it correctly, by optimizing the proposed KL objective, we fit both \\pi^i and \\rho^{-i} to the distribution of *optimal trajectories* with respect to r^i reward. That makes sense in a cooperative setting, but the problem arises when opponent's reward r^{-i} is different from r^i, in which case I don't understand how \\rho^{-i} happens to approximate the actual policy of the opponent(s). Am I missing something here?\n\nA minor point: shouldn't \\pi^{-i} in eq. 9 be actually \\rho^{-i}? (The derivations in appendix C suggest that.)\n\n## Regarding alternative approaches (section 4.5)\nThe authors point out intractability of trying to directly approximate \\pi^{-i}. The argument here is a little unclear. Wouldn't simple behavioral cloning work? Also, could we minimize KL(\\pi^{-i} || \\rho^{-i}) instead of KL(\\rho^{-i} || \\pi^{-i})?\n\n# Minor\n- I might be misreading it, but the last sentence of the abstract seems to suggest that this paper introduces opponent modeling to MARL, which contradicts the first sentence of paragraph 2 in the introduction.\n- It is very hard to read plots in Figure 3. Would be nice to have them in a larger format.\n\nOverall, I find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "cdate": 1542234193268, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335983034, "tmdate": 1552335983034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxuUWYYJ4", "original": null, "number": 13, "cdate": 1544290639950, "ddate": null, "tcdate": 1544290639950, "tmdate": 1544290639950, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rkgm3P2tCX", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Concerns addressed", "comment": "Authors have addressed my concerns."}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "S1gKjx-cCQ", "original": null, "number": 11, "cdate": 1543274656924, "ddate": null, "tcdate": 1543274656924, "tmdate": 1543274656924, "tddate": null, "forum": "rkl6As0cF7", "replyto": "S1xJ4tgc0m", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Thanks for the comments.", "comment": "Emphasizing K=1 is a critical point that we would surely incorporate in the final version, we thank the reviewer for highlighting this."}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "rJeOPGyda7", "original": null, "number": 3, "cdate": 1542087263566, "ddate": null, "tcdate": 1542087263566, "tmdate": 1543273828644, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "content": {"title": "A good first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities", "review": "The high-level problem this paper tackles is that of endowing RL agents with recursive reasoning capabilities in a multi-agent setting, based on the hypothesis that recursive reasoning is beneficial for the agents to converge to non-trivial equilibria.\n\nThe authors propose the probabilistic recursive reasoning (PR2) framework for an n-agent stochastic game. The conceptual difference between PR2 and non-correlated factorizations of the joint policy is that, from the perspective agent i, PR2 augments the joint policy of all agents by conditioning the policies of agent i's opponents on the action that agent i took. The authors derive the policy gradient for PR2 and show that it is possible to learn these action-conditional opponent policies via variational inference in addition to learning the policy and critic for agent i.\n\nThe proposed method is evaluated on two experiments: one is an iterated matrix game and the other is a differential game (\"Max of Two Quadratics\"). The authors show in the iterated matrix game that baselines with non-correlated factorization rotate around the equilibrium point, whereas PR2 converges to it. They also show in the differential game that PR2 discovers the global optimum whereas baselines with non-correlated factorizations do not.\n\nThis paper is clear, well-motivated, and well-written. I enjoyed reading it. I appreciated the connection to probabilistic reinforcement learning as a means for formulating the problem of optimizing the variational distribution for the action-conditional opponent policy and for making such an optimization practical. I also appreciated the illustrative choice of experiments that show the benefit of recursive reasoning. \n\nCurrently, PR2 provides a proof-of-concept of recursive reasoning in a multi-agent system where the true equilibrium is already known in closed form; it remains to be seen to what extent PR2 is applicable to multi-agent scenarios where the equilibrium the system is optimizing is less clear (e.g. GANs for image generation). Overall, although the experiments are still small scale, I believe this paper should be accepted as a first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities.\n\nBelow are several comments.\n\n1. Discussion of limitations: As the authors noted in the Introduction and Related Work, multi-agent reinforcement problems that attempt to model opponents' beliefs often become both expensive and impractical as the number of opponents (N) and the recursion depth (k) grows because such complexity requires high precision in the approximate the optimal policy. The paper can be made stronger with experiments that illustrate to what extent PR2 practically scales to problems with N > 2 or K > 1 in terms of how practical it is to train.\n2. Experiment request: To what extent do the approximation errors affect PR2's performance? It would be elucidating for the authors to include an experiment that illustrates where PR2 breaks down (for example, perhaps in higher-dimensional problems).\n3. Minor clarification suggestion: In Figure 1: it would be clearer to replace \"Angle\" with \"Perspective.\"\n4. Minor clarification suggestion: It would be clearer to connect line 18 of Algorithm 1 to equation 29 on Appendix C.\n5. Minor clarification suggestion: In section 4.5: \"Despite the added complexity\" --> \"In addition to the added complexity.\"\n6. Minor clarification: How are the importance weights in equation 7 reflected in Algorithm 1?\n7. Minor clarification: In equation 8, what is the significance of integrating over time rather than summing?\n8. Minor clarification: There seems to be a contradiction in section 5.2 on page 9. \"the learning outcomes of PR2-AC and MASQL are extremely sensitive to the way of annealing...However, our method does not need to tune the the annealing parameter at all...\" Does \"our method\" refer to PR2-AC here?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "cdate": 1542234193268, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335983034, "tmdate": 1552335983034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xJ4tgc0m", "original": null, "number": 10, "cdate": 1543272743366, "ddate": null, "tcdate": 1543272743366, "tmdate": 1543272743366, "tddate": null, "forum": "rkl6As0cF7", "replyto": "SkxO7Het6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Revisions address most of my concerns", "comment": "The new version of the paper addresses most of my concerns. For the final version of the paper, I would suggest that the authors emphasize that they focus only level-1 recursion in the introduction to be precise about the scope of their contribution. It is a good first step towards level-k recursion for k > 1, but because the PR2 framework as presented captures only level-1 recursion, stating upfront that the paper is concerned with level-1 recursion would avoid confusing the reader."}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "r1g7Ly5V0m", "original": null, "number": 2, "cdate": 1542917962682, "ddate": null, "tcdate": 1542917962682, "tmdate": 1542917962682, "tddate": null, "forum": "rkl6As0cF7", "replyto": "BygPe4xt67", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Public_Comment", "content": {"comment": "I agree with the reviewer 3: It would be interesting to see results for the iterated prisoner's dilemma . I would also suggest adding matching pennies. \nPR2 looks like a very promising algorithm and seeing results on these standard iterated matrix games will help understand more about its functioning.\n\nTwo small points regarding LOLA:\n1) LOLA was developed and tested with opponent modelling, so access to the opponent's policy is not required. Please see the results in the original paper. \n2) \"..pre-defined opponent strategies (e.g. Tit-fot-Tat in iterated Prisoner\u2019s Dilemma (Foerster et al., 2018))..\": This is inaccurate. Tit-for-Tat is an emergent strategy that LOLA agents end up discovering, rather than pre-defined. ", "title": "Prisoner's dilemma results + small issues around presentation of LOLA."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311717032, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkl6As0cF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311717032}}}, {"id": "rJehIcWt2Q", "original": null, "number": 1, "cdate": 1541114451782, "ddate": null, "tcdate": 1541114451782, "tmdate": 1542171316905, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "content": {"title": "Significant updates to the new version", "review": "The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.\n\n(\nUpdate: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. \n\nHowever, I wonder if the reasoning for PR2 is limited to \"self-play\", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.\n\nAlso, maybe explain self-play mathematically to make the paper self contained?\n)\n\n1. From the abstract, \" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario\". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the \"convergence\" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?\n\n2. Theorem 2 also assumes that the opponent model $\\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\\rho$ or $Q_\\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.\n\n3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.\n\nMinor points:\n- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?\n- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?\n- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.\n- Figure 4(a): what are the blue dots?\n- Does (11) depend on the amount of data collected from the opponents? If so, how?\n- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.\n- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?\n- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Review", "cdate": 1542234193268, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl6As0cF7", "replyto": "rkl6As0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335983034, "tmdate": 1552335983034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklp6Elta7", "original": null, "number": 4, "cdate": 1542157509143, "ddate": null, "tcdate": 1542157509143, "tmdate": 1542160203078, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rJehIcWt2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "additional experiments added with other detailed questions addressed (part 2/2)", "comment": "\n(This is part 2/2)\n === AnonReviewer2 ===\n\nQuestion_2: \u201cdemonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper.\u201d\n\nAnswer_2: We have managed to add all 4 particle games that are used in the MADDPG paper. Please find Fig. 6 in the updated paper. Our method can achieve the best performance in the fully-cooperative task, and comparative performance in the other three competitive tasks comparing to the MADDPG that is a centralized method. We also tested the adversarial setting in those three competitive tasks (physical deception, predator-prey, and keep-away), where our method can beat the independent methods (DDPG, DDPG-OM), and keep better performance. However, we did find that it is hard to outperform on the task of physical deception; we believe it is mainly due to the fact that during training, centralized critic can access the full knowledge of the exact policies of PR2-AC, but PR2-AC cannot access the models of its opponents in a reverse way; this could potentially place PR2-AC in an inferior position in the testing time as the way how PR2-AC would deceive the opponent is known by the opponent already.\n\n\nQuestion_3:\u201ddifference between PR2 methods with single-agent RL and IGA\u201d\n\nAnswer_3: Both the single-agent RL and IGA can be categorised into the case of non-correlated factorization on the joint policy, as it is described by Eq.2 in Section 3.1, whereas PR2 methods conduct a different way of decomposition on the joint policy, as shown by Eq 3 in Section 4.1. Even equipped with full knowledge of the opponent, PR2 is still markedly different from single-agent RL or IGA because PR2 takes into account opponent\u2019s behavior in its own decision making, i.e. $\\pi(a^i | s, a^-i)$, and assuming all the agents will do so, as such, a recursive loop is formed, while the former two methods do not have certain dependency in the policies. We believe having such dependency in agent\u2019s policy is critical for multiagent reinforcement learning tasks; it is justified by three different types of experiments, including matrix games, different games, and cooperative-competitive games where IGA or independent learning simply fails. \n\n\nQuestion_4: \u201cblue dots in Figure 4a\u201d\n\nAnswer_4: those are the 1000 exploratory steps in the start phase before training.\n\n\nQuestion_5: \u201cDoes (11) depend on the amount of data collected from the opponents? If so, how?\u201d\n\nAnswer_5: The results in the original submission was reported by sampling the opponent policy 16 times during each update. We have tried sampling 32 times during the rebuttal period and find no major difference on the performance.\n\n\nQuestion_6: \u201ccompare with SGA (Balduzzi et al) or Optimistic mirror descent\u201d\n\nQuestion_6: We added the SGA experiment on the differential game in Section 5.2, where the centralised SGA optimization with the two independent DDPG agents. In Fig.4b and Fig.5c,  SGA can steadily converge to the local maximum as promised in original paper, but compared to PR2-AC, it still fails to find the global maximum in that differential game. \n\n\nQuestion_7: \u201cablation study on inferring the opponent policies\u201d\n\nQuestion_7: Instead of setting rule-based opponent strategy as baselines, we believe it is more illustrative to conduct the ablation study in such a way that using PR2 method to play against MADDPG which requires to know the exact policy parameters and actions that PR2 will take. Such nature of MADDPG makes it a perfect baseline model to understand the opponent module of PR2 methods.  The effectiveness of the opponent modeling module in PR2 is evaluated by Fig. 5h on the differential game, and by Fig. 6 on the multi-state cooperative-competitive games.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "SkeFiNgF6X", "original": null, "number": 3, "cdate": 1542157473029, "ddate": null, "tcdate": 1542157473029, "tmdate": 1542160188009, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rJehIcWt2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "additional experiments added with other detailed questions addressed (part 1/2)", "comment": "\n(This is part 1/2)\n === AnonReviewer2 ===\n\nWe first thank the reviewer for the constructive reviews that would surely help improving the paper. The main issue is that the experiment appears to be insufficient. We have addressed the issue. Following the comments, we have included additional comparisons of the following baselines:\nSGA (Balduzzi et al, 2018),\nWoLF-IGA (Bowling and Veloso, 2002),\nLOLA (Foerster et al, 2018),  \non the iterated matrix game. We add one extra zero-sum matrix game: matching pennies. We further evaluate our PR2 methods on multi-state multiplayer tasks that are more complicated. They include four different cooperative and competitive scenarios, i.e.\nCooperative navigation,\nPhysical deception,\nPredator-prey,\nKeep-away.\nThe above scenarios settings follow those in MADDPG (Lowe, Ryan, et al. 2017). \n\nBased on the Fig. 6 in the updated submission, our conclusion is that the proposed PR2 method can achieve the best performance over all baselines on the fully-cooperative tasks. On the competitive tasks, PR2 still performs much better than all the independent learners, but slightly worse than MADDPG. We believe the main reason is that MADDPG is centralized method that requires to know the opponent\u2019s exact policies whereas PR2 are fully decentralized methods. Due to the nature of competition, centralized methods that can know opponent policies in advance will have great advantages on the performance than decentralized algorithms.\n\nOur code has anonymously been uploaded onto the github (https://github.com/ml3705454/mapr2).\n\n\nQuestion_1: \u201cdiscuss the case where $\\rho$ or $Q_\\theta$ in question is imperfect\u201d\n\nAnswer_1: We fully agree with the reviewer that there is possibility that the approximation error coming from the opponent modeling $\\rho$ could destroy the whole sampling trajectory, and drag both agents to move away from the equilibrium. This explains why our current convergence proof requires the (strong) assumption of single Nash equilibrium under self-play, and the proof is in fact only valid when the $\\rho$ is very close to the global optimum, or the $\\rho$ should find the exact opponent policy. However, we believe that this is a fundamental challenge that current MARL community faces, for example, Nash-Q learning (Hu, Junling, et al. 2003), MADDPG(Lowe, Ryan, et al. 2017), LOLA  (Foerster et al, 2018), or the most recent SGA (Balduzzi et al, 2018), they all assume each agent has access to the exact parameters of the opponent; however, in adversarial settings, the opponent\u2019s parameters are typically obscured,  and have to be inferred from the opponent\u2019s state-action trajectories, such as by behavior cloning. In this sense, the approximation error from the inferring policies of other agents is  inevitable. However, our work is still one step further upon those methods. While those methods lose the convergence guarantee when the opponent policy is not exact, we have proved in theory and demonstrated by experiments that, by adopting the variational inference on approximating the opponent conditional policy and its subsequent soft-Q updates, the PR2 methods has nice convergence property in the game with one Nash equilibrium, without knowing the opponent\u2019s exact policy functions. \n\nIn fact, we tried to understand the impact of the approximation error on the matrix game on Section 5.1, it seems that under the self-play scenario with one single equilibrium, the approximation error decreases as the central agent heads towards the equilibrium. We believe understanding the impact of approximation error of opponent policy becomes critical especially in the multi-state multi-player environment, and this leaves considerable space for future work. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "rJe6zkZK6m", "original": null, "number": 6, "cdate": 1542160149292, "ddate": null, "tcdate": 1542160149292, "tmdate": 1542160167658, "tddate": null, "forum": "rkl6As0cF7", "replyto": "Syee-B352X", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Additional experiment added in the updated version of the paper (part 2/2) ", "comment": "\n(This is part 2/2)\n === AnonReviewer3 ===\n\nQuestion_3: \u201cThe argument is unclear on the intractability of directly approximate \\pi^{-i}, could we minimize KL(\\pi^{-i} || \\rho^{-i})\u201d\n\nAnswer_3: Our work assume no access to opponent\u2019s exact policy or value functions; therefore, any expectation over \\pi^{-i} would require an additional layer of modelling the \\pi^{-i} first, which we are concerned of introducing more approximation errors on top of the variational inference.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "BygPe4xt67", "original": null, "number": 2, "cdate": 1542157294820, "ddate": null, "tcdate": 1542157294820, "tmdate": 1542160090661, "tddate": null, "forum": "rkl6As0cF7", "replyto": "Syee-B352X", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "Additional experiment added in the updated version of the paper (part 1/2)", "comment": "\n(This is Part 1/2)\n === AnonReviewer3 ===\n\nWe first thank the reviewer for the constructive reviews that would surely help improving the paper. The main issue is that the experiment appears to be insufficient. We have addressed the issue. Following the comments, we have included additional comparisons of the following baselines:\nSGA (Balduzzi et al, 2018),\nWoLF-IGA (Bowling and Veloso, 2002),\nLOLA (Foerster et al, 2018),  \non the iterated matrix game. We add one extra zero-sum matrix game: matching pennies. We further evaluate our PR2 methods on multi-state multiplayer tasks that are more complicated. They include four different cooperative and competitive scenarios, i.e.\nCooperative navigation,\nPhysical deception,\nPredator-prey,\nKeep-away.\nThe above scenarios settings follow those in MADDPG (Lowe, Ryan, et al. 2017). \n\nBased on the Fig. 6 in the updated submission, our conclusion is that the proposed PR2 method can achieve the best performance over all baselines on the fully-cooperative tasks. On the competitive tasks, PR2 still performs much better than all the independent learners, but slightly worse than MADDPG. We believe the main reason is that MADDPG is centralized method that requires to know the opponent\u2019s exact policies whereas PR2 are fully decentralized methods. Due to the nature of competition, centralized methods that can know opponent policies in advance will have great advantages on the performance than decentralized algorithms.\n\nOur code has anonymously been uploaded onto the github (https://github.com/ml3705454/mapr2).\n\nQuestion_1: \u201cA comprehensive comparison of PR2 vs alternatives in at least 1 such scenario, e.g. \u201cWin or Learn Fast\" (WoLF) algorithms (Bowling and Veloso, 2002) and the recent LOLA (Foerster et al, 2018) compare with PR2 on the iterated matrix game (section 5.1).\u201d\n\nAnswer_1: We have further added both of them (i.e. WolF-IGA and LOLA) on the matrix game in sec 5.1 in Appendix E. Note that that although both WolF-IGA and LOLA  (Fig.7 ) can converge to the Nash equilibrium, both of them require additional information, either the exact the equilibrium/payoff information, or the exact opponent\u2019s value function and its gradient, whereas our PR2 methods do not need either of these information. Although this matrix game is simple; we believe it can help us understand the learning dynamics of the conditional opponent policy (see Fig. 3), especially under the condition that it can converge to non-trivial equilibrium with only the information of historical actions.\n\n\nQuestion_2: \u201cby optimizing the proposed KL objective, we fit both \\pi^i and \\rho^{-i} to the distribution of *optimal trajectories* with respect to r^i reward. That makes sense in a cooperative setting, but the problem arises when opponent's reward r^{-i} is different from r^i, in which case I don't understand how \\rho^{-i} happens to approximate the actual policy of the opponent(s). \u201d\n\nAnswer_2: Despite the high level similarity between single-agent energy-based RL framework and our probabilistic recursive reasoning framework, the fundamental graphical model is different (see Fig. 8 in Appendix E). The \u201cmost probable trajectory\u201d in the multi-agent case, represented together by the variables {O, O^-i}, does not necessarily stand for the trajectory where each agent just chooses the action that will give him the maximum reward (namely the \u201coptimal trajectory\u201d in single-agent case), but rather some kinds of equilibrium that no one would want to deviate from. In the example of the matrix game in Section 5.1, both agents reach the Nash equilibrium at (0.5, 0.5) in the end, that is because agent 1 knows choosing the action 1 which gives the maximum reward 3 (at the same time assuming agent 2 choose action 2) will not last because agent 2 will simply defect to choose action 1 to avoid the case of reward 0 for itself; therefore, the trajectory of (action 1, action 2) is not optimal to agent 1 anymore after considering the consequent influence on agent 2\u2019s action. Another example is to think about the prisoner\u2019s dilemma, (cooperate, cooperate) is not a probable trajectory, because it is always agent\u2019s interest to defect, thereby the {O=1, O^-i =1} will only occur at the (defect, defect) instead. To sum up,  {O, O^-i} describes the likelihood of certain trajectory being observed, in the multi-agent scenario, the goal of equilibrium certainly allows the case where the opponent reward is different from r^i. \n\nTheoretically speaking, we have also proved in Theorem 2 that PR2 methods converge in the games with either fully-cooperative equilibrium or fully-competitive equilibrium. In addition, we have further added one extra zero-sum game, i.e. matching penny, in Fig. 9 of Appendix E. PR2 methods present convergent results as expected.          \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}, {"id": "SkxO7Het6Q", "original": null, "number": 5, "cdate": 1542157600486, "ddate": null, "tcdate": 1542157600486, "tmdate": 1542159878513, "tddate": null, "forum": "rkl6As0cF7", "replyto": "rJeOPGyda7", "invitation": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "content": {"title": "additional experiment results added with limitation of PR2 method discussed", "comment": "\n\n === AnonReviewer4 ===\n\nWe first thank the reviewer for the constructive reviews that would surely help improving the paper. The main issue is that the experiment appears to be insufficient. We have addressed the issue. Following the comments, we have included additional comparisons of the following baselines:\nSGA (Balduzzi et al, 2018),\nWoLF-IGA (Bowling and Veloso, 2002),\nLOLA (Foerster et al, 2018),  \non the iterated matrix game. We add one extra zero-sum matrix game: matching pennies. We further evaluate our PR2 methods on multi-state multiplayer tasks that are more complicated. They include four different cooperative and competitive scenarios, i.e.\nCooperative navigation,\nPhysical deception,\nPredator-prey,\nKeep-away.\nThe above scenarios settings follow those in MADDPG (Lowe, Ryan, et al. 2017). \n\nBased on the Fig. 6 in the updated submission, our conclusion is that the proposed PR2 method can achieve the best performance over all baselines on the fully-cooperative tasks. On the competitive tasks, PR2 still performs much better than all the independent learners, but slightly worse than MADDPG. We believe the main reason is that MADDPG is centralized method that requires to know the opponent\u2019s exact policies whereas PR2 are fully decentralized methods. Due to the nature of competition, centralized methods that can know opponent policies in advance will have great advantages on the performance than decentralized algorithms.\n\nOur code has anonymously been uploaded onto the github (https://github.com/ml3705454/mapr2).\n\n\nQuestion_1: \u201cDiscussion of limitations: experiments that illustrate to what extent PR2 practically scales to problems with N > 2 or K > 1 in terms of how practical it is to train.\u201d\n\nQuestion_2:  \u201cExperiment request: To what extent do the approximation errors affect PR2's performance? It would be elucidating for the authors to include an experiment that illustrates where PR2 breaks down\u201d\n\nAnswers to 1 & 2: We have added experiments on more complicated multi-state multi-player game where we have 2-4 agents, and more complex action space (5 dimensions). Please find the result shown in Fig. 6 in the updated paper. Our method can achieve the best performance in the fully-cooperative task, and comparative performance in the other three competitive tasks comparing to the MADDPG that is a centralized method and require to know the exact opponent policy during training, while our methods do not. We also tested the adversarial setting in those three competitive tasks (physical deception, predator-prey, and keep-away), where our method can beat the independent methods (DDPG, DDPG-OM), and keep better performance. However, we did find that it is hard to outperform the MADDPG;  we believe it is mainly due to the fact that during training, centralized critic can access the full knowledge of the exact policies of PR2-AC, but PR2-AC cannot access the models of its opponents in a reverse way; this could potentially place PR2-AC in an inferior position in the testing time as the way how PR2-AC would deceive the opponent is known by the opponent already. On the other hand, we believe PR2 methods will face more challenges when the number of agents scales to hundreds. As each agent\u2019s policy is dependent on the opponent\u2019s behaviors $\\pi (a^i | s, a^1, a^2, \u2026 , a^99)$, such dependency will become very hard even to sample from.\n\n\nQuestion_3:. How are the importance weights in equation 7 reflected in Algorithm 1?\n\nAnswer_3: Since PR2 methods assume no knowledge about the opponents policies, the Algorithm 1 does not actually use the importance weights, but learns an approximated conditional policy \\rho_{-i}.\n\nAll the other comments have been addressed in the updated paper directly. We thank the reviewer for pointing out.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper938/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning", "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n", "keywords": ["Multi-agent Reinforcement Learning", "Recursive Reasoning"], "authorids": ["ying.wen@cs.ucl.ac.uk", "yaodong.yang@cs.ucl.ac.uk", "rui.luo@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "wei.pan@tudelft.nl"], "authors": ["Ying Wen", "Yaodong Yang", "Rui Luo", "Jun Wang", "Wei Pan"], "TL;DR": "We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.", "pdf": "/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf", "paperhash": "wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper938/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620567, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl6As0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper938/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper938/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper938/Authors|ICLR.cc/2019/Conference/Paper938/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper938/Reviewers", "ICLR.cc/2019/Conference/Paper938/Authors", "ICLR.cc/2019/Conference/Paper938/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620567}}}], "count": 15}