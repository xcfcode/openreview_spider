{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1491612077681, "tcdate": 1487355613250, "number": 105, "id": "BJBkkaNYe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BJBkkaNYe", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "content": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028603003, "tcdate": 1490028603003, "number": 1, "id": "HJXHdKpsx", "invitation": "ICLR.cc/2017/workshop/-/paper105/acceptance", "forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028603533, "id": "ICLR.cc/2017/workshop/-/paper105/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028603533}}}, {"tddate": null, "tmdate": 1489418818256, "tcdate": 1489418818256, "number": 1, "id": "HycScE4ie", "invitation": "ICLR.cc/2017/workshop/-/paper105/official/comment", "forum": "BJBkkaNYe", "replyto": "rJke6qlil", "signatures": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3"], "content": {"title": "Ok, increasing score by 1", "comment": "Thanks for the reply. I'll bump up my score by 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487355613899, "tcdate": 1487355613899, "id": "ICLR.cc/2017/workshop/-/paper105/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "BJBkkaNYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper105/reviewers", "ICLR.cc/2017/workshop/paper105/areachairs"], "cdate": 1487355613899}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489418737507, "tcdate": 1489180131221, "number": 1, "id": "BJikI5lsx", "invitation": "ICLR.cc/2017/workshop/-/paper105/official/review", "forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "signatures": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3"], "content": {"title": "Reasonable start, but not there yet", "rating": "5: Marginally below acceptance threshold", "review": "This paper seeks to build a neural network component for subsampling data. The idea is to build a layer that takes as input a discrete sequence s_1, ..., s_T along with a probability e_t of keeping each element t of the sequence.  The layer is meant to independently choose whether or not to keep each element s_t based on the probability e_t, and then to assemble the kept inputs into a subsequence.  Rather than execute the layer by sampling, the paper proposes to instead compute a marginal distribution over the outputs under this model. It proposes a dynamic program that runs in O(T^3) time and evaluate the method on a simple toy problem.\n\nWhile the big idea seems reasonable and the paper is written clearly, I don't think it's developed enough to warrant publication at the workshop at this point. The main issues are as follows:\n\n- I'm not convinced the algorithm is optimal:\n-- I'm not convinced the O(T^3) cost is necessary.  I would think that a matrix of $output position$ x $input symbol$ could be computed in O(T^2) time using dynamic programming, and then after having computed this, the expected output could be computed in O(T^2) time by summing over the $input symbol$ dimension. Am I missing something?\n-- The algorithm should be implemented in a numerically stable way using log-sum-exps.\n\n- The experiment is very simple and there are no baselines.\n\n- One motivation for subsampling is to shorten a sequence. However, under the marginalization approach, the sequence doesn't actually get shortened. Please discuss this.\n\n\nTypos:\n\"extented\"\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489182108335, "id": "ICLR.cc/2017/workshop/-/paper105/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper105/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3", "ICLR.cc/2017/workshop/paper105/AnonReviewer1"], "reply": {"forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489182108335}}}, {"tddate": null, "tmdate": 1489182906462, "tcdate": 1489182906462, "number": 2, "id": "ByMpgjljl", "invitation": "ICLR.cc/2017/workshop/-/paper105/public/comment", "forum": "BJBkkaNYe", "replyto": "Hk4ip5esl", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Response", "comment": "Thanks for your detailed review!  To address your comments individually:\n\n> The authors propose a dynamic programming algorithm with the computational complexity of O(T^3) and provide some results on toy task.\n\nIn fact, the dynamic program is O(T^2) - this was an error in the originally-posted version of the manuscript, which has been recently fixed.  Sorry that it was not updated early enough for you to see this change.\n\n> The writing of the paper needs some work and the ideas need to be presented more clearly, though I understand that the space limitation makes it difficult to explain everything coherently. Especially, the notation is vague.\n\nThanks for this feedback.  I will add the changes you suggested, and also try to clean up the notation.  I think with a bit more additional space the exposition could be made better.\n\n> Actually, we have tried a very similar idea for character-level neural machine translation a few years ago in order to learn hierarchical alignments, but we never managed to make it work...\n\nVery cool that you were trying a similar idea!  Would be interested in discussing further.  When applying this approach and related ideas, we also experienced non-monotonic alignments and vanishing gradients.  We have some current ongoing work for mitigating both issues.\n\n> In the first page, you say \u201cU \\le T\u201d, but neither U nor T has not defined anywhere in the document.\n\nThey are listed in the definitions of the input and output sequence, as s = {s0, s1, . . . , sT \u22121},  y = {y0, y1, . . . , yU\u22121}, but you are right that this could be made clearer; we will address this.\n\n> Please present a more precise probabilistic for e_t. You just say p(y_0=s_0) = e_0, but a more general formal definition would be useful.\n\nGood idea.  The basic idea is that they e_t is the \"probability of including element s_t in the output sequence\".  In practice, they are computed as a function of the network states.  We will be sure this information is clear in the paper.\n\n> A figure or the visualization of the automata/algorithm that generates the task would be useful(perhaps in the appendix).\n\nWe do in fact have such a diagram.  We will add it in an appendix.\n\n> This is more of a curious empirical question, but can this algorithm generalize to the sequences longer than the ones that it has been trained on?\n\nIn practice, we found that it was able to generalize.  In particular, because of the curriculum learning strategy we employed, we found that it was able to learn the correct algorithm on short examples and apply them to longer examples.\n\n\nThank you again for your comments.  We hope we have addressed your concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487355613900, "tcdate": 1487355613900, "id": "ICLR.cc/2017/workshop/-/paper105/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper105/reviewers"], "reply": {"forum": "BJBkkaNYe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487355613900}}}, {"tddate": null, "tmdate": 1489182107807, "tcdate": 1489182107807, "number": 2, "id": "Hk4ip5esl", "invitation": "ICLR.cc/2017/workshop/-/paper105/official/review", "forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "signatures": ["ICLR.cc/2017/workshop/paper105/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper105/AnonReviewer1"], "content": {"title": "Interesting and plausible idea but needs more work.", "rating": "6: Marginally above acceptance threshold", "review": "Subsampling In Expectation\n\nSummary: \n\nThis paper proposes a way to sample a shorter sequence y=(y_0, y_1, ..., y_t) from the input sequence x=(x_0, ..., x_k) according to the probabilities e=(e_0, ..., e_k). The authors propose a dynamic programming algorithm with the computational complexity of O(T^3) and provide some results on toy task.\n\nA General Comment: The writing of the paper needs some work and the ideas need to be presented more clearly, though I understand that the space limitation makes it difficult to explain everything coherently. Especially, the notation is vague. However, the idea makes sense and it is correct in principal. Actually, we have tried a very similar idea for character-level neural machine translation a few years ago in order to learn hierarchical alignments, but we never managed to make it work. One main limitation we observed at the time for char-level NMT that, this kind of algorithm can only generate monotonic alignments, and for the language pairs such as, Ch-En or Tr-En where the alignments can be highly non-monotonic, we could not observe much improvements and also the vanishing gradients arising from the products and the sigmoids were crippling the training. Efficiency was also another issue for us. But authors of this paper shows that in principle this idea works in the toy cases, I guess the challenge remains to find the right architecture and a way to scale the algorithm to right tasks.\n\nMore detailed comments:\n\nIn the first page, you say \u201cU \\le T\u201d, but neither U nor T has not defined anywhere in the document.\nPlease present a more precise probabilistic for e_t. You just say p(y_0=s_0) = e_0, but a more general formal definition would be useful.\nA figure or the visualization of the automata/algorithm that generates the task would be useful(perhaps in the appendix).\nThis is more of a curious empirical question, but can this algorithm generalize to the sequences longer than the ones that it has been trained on?\n\nConclusion,\nPros: - A simple algorithm to subsample the sequences.\n- Interesting results on a toy task.\n\nCons:\n- The proposed algorithm is O(T^3) which is quite difficult to scale for the long sequences and realistic tasks.\n- The experiments are not convincing enough.\n- The writing is not clear enough and needs some more work(this is a minor cons).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489182108335, "id": "ICLR.cc/2017/workshop/-/paper105/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper105/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper105/AnonReviewer3", "ICLR.cc/2017/workshop/paper105/AnonReviewer1"], "reply": {"forum": "BJBkkaNYe", "replyto": "BJBkkaNYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper105/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489182108335}}}, {"tddate": null, "tmdate": 1489181927459, "tcdate": 1489181927459, "number": 1, "id": "rJke6qlil", "invitation": "ICLR.cc/2017/workshop/-/paper105/public/comment", "forum": "BJBkkaNYe", "replyto": "BJikI5lsx", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Responses", "comment": "Hi, thanks for your thorough review!\n\nTo immediately address your first concern, I believe you reviewed the initially posted version of this manuscript, not the most recent one - we recently updated it to reflect the error in stating that the dynamic program had cubic complexity.  We apologize for not having the corrected version online early enough for you to consider it.\n\nTo address your second concern, this is indeed how it is implemented - you can see here in the example code posted along with the paper:\nhttp://nbviewer.jupyter.org/github/craffel/subsampling_in_expectation/blob/master/Subsampling%20in%20Expectation.ipynb#TensorFlow-example\nIf you think it's appropriate, we can include this information in the manuscript.\n\nIn terms of the experiment, we appreciate the criticism that it is overly simple and there are no baselines.  The purpose of this abstract was solely to propose the approach and show a proof-of-concept that it works; unfortunately, there was not sufficient space for further experiments.\n\nFinally, while as you suggest marginalization does not actually shorten the sequence as you say, it does have the effect of placing sequence elements closer together in the output sequence.  For example, if the input sequence was\n[a, b, c, d, e]\nand the subsampling probabilities were\n[1, 0, 0, 1, 0]\nthen the expected output would be\n[a, d, 0, 0, 0]\nAs you say, this sequence is not shorter, but if there is an important dependency between a and d and the remaining symbols are distractors, the resulting timelag between them has been made substantially shorter.  We tried to mention this effect in the bullet points at the beginning of the abstract, but we can try to make it clearer in later versions.\n\nThank you again for the review, we hope we have addressed your concerns!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training a Subsampling Mechanism in Expectation", "abstract": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.  We test this approach on a simple toy problem and discuss its shortcomings.", "pdf": "/pdf/846d61a2ca00088450355aed255730c0ce7c9d7f.pdf", "TL;DR": "We describe a mechanism for subsampling sequences which can be trained in expectation with standard backpropagation.", "paperhash": "raffel|training_a_subsampling_mechanism_in_expectation", "conflicts": ["google.com"], "keywords": ["Theory", "Deep learning", "Structured prediction"], "authors": ["Colin Raffel", "Dieterich Lawson"], "authorids": ["craffel@gmail.com", "dieterichl@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487355613900, "tcdate": 1487355613900, "id": "ICLR.cc/2017/workshop/-/paper105/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper105/reviewers"], "reply": {"forum": "BJBkkaNYe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487355613900}}}], "count": 7}