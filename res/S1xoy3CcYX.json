{"notes": [{"id": "HJxwxAcAEV", "original": null, "number": 6, "cdate": 1549868527371, "ddate": null, "tcdate": 1549868527371, "tmdate": 1549868527371, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "SJezy1XagN", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "Apologies for the late response, I did not receive a reply notification.\n\nNo such separation between robust-to-adversary and robust-to-large-noise learning arises from the results of our paper. Note that the optimal classifier for the Gaussian setting is a linear classifier. For linear classifiers, adversarial robustness and robustness to (large) noise are both directly dependent on the margin achieved by the classifier and are hence very closely connected. ", "title": "Clarification"}, "signatures": ["~Dimitris_Tsipras1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dimitris_Tsipras1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}, {"id": "SJezy1XagN", "original": null, "number": 9, "cdate": 1545576153585, "ddate": null, "tcdate": 1545576153585, "tmdate": 1545576153585, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1gkxnGTeV", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "thanks for clarification", "comment": "Thanks for the clarification. Just to be clear, this \"apparent contradiction\" was just something I wanted the authors to engage with. It's not mentioned by any of the referees, who raise different issues. I am certain that a rewrite of this paper (designed to anticipate confusion that will arise by readers, such as those by these referees) will produce a much stronger and more impactful paper.\n\nRegarding the apparent contradiction, I was imagining a separation between robust-to-adversary and robust-to-large-noise learning, which you do not address. Can you comment on this?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "S1gkxnGTeV", "original": null, "number": 5, "cdate": 1545575399488, "ddate": null, "tcdate": 1545575399488, "tmdate": 1545575399488, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HyggiieNx4", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "I am an author of the mentioned work (https://arxiv.org/abs/1804.11285). I would like to say that there is no contradiction between the results of our paper and this one. One can adapt our bounds separating standard learning from adversarially robust learning to separate standard learning from large-noise-robust learning. \n\nLearning a classifier that is robust to noise (of appropriately large magnitude) is harder in terms of sample complexity compared to learning a standard low-risk classifier.", "title": "No contradiction with mentioned work"}, "signatures": ["~Dimitris_Tsipras1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dimitris_Tsipras1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}, {"id": "S1xoy3CcYX", "original": "H1lHEBj9Ym", "number": 1013, "cdate": 1538087906575, "ddate": null, "tcdate": 1538087906575, "tmdate": 1545355427132, "tddate": null, "forum": "S1xoy3CcYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HyggiieNx4", "original": null, "number": 1, "cdate": 1544977303742, "ddate": null, "tcdate": 1544977303742, "tmdate": 1545354488825, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Meta_Review", "content": {"metareview": "In light of the reviews and the rebuttal, it seems that the paper needs to be rewritten to head off some of the confusions and criticisms that the reviewers have made. That said, the main argument seems to contradict some of the lower bounds recently established by Madry and colleagues, showing the existence of distributions where the sample complexity for finding robust classifiers is arbitrarily larger than that for finding low-risk classifiers. I recommend the authors take a closer look at this apparent contradiction when revising.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Major revisions required."}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1013/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353000139, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353000139}}}, {"id": "BkgC_UJ0CX", "original": null, "number": 8, "cdate": 1543530102355, "ddate": null, "tcdate": 1543530102355, "tmdate": 1543530102355, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HyxZTVo9hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "Can you clarify why you find the conclusion not interesting", "comment": "Thanks for your comments. However, we feel you haven\u2019t addressed the main point of our paper in your review. \n\nOur results draw a close connection between the adversarial defense literature and the robustness literature [1,2,3,4]. This is important and significant because it suggests that methods useful for improving one metric (adversarial robustness) should also be useful for improving another metric (test error as measured in corrupted image distributions) and vice-versa. Previously, these two communities have largely studied model robustness independently of each other but it turns out in fact, they are effectively studying the same phenomenon through the lens of two different metrics. This connection dramatically changes how we view \u201cadversarial examples\u201d, most adversarial example papers do not discuss their existence or connection with respect to traditional notions of model robustness to noise. \n\nDo you not find the connection we are drawing interesting? \n\n\u201cUnderstanding How Image Quality Affects Deep Neural Networks\u201d: https://arxiv.org/pdf/1604.04004.pdf\n\u201cBenchmarking Neural Network Robustness to Common Corruptions and Surface Variations\u201d: https://arxiv.org/abs/1807.01697\n\u201cComparing deep neural networks against humans: object recognition when the signal gets weaker\u201d https://arxiv.org/abs/1706.06969\n\u201cAn Overview of Noise-Robust Automatic Speech Recognition\u201d: https://ieeexplore.ieee.org/document/6732927\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "rygto--BAQ", "original": null, "number": 7, "cdate": 1542947233338, "ddate": null, "tcdate": 1542947233338, "tmdate": 1542947597787, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "r1xOqb-HR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "part 2", "comment": "Can you clarify what you mean by algorithms too similar to prior work? We aren\u2019t introducing any algorithms in this work, we are presenting a theoretical understanding of the adversarial example phenomenon. While the isoperimetric inequality has been studied in some prior works, we are the first work to empirically compare the robustness of vision models with respect to the bounds applied by the isoperimetric inequality. The reason we considered models trained in additive noise was to confirm the connection between small worst-case perturbations and large average case perturbations, rather than propose a new defense. The fact that standard Gaussian data augmentation improved adversarial robustness, and that adversarial training improved robustness in Gaussian noise solidifies the connection we are making.\n\nThanks again for your comments. If you have further questions, or comments on the paper please don\u2019t hesitate to ask. Your comments so far have already helped improve this work. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "r1xOqb-HR7", "original": null, "number": 6, "cdate": 1542947216312, "ddate": null, "tcdate": 1542947216312, "tmdate": 1542947402568, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HkxpwrWcn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "Thank you for the detailed review, if you have time can you answer some further questions so we can improve upon this work?", "comment": "It would help us improve the paper to get some additional clarity on the parts you find confusing or not interesting. Would you mind responding to a few of our questions?\n\n\"7. In the conclusion, the paper states \"we proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations\". I believe a formal proof is only given to the case of existence of small adversarial perturbations to examples to the \"noisy examples\" from the Gaussian distribution, and in this case, it is a rather direct corollary from the Gaussian Isoperimetric Inequality. For the more \"practical case\" (in the sense that is more related to the usual notion of adversarial examples) of existence of small adversarial examples, it seems only the case of the linear classifier is formally discussed.\"\n\nYou are absolutely right that the isoperimetric inequality does not apply to small perturbations of the clean image. However for the adversarial examples shown in Figure 2 it directly applies. Is there a reason you don't find the existence of the adversarial examples in Figure 2 interesting? It's still a small perturbation of a correctly classified image. \n\n\"Moreover, I'm a little bit worried some important pieces might be lost and create potentially misleading or seemly strong conclusion. Maybe it would be helpful if a concrete example could be given in the paper that shows the full path from the error-in-noise to existence of adversarial example, by showing all the constants involved. I'm a bit confused here because (in order for the isoperimetric inequality to have favorable bounds?) the Gaussian distributions used in error-in-noise seem to have rather large variance.\"\n\nLet us walk you through the adversarial example shown in the left side of Figure 5, because it should help illustrate the full argument of the paper. Figure 5 shows a 2d-slice of the decision boundary through 3 images, the clean image, an adversarial example found via PGD, and a randomly found error in noise at sigma=.08. The PGD adversarial example has l2 distance of .274 from the clean image, the error in noise as l2 distance of 31.4. Note that despite the noisy images having an l2 distance of 31.4, it still is easily identified as a panda. We found that .1% of the random gaussian perturbations are misclassified by this model. Visually, this means that .1% of the sphere at radius 31.4 is misclassified, although in this 2d slice it looks like most of the sphere is misclassified. The isoperimetric inequality then implies that the median distance from a noisy image to an error is at most -.08 * phi^{-1}(.1) = .24. This means that most random Gaussian perturbations, although correctly classified must be very close to the error set (50% are within distance .24).\n\nNow you are correct that this doesn\u2019t say anything about the clean image, however if we make a linearity assumption (which the CIFAR-10 and Imagenet models all satisfied empirically, as shown in Figures 3 and 4), then errors in noise will imply small perturbations of the clean image and the relationship between the two is the same bound implied by the isoperimetric inequality. This helps explain why adversarial training on small perturbations reduced test error in large random noise, by pushing the decision boundary away the errors are both farther away from the clean image and the error rates in noise decreased. This is one of the reasons we recommend defense papers consider evaluating on more general noise distributions, at least in addition to the standard practice of evaluating small lp adversarial examples. The theory suggests that defenses which are truly making progress on this problem, say by enforcing a better model prior for images, should also help improve generalization in the presence of more general image corruptions.\n\nFor most security settings of interest, the Gaussian isoperimetric inequality does imply that achieving perfectly robust models requires reducing test error in noise to essentially 0 (note for many settings of interest attackers will not be restricted to making only small perturbations of the clean image). For example: defending against worst case perturbations of distance .6 requires the error rate at sigma=.1 to be less than 10^(-10). Now we think this is significant because for most security settings of interest, an attacker will not be restricted to small perturbations of the clean image. Furthermore, the small perturbations of noisy images expose the same sort of blind spots that perturbations of the clean images do. Is there a reason you find small perturbations of noisy images less surprising or interesting than small perturbations of the clean image?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "BygMs4cPTX", "original": null, "number": 5, "cdate": 1542067354510, "ddate": null, "tcdate": 1542067354510, "tmdate": 1542067354510, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "Rebuttal", "comment": "Thank you to all three reviewers for taking the time to read and comment on our paper. We would like to clarify the significance of our results.\n\nMany members of the machine learning community treat the existence of small worst-case perturbations as surprising, while far fewer would react in the same way to the fact that vision models perform imperfectly in the presence of noise. Some authors have tried to blame the existence of nearby errors on strange properties of the error set itself. We argue that there is neither any evidence for any such strangeness nor any need to invoke it to explain the results we see: given the error rates we observe in noise, an error set whose boundary is almost linear, combined with some unintuitive but mathematically straightforward facts about high-dimensional geometry, is already enough to predict the existence of adversarial examples at the scales we see in practice.\n\nOur goal is not to propose yet another adversarial defense. Rather, it is to demonstrate that, given the (unsurprising) performance of vision models on noisy images, one should in fact not be surprised that there are also errors very close to a clean image. We show that the measurement we propose --- measuring the model\u2019s performance on noisy perturbations of test images --- is (a) highly correlated with adversarial robustness, (b) easier to measure in practice, and (c) interesting in its own right.\n\nWe establish the relationship between these two quantities --- distance to the nearest error and error rate in noisy image distributions --- in two different settings. In the first setting, where we use a Gaussian centered at a test point as the image distribution, the relationship is tight: for a given error rate, the Gaussian isoperimetric inequality gives a model-independent bound on how far away the nearest error can be from a typical sample. (To address a question that came up a few times in the reviews, we assume throughout the entire paper that the noise scales we consider are small enough not to change the label; some pictures of noisy images at different scales can be found in the appendix. There have been numerous studies of the performance of machine learning models under various image corruptions which all make this assumption[1,2,3,4,5,6].) Moreover, we saw that, for the error rates in noise that we observe in practice, the distance from most noisy points to the nearest error is not far from the bound. So, in this image distribution, it is impossible to improve adversarial robustness without simultaneously reducing test error.\n\nIn the second setting, where we use the ordinary distribution, we can\u2019t give a hard bound of this type, but we can establish the relationship between the two quantities of interest empirically. (In particular, to answer reviewer #1\u2019s last question, we are not claiming to somehow use the Gaussian isoperimetric inequality from the previous section to deduce a bound for the clean point.) But the empirical case is quite strong: in every case we examined, models which showed improvement in one of these two metrics also showed improvement in the other. This was true even when, as in the case of adversarial training or training on noise, we intervened in an attempt to improve one and not the other. (To address a question from reviewer #3, the models trained in noise used the same number of samples as the ordinarily trained models, but again, the point of this experiment was to establish the relationship between the two quantities of interest, not to propose a new defense.) We also show, in Table 1, that this remains true when we examine a few other noise distributions, and, in Table 2, we looked at six broken adversarial defenses and saw that measuring performance in noise would have been enough to identify that they did not work.\n\nMost adversarial example work operates under the assumption that the errors they are trying to eliminate are \u201cspecial\u201d in some way, and that the task of eliminating them is different from the task of reducing test error. We are arguing for a different perspective: these nearby errors are not surprising or complicated, and are in fact a natural consequence of test error in noise. The situation is, in a sense, much simpler than commonly assumed: the defense community and the community studying model robustness to additive noise are both trying to reduce test error; they are just measuring it in two different ways.\n\nhttps://openreview.net/pdf?id=HJz6tiCqYm\nhttps://arxiv.org/abs/1705.02498\nhttps://arxiv.org/pdf/1604.04326.pdf\nhttps://arxiv.org/pdf/1706.06969.pdf\nhttps://arxiv.org/pdf/1611.05760.pdf\nhttps://arxiv.org/pdf/1703.08119.pdf\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "HyxZTVo9hQ", "original": null, "number": 3, "cdate": 1541219513281, "ddate": null, "tcdate": 1541219513281, "tmdate": 1541533496564, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "content": {"title": "not very interesting conclusion", "review": "The paper tries to make a connection between the functionality of Gaussian noise to adversarial examples. It shows that data augmentation with added Gaussian noise could also improve the model robustness. \nMeanwhile, it shows that in a high dimensional space, even with a small (error) set, its bounding ball of \\epsilon l_p distance could be large. It explains why even with a small test error, the model could still be vulnerable to adversarial examples. \n\nAlthough the paper has some good intuitions and some nice experiments, I find the main conclusion of this paper not very interesting. \nAlthough I partially buy the second point about the high dimensional geometry, this is an obvious observation and does not give rise to much meaningful result for the future work. It would be more interesting to see the different geometry structure of robust versus not robust models. \n\nMeanwhile, though a formal definition of error set is not presented in the paper, it seems the authors are simply dividing the data space to the set where the model gives a correct label, and the \u201cerror set\u201d the other way around. However, since the paper is considering a data distribution (q) rather than a dataset, the separation could be more complicated than that. For instance, a noise image should also in your data space, but does it belong to an error set or not? It isn\u2019t necessarily attached to any labels. Or does your model only consider meaningful images? But what if adding noise simply get you out of the space? It\u2019s better to make this concept clearer. \n\n\nIt is not a surprising result that there is one randomly chosen direction mimicking the performance of adversarial examples as in Figure 2. By \u201ccarefully crafted imperceptible noise\u201d, I assume it means choosing one random sample that will change the model output the most. This is exactly a way of choosing an adversarial example. Since even in high dimensional space, out of a lot of random vectors , one could approximate a target (adversarial) direction.\n\nSimilar explanations also apply to the training with error part. How much more data do you use for the data augmentation? If you use much more data with Gaussian noise than what your use for adversarial training, it is not surprising at all to get a more robust network, with a similar argument as above.\n\n\nminor issue: Section 3 should not be an isolated section.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "cdate": 1542234326051, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335853559, "tmdate": 1552335853559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxpwrWcn7", "original": null, "number": 2, "cdate": 1541178725275, "ddate": null, "tcdate": 1541178725275, "tmdate": 1541533496360, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "content": {"title": "Interesting but algorithms too similar to previous work", "review": "This paper propose an alternative view for adversarial examples in high dimension spaces by considering the \"error rate\" in a Gaussian distribution centered at each test point. However, as mentioned in the related work, adversarial examples through the lens of isoperimetric inequality is not new to this paper; the implication of adversarial sensitivity by error rate in the test-sample-centered-Gaussian in general non-linear case is rather weak; and the empirical results does not show advantage over simple adversarial training against lp constrained adversarial attacks.\n\nHere are some more detailed comments and feedbacks:\n\nThe clarity could be improved by making clear use of notations and define some key terms explicitly:\n\n1. For example, the error rate sometimes refer to the test error, sometimes refer to the error rate under a special distribution centered at a particular test example.\n\n2. Similarly, the distribution q sometimes refer to the original (unknown) input data distribution, but the same notation is also used to refer to this Gaussian distribution centered on each test example. Although the paper says that \"q need not be restricted to the distribution from which the training set was sampled\", it could potentially confuse the reader less if there is a symbol for the \"usual\" test error and a different one for this test-error-under-Gaussian-centered-at-a-particular-test-example.\n\n3. It would be good if the paper could make a formal definition of the problem being studied and explicitly specify the assumptions on the existence of a deterministic target function (concept) and explicitly define the error set E.\n\n4. It seems to assume the input distribution is continuous everywhere but not stated. If for example, the original data is supported on disconnected manifolds separated with low density or even zero-density margins, then the Gaussian distribution centered on test example argument will need to be modified to talk about the intersection of the Gaussian with the data manifold instead. If the paper does decide to make this kind of assumption, some empirical study on the data to verify the fidelity of the assumptions would be great.\n\n5. The paper does not mention how measurement against the error set E. Under the original data distribution, it is natural to measure the error rate with the provided training or test data with labels. However, under each newly formed Gaussian distribution centered at each test point, the labels for the newly sampled examples from this Gaussian is unknown, and since there is no \"ground truth classifier\" for MNIST or CIFAR10 available, it seems impossible to \"calculate\" the true label for those samples, which are needed to calculate the error rate. It is not very clear from the paper how this issue is solved. I'm guess it uses the label from the Gaussian center for all the samples from the Gaussian. While this might be reasonable assumption for Gaussian with tiny variances, it is less clear how reasonable it is for the large variance Gaussian distributions considered in the paper. If this assumption is made, please state it explicitly and empirically or theoretically study how reasonable this assumption is in the regime of variances considered in this paper. If my guessing is wrong, please also explicitly what approach is used to get around this issue.\n\nThe followings are some feedbacks on the contents and ideas of the paper:\n\n6. I think one sentence in the text summarize a large part of the paper very well: \"to measure adversarial robustness is to ask whether or not there are any errors in the linf ball, ... and to measure test error in noise is to measure the volume of the error set in the defined noise distribution\". However, this looks like a rather roundabout approach to attack another problem (measuring volume of an unknown set in a very high dimension space) in order to solve the original problem, while the implication is rather weak (a precise implication can only be obtained for linear separating hyper-plane, while for non-linear classifiers it is much less clear).\n\nNote while exact adversarial robustness is NP-hard, volume estimation in high dimension is not easy (if not harder). For cifar-10, the inputs are in dimension 32x32x3 = 3072, the 1,000 samples used in the paper to estimate the volume of a set in this high dimension seem to be quite inaccurate. I would appreciate if variances could be reported in those studies to show the confidence of the estimations. For imagenets, the inputs are in even larger spaces.\n\nGiven the difficulty (in terms of sample complexity) to estimate the \"error-in-noise\", it might not be very surprising that the noise augmentation does not show advantages to lp constrained adversarial attacks (comparing to adversarial training).\n\n7. In the conclusion, the paper states \"we proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations\". I believe a formal proof is only given to the case of existence of small adversarial perturbations to examples to the \"noisy examples\" from the Gaussian distribution, and in this case, it is a rather direct corollary from the Gaussian Isoperimetric Inequality. For the more \"practical case\" (in the sense that is more related to the usual notion of adversarial examples) of existence of small adversarial examples, it seems only the case of the linear classifier is formally discussed.\n\nMoreover, I'm a little bit worried some important pieces might be lost and create potentially misleading or seemly strong conclusion. Maybe it would be helpful if a concrete example could be given in the paper that shows the full path from the error-in-noise to existence of adversarial example, by showing all the constants involved. I'm a bit confused here because (in order for the isoperimetric inequality to have favorable bounds?) the Gaussian distributions used in error-in-noise seem to have rather large variance. As mentioned in the paper, the majority of the mass in the Gaussian distribution considered will be in a thin sphere of radius sigma * sqrt(n) centered at the test example x. If sigma = 0.1 and dimension n = 3072 (cifar-10), then the radius is around 5.5 (in l2 distance) which is probably quite far from the test point x (is it?). It is then less clear how \"a large majority of this thin sphere far away from x is epsilon close to the error set\" could tightly imply properties of adversarial robustness of x itself in its close vicinity. Maybe a specific example with all the numerical constants spelled out would help illustrate this.\n\nIn summary, I think this paper takes an interesting but roundabout perspective to adversarial robustness, and the implication is weak in the non-linear case. (Potentially because of the weak implication), the suggested approach for defenses by augmenting with noises does not show advantage over adversarial training.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "cdate": 1542234326051, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335853559, "tmdate": 1552335853559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gVOsyuh7", "original": null, "number": 1, "cdate": 1541041003547, "ddate": null, "tcdate": 1541041003547, "tmdate": 1541533496117, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "content": {"title": "A connection between training with noisy images and the adversarial training is not convincing", "review": "The paper suggests a connection between training with noisy images and the adversarial training. The observation is original to me. It has several cons.\n\n1. The paper is hard to follow because of  too many vague descriptions and unnecessary contrast clauses. Here are some.\n 1) The title of section 4: ERRORS IN NOISE IMPLY ADVERSARIAL EXAMPLES FOR NOISY IMAGE.  \n 2) \"The discussion of high-dimensional geometry suggests that adversarial examples may actually not be in contradiction to high generalization performance. Indeed, high generalization performance does not mean perfect generalization\" The uncertain tone \"may not\" and the vague statement \"does not mean perfect generalization\" make readers hard to get the solid understanding about what the paper tries to say. \n 3)  \"Adversarial training on small perturbations on CIFAR-10 also improved generalization in noise, and training on noise improved robustness to small perturbations.\" \nMany other sentences like the above make the paper not technically sound.\n\n2. It is problematic that the paper uses Euclidean l2 distance to measure the error set and its surface as it is believed that the dataset lives on low-dimensional manifold. Moreover, the adversarial examples often constructed by moving the legal images towards a specific direction rather than adding the Gaussian isotropic noise. \n3. The advocates that using test error in noise as a measure of adversarial robustness  is misleading  as test error in noise has a large number of different combinations: noise type, noise amplitude. One may find one type of test error in noise coinciding with adversarial robustness but  in general it is not a good measure for adversarial robustness because of its varying nature.\n4. Several terms are referred without definitions: errors in noise,  adversarial robustness. From the definition of E_epsilon, it should include the interior of E.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Review", "cdate": 1542234326051, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335853559, "tmdate": 1552335853559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1ezN5f9nQ", "original": null, "number": 4, "cdate": 1541184041883, "ddate": null, "tcdate": 1541184041883, "tmdate": 1541184391607, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "Ske_pCTe2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "Thanks for responding to these concerns, I look forward to the updated version.\n\nI maintain that the trade-off between clean test accuracy and robustness still applies despite the additive white noise, as this cannot \"cancel out\" the noise inherent in the dataset wrt the label without significant destruction of the relevant information that describes the rule to be learned.", "title": "Thank you"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["~Angus_Galloway1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}, {"id": "Ske_pCTe2Q", "original": null, "number": 4, "cdate": 1540574911970, "ddate": null, "tcdate": 1540574911970, "tmdate": 1540574911970, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HyevjK2U5X", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "Updating the defense table.", "comment": "Hi Angus,\n   Just an update, we reran each of the defenses N=10 times and took the mean accuracies for each and computed the standard error of these measurements. We found that only one of the defenses (Pixel Deflection) showed any statistically significant improvement over the baseline undefended model, the rest actually increase test error in additive noise. For Pixel Deflection the improvement was still minimal when compared with what can be achieved by performing standard Gaussian data augmentation. For sigma=.1 Pixel deflection had a mean accuracy of 70.3% with standard error of 1.1 while the undefended model (InceptionV3) mean accuracy was 69.1%.  Gaussian data augmentation achieved 73.3% accuracy.\n\nWe also will correct a mistake we made when transcribing the numbers for tv-minimize in the original table. The mean accuracies for this defense at sigma=.1 is 68% and at sigma=.2 is 50.5%. Both values are lower than the undefended baseline. The original table incorrectly reported higher numbers for this particular defense. We will update the paper with the exact numbers, using a bar plot to visualize the mean accuracies and corresponding standard errors more easily.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "BklKxJBF57", "original": null, "number": 3, "cdate": 1539030768800, "ddate": null, "tcdate": 1539030768800, "tmdate": 1539031168486, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HyevjK2U5X", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "clarification about the tradeoff and recommendations", "comment": "Hi, Angus,\n\nThanks again for the comments. We\u2019re still not exactly sure how the work about trading off clean accuracy for robustness is relevant to the main claims of our paper. At least for the noisy image distribution that we considered in Section 4, we prove a theorem that shows that the existence of test error *implies* the existence of small worst-case l2 perturbations. For example, if sigma=.1, an error rate of 10^-10 implies that the median distance to the nearest error is at most -.1 * phi^{-1}(10^{-10}) = .624. We agree that getting zero test error does not seem practical, but unfortunately, at least for the noisy image distribution, our theorem implies that obtaining the robustness results that the defense literature seems to be aiming for requires the test error to be very close to zero.\n\nNone of the work on the tradeoff between accuracy and robustness applies directly to the case we consider here, and we are in fact arguing that in our case the opposite is true: our experiments showed that lower error rates in additive noise do seem to correlate with increased robustness. This doesn\u2019t contradict the \u201ctradeoff\u201d literature you are citing; it\u2019s just that we are discussing a different problem --- they are concerned with accuracy on the clean test set where we are concerned with accuracy in the presence of additive noise.\n\nIn general, we don\u2019t have a recommendation for how to solve either the adversarial example problem or the problem of increasing accuracy under additive noise. Rather, we are saying that these are provably the same problem for the noisy image distribution, and they seem to be the same problem for the clean image distributions we examined. In light of this, we believe that completely solving the adversarial example problem is probably going to be incredibly difficult.\n\nIt has been helpful to talk to you about your reactions to the paper; it has shown us a few places where we think we can make the exposition clearer. Please let us know if there\u2019s anything else we can answer.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "HyevjK2U5X", "original": null, "number": 3, "cdate": 1538865566763, "ddate": null, "tcdate": 1538865566763, "tmdate": 1538877423257, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "Hkl2ae_r5X", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "Thanks for the detailed response.\n\nI mention the literature re. clean accuracy vs robustness trade-off because it applies to absolute statements that appear in the context of reducing the error rate in Gaussian noise to zero, and \u201cimperfect generalization\u201d. I do not believe that making any error rate zero is practical, nor that obtaining this for Gaussian noise is desirable. Perhaps some of the absolute statements can be clarified.\n\nRegarding \u201cThe only way to defend against such adversarial perturbations is to reduce the error rate in Gaussian noise\u201d, I understand what was meant now, and initially interpreted this as suggesting that the only/best way to do this is also by training on Gaussian noise, as this is one of the strategies explored in the paper. I agree that testing on additive Gaussian noise is a good idea, but the best way to reduce the error rate in this noise is not likely to be by training on the same, similarly to how adversarial training is generally not an optimal way to reduce epsilon-adversarial test error while improving generalization (although the reasons for the respective approaches being suboptimal are unique).\n\nThe differences in test accuracy for the previously proposed defenses benchmarked in Table 2 are rather small, and the statement that none of them improve generalization in noise should be supported by several independent runs to show the variance of the results to help establish if the differences are significant. There are different ways of expressing the random error, one way would be to repeat the experiments over n=5 random seeds and report the mean to a number of decimal places given by the precision, where precision could be the standard error of the mean for n samples and is usually rounded to 1 sig fig. This repetition should still be done even if the differences do not seem small, and is especially important when drawing conclusions about several different methods. I could be convinced that this might be less important when sweeping a single variable where a clear trend can be observed.\n\nSetting aside the issue of random fluctuations, Table 2 shows that \u201ctv-min\u201d does 3.1% (absolute) better than the undefended InceptionV3 model for sigma=0.1, and 14.9% (absolute) better for sigma=0.2. If we\u2019re comparing to the undefended InceptionV3 model, test performance on additive noise is approximately upper bounded by that for the clean test set (76%) by the data processing inequality, so an increase from 68.4% to 71.5% for sigma=0.1 represents about 40% of the available room for improvement for this model, and 62% in terms of the same for sigma=0.2. The approximate upper bound is exact if we assume that the undefended model was trained to maximize test accuracy and the defenses do not increase the effective capacity. Only one of the defenses exceeds this upper bound (\"HGD\" by 1.2%), so these seem like reasonable assumptions, but again difficult to know without random error. \u201cPixel\u201d also achieves higher accuracy than the baseline for the noisy datasets, although the improvement is less than for \u201ctv-min\u201d. Is there a reason these defenses are being excluded? Otherwise I believe the statement that none of the defenses improve the test error in noise needs adjustment. Some of the defenses seem to reduce generalization error, e.g., \u201cRandom\u201d, because it has less clean test accuracy for the same accuracy in additive noise sigma=0.2 as the baseline. I'm assuming this is one of the places where it was previously acknowledged that \"test error\" would be more appropriate than \"generalization\".\n\nSorry to insist on so many details, i'm mainly seeking clarity as to the recommendations/results, and hoping to give the previously proposed defenses a fair shake even if they aren't optimal.", "title": "Clarification, and re. the conclusion about previously proposed defenses"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["~Angus_Galloway1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}, {"id": "Hkl2ae_r5X", "original": null, "number": 2, "cdate": 1538781380206, "ddate": null, "tcdate": 1538781380206, "tmdate": 1538781380206, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HJeOIqrr57", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "content": {"title": "Thanks for the comments!", "comment": "Hi, Angus,\n\nThank you very much for taking the time to read the paper so closely! We\u2019re happy that you agree with the main argument we make in the paper, and we are grateful for all the thoughtful comments. We are happy to revise the paper to clarify some of the more confusing parts you pointed to. We can also address some of them here.\n\nThe main thrust of the paper is a connection between two metrics: adversarial robustness and test performance under additive noise. We don\u2019t intend to make any absolute statements about either of these quantities individually; for example, we\u2019re not saying that it would be impossible to construct a perfectly adversarially robust classification model, only that, if you observe lots of errors in additive noise, you should not expect much robustness, and vice versa.\n\nYou make a very good point about our use of the word \u201cgeneralization\u201d; we do indeed use this in several places where \u201ctest error\u201d would be more appropriate. We will definitely fix this in future drafts. Moreover, regarding the tradeoff you mentioned, the test error we are most concerned with in this paper is test error under additive noise, not test error on the clean image distribution. We don\u2019t intend to claim anything about a relationship between clean test error and adversarial robustness, but we can see a few places in the paper, especially the introduction, where a reasonable reader might think that we did. We will be sure to address this in our revisions.\n\nYou asked about the definition of the error set. There are two parts to the answer. First, when considering images from the test set under additive noise, we always assume that the \u201ccorrect\u201d label of the noisy point is the same as that of the clean point; we ran all our experiments on scales of noise at which this is reasonable. We agree that the paper should be edited to make this clearer.\n\nThe second part of the answer is that, for all figures and tables, a point is an \u201cerror\u201d if the largest logit belongs to an incorrect class regardless of the confidence. However, the isoperimetric inequality is just a fact about sets in R^n, and could just as easily apply to the set of \u201cconfident errors\u201d made by a model. In practice, we found that this makes very little difference to the overall story; the error sets we examined all contain less confident errors on the boundary and more confident errors very closeby in the interior. (There are in fact some visualizations of this in the appendix.) We agree, though, that this might be worth discussing in the main body of the paper.\n\nThank you for pointing out the terminology overload with \u201ctypical set\u201d; we can certainly be more careful about this.\n\nYou say: \u201cIt is unnecessary to suggest that the only way to defend against small perturbations is to reduce the error rate in Gaussian noise, this is only one way that provably doesn\u2019t scale.\u201d We are not entirely sure what you are asking here, but if you are saying that Gaussian data augmentation isn\u2019t the only way to improve robustness, then we agree. The Gaussian isoperimetric inequality merely implies that if you have not improved robustness to Gaussian noise then you have not improved adversarial robustness beyond the bound given by the theorem. This has nothing to do with any particular training procedure; in fact, what you say --- \u201capproaches which improve adversarial robustness also improve robustness to Gaussian noise\u201d --- is exactly what we meant.\n\nRegarding the question about failed defenses not improving generalization in Gaussian noise, we again mean the thing that you suggest. You are of course correct that adding noise can only make the problem harder; the thing to notice in Table 2 is that the failed defenses, unlike adversarial training, did not perform better than the *undefended* model on noisy images. The general takeaways from Tables 1 and 2 are (a) the defense that seems to improve adversarial robustness also showed improvement (over the undefended model) in most of the noise distributions we tested, (b) failed defenses don\u2019t, and (c) training on noise seems to improve adversarial robustness.\n\nWhat do you mean by \u201cIt is difficult to draw definitive conclusions from Tables 1 and 2 because the random error was not reported.\u201d? What is \u201crandom error\u201d? We do report error rates on the clean test set in the rows labelled \u201cClean\u201d.\n\nThanks again for reading, and let us know if there\u2019s anything else we can help clarify or you find any other mistakes.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1013/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607360, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1xoy3CcYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1013/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1013/Authors|ICLR.cc/2019/Conference/Paper1013/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607360}}}, {"id": "HJeOIqrr57", "original": null, "number": 2, "cdate": 1538771536309, "ddate": null, "tcdate": 1538771536309, "tmdate": 1538771536309, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "HJxH79BB97", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "In list item 3, it is stated that the failed defense strategies \u201cfailed to improve generalization in Gaussian noise\u201d it is unclear what kind of improvement is expected here, and if we\u2019re really talking about generalization or test error. It is impossible to do better in terms of test error on a test set with additive Gaussian noise, than without, because adding Gaussian noise can only destroy information. The interesting question is by how little test error degrades in the presence of this additive noise. It is difficult to draw definitive conclusions from Tables 1 and 2 because the the random error was not reported.\n\nUltimately, while training on additive noise acts as a form of regularization on the dataset itself, it is insufficient to prevent the model from overfitting, so it\u2019s harder to comment generally about the effect it may have on generalization gaps. Section 6 refers to a 22% gap in terms of accuracy (99 train, 77 test) on CIFAR-10 with additive Gaussian noise with sigma=0.15, this suggests that the model has excess capacity. ", "title": "comments pt 2"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["~Angus_Galloway1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}, {"id": "HJxH79BB97", "original": null, "number": 1, "cdate": 1538771485230, "ddate": null, "tcdate": 1538771485230, "tmdate": 1538771485230, "tddate": null, "forum": "S1xoy3CcYX", "replyto": "S1xoy3CcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "content": {"comment": "This paper makes some reasonable recommendations that can be used in works that aim to defend against adversarial perturbations, such as reporting test performance for additive noise, supplementing that for worst case perturbations. To my knowledge, this is a common benchmark in classical computer vision, but not common in adversarial examples literature. It is convincing that this practice could have partly mitigated the false sense of improved \u201csecurity\u201d due to gradient masking/overfitting specific attack methods, previously reported by Carlini and Athalye. It is similarly convincing that training on isotropic Gaussian noise can reduce sensitivity to \u201csmall\u201d adversarial perturbations, and be competitive with adversarial training. \n\nUnfortunately, this work makes some overly general statements.\n\nI agree that improving adversarial robustness is complementary to generalization in the i.i.d statistical learning theory sense. However, this work seems to use \u201ctest error\u201d and \u201cgeneralization\u201d (the difference between the empirical and expected loss) interchangeably, but the distinction between the two has major implications. The inherent trade-off between test error and adversarial robustness has been reported extensively.\n\nhttps://arxiv.org/abs/1412.2309 \u2013 See discussion of spurious correlate and visual causes\nhttps://arxiv.org/abs/1608.07690 \u2013 See Type 0, 1 adversarial examples in their taxonomy.\nhttps://arxiv.org/abs/1804.03308 \u2013 Shows an accuracy vs robustness trade-off.\nhttps://arxiv.org/abs/1805.12152 \u2013 The focus of their work\nhttps://arxiv.org/abs/1808.01688 -  Characterizes the trade-off between accuracy and robustness for a variety of DNNs for image classification\n\nThe title of the present work mentions \u201ctest error in noise\u201d. This is confusing because natural images are already noisy by default, so a possible interpretation is that we should fit the noise inherent in the dataset, which is clearly not a good idea. It is intuitive that adding isotropic Gaussian noise to the data acts as a kind of regularizer due to making the input more spherical and attenuating spurious correlates, but I am unaware of a compelling way to scale this to larger perturbations since this necessarily destroys legitimate information about the relevant variable. In any case, an important baseline would be sphering the data with ZCA, which preserves more of the relevant information for natural images, while also removing some spurious correlate and redundancy between neighboring pixels.\n\nWhat is the definition of the error set E? An example is given in the context of a toy example on p2 for an input x having L2 norm less than 1, but it is unclear what this has to do with image classification? I understand that volume concentrates in a thin shell in high dimensional spaces, but I do not see how it immediately follows from the example that we are bound to have adversarial examples for high dimensional inputs in DNNs. This doesn\u2019t tell me about the confidence with which adversarial examples may be classified (is an adversarial example corresponding to a nearly uniform softmax output really a problem?), and how the model will classify inputs that don\u2019t cluster near any of the legitimate image volumes. Also, I think there might be an accidental overloading of terminology in reference to \u201ctypical sets\u201d, which is a technical term in information theory, see e.g., Ch. 3 in Cover & Thomas (1991).\n\nIt is unnecessary to suggest that the only way to defend against small perturbations is to reduce the error rate in Gaussian noise, this is only one way that provably doesn\u2019t scale. I don\u2019t see how the Theorem regarding the Gaussian Isoperimetric Inequality precludes other approaches (e.g. preprocessing, regularization) from achieving similar results. Maybe this could be revised to say something like \u201cit follows from \u2026 that approaches which improve adversarial robustness also improve robustness to Gaussian noise\u201d. Also, \u201csmall\u201d in the context of epsilon-adversarial examples was not defined, and there is no explanation as to how to choose the noise parameters, e.g., in Fig. 3, why sigma 0.1 test versus 0.4 training?", "title": "comments pt 1"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1013/Reviewers/Unsubmitted"], "writers": ["~Angus_Galloway1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.", "keywords": ["Adversarial examples", "generalization"], "authorids": ["nicf@google.com", "gilmer@google.com", "cubuk@google.com"], "authors": ["Nicolas Ford", "Justin Gilmer", "Ekin D. Cubuk"], "TL;DR": "Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.", "pdf": "/pdf/9960667397256afcc527f8bb3c019f018d62cb05.pdf", "paperhash": "ford|adversarial_examples_are_a_natural_consequence_of_test_error_in_noise", "_bibtex": "@misc{\nford2019adversarial,\ntitle={Adversarial Examples Are a Natural Consequence of Test Error in Noise},\nauthor={Nicolas Ford and Justin Gilmer and Ekin D. Cubuk},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xoy3CcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1013/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311699469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1xoy3CcYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1013/Authors", "ICLR.cc/2019/Conference/Paper1013/Reviewers", "ICLR.cc/2019/Conference/Paper1013/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311699469}}}], "count": 19}