{"notes": [{"id": "Syl-_aVtvH", "original": "S1xvKsiwwr", "number": 621, "cdate": 1569439080680, "ddate": null, "tcdate": 1569439080680, "tmdate": 1577168289845, "tddate": null, "forum": "Syl-_aVtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-LovxiaeFf", "original": null, "number": 1, "cdate": 1576798701613, "ddate": null, "tcdate": 1576798701613, "tmdate": 1576800934389, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Decision", "content": {"decision": "Reject", "comment": "This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. Empirical results are provided which suggests improved performance.\n\nThe reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning.  However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Reviewers were also unconvinced by the provided empirical evaluation results. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729063, "tmdate": 1576800281591, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper621/-/Decision"}}}, {"id": "B1lEfksosH", "original": null, "number": 4, "cdate": 1573789452252, "ddate": null, "tcdate": 1573789452252, "tmdate": 1573789452252, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment", "content": {"title": "Response to reviews", "comment": "Dear reviewers,\n\nThank you for the thorough reviews!\n\nFor the comments on novelty, although our method is not based on a complex theoretical ideas, it is simple yet practical. With minimal modification, the any personalization models which satisfy the split-personalization constraint can be used in FL in a scalable manner. We plan to change the title to \"Embarrassingly Simple Approach to Personalization in Federated Learning\" to emphasize this aspect. \n\nIn addition, we also plan to add a section to highlight the contrast with Federated Multi-Task Learning [1] and show that FURL is more scalable.\n\n[1] Smith, Virginia, et al. \"Federated multi-task learning.\" Advances in Neural Information Processing Systems. 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper621/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl-_aVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper621/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper621/Authors|ICLR.cc/2020/Conference/Paper621/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168733, "tmdate": 1576860531337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment"}}}, {"id": "ByxxiNE8iH", "original": null, "number": 3, "cdate": 1573434519548, "ddate": null, "tcdate": 1573434519548, "tmdate": 1573434519548, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "SJeGpxFFYB", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment", "content": {"title": "Additional notes after reading other anonymous reviews.", "comment": "I did see the same concern raised by anon-reviewer #2 and #4 when reading the paper, but was not confident judging actual novelty since I'm less familiar with most recent FL research. So I chose to base on the author's own claim on their contrast with previous work.\n\nI'm very ok with adjusting it to weak accept and would lean more on the opinion of #2/#4 given their record of publication in the field.\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl-_aVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper621/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper621/Authors|ICLR.cc/2020/Conference/Paper621/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168733, "tmdate": 1576860531337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment"}}}, {"id": "SJeGpxFFYB", "original": null, "number": 1, "cdate": 1571553465567, "ddate": null, "tcdate": 1571553465567, "tmdate": 1572972572498, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Authors proposed a formal training scheme (FURL) for personalized federated models. The claimed benefits of such models are 1) preservation of user privacy by keeping the personalized parameters locally on each user's device, and 2) reduced data exchange to make the training complexity grow linearly with the number of users. Authors approached the problem with defining the constraint of split personalization, and argued that common FL setting such as Federated Averaging could satisfy this constraint. \n\nAuthors designed a personalized classification deep network for two data sets, namely Stickers and SubReddit. Both tasks could benefit personal preference in addition to textual features: Stickers CTR depends on user's adoption of the feature; SubReddit categorization depends on user's past activities in each sub-Reddit. A clearly conducted experiment showed that personalization has a significant contribution in non-federated setting, and using FURL in federated setting achieved similar performance while non-personalized FL may suffer bigger loss (in the case of SubReddit). Authors also compared the conversage curve and visualized final embeddings to show that federated learning produces acceptable convergence and equally reasonable embeddings.\n\nThe paper is well written and all claimed contributions are well articulated. Reviewer didn't find any significant problems.\n\nReviewer has limited knowledge of previous work in the personalized FL field, thus is only able to confirm the novelty from Authors' related work section.\n\nOne comment about formatting: in Figure 5, the color dots in the legend could be larger for easier identification. Please also consider some color-independent label/description to help readers with difficulties in color perception. For example, you can name the color in the legend (i.e. \"Red\") and provide some text labels in the embedding chart to tell which part is mostly red.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137388502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper621/Reviewers"], "noninvitees": [], "tcdate": 1570237749480, "tmdate": 1576137388516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Review"}}}, {"id": "BkgTC6L6KS", "original": null, "number": 2, "cdate": 1571806676998, "ddate": null, "tcdate": 1571806676998, "tmdate": 1572972572463, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors propose using federated learning (FL) to train personalized models, which improves the scalability and privacy preservation of the existing personalization techniques. The empirical results show good performance.\n\nHowever, in general, I think the contribution is limited. The reasons are as follows:\n\n1. The proposed algorithm, FURL, is a direct and simple combination of personalized model and FL. Although the authors claim that there is significant improvement in the performance, such improvement comes from the personalization. And, the personalization itself is not a novel thing (I think the personalized model used in this paper is similar to [1] or some other references. Please correct me if the personalized model used in this paper is new, since I'm not an expert in personalization.) Thus, in general, this paper simply use FL to replace fully synchronous SGD in the training of the personalized models. All the benefits claimed in the introduction, including scalability, privacy preservation, and improvement of performance, come from either vanilla personalization or vanilla FL. I fail to find any new contribution in this combination.\n\n2. The authors emphasize a lot on the \"independent aggregation constraint\". Although it sounds like such constraint is designed especially for FL + personalization, it is actually a feature only for personalization, which has nothing to do with FL. Note that when doing inference/prediction, each user uses his/her own private part of the model. Different users' private part of models will never affect each other. It is equivalent to training a global model, which concatenates the private parts of models into a big model, and each user update the global model in a sparse manner. Thus, we can also train such personalized model with fully synchronous SGD with sparse gradients, which also does not synchronize the private parts. The private part is never shared by different users, no matter trained by fully synchronous SGD or FL.\n\n\n------------\nReferences\n\n[1] Jaech, Aaron, and Mari Ostendorf. \"Personalized language model for query auto-completion.\" arXiv preprint arXiv:1804.09661 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137388502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper621/Reviewers"], "noninvitees": [], "tcdate": 1570237749480, "tmdate": 1576137388516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Review"}}}, {"id": "HJx8Dzey5S", "original": null, "number": 3, "cdate": 1571910238129, "ddate": null, "tcdate": 1571910238129, "tmdate": 1572972572418, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes the use of Federated Averaging for achieving personalised user embedding. Federated Learning is used whether they propose a particular split of model parameters with user embedding (private) and the overall BLSTM model (shared). Federated Averaging is used for the global update.\n\nThe key contribution of this paper is not clear. It seems to be the introduction of the notion of split-personalisation-constraint, and it shows that the modeling each user with a \u201cprivate\u201d embedding that feeds to a global MLP with a global BLSTM as another input (named as FURL) can achieve the constraint so that FL can be used. The originality is limited.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137388502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper621/Reviewers"], "noninvitees": [], "tcdate": 1570237749480, "tmdate": 1576137388516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Review"}}}, {"id": "SyggxlxnKS", "original": null, "number": 2, "cdate": 1571713000140, "ddate": null, "tcdate": 1571713000140, "tmdate": 1571713000140, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "r1x97E5B_S", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment", "content": {"title": "Interesting discussion", "comment": "For privacy, I think the shared part follows the standard FL setting. The private part is never synchronized. So, in overall, the guarantee of privacy preservation is the same as the standard FL.\n\nFor validity, I think the improvement of the accuracy comes from the personalization. As the experiments shows, personalization + fully synchronous training (I think it means the shared part is trained by fully synchronous SGD, while the private parts are still never synchronized) has better performance compared to personalization + FL. \nSo, as you mentioned, FL does lose some accuracy.\nAnd, I guess you misunderstand \"personalization\". Basically, for personalization, each user has his/her own embedding layer, which is not shared by each other. And, this \"personalization\" is sth. orthogonal to FL (see the personalized language model: https://arxiv.org/pdf/1804.09661.pdf). \nThus, the improvement comes from:\n1. Since each user has his/her own user embedding layer, compared to the fully shared model, the personalized model has a much larger number of parameters. Thus, it is reasonable that the personalized model has better performance, due to the stronger representation power.\n2. When testing on a new data, the model knows which user the data comes from, and use the corresponding user embeddings. The users' identity could be viewed as some extra information, which is not utilized by the fully shared model.\nI also believe that the personalized model has some limitations/tradeoffs. Basically, in testing, such personalization only works when the users already exist in the training data."}, "signatures": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper621/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl-_aVtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper621/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper621/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper621/Authors|ICLR.cc/2020/Conference/Paper621/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168733, "tmdate": 1576860531337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper621/-/Official_Comment"}}}, {"id": "r1x97E5B_S", "original": null, "number": 1, "cdate": 1570247713800, "ddate": null, "tcdate": 1570247713800, "tmdate": 1570247713800, "tddate": null, "forum": "Syl-_aVtvH", "replyto": "Syl-_aVtvH", "invitation": "ICLR.cc/2020/Conference/Paper621/-/Public_Comment", "content": {"comment": "I have following questions\n\nPrivacy:\nIn this paper, the authors claim that local user will not update local embedding  but need to locally update the parameters. The update of the local parameters is jointly with the global shared parameters. \nHowever, during the update process, the update of the global shared parameters is already influenced by the local private parameters and will carry somewhat information of the local private parameters. As a result, you cannot claim you protect privacy just because you didn't upload it. \n\nValidity:\nIn the abstract, you write as: we show theoretically that this parameter split does not affect training for most model personalization approaches. Unfortunately, I didn't see any proof about it. Second, for federated setting, you have to update the parameters many steps before uploading. Even though you share all of the parameters, you will lose high accuracy. Now, you only share part of the parameters, you will lose much higher accuracy. So I think the result maybe \"too good\"\n\nOne more thing about writing:\nI think the logic flow of this paper is rather mixed, and I spend lots of time to understand what the author is talking about even though I work on federated learning.\n\nThanks\n", "title": "Several questions about privacy concern and validity"}, "signatures": ["~Stone_Jamess1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stone_Jamess1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ducbui@umich.edu", "kmalik2@fb.com", "jrgoetz@umich.edu", "shanemoon@fb.com", "honglei@fb.com", "anujk@fb.com", "kgshin@umich.edu"], "title": "Federated User Representation Learning", "authors": ["Duc Bui", "Kshitiz Malik", "Jack Goetz", "Seungwhan Moon", "Honglei Liu", "Anuj Kumar", "Kang G. Shin"], "pdf": "/pdf/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "TL;DR": "We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.", "abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.", "keywords": ["Machine Learning", "Federated Learning", "Personalization", "User Representation"], "paperhash": "bui|federated_user_representation_learning", "original_pdf": "/attachment/cb37b78a0231af2cd7f44dd536a76366968522f9.pdf", "_bibtex": "@misc{\nbui2020federated,\ntitle={Federated User Representation Learning},\nauthor={Duc Bui and Kshitiz Malik and Jack Goetz and Seungwhan Moon and Honglei Liu and Anuj Kumar and Kang G. Shin},\nyear={2020},\nurl={https://openreview.net/forum?id=Syl-_aVtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syl-_aVtvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504206525, "tmdate": 1576860564992, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper621/Authors", "ICLR.cc/2020/Conference/Paper621/Reviewers", "ICLR.cc/2020/Conference/Paper621/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper621/-/Public_Comment"}}}], "count": 9}