{"notes": [{"id": "rkxawlHKDr", "original": "Byl_f2eFPr", "number": 2378, "cdate": 1569439845202, "ddate": null, "tcdate": 1569439845202, "tmdate": 1583912034056, "tddate": null, "forum": "rkxawlHKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "CDwLynS9UX", "original": null, "number": 9, "cdate": 1578989715410, "ddate": null, "tcdate": 1578989715410, "tmdate": 1578989715410, "tddate": null, "forum": "rkxawlHKDr", "replyto": "saqlhjcpp", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "Related work", "comment": "This is unfortunate and does not seem right. We will treat your paper as concurrent to our work and think that it should be treated as such by all future reviewers."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "saqlhjcpp", "original": null, "number": 3, "cdate": 1578871199596, "ddate": null, "tcdate": 1578871199596, "tmdate": 1578871199596, "tddate": null, "forum": "rkxawlHKDr", "replyto": "RRQ6G3k2_", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment", "content": {"title": "Related Work", "comment": "Thank you. We originally submitted our paper, as it appears on arXiv, to ICCV 2019, back in March 2019, but it was ultimately rejected for perplexing reasons, despite the fact that our model significantly outperformed the then state-of-the-art building segmentation methods. We will cite your paper in the published version of our work."}, "signatures": ["~Ali_Hatamizadeh1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ali_Hatamizadeh1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504181149, "tmdate": 1576860593267, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment"}}}, {"id": "RRQ6G3k2_", "original": null, "number": 8, "cdate": 1578219660938, "ddate": null, "tcdate": 1578219660938, "tmdate": 1578219660938, "tddate": null, "forum": "rkxawlHKDr", "replyto": "e9Nw9mdqfH", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "Post-submission related-work", "comment": "Thank you for letting us know about your paper, which we will cite in the next version. \n\nWe ask that you would also cite our work, noting that it was made public on open review before the publication date of your arxiv manuscript (obviously the two efforts are concurrent)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "e9Nw9mdqfH", "original": null, "number": 2, "cdate": 1576973736694, "ddate": null, "tcdate": 1576973736694, "tmdate": 1576973736694, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment", "content": {"title": "More Related Work", "comment": "We appreciate your work and would like to bring to your attention our paper published as:\n\n@article{hatamizadeh2019endtoend,\ntitle={End-to-End Deep Convolutional Active Contours for Image Segmentation},\nauthor={Ali Hatamizadeh and Debleena Sengupta and Demetri Terzopoulos},\njournal={arXiv preprint arXiv:1909.13359},\nmonth={September},\nyear={2019}\n}\n\nIt introduced the first fully automatic, end-to-end trainable CNN-ACM combination, where the Active Contour Model (ACM) is defined implicitly, as a level set. This has important advantages relative to your use of the explicit ACM formulation. Among them is the fact that our DCAC model can simultaneously segment multiple object instances, as opposed to just a single instance, while dealing with arbitrary shapes and capturing sharp corners as necessary. Our DCAC model is implemented entirely in Tensorflow and is thus end-to-end differentiable and backpropagation trainable. It requires no user intervention either during training or during image segmentation. We trained and tested DCAC on the Vaihingen and Bing Huts datasets and our results established a new state-of-the-art performance by a wide margin at the time (March 2019). \n\nOur foregoing publication is highly relevant to your work and should be discussed in your Related Work section, such as in your paragraph on \"Building segmentation and recent active contour solutions \". Thanks."}, "signatures": ["~Ali_Hatamizadeh1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ali_Hatamizadeh1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504181149, "tmdate": 1576860593267, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment"}}}, {"id": "OwGtF3nhon", "original": null, "number": 1, "cdate": 1576798747541, "ddate": null, "tcdate": 1576798747541, "tmdate": 1576800888535, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The submission presents a differentiable take on classic active contour methods, which used to be popular in computer vision. The method is sensible and the results are strong. After the revision, all reviewers recommend accepting the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721177, "tmdate": 1576800272155, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Decision"}}}, {"id": "Syl8NXg6tH", "original": null, "number": 2, "cdate": 1571779374020, "ddate": null, "tcdate": 1571779374020, "tmdate": 1574442068020, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper investigates an image segmentation technique that learns to evolve an active contour, constraining the segmentation prediction to be a polygon (with a predetermined number of vertices).  The advantage of active contour methods is that some shapes (such as buildings) can naturally be represented as closed polygons, and learning to predict this representation can improve over pixelwise segmentation.\n\nThe authors propose to learn an image-level displacement field to evolve the contour, and a neural mesh renderer to render the resulting mask for comparison with the ground truth mask.  The performance compared to prior learning-based active contour methods is impressive.\n\nIn section 4.3, there\u2019s a reference to a \u201cgap in performance\u201d between the proposed method and DARNet and a reference to a \"low number of vertices,\" but a comparison between the two methods as the numbers of vertices is varied seems to only be present in Fig. 6 -- it would be interesting to see an explanation of the discrepancy for the lower number of vertices seen in this figure.\n\nOverall, due to the relative simplicity of the approach and impressive performance compared to prior learning-based approaches I recommend to accept.\n\nPost-rebuttal:  I maintain my recommendation.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244802611, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Reviewers"], "noninvitees": [], "tcdate": 1570237723674, "tmdate": 1575244802624, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review"}}}, {"id": "H1xhlGTQqH", "original": null, "number": 3, "cdate": 1572225523744, "ddate": null, "tcdate": 1572225523744, "tmdate": 1573948043591, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes a straightforward method for end-to-end learning of active contours, based on predicting a dense field of 2D offsets, and then iteratively evolving the contour based on these offsets. A differentiable rendering formulation by Kato et al is employed to make the process of aligning a contour to a GT mask differentiable. \n\nThe model shows rather compelling results on small datasets, and is very simple, with very strong parallels to active contours, which is a strength. The results improve those of DARNet, which to the best of my knowledge is the main published work in the space other than Curve-GCN. One thing that would be helpful, is  to have an experiment on a large dataset, such as Cityscapes -- right now all the datasets are testing the model in only the small-data regime. Perhaps in a supplement, it would also help to do ablation of how input image / dense deformation resolution affects the result quality -- the input can be subsampled by powers of 2 for the experiment. \n\nAs Amlan Kar helpfully points out, the work heavily overlaps with his approach \"Fast Interactive Object Annotation with Curve-GCN\", CVPR 2019, which is not cited or compared to. Curve-GCN similarly utilizes differential rendering (only a different variant) to match the GT masks. To me, the main difference wrt Curve-GCN is that explicit dense displacement fields are generated by the net and used directly for the iterative refinement steps, while Curve-GCN leverages implicit feature embeddings and uses GCN layers for their iterative updates. A second main difference is that Curve-GCN supports splines and interactive editing, while the proposed approach does not. Beyond these, there are multiple other differences that the authors point out, but those are more of a technical nature. Unfortunately, without a more direct comparison, it is very difficult to evaluate the design choices in the two approaches, which I feel is necessary for proper understanding of the paper. \n\nAFTER REBUTTAL: The authors made additions that covered my concerns, so I have switched my recommendation. \n\nA few more minor clarity / presentation issues. \n-- \u201cThe recent learning-based approaches are either non-competitive or proven to be effective in the specific settings of building segmentation\". It's not exactly clear what the point is in the context. Which \"learning-based approaches\"? \n-- Typo 'backpropogation'. \n-- A little better explanation of how a differentiable renderer of Kato works would have been helpful. \n-- Figure 3 is not referenced in the text, takes a little bit of thought why it is relevant (helps explain Fig 1, but maybe better to show it prior to Fig 1). \n-- In Eq 4 it\u2019s not clear what F is.  (I see it is explained in Algorithm box, but that's much later)\n\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244802611, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Reviewers"], "noninvitees": [], "tcdate": 1570237723674, "tmdate": 1575244802624, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review"}}}, {"id": "S1gg0BrviH", "original": null, "number": 6, "cdate": 1573504455711, "ddate": null, "tcdate": 1573504455711, "tmdate": 1573504455711, "tddate": null, "forum": "rkxawlHKDr", "replyto": "r1xFi7sFFS", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "Thank you for the additional feedback and for upgrading the paper\u2019s rating", "comment": "Thank you for the additional feedback and for upgrading the paper\u2019s rating. We appreciate the timely response and apologize for neglecting to proofread the submitted version more carefully. \n\nRegarding the number of iterations. In the original (and revised) submission, Section 4.4 \"Number of Iterations\u201d and Fig.8 (as numbered in the revised version), we experiment with different number of iterations. We noticed that a single iteration is less beneficial across all datasets, while 2-3 iteration results in higher performance.  Nevertheless, as the reviewer hypothesised, a single iteration can already produce very good results, as can be seen from our experiments. \n\nWe have released a new revision, elaborating on two subjects: (i) The initial guess, and (ii) the effect of the number of iterations T."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "r1xFi7sFFS", "original": null, "number": 1, "cdate": 1571562401150, "ddate": null, "tcdate": 1571562401150, "tmdate": 1573298081193, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "EDIT: The rating changed from '1: Reject' to '6: Weak accept' after the rebuttal. See below for my reasoning.\n\nThe submission considers two-class image segmentation problems, where a closed-contour image region is to be specified as the 'object'/region of interest, vs. 'no-object'/background. The approach taken here is end-to-end learning with an active-contour type approach. The main loss, in contrast to other active contour approaches, contains a direct difference of the estimated polygon area vs. ground truth polygon area.\n\nThe applied method seems conceptually quite simple (as admitted by the authors in Section 5), and the neural rendering approach seems quite neat, but both method presentation (Section 3) and evaluation (Section 4) seem incomplete and leave significant open questions.\n\nOne of my main concerns is related to the fact that the displacement field is static and, according to Figure 1 and Algorithm 1, is evaluated only once per image.\nIf the displacement field J is not conditioned on the current polygon shape (and this does not seem to be the case), then I am wondering why T iterations in the sampling/rendering part are necessary at all. When only considering L_seg, the optimal solution should be found within one iteration, since the displacement field will be able to provide the optimal answer. So maybe these iterations are only necessary when L_B and L_K are incorporated?\nIn any case, it is unclear why even L_seg is accumulated (using unweighted mean) over all T iterations before being backpropagated. Does this mean that these iterations are not meant to yield shape improvements? Why is ||M^t-M|| not evaluated per iteration, for the purpose of minimization?\nIt is also not sufficiently clear whether M^t in Equation 4 is a filled polygon mask, or if the mask is just related to the boundary (with a certain width). In absence of explanatory image material, I am assuming the former.\nOverall the method description remains weak, since obvious questions/concerns such as the above are not addressed.\n\nThe experimental results look good from a quantitative point of view, and indeed, the strongest baselines, e.g. DARNet, are outperformed significantly in many cases.\nSection 4 mostly focuses on quantitative evaluation and lots of picture examples, but fails to give insight into particular behaviors, failure cases, etc.\nThe evaluation procedure is cast a bit into doubt by two things: 1) In Figure 4, the initializations (blue circles) between the DARNet method and the proposed method are very different in size. I am wondering if this then still constitutes a fair comparison, and I have some doubts there. 2) In Figure 6, the proposed method consistently looks much worse than the DARNet baseline (and, in contrast to the baseline, completely fails for 4 vertices), unless the colors were swapped in the description.\n\nOverall, I do not think the submission is in a good enough shape for acceptance.\n\nMinor remarks:\n- The values for lambda_1 and lambda_2 seem to come out of thin air, and they also seem quite small. It needs to be mentioned how they were determined.\n- Data augmentation by rotation seems to be missing several values (between 270 and 260 degrees) and also not evenly spaced. Is this a typo or on purpose? In the latter case, an explanation is needed, since this seems weird.\n- Section 4.3: There is no \"Figure 4.2\", I assume you mean Figure 6, which otherwise remains unreferenced.\n- Section 4.3, Ablation Study: Don't use the word \"derivatives\" when you're talking about variations.\n- Section 4.3, Ablation Study: \"even without no auxiliary loss\" -> remove \"no\" or change \"without\" -> \"with\"\n\n-------------\nPost-rebuttal comments:\n\nI have read the revised version, as well as the other reviews and all authors' comments. The inclusion of an evaluation on a larger-size data set is highly appreciated, and seems to indeed validate the robustness of the method. Typos were fixed, including the switched color descriptions in Figure 7 (which should not have passed initial submission in the first place, if the text had been proofread properly).\n\nSeveral of the open questions (e.g. \"Why is L_seg accumulated before backpropagation?\", \"Why is the algorithm iterative if the displacement map is computed only once, if not for the other loss terms?\", \"Choice of values for lambda_1, lambda_2\", Initial diameter of initialization\") have been somewhat addressed by the authors in the rebuttal comment, though not in great detail.\n\nBased on the quality of the results across data sets, and because I believe that the timely publication of this rather simple method can benefit further research in this area, I have adjusted my score to a 'Weak accept'. That said, I still do not think it is a good manuscript, and my score should be seen as a massive benefit of the doubt toward the authors.\n\nMost importantly, above questions have NOT been adequately addressed in the actual revised text. The authors claim they have \"improved the manuscript considerably\", but yet I see more reasoning for certain choices described in the comment here than in the actual manuscript. Most of the changes are in Section 2 and the new Section 4.3, but not much relevant to my comments changed in Section 3.\n\nFor example, balloon and curvature losses aside, it is still not clear why an iterative approach would be helpful past the first iteration. An ideal displacement map that is not conditioned on the polygon should point, for each pixel, straight to the closest contour pixel. It is clear to me that this may not be what is being learned when multiple iterations are forced, yet it is not addressed why multiple iterations should be beneficial. (I could see why they could be beneficial if the approach was conditioned on the polygon vertices, to avoid vertex collapsing, but it's not.)\n\nA good submission preempts these kinds of questions by addressing them carefully. What seems crystal clear to the authors will not be crystal clear to every reader. The authors should be more careful to include their reasoning in the actual text, which I believe this is essential for proper, easy understanding of the paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244802611, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Reviewers"], "noninvitees": [], "tcdate": 1570237723674, "tmdate": 1575244802624, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Review"}}}, {"id": "HkxNP0b7oH", "original": null, "number": 5, "cdate": 1573228123533, "ddate": null, "tcdate": 1573228123533, "tmdate": 1573228123533, "tddate": null, "forum": "rkxawlHKDr", "replyto": "r1xFi7sFFS", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "We have improved the manuscript considerably following the feedback", "comment": "Thank you for the detailed review.\n\nIndeed, our approach predicts the displacement field only once. We believe that this is a strength of our approach and is part of its simplicity. Similarly to RNNs with fixed weights, having the displacement map computed only once, does not mean that iterations are not beneficial. Note also that the error is backpropagated from all iterations. \n\nRegarding the use of the balloon and curvature term, please see the ablation study, which shows that while our method is extremely competitive even without these losses, the two losses contribute to the results.\n\n\u201cWhy is ||M^t-M|| not evaluated per iteration\u201d -- As mentioned in the text before Eq.5., M^t is evaluated at each iteration given the updated set of points. Therefore, ||M^t-M|| is evaluated per iteration. The backpropagation is done on the accumulated loss.\n\nClarity regarding M^t in Equation 4 - the mask M^t is a filled polygon rendered from the set of points P^t. We have further clarified this in the revision.\n\nWe did not search for the best initial diameter, and simply fixed it to the size of 16 pixels across all datasets. Please note that DARNet uses multiple initializations (circles) or different sizes for each dataset as can be seen in Fig.4, while we use only one fixed-size circle. This further supports the robustness of our method.\n\nThe caption of Fig.6 (of the original paper, 7 in the revised) is indeed a typo, and the colors were switched. We apologize for this and have fixed it in the revision. The quantitative results in the graphs of Fig.7 (of the original submission, now Fig. 8) support the fact that our method yields better segmentation for simple polygons as well.\n\nThe values of Lambda1 and Lambda2 were fixed early during the development process and used across datasets. These reflect the relatively smaller part that the ballooning force and the curvature loss play, in the optimization. This is further supported by the ablation analysis that demonstrates that our method is extremely competitive even without these. Similarly, the set of rotations was set without much thinking early on during training, and since it worked, we kept it as is. We believe that changing the augmentation would contribute little to the results, and does not justify the pitfalls of multiple hypothesis testing.\n\nOverall, we hope that the simplicity and elegance of our method are not interpreted as a disadvantage. We believe that the power of our method over previous work (as complicated as they\u2019ll be) is in the straightforward approach. Following the reviews, we have provided an additional dataset for comparison, and clarified and fixed the relevant sections and figures.\n\nWith the CVPR deadline in a week, we would appreciate a timely response, in order for us to be able to plan our submission strategy."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "SJxyGCWQsB", "original": null, "number": 4, "cdate": 1573228038945, "ddate": null, "tcdate": 1573228038945, "tmdate": 1573228038945, "tddate": null, "forum": "rkxawlHKDr", "replyto": "Syl8NXg6tH", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "Thank you for the supportive review", "comment": "Thank you for the supportive review.  We are sorry for the reference mistakes, these are all fixed in the new revised version. \n\nThere was a typo in the caption of Fig.6 (Fig. 7 in the revised version), which switched the association between the methods and the colors. We believe that this mistake has led to the remark concerning this figure."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "ryx4aT-Xir", "original": null, "number": 3, "cdate": 1573227963581, "ddate": null, "tcdate": 1573227963581, "tmdate": 1573227963581, "tddate": null, "forum": "rkxawlHKDr", "replyto": "H1xhlGTQqH", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "We have improved the manuscript considerably following the detailed feedback", "comment": "We thank the reviewer for the comprehensive review. \n\nWe apologize for the typos in the previous draft. These have been corrected.\n\nTo your comments:\n\nComparison to Curv-GCN: as noted by the reviewer, the differences between the methods are in the support of splines and working with an embedding space in Curve-GCN, vs. displacement map. To emphasize: our method employs a single learned network that produces a displacement image in a single forward pass. The CNN used by Curve-GCN predicts an embedding space of size 28x28 that is further processed by graph neural networks.\n\nFollowing the review, we have conducted experiments on Cityscapes, which is the only public dataset available from Curve-GCN experiments (their code is not available). In this dataset, our method obtains SOTA for 6/8 classes and SOTA, by a sizable margin that is larger than the difference between the performance of previous work, in the overall mean mIoU. We believe that this also directly addressed the reviewer\u2019s comment regarding larger datasets.\n\nAll of our models are trained at the resolution of 64x64 pixels. As noted in the original submission when discussing the Vaihingen dataset \u201cwe experiment with different resizing factors during training\u201d. Following the review, we share these results in Tab.5 of the revised submission. As can be seen, there is a steep degradation in performance below 64x64, which we attribute to the lack of details. When doubling the resolution to 128x128, the performance slightly degrades, however, that model could improve with further training epochs.\n\n\nThe recent learning-based approaches are either non-competitive or proven to be effective in the specific settings of building segmentation\" \u2014 we have clarified in the text that we mean learning-based active contour methods and have limited the scope of the claim. \n\nWe have added a paragraph regarding the use of a 3D renderer for 2D maps. We simply fix the third coordinate and use the code of Kato as is.\n\nThe letter \u201cF\u201d (for faces) is defined at the beginning of the \u201cMethod\u201d section.\n\nWe believe that various issues raised by the reviewer were fully addressed in a way that considerably improved the manuscript. With the CVPR deadline in a week, we would appreciate a timely response, in order for us to be able to plan our submission strategy. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "SJxs76WmjH", "original": null, "number": 2, "cdate": 1573227811341, "ddate": null, "tcdate": 1573227811341, "tmdate": 1573227811341, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "A revised version, including new experiments comparing with Curve-GCN", "comment": "Following the reviews, we have revised our manuscript to correct the various typos, to clarify some issues and, most importantly, to update the related work section and to compare experimentally with the CVPR 2019 work of Ling et al. As detailed on open review, which this work indeed narrows our novelty claims, there are important differences and the two methods are very much different.\n\nWe are happy to report that on the public dataset on which the CVPR 2019 work has been tested, our method outperforms all previous work in 6/8 categories and shows a clear advantage in the mean performance. This, without performing any modification to our method and despite our method being considerably less involved than the other methods."}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "SygWvXz19S", "original": null, "number": 1, "cdate": 1571918680702, "ddate": null, "tcdate": 1571918680702, "tmdate": 1571918680702, "tddate": null, "forum": "rkxawlHKDr", "replyto": "B1lq6PWaYr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment", "content": {"title": "Thank you for pointing us to the missing related work", "comment": "Thank you very much for pointing us to [1], which is indeed related and would be cited appropriately. We would like to enumerate some of the important differences between the methods. \n1. Supervision and training: We supervise with GT masks only during training, while [1] learns an additional edge branch and a vertex branch. Vertex-based supervision constraints their model to learn a specific location for each point. \n2. Training: While we train our model end-to-end with a single supervision, [1] performs a two-phase learning, where they first train using edges and vertices supervision, followed by fine-tuning with GT masks. [1] also points out that their rendering process is too slow for training end-to-end using only GT masks (Sec. \u201cTraining Details\u201d), while we use a fast, fully differentiable renderer.\n3. CNN role: Our method employs a single learned network that produces a displacement image in a single forward pass. The CNN used by [1] predicts an embedding space of size 28x28 that is further processed by other networks.\n4. CNN architecture: We employ a fully convolutional CNN that produces an output that is the same size as the input image, while [1] scales a fixed-sized input to a spatially-limited 28x28 image.\n5. To emphasize: our method is considerably more direct, and learns a 2-D displacement field in the scale of the input by a fully convolutional network. The update in [1] is by a learned GCN that is applied over graph nodes that employ the 28x28 embedding.\n6. Loss: we incorporate two loss terms that are based on time-tested pulling forces from the classical active contour literature: the Balloon and Curvature terms. This allows us to work directly with the contour.\n\n[1] Fast Interactive Object Annotation with Curve-GCN: https://arxiv.org/abs/1903.06874 - CVPR 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper2378/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2378/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2378/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2378/Authors|ICLR.cc/2020/Conference/Paper2378/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142252, "tmdate": 1576860560254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Official_Comment"}}}, {"id": "B1lq6PWaYr", "original": null, "number": 1, "cdate": 1571784642512, "ddate": null, "tcdate": 1571784642512, "tmdate": 1571797886492, "tddate": null, "forum": "rkxawlHKDr", "replyto": "rkxawlHKDr", "invitation": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment", "content": {"comment": "Thanks for the nice work! I would like to point out our related work [1], published at CVPR '19 here, that also utilizes a differentiable renderer to render polygons into masks similar to your work. It would be nice to discuss contributions in light of our paper as well, thanks!\n\n[1] Fast Interactive Object Annotation with Curve-GCN: https://arxiv.org/abs/1903.06874 - CVPR 2019", "title": "Related Work"}, "signatures": ["~Amlan_Kar2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Amlan_Kar2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["shiretzet@gmail.com", "shaharabany@mail.tau.ac.il", "wolf@fb.com"], "title": "End to End Trainable Active Contours via Differentiable Rendering", "authors": ["Shir Gur", "Tal Shaharabany", "Lior Wolf"], "pdf": "/pdf/8fd896acc91bd1f06a713396633ab37e62e6dd46.pdf", "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "keywords": [], "paperhash": "gur|end_to_end_trainable_active_contours_via_differentiable_rendering", "_bibtex": "@inproceedings{\nGur2020End,\ntitle={End to End Trainable Active Contours via Differentiable Rendering},\nauthor={Shir Gur and Tal Shaharabany and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxawlHKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/157e71c4cf9f7bdf6ba239a81cbbc4a7073bde17.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxawlHKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504181149, "tmdate": 1576860593267, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2378/Authors", "ICLR.cc/2020/Conference/Paper2378/Reviewers", "ICLR.cc/2020/Conference/Paper2378/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2378/-/Public_Comment"}}}], "count": 16}