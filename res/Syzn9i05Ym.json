{"notes": [{"id": "B1lq3p5cxN", "original": null, "number": 9, "cdate": 1545412017578, "ddate": null, "tcdate": 1545412017578, "tmdate": 1569769138927, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "H1eLkAcWxV", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "We are very surprised and confused by the decision, given that \u201cReviewers generally agree that the experiments are sufficient and convincing, and that the method is evaluated well ... The paper is reasonably well-written.\u201d", "comment": "Regarding novelty, this paper represents the first neural random field paper achieving state of the art results. We have addressed the reviewers' concern on novelty, but received no further concrete response.\n\nUn-familiarity with the new approach should not be the reason to reject the paper. Rather, given that this learning problem is clearly under-appreciated and the proposed method is theoretically sound and empirically promising, it is more worthwhile to help promote further research in this NEW approach.\n\nWith full respect, we would greatly appreciate it if the paper could be reconsidered."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "Syzn9i05Ym", "original": "BkeDhrOqKQ", "number": 569, "cdate": 1538087828049, "ddate": null, "tcdate": 1538087828049, "tmdate": 1545355380175, "tddate": null, "forum": "Syzn9i05Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1eLkAcWxV", "original": null, "number": 1, "cdate": 1544822237724, "ddate": null, "tcdate": 1544822237724, "tmdate": 1545354529250, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Meta_Review", "content": {"metareview": "This paper proposes a method for learning neural RFs with the inclusive-divergence minimization problem.\n\nReviewers generally agree that the experiments are sufficient and convincing, and that the method is evaluated well. Results are comparable with SOTA methods for image generation. The paper is reasonably well-written.\n\nThe paper is also somewhat lacking in background; most people at ICLR will not be very familiar with this learning problem. More information on the inclusive-divergence minimization problem would be helpful. A major concern of reviewers is whether novelty of the method is sufficient for publication.\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper569/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353170624, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353170624}}}, {"id": "HJl2-vta0Q", "original": null, "number": 8, "cdate": 1543505667865, "ddate": null, "tcdate": 1543505667865, "tmdate": 1543934832329, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "We make substantial contribution, instead of incremental contribution.", "comment": "Dear Reviewers,\n\nThank you again for your valuable comments and for considering our responses and revisions. The main revisions of the manuscript  are: revising the Abstract, expanding the Related Work section to more clearly reveal the differences between this paper and previous studies (to respond to reviewer 1 & 2), adding the new Proposition 1 in the Supplement to prove the old Eq. 4 (to respond to reviewer 2), updating sec 4.4 and the caption of Table 4 to make them clearer (to respond to reviewer 3).\n\nBasically, we do not think the comment on our lack of substantial contributions is reasonable, considering the contributions clarified below [Please read our updated response for novelty and contribution below after receiving reviewer-1's feedback to our initial response.]. In a word, our major contribution/our major claim is:  we propose the inclusive-NRF approach for continuous data, with _solid theoretical examination_ on exploiting gradient information in model sampling and with _extensive strong experimental results_ compared to the state of the art.\n\nIn addition to the strong experimental results of inclusive-NRFs, one fundamental benefit of our inclusive-NRF approach is that, unlike in GANs, we can learn density estimate about the data manifold (partly illustrated in Figure 1).\n\nGiven reviewer-1 acknowledging that modeling continuous data can be substantially different from modeling discrete data, \u201cThe target NRF model, the generator and the sampler are all different\u201d, and the theoretical guarantees contributed in this paper for applying SGLD/SGHMC, we hope that we have addressed your concern on our contribution.\n\nThe discussion period is coming to an end. If you have any questions or would like to provide more specific context behind your scores, we would be happy to provide feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "BygoQnV9CQ", "original": null, "number": 7, "cdate": 1543289890734, "ddate": null, "tcdate": 1543289890734, "tmdate": 1543633489480, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "r1g2osJOnX", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Author Response to Reviewer-1 feedback", "comment": "Thank you very much for taking time to read our response and give feedback.\n\nWe are encouraged by your acknowledgement of contributions made by this paper - noticing that modeling continuous data can be substantially different from modeling discrete data, and theoretical guarantees for applying SGLD/SGHMC.\n\n> The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.\n\nBetter mode exploration of the proposed method is experimentally demonstrated in the paper, especially in the GMM experiment (Figure 1 and Table 1), as also commented by both Reviewer 1 - \"Experiments are sufficient and convincing, especially the synthetic data experiments with GMM distributions.\" and Review 2 - \"The toy example with mixture of gaussians is convincing showing the contrast in results between the exclusive NRF, inclusive, and sampling gradient revision steps.\".\n\nOur explanation for this improvement is mainly based on different properties between inclusive and exclusive divergences, which is commonly acknowledged when discussing the issue of mode exploration.\n\n> In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.\n\nBesides the strong experimental results (better/on par with state-of-the-art), one fundamental additional benefit of our inclusive-NRF approach is that, unlike in GANs, it enables us to learn density estimate about the data manifold (partly illustrated in Figure 1).\n\n> A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.\n\nPlease read our updated response for novelty and contribution above after receiving your feedback to our initial response.\n\nExaminations under multiple learning settings are better viewed as a plus rather than a minus, which elaborates our major contribution - the proposed inclusive-NRF approach, as we explained in our response above.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "r1lMZ--J0m", "original": null, "number": 2, "cdate": 1542553850307, "ddate": null, "tcdate": 1542553850307, "tmdate": 1543632712191, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Author Response: for novelty and contribution", "comment": "We would like to thank all of the reviewers for their thoughtful and helpful comments. We have uploaded a new version of our manuscript with improvements based on reviewer feedback. \n\nFor the novelty and contribution of the paper:\n\nThe Related Work section in the updated paper is expanded to more clearly reveal the differences between this paper and previous studies, e.g. Xie et al (2016) and Wang & Ou (2017). \n\nLearning in Wang & Ou (2017) and in this paper minimizes the inclusive-divergence $KL[p_\\theta||q_\\phi]$ w.r.t. $\\phi$.  But noticeably, this paper presents our innovation in development of NRFs for continuous data, which is fundamentally different from Wang & Ou (2017) for discrete data. The target NRF model, the generator and the sampler are all different.\nWang & Ou (2017) mainly studies random field language models, using LSTM generators (autoregressive with no latent variables) and employing Metropolis independence sampler (MIS) - applicable for discrete data (natural sentences).\nIn this paper, we mainly develop random field models for continuous data (images), using latent-variable generators and utilizing SGLD/SGHMC (with solid theoretical examination) to exploit gradient information in the continuous space.\n\nIn Xie et al (2016), motivated by interweaving maximum likelihood training of the random field $p_\\theta$ and the latent-variable generator $q_\\phi$, a joint training method is introduced. Operationally, in learning $\\theta$ and $\\phi$, this method also uses Langevin sampling to generate samples. Two Langevin sampling steps are intuitively interleaved according to gradients w.r.t. x and h separately. This is different from our sampling step, which moves $(h,x)$ jointly, as theoretically justified in section 2.2. Moreover, interpretation presented in Xie et al (2016) relates their method to a joint optimization problem, which is also different from ours as shown in Eq. (4). Thus, learning in Xie et al (2016) does not aim to minimize the inclusive-divergence $KL[p_\\theta||q_\\phi]$ w.r.t. $\\phi$. [We correct our misunderstanding of Xie et al (2016) based on its recent version. See our updated paper for details.]\n\nPlease refer to the updated related work for more comparisons. It can be seen that there are non-trivial differences in model formulation, algorithm development, and theoretical examination between this paper and previous studies. This paper makes significant contribution in learning NRFs, instead of incremental contribution, as explained below.\n\nIn a word, our major contribution/our major claim is:  we propose the inclusive-NRF approach for continuous data, with _solid theoretical examination_ on exploiting gradient information in model sampling and with _extensive strong experimental results_ compared to the state of the art.\n* The major contribution is substantiated by the following developments, which are significantly new and have never been reported/obtained before:\n - The Eq. (5) in section 2.1 with Proposition 1 in the Supplement (model formulation);\n - The whole section 2.2 (solid theoretical examination on applying SGLD/SGHMC);\n - The whole section 2.3 (SSL with inclusive-NRFs).\n* We show that inclusive-NRFs can be flexibly used unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks.\n* Extensive empirical evaluations show that inclusive-NRFs obtain state-of-the-art sample generation quality and achieve strong semi-supervised learning results on par with state-of-the-art DGMs.\n\nSpecific responses to each reviewer are provided in the following. We are happy to discuss any questions that you may have during the discussion period."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "r1g2osJOnX", "original": null, "number": 1, "cdate": 1541041060119, "ddate": null, "tcdate": 1541041060119, "tmdate": 1543199521502, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "content": {"title": "Incremental Contribution", "review": "The paper proposes the inclusive neural random field model. Compared the existing work, the model is different because of the use of the inclusive-divergence minimization for the generative model and the use of stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo  (SGHMC) for sampling. Experimental results are reported for unsupervised, semi-supervised, and supervised learning problems on both synthetic and real-world datasets. Specific comments follow:\n\n1. A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.\n\n2. A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.\n\nOther points:\n3. the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.\n\n4. more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.\n\n5. what makes some of the statistics in the tables unobtainable or unreported?\n\n\n============= After Reading Response from Authors ====================\n\nThe reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors\u2019 argument. \n\n\u201cThe target NRF model, the generator and the sampler are all different.\u201d\nIt is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.\n\nAs for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2\u2019s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.\n\nThe explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.\n\nIn terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.\n\nA general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper569/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "cdate": 1542234431385, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335754106, "tmdate": 1552335754106, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1l33IW10Q", "original": null, "number": 3, "cdate": 1542555316379, "ddate": null, "tcdate": 1542555316379, "tmdate": 1542904855552, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Hkxe-51c3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Author Response", "comment": "We appreciate that you found our paper to be interesting. Please refer to the above for our novelty and contribution.\n\n>the idea of using neural network to learn the random field is not new.\n\nYes but received less attention with slow progress. We significantly advance the learning of NRFs, both theoretically and empirically. Please refer to the above for our novelty and contribution.\n\n> Using inclusive-divergence is also not new, e.g. Xie et al (2016) and Wang & Ou (2017) already proposed to use the inclusive-divergence.\n\nThe Related Work section in the updated paper is expanded to more clearly reveal the differences between this paper and previous studies, e.g. Xie et al (2016) and Wang & Ou (2017).  Also please refer to the above for our novelty and contribution.\n\n> The overall technical quality of the paper is sound but I am not 100% sure about the equations, e.g. the second line in Eq. 4. \n \nWe add the proof for the second line of Eq. 4 in the new Proposition 1 in the Supplement."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "SJgY-YW1R7", "original": null, "number": 5, "cdate": 1542555904930, "ddate": null, "tcdate": 1542555904930, "tmdate": 1542904052817, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "r1g2osJOnX", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Author Response", "comment": "> A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.\n\nPlease refer to the above for our novelty and contribution.\n\n> A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.\n\nThis claim is empirically validated and conceptually plausible by the following reasoning. By the property of minimizing inclusive-divergence, the generator tends to cover modes of the target density $p_\\theta$. The SGLD/SGHMC sampling further pushes the samples towards the modes of $p_\\theta$. Presumably, this helps to produce Markov chains that mix fast between modes and facilitate model learning.\n\n> the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.\n\nIn addition to the strong experimental results of inclusive-NRFs, one fundamental benefit of our inclusive-NRF approach is that, unlike in GANs, we can learn density estimate about the data manifold (partly illustrated in Figure 1).\n\n> more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.\n\nThanks for your suggestion. We expand section 2.1 and provide the new Proposition 1 in the Supplement.\n\n> what makes some of the statistics in the tables unobtainable or unreported?\n\nThanks for your suggestion. We update Table 3 to differentiate the two cases - 1) the results are not reported in the original paper and without released code; 2) not applicable, e.g. the models cannot generate samples stochastically."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "Bkxlxu-10X", "original": null, "number": 4, "cdate": 1542555624454, "ddate": null, "tcdate": 1542555624454, "tmdate": 1542557408100, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "S1gFepIt27", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Author Response ", "comment": "We appreciate that you found our paper to be effective.\n\n> While the paper claims the results show classification and generation performance are complementary, Table 3 appears to validate the opposite claim, that these are to a large degree a trade-off.  The fact that this system performs well at both is good, but to me it looks like it may be on the \"shoulder\" of a frontier curve if one were to plot the classification vs generation performance of the different current systems.\n\nWe do not claim the results show classification and generation performance are complementary. As we comment in the footnote under section 4.3, the conflict of good classification and good generation is reported when using the (K + 1)-class GAN-like discriminator objective for SSL, and does not seem to be reported in previous generative SSL methods (Zhu (2006); Larochelle et al. (2012)) which use the K-class classifier like in semi-supervised inclusive-NRFs.\n\nWhat we claim is that the conflict of GAN-based SSL is embarrassing and in fact obviates the original idea of generative SSL - successful generative training, which indicates good generation, provides regularization for finding good classifiers (Zhu, 2006; Larochelle et al., 2012). In this sense, Bad-GANs could hardly be classified as a generative SSL method.\n\nThe raised viewpoint of trade-off between classification and generation is interesting. Different SSL methods and models, e.g. generative model based SSL (e.g. Zhu, 2006; Larochelle et al., 2012) and discriminative SSL (e.g. Miyato et al., 2017), use different regularization. Does this trade-off exist for all SSL methods and models, and if not, for what type of SSL methods and models? Is the trade-off due to the regularization used? Exploration of such problem is interesting but outside the scope of this paper.\n\n> Table 4 and sec 4.4:  I think these could be clearer. \n\nThanks for your suggestion. We update sec 4.4 and add explanation in the caption of Table 4.\n\n> There does appear to be a consistent increase from the first column (generation) to second (revision), though -- is this what this observation refers to?  \n\nYes.\n\n> In addition, I'm not entirely clear what the \"Generation IS\" vs \"Revision IS\" column refers to --- I believe \"generation\" is the initial sampling of x=g(h) (i.e. h followed by q(x | h)), and \"revision\" is the application of gradient revision.  But then how does the generation IS results change from row to row (which only modify the revision step)?\n\nColumn-wise \"Generation IS\" vs \"Revision IS\" is to compare the two manners to generate samples  - whether applying sample revision or not in inference (generating samples) given a trained NRF, as previously illustrated in Figure 1 over synthetic GMM data. For both manners, the NRF model is trained with the sample revision step.\n\nEach row in Table 4 represents a specific setting in model training, such as using SGLD or SGHMC and the revision step $L=1/5/10$ used. Thus, each row produces a specific different NRF model."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "S1lXy5-yAQ", "original": null, "number": 6, "cdate": 1542556123240, "ddate": null, "tcdate": 1542556123240, "tmdate": 1542556123240, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "BJxF9DHApQ", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "content": {"title": "Dear AC", "comment": "Thanks for your reminder. We have added our response."}, "signatures": ["ICLR.cc/2019/Conference/Paper569/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syzn9i05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper569/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper569/Authors|ICLR.cc/2019/Conference/Paper569/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers", "ICLR.cc/2019/Conference/Paper569/Authors", "ICLR.cc/2019/Conference/Paper569/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614452}}}, {"id": "Hkxe-51c3Q", "original": null, "number": 3, "cdate": 1541171704357, "ddate": null, "tcdate": 1541171704357, "tmdate": 1541533881773, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "content": {"title": "Interesting but incremental", "review": "This paper addresses an important problem of learning the random field using neural networks by using a inclusive auxiliary generator. Comparing to existing state-of-the-art methods for learning neural random fields, this paper used a the inclusive-divergence (KL divergence of the density approximate and the auxiliary generator) which avoids the intractable entropy term. SGLD/SGHMC are used to revise samples drawn from the auxiliary generator and these two sampling methods are examined theoretically.  \n\nIn generally, the paper is well motivated and well written. Experiments are sufficient and convincing, especially the synthetic data experiments with GMM distributions. \n\nHowever, I am a little bit concerned that the theoretical contribution seems weak. As discussed in the related work, the idea of using neural network to learn the random field is not new. Using inclusive-divergence is also not new, e.g. Xie et al (2016) and Wang & Ou (2017) already proposed to use the inclusive-divergence. If I understand it correctly, the only contribution here is to apply the SGLD/SGHMC to revise the samples and authors provided some theoretical analysis of SGLD/SGHMC.\n\nThe overall technical quality of the paper is sound but I am not 100% sure about the equations, e.g. the second line in Eq. 4. \n\nIn summary, this paper is well written and authors have done a good job. But I will appreciate if authors can elaborate\nmore on the novelty and innovation of the paper. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper569/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "cdate": 1542234431385, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335754106, "tmdate": 1552335754106, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gFepIt27", "original": null, "number": 2, "cdate": 1541135600542, "ddate": null, "tcdate": 1541135600542, "tmdate": 1541533881562, "tddate": null, "forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "content": {"title": "appears effective, though unfamiliar with this type of model", "review": "This paper describes a method of training NRFs with auxiliary generator networks, using an error that minimizes KL(NRF || generator).  This formulation enables the use of iterative gradient-based stochastic sampling of image samples from the model distribution using SGLD/SGHMC.  Applications to both unsupervised sample generation and semi-supervised classification are evaluated.\n\nI'm not very familiar with these types of NRFs or random sampling techniques, but the approach appears sound and is evaluated rather well.  I would have liked some more background and explicit description and contrast compared to the explicit NRF.  While this is described already, I think the contrasts could potentially be spelled out even more explicitly, particularly in the descriptions of the sampling algorithms.\n\nThe toy example with mixture of gaussians is convincing showing the contrast in results between the exclusive NRF, inclusive, and sampling gradient revision steps.\n\nExperimental evaluations on MNIST, SVHN and CIFAR show that the system obtains performance similar to SOA generative systems, in both semi-supervised classification and sample generation.\n\n\nQuestions and comments:\n\n- While the paper claims the results show classification and generation performance are complementary, Table 3 appears to validate the opposite claim, that these are to a large degree a trade-off.  The fact that this system performs well at both is good, but to me it looks like it may be on the \"shoulder\" of a frontier curve if one were to plot the classification vs generation performance of the different current systems.\n\n- Table 4 and sec 4.4:  I think these could be clearer.  The first observation states that revision improves IS.  But using more iterations (increasing L) does not appear to increase IS.  There does appear to be a consistent increase from the first column (generation) to second (revision), though -- is this what this observation refers to?  In addition, I'm not entirely clear what the \"Generation IS\" vs \"Revision IS\" column refers to --- I believe \"generation\" is the initial sampling of x=g(h) (i.e. h followed by q(x | h)), and \"revision\" is the application of gradient revision.  But then how does the generation IS results change from row to row (which only modify the revision step)?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper569/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neural Random Fields with Inclusive Auxiliary Generators", "abstract": "Neural random fields (NRFs), which are defined by using neural networks to implement potential functions in undirected models, provide an interesting family of model spaces for machine learning. In this paper we develop a new approach to learning NRFs with inclusive-divergence minimized auxiliary generator - the inclusive-NRF approach, for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. We show that inclusive-NRFs can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Particularly, inclusive-NRFs achieve state-of-the-art sample generation quality on CIFAR-10 in both unsupervised and supervised settings. Semi-supervised inclusive-NRFs show strong classification results on par with state-of-the-art generative model based semi-supervised learning methods, and simultaneously achieve superior generation, on the widely benchmarked datasets - MNIST, SVHN and CIFAR-10.", "keywords": ["Neural random fields", "Deep generative models", "Unsupervised learning", "Semi-supervised learning"], "authorids": ["769414284@qq.com", "ozjthu@gmail.com"], "authors": ["Yunfu Song", "Zhijian Ou"], "TL;DR": "We develop a new approach to learning neural random fields and show that the new approach obtains state-of-the-art sample generation quality and achieves strong semi-supervised learning results on par with state-of-the-art deep generative models.", "pdf": "/pdf/8e42e28c69bd73d512c02a533c9b2368568b7081.pdf", "paperhash": "song|learning_neural_random_fields_with_inclusive_auxiliary_generators", "_bibtex": "@misc{\nsong2019learning,\ntitle={Learning Neural Random Fields with Inclusive Auxiliary Generators},\nauthor={Yunfu Song and Zhijian Ou},\nyear={2019},\nurl={https://openreview.net/forum?id=Syzn9i05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper569/Official_Review", "cdate": 1542234431385, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "Syzn9i05Ym", "replyto": "Syzn9i05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper569/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335754106, "tmdate": 1552335754106, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper569/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}