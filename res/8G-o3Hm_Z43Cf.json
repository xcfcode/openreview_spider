{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1391900340000, "tcdate": 1391900340000, "number": 4, "id": "JSwPJZLj9ASjN", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8G-o3Hm_Z43Cf", "replyto": "8G-o3Hm_Z43Cf", "signatures": ["Qiang Qiu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for all these valuable and insightful suggestions. In what follows our responses. All responses marked with an * have already been included in the updated version of the arXiv submission. The others are currently been investigated.\r\n\r\n- '...Does the framework easily extend to other classification techniques?'\r\n\r\n*Response: The learned transformation at each node reduces intra-class variation and increases inter-class separation. Thus, such learned (transformed) representation can potentially help in other classification tasks as well, e.g., subspace based methods [A]. We have incorporated such comment in the revised paper, see Sec.1.\r\n\r\n\r\n- 'All dataset images used were scaled down to 16x16. How does this method perform on truly high-dimensional data?'; 'All the images were resized to 16 x 16': It's ambiguous whether this was for just YaleB, or all the datasets?; '... the feature space is very large ...'\r\n\r\n*Response: Such image resize is only applied to the Extend YaleB and MNIST, which gives a 256-dimensional feature. Given d-dimensional features, in this paper, we only focus on a d*d-sized square transformation matrix. A potential solution to handle high-dimensional data is to learn a 'fat' r*d-sized transformation matrix (r<<d) to compress the data while enhancing the discriminability, as discussed in [A]. This has been mentioned in the revised version, see Sec.2.4.\r\n\r\n\r\n- 'Is there a reason that not all experiments were performed with each of the datasets?'; '... in all 3 of these plots.'\r\n\r\n*Response: We first compare as many learners in a tree context for accuracy and testing time; then we only compare with learners that are widely adopted for random forests. See revised version, Sec.3, for a clarification.\r\n\r\n\r\n- '...timing results aren't too informative when reported for a single dataset.'\r\n\r\nResponse: We can observe from the second paragraph in Sec.2.4, the testing time is determined only by the feature dimension and the tree depth, as explained in detail in another response.\r\n\r\n\r\n- 'Experiments showing the effect of noise/mislabeld examples would be interesting as the impact of such imperfections on this scheme is unclear.'\r\n\r\n*Response: The first two terms in (3) will significantly reduce the intra-class variation/noise; and the final results combines multiple random trees, thereby expecting the proposed framework to be very robust to noise. This is the subject of future investigation. We have clarified this in the discussion, Sec.4.\r\n\r\n\r\n- 'Section 3.2, Paragraph 2, Sentence 1: What is the reason for mentioning that classes arriving at a node are randomly split into two categories? Why is it being introduced here as a new source of randomness?'\r\n\r\n*Response: In (3), we learn a transformation optimized for two class problem. The proposed strategy reduces a multi-class problem into a two-class problem at each node for transformation learning; furthermore, it introduces node randomness to avoid duplicated trees generated. This has been further clarified in Sec.2.5.\r\n\r\n\r\n- 'For the Kinect experiment, ... It would have been better to follow the setup in (Denil et al., 2013) ...'; '...but it would be nice to include a comparison to Denil et al (the dataset providers).'\r\n\r\nResponse: As training using the complete Kinect dataset requires considerable time and computation resources, we decided on a reduced scale of experiments, sufficient to show the advantage of the proposed approach over popular learners. Further experiments are ongoing.\r\n\r\n\r\n- 'Did you consider applying a difference of convex programming approach to (1) instead of gradient descent?'\r\n\r\nResponse: We adopted a gradient descent approach only for its efficiency and simplicity. It is probably not the best approach, and we leave it open for discussions and improvements.\r\n\r\n\r\n- ' Sec 2.4: '... only involves matrix multiplication, which is virtually computationally free at the testing time.' This seems like a odd claim. If d is large, then these matrix multiplications are far from free.'\r\n\r\n*Response: Assume each learned transformation matrix is of size r*d (r=d in the paper, but r is allowed to be a constant much smaller than d). Each split test consists of two matrix multiplications of complexity O(r*d) using a sequential implementation. With the modern architecture, p processes are usually assigned to each multiplication, which reduces the complexity to O(r*d/p) plus some small overhead. Thus, each split test can be reduced to linear complexity. However, we agree 'virtually computationally free' is over-stressed, and we revised it to 'low computational complexity'.\r\n\r\n\r\n- 'One question I have is how dependent this method is on image alignment.'\r\n\r\n*Response: The proposed method is robust to misalignment. One main objective of the learned transformation is to reduce the intra-class variations, which includes misalignment. As an example, in a separate work, we demonstrate the success in face recognition across poses using the learned transformation, see [A].\r\n\r\n\r\n- 'Pixel descriptors for kinect ... forming a 96-dim descriptor -- is this correct? ... '\r\n\r\n*Response: Each pixel is represented using depth difference from its 96 equally spaced neighbors with radius 8, 32 and 64 repletely, forming a 288-dim descriptor. We have further clarified this in Sec.3.3.\r\n\r\n\r\n[A] Q. Qiu and G. Sapiro, 'Learning transformations for clustering and classification', CoRR, vol. abs/1309.2074, 2013.  http://arxiv.org/abs/1309.2074"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Transformations for Classification Forests", "decision": "submitted, no decision", "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "pdf": "https://arxiv.org/abs/1312.5604", "paperhash": "qiu|learning_transformations_for_classification_forests", "keywords": [], "conflicts": [], "authors": ["Qiang Qiu", "Guillermo Sapiro"], "authorids": ["qiang.qiu@duke.edu", "guillermo.sapiro@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391507100000, "tcdate": 1391507100000, "number": 3, "id": "ee-Bvr_jOc49x", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8G-o3Hm_Z43Cf", "replyto": "8G-o3Hm_Z43Cf", "signatures": ["anonymous reviewer 508e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Transformations for Classification Forests", "review": "This paper studies a new split rule for classification trees, based on the distances to two linear subspaces constructed at each node.  Subspaces are constructed by partitioning a data subset by class, and finding a linear transformation that attempts to concentrate each same-class space, while maximizing angle distance between the two (different-class) spaces.  At test time, a left/right decision is made based on which subspace the test point is closer to (based on projected reconstruction).  For multiclass problems, classes are randomly assigned at each node to one of the two subspaces.  The method is supported by theoretical analysis, toy examples, simple image classification datasets, and a pixel labeling task.\r\n\r\nI'm not very knowledgeable in random forests, but this seems like a decent proposal to me, effectively integrating a linear subspace method into the decision rule.\r\n\r\nThe image classification tasks are on fairly simple datasets, but nonetheless may be good fits for the approach.  One question I have is how dependent this method is on image alignment.  The three datasets all have fairly well aligned images, potentially lending themselves to gains by integrating linear subspace methods (in pixel-space).  Notably, the proposed method performs quite well on 15-Scenes -- I wonder if there may be any additional take-aways concerning alignment of the images in this dataset and how else this might be used in classification of such scene data.\r\n\r\nThe results from the kinect pixel labeling look pretty good, and strengthens the paper with an application different from classifying aligned images, but it would be nice to include a comparison to Denil et al (the dataset providers).\r\n\r\n\r\nPros:\r\n\r\n- Method is well motivated and theoretically analyzed\r\n- Demonstrated to be effective in several tasks\r\n- Very well written\r\n\r\nCons:\r\n\r\n- Applications are somewhat simple\r\n\r\n\r\nMore questions:\r\n\r\n\r\n- Fig. 5:  It would be interesting to put the 'Identity learner' (subspace assignment without the transformation T) in all 3 of these plots.  I'd be particularly interested to know the effect of this step on MNIST, which according to Fig 6 seems reasonably separated in the original space as well.\r\n\r\n- Pixel descriptors for kinect:  I think they are depth differences from the center pixel obtained at 32 equally spaced points along circles at 3 different radii (8, 32, and 64), forming a 96-dim descriptor -- is this correct?  I wasn't entirely sure based on the description.\r\n\r\n- 'All the images were resized to 16 x 16':  It's ambiguous whether this was for just YaleB, or all the datasets?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Transformations for Classification Forests", "decision": "submitted, no decision", "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "pdf": "https://arxiv.org/abs/1312.5604", "paperhash": "qiu|learning_transformations_for_classification_forests", "keywords": [], "conflicts": [], "authors": ["Qiang Qiu", "Guillermo Sapiro"], "authorids": ["qiang.qiu@duke.edu", "guillermo.sapiro@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391482140000, "tcdate": 1391482140000, "number": 2, "id": "jjhlkbWNtA_8m", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8G-o3Hm_Z43Cf", "replyto": "8G-o3Hm_Z43Cf", "signatures": ["anonymous reviewer fd4c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Transformations for Classification Forests", "review": "Summary\r\n\r\nThis paper introduces a new split node learner for use in classification trees. The learner is parameterized by a d x d dimensional matrix T (d is the input dimension) and two subspaces, one for each class in a binary classification problem. At test time, a data point is transformed by T and assigned to the class whose subspace is best able to approximate the transformed point (least L2 residual). For each split node, the transformation T is learned by (locally) minimizing a non-convex objective function that is a difference of nuclear norms of transformed data matrices. Theoretical justification is given for the objective, showing that if the objective is 0 then the transformed subspaces for the two classes is orthogonal.\r\n\r\nEmpirical results are shown on four datasets and indicate that the proposed method obtains good performance.\r\n\r\nNovelty and Quality\r\n\r\nTo my knowledge, the proposed split objective is novel. The paper is very well written and the method is explained in a clear manner. \r\n \r\nPros\r\n\r\n+ The split objective and learner is novel and well motivated.\r\n+ Experimental performance is convincing compared to other split learners.\r\n+ The theoretical justification for the method is compelling and the presentation is clear.\r\n\r\nCons / Questions for author feedback\r\n\r\nDetails of the experimental setup are missing in some cases:\r\n - What d-dimensional feature vectors are computed for each experiment? And what is d in each case? This is explained for the Kinect experiment, but not for the others.\r\n - Sec. 3.1 \u201cwe further evaluate the effect of randomness introduced by randomly dividing classes arriving at each split node into two categories.\u201d I don\u2019t follow how the experiment described here evaluates this source of randomness. This simply reports accuracy on the 15-scenes dataset. I was expected an experiment that controlled for this source of randomness.\r\n - For the Kinect experiment, why did you use only test images (450 for training/50 for testing)? It would have been better to follow the setup in (Denil et al., 2013) so that the results in the paper are comparable to Denil et al.\u2019s results.\r\n\r\nThe paper is missing a discussion of the drawbacks of the proposed approach. For example, decision trees with decision stumps are often applied to problems where the feature space is very large (or even infinite). How could the proposed approach be modified to work in these cases?\r\n\r\nDid you consider applying a difference of convex programming approach to (1) instead of gradient descent?\r\n\r\nSec 2.4: \u201c... only involves matrix multiplication, which is virtually computationally free at the testing time.\u201d This seems like a odd claim. If d is large, then these matrix multiplications are far from free."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Transformations for Classification Forests", "decision": "submitted, no decision", "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "pdf": "https://arxiv.org/abs/1312.5604", "paperhash": "qiu|learning_transformations_for_classification_forests", "keywords": [], "conflicts": [], "authors": ["Qiang Qiu", "Guillermo Sapiro"], "authorids": ["qiang.qiu@duke.edu", "guillermo.sapiro@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390989300000, "tcdate": 1390989300000, "number": 1, "id": "sHezsRlPzgm2S", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8G-o3Hm_Z43Cf", "replyto": "8G-o3Hm_Z43Cf", "signatures": ["anonymous reviewer 461d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Transformations for Classification Forests", "review": "The manuscript presents a novel approach to random forests in which a linear discriminative transformation is learned at each split node in order to enhance class separation of the weak learners. The work offers a merger of existing ideas in an innovative way. The experimental results, which include synthetic and real-world datasets, adequately support the theoretical claims. Also noted is the comparison to other classification tree models, clearly differentiating the proposed scheme from prior state-of-the-art subspace learning methods. A core argument made is that the proposed method can perform better with fewer and more shallow trees than other popular, yet relatively simpler, classification tree methods.\r\n\r\nWeaknesses:\r\n(1) A single classifier type coupled with the linear transformation was described. It would be valuable if other classifier/transformation pairs were explored (e.g. SVNs). Does the framework easily extend to other classification techniques? \r\n(2) All dataset images used were scaled down to 16x16. How does this method perform on truly high-dimensional data? \r\n(3) Is there a reason that not all experiments were performed with each of the datasets?\r\n(4) Timing and accuracy comparison results would be valuable if presented for additional datasets, thereby establishing the scalability properties of the algorithm; timing results aren't too informative when reported for a single dataset. \r\n(5) Experiments showing the effect of noise/mislabeld examples would be interesting as the impact of such imperfections on this scheme is unclear.\r\n(6) Section 3.2, Paragraph 2, Sentence 1: What is the reason for mentioning that classes arriving at a node are randomly split into two categories? Why is it being introduced here as a new source of randomness?\r\n\r\nMinor editorial comments:\r\n - Section 3.1, Paragraph 1, Sentence 1: Could be worded to flow better.\r\n - Section 3.1, Paragraph 2, Sentence 2: '... the paper ...' ->\r\n   '...  this paper ...\r\n - Section 3.1, Paragraph 2, Sentence 4: Missing word (termination 'criteria'?)\r\n - Section 3.2, Paragraph 2, Sentence 2: Missing period.\r\n - Section 3.2, Paragraph 2, Sentence 3: '... the paper ...' ->\r\n   '...  this paper ...\r\n - Section 3.2, Paragraph 2, Sentence 5: '... in details ...' ->\r\n   ' ... in detail ...'\r\n - Section 3.2, Paragraph 2, Sentence 1: Not clear that its actually the\r\n   samples from the dataset that the referenced figure contains."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Transformations for Classification Forests", "decision": "submitted, no decision", "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "pdf": "https://arxiv.org/abs/1312.5604", "paperhash": "qiu|learning_transformations_for_classification_forests", "keywords": [], "conflicts": [], "authors": ["Qiang Qiu", "Guillermo Sapiro"], "authorids": ["qiang.qiu@duke.edu", "guillermo.sapiro@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387520340000, "tcdate": 1387520340000, "number": 13, "id": "8G-o3Hm_Z43Cf", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "8G-o3Hm_Z43Cf", "signatures": ["qiang.qiu@duke.edu"], "readers": ["everyone"], "content": {"title": "Learning Transformations for Classification Forests", "decision": "submitted, no decision", "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "pdf": "https://arxiv.org/abs/1312.5604", "paperhash": "qiu|learning_transformations_for_classification_forests", "keywords": [], "conflicts": [], "authors": ["Qiang Qiu", "Guillermo Sapiro"], "authorids": ["qiang.qiu@duke.edu", "guillermo.sapiro@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 5}