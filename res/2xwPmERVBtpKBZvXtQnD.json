{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457549323488, "tcdate": 1457549323488, "id": "L7VjZ5r7BHRNGwArs4mj", "invitation": "ICLR.cc/2016/workshop/-/paper/123/review/12", "forum": "2xwPmERVBtpKBZvXtQnD", "replyto": "2xwPmERVBtpKBZvXtQnD", "signatures": ["~Sander_Dieleman1"], "readers": ["everyone"], "writers": ["~Sander_Dieleman1"], "content": {"title": "ICLR 2016 paper 123 review 12", "rating": "6: Marginally above acceptance threshold", "review": "The paper introduces a heuristic which aims to revive \"dead\" units in neural networks with the ReLU activation. In such networks, units that are less useful may be abandoned during training because they no longer receive any gradient. This wastes capacity. The proposed heuristic is to detect when this happens and to reinitialize the units in question, so they get another shot at learning something useful.\n\nIt's a simple idea that is definitely worth exploring, but I feel the paper needs some work. My main concern is with the experimental evaluation -- although I appreciate that this is a workshop submission about preliminary work, the choice of dataset and model reduce the relevance of the results. The paper would also benefit from proofreading, there were quite a few spelling and grammar mistakes. The second paragraph of section 2 in particular has a lot of redundancy.\n\n+ simple idea, reasonably clearly explained.\n\n+ results are reported across large ranges of the hyperparameters (Fig. 3).\n\n- the \"two datasets\" used in the experiments actually seem to be two halves of a single dataset, which is a bit misleading. The authors claim this dataset is well-known, but I don't think it is (at least not in the ICLR community). It is also quite small. I think MNIST would have been a much better choice, since it is the accepted \"toy dataset\" for experiments like these nowadays. This would make the results much easier to interpret.\n\n- the network architecture used for this problem is really small (only two convolutional layers with 20 filters each), and since the point of the heuristic is to reduce the required capacity to solve a given task, evaluating it only on such a tiny model reduces the relevance of the results. I think scaling behaviour is especially important here.\n\n\n* I'm not entirely clear on how the validation was handled, as it is only mentioned that the datasets are split into equal parts for train and test (no separate validation set). I think this is not a huge issue here because results are reported across large ranges of values for the hyperparameters, but of course it is important to be careful with this. It would be useful if this could be clarified in the paper.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "RandomOut: Using a convolutional gradient norm to win The Filter Lottery", "abstract": "Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will ``win'' and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.  This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. ", "pdf": "/pdf/2xwPmERVBtpKBZvXtQnD.pdf", "paperhash": "cohen|randomout_using_a_convolutional_gradient_norm_to_win_the_filter_lottery", "conflicts": ["cs.umass.edu", "umb.edu"], "authors": ["Joseph Paul Cohen", "Henry Z. Lo", "Wei Ding"], "authorids": ["joecohen@cs.umb.edu", "henryzlo@cs.umb.edu", "ding@cs.umb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580012364, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580012364, "id": "ICLR.cc/2016/workshop/-/paper/123/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "2xwPmERVBtpKBZvXtQnD", "replyto": "2xwPmERVBtpKBZvXtQnD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/123/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1456640330132, "tcdate": 1456640330132, "id": "yov343335ir682gwszQx", "invitation": "ICLR.cc/2016/workshop/-/paper/123/review/10", "forum": "2xwPmERVBtpKBZvXtQnD", "replyto": "2xwPmERVBtpKBZvXtQnD", "signatures": ["~Anelia_Angelova1"], "readers": ["everyone"], "writers": ["~Anelia_Angelova1"], "content": {"title": "ICLR 2016 paper 123 review 10", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper proposes an approach to re-set convolutional filters that are apparently not being trained well (and randomly reinitialize them)\nIt proposes a criterion that is based on the gradients propagated to these filters. \n\nThe approach is not entirely novel, since it is well known that very low gradients indicate no learning. Regardless, forming the idea to eliminate and reset the filters during the training and evaluating the contributions on the entire performance is the contribution of this work.\n\n+ The paper is well organized and written clearly.\n+ The approach is motivated well. \n+ The overall approach makes sense.\n+ The advantage is that one can potentially train more compact networks (fewer but more useful filters and less redundancy among them) \n\n- The approach itself is a little bit heuristic; also, looking at figure 3, if you cross-validated on the East crater region but tested on the West, there are regions of the parameter space where the gains in one set may be misleading for the other. \n\n\nOverall the proposed method seems to be useful for the purposes intended.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "RandomOut: Using a convolutional gradient norm to win The Filter Lottery", "abstract": "Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will ``win'' and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.  This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. ", "pdf": "/pdf/2xwPmERVBtpKBZvXtQnD.pdf", "paperhash": "cohen|randomout_using_a_convolutional_gradient_norm_to_win_the_filter_lottery", "conflicts": ["cs.umass.edu", "umb.edu"], "authors": ["Joseph Paul Cohen", "Henry Z. Lo", "Wei Ding"], "authorids": ["joecohen@cs.umb.edu", "henryzlo@cs.umb.edu", "ding@cs.umb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580012988, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580012988, "id": "ICLR.cc/2016/workshop/-/paper/123/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "2xwPmERVBtpKBZvXtQnD", "replyto": "2xwPmERVBtpKBZvXtQnD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/123/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455822250199, "tcdate": 1455822250199, "id": "2xwPmERVBtpKBZvXtQnD", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "2xwPmERVBtpKBZvXtQnD", "signatures": ["~Joseph_Paul_Cohen1"], "readers": ["everyone"], "writers": ["~Joseph_Paul_Cohen1"], "content": {"CMT_id": "", "title": "RandomOut: Using a convolutional gradient norm to win The Filter Lottery", "abstract": "Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will ``win'' and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.  This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. ", "pdf": "/pdf/2xwPmERVBtpKBZvXtQnD.pdf", "paperhash": "cohen|randomout_using_a_convolutional_gradient_norm_to_win_the_filter_lottery", "conflicts": ["cs.umass.edu", "umb.edu"], "authors": ["Joseph Paul Cohen", "Henry Z. Lo", "Wei Ding"], "authorids": ["joecohen@cs.umb.edu", "henryzlo@cs.umb.edu", "ding@cs.umb.edu"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}