{"notes": [{"id": "HJxNAnVtDS", "original": "r1x_sfcNPB", "number": 261, "cdate": 1569438924237, "ddate": null, "tcdate": 1569438924237, "tmdate": 1583912029734, "tddate": null, "forum": "HJxNAnVtDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "0Jl3t5ayC5", "original": null, "number": 1, "cdate": 1576798691772, "ddate": null, "tcdate": 1576798691772, "tmdate": 1576800943539, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This manuscript analyzes the convergence of federated learning wit hstragellers, and provides convergence rates. The proof techniques involve bounding the effects of the non-identical distribution due to stragglers and related issues. The manuscript also includes a thorough empirical evaluation. Overall, the reviewers were quite positive about the manuscript, with a few details that should be improved. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729972, "tmdate": 1576800282671, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper261/-/Decision"}}}, {"id": "Bkxft2PptH", "original": null, "number": 2, "cdate": 1571810425908, "ddate": null, "tcdate": 1571810425908, "tmdate": 1574449771582, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper analyzes the convergence of FedAvg, the most popular algorithm for federated learning. The highlight of the paper is removing the following two assumptions: (i) the data are iid across devices, and (ii) all the devices are active. For smooth and strongly convex problems, the paper proves an O(1/T) convergence rate to global optimum for learning rate decaying like 1/t with time. It is also shown that with constant learning rate eta, the solution found can be necessarily Omega(eta) away from the optimum (for a specific problem instance), thus justifying the decaying learning rate used in the positive result.\n\nFederated learning has been an important and popular research area since it models a highly distributed and heterogeneous learning system in real world. Previous theoretical analysis of FedAvg was quite scarce and either made the iid data assumption or required averaging all the devices. This work is the first to prove a convergence guarantee without these two assumptions. In particular, it only requires averaging a (random) subset of devices each round, which is much more realistic than averaging all.\n\nI don't quite have an intuition for why you need strong convexity. I hope the authors could explain this in words and maybe comment on what are the challenges of removing this assumption.\n\n\n------\nThanks to the authors for their response.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139351808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper261/Reviewers"], "noninvitees": [], "tcdate": 1570237754687, "tmdate": 1576139351824, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Review"}}}, {"id": "SkxS2tZ-sH", "original": null, "number": 1, "cdate": 1573095852783, "ddate": null, "tcdate": 1573095852783, "tmdate": 1573762573075, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "rkl4PWCsKB", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment", "content": {"title": "Thank you for the supportive review", "comment": "We greatly appreciate the reviewer's effort. Thanks for your positive reviews."}, "signatures": ["ICLR.cc/2020/Conference/Paper261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiang_Vincent_Li1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxNAnVtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper261/Authors|ICLR.cc/2020/Conference/Paper261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174015, "tmdate": 1576860543453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment"}}}, {"id": "Hyx97qWbsS", "original": null, "number": 2, "cdate": 1573095970063, "ddate": null, "tcdate": 1573095970063, "tmdate": 1573762552435, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "Bkxft2PptH", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment", "content": {"title": "Thank you for your supportive review", "comment": "We greatly appreciate the reviewer's effort. Here are our responses to your comments.\n\nThere are several benefits when we assume strong convexity.\nFirst, it facilitates our analysis.\nWe have more ways to prove the convergence since there is no difference when we prove convergence for $\\|w_t - w^*\\|$ or $f(w_t) - f(w^*)$.\nSecond, strong convexity and smoothness mean fast convergence rate $O(1/T)$. \nSince FedAvg is a distributed variant of SGD, it couldn't achieve faster convergence than SGD.\nWe can remove the strong convexity assumption but the convergence rate is $O(1/\\sqrt{T})$ (see Khaled et al. (2019)).\nThird, in Theorems 1 and 2, we require the learning rate $\\eta_t = 2/(\\gamma + t) 1/\\mu$, which makes the use of a positive $\\mu$."}, "signatures": ["ICLR.cc/2020/Conference/Paper261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiang_Vincent_Li1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxNAnVtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper261/Authors|ICLR.cc/2020/Conference/Paper261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174015, "tmdate": 1576860543453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment"}}}, {"id": "B1gL_cb-jS", "original": null, "number": 3, "cdate": 1573096046321, "ddate": null, "tcdate": 1573096046321, "tmdate": 1573762505547, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "SyeCOwkk9r", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment", "content": {"title": "Thank you for your valuable review", "comment": "We greatly appreciate the reviewer's effort. Here are our responses to your comments.\n\n1. Yes, transformed Scheme II is the scaling trick described at the end of Section 3.3. We will clarify its definition.\n\n2. Since FedAvg is a distributed variant of SGD, it couldn't achieve faster convergence than SGD.\nFor strongly convex and smooth optimization problems, the best convergence of SGD is $\\Theta(1/t)$.\nIn our case, if $\\eta_t$ that is decaying slower than $\\Theta(1/t)$, you might not obtain the convergence rate $O(1/t)$.\nBased on Eq. (11), when $\\eta_t = 1/(\\mu\\sqrt{t})$, we will obtain the convergence rate $O(1/\\sqrt{t})$ by a similar argument.\n\n3. We will correct these typos in its revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiang_Vincent_Li1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxNAnVtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper261/Authors|ICLR.cc/2020/Conference/Paper261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174015, "tmdate": 1576860543453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper261/Authors", "ICLR.cc/2020/Conference/Paper261/Reviewers", "ICLR.cc/2020/Conference/Paper261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Comment"}}}, {"id": "rkl4PWCsKB", "original": null, "number": 1, "cdate": 1571705179776, "ddate": null, "tcdate": 1571705179776, "tmdate": 1572972618123, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents convergence rates for straggler-aware averaged SGD for non-identically but independent distributed data. The paper is well-written and motivated with good discussions of the algorithm and the related works. The proof techniques involve bounding how much worse can the algorithm do because of non-identical distribution and introduction of stragglers into the standard analysis of SGD-like algorithms. The presented theory is useful, and also provides new insights such as a new sampling scheme and an inherent bias for the case of non-decaying step size. The empirical evaluation is adequate and well-presented. I think this paper is a strong contribution and should spark further discussions in the community. "}, "signatures": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139351808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper261/Reviewers"], "noninvitees": [], "tcdate": 1570237754687, "tmdate": 1576139351824, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Review"}}}, {"id": "SyeCOwkk9r", "original": null, "number": 3, "cdate": 1571907445783, "ddate": null, "tcdate": 1571907445783, "tmdate": 1572972618044, "tddate": null, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "invitation": "ICLR.cc/2020/Conference/Paper261/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Federated learning is distinguished from the standard distributed learning in the following sense: \n1) training is distributed over a huge number (say N) of devices and communication between the central server and devices are slow.\n2) The central server has no control of individual devices, and there are inactive devices that does not respond to the server; full participation of all devices is unrealistic.\n3) The local data distribution at each device is different from each other; i.e., the data is non-iid.\n\nDue to property 1), communication-efficient algorithms such as Federated Averaging (FedAvg) have been proposed and studied. FedAvg runs SGD in parallel on K (\u2264N) local devices using their local datasets, and updates the global parameter after E local iterations by aggregating the updates from the local devices.\n\nProperties 2) and 3) makes analysis of FedAvg difficult, and previous results have proven convergence of FedAvg assuming that the data is iid and/or all devices are active. In contrast, this paper studies FedAvg on the non-iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T). \n\nOverall, I enjoyed reading this paper and I would like to recommend acceptance. This is the first result showing convergence rate analysis of FedAvg under presence of properties 2) and 3), which is a nontrivial, important, and timely problem. The paper is well-written and reads smoothly, except for some minor typos. The convergence bounds provide insights of practical relevance, e.g., the optimal choice of E, the effect of K in convergence rate, etc. The authors also provide empirical results supporting their theoretical analysis.\n\nSome questions I have in mind:\n- What is \"transformed Scheme II\"? Is it the scaling trick described at the end of Section 3.3? The name appears in the experiment section before being defined.\n- What happens if we choose \\eta_t that is decaying but slower than O(1/t), say O(1/\\sqrt t)? Can convergence be proved? If so, in what rate?\n\nMinor typos:\n- Footnote 3: know -> known\n- Assumptions 1 & 2: f in $f(w)$ is math-bold\n- Choice of sampling schemes: \"If the system can choose to active...\" -> activate\n- mnist balanced and mnist unbalanced: the description after them suggests they should be switched\n- Apdx D.1: widely -> wide, summary -> summarize"}, "signatures": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper261/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of FedAvg on Non-IID Data", "authors": ["Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang"], "authorids": ["smslixiang@pku.edu.cn", "hackyhuang@pku.edu.cn", "yangwhsms@gmail.com", "shusen.wang@stevens.edu", "zhzhang@math.pku.edu.cn"], "keywords": ["Federated Learning", "stochastic optimization", "Federated Averaging"], "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal.", "pdf": "/pdf/dcd68da80bc99678b1254ec0b4d49dee872c7898.pdf", "paperhash": "li|on_the_convergence_of_fedavg_on_noniid_data", "code": "https://github.com/lx10077/fedavgpy", "_bibtex": "@inproceedings{\nLi2020On,\ntitle={On the Convergence of FedAvg on Non-IID Data},\nauthor={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxNAnVtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7ebf4085003e4d555a0668dcc5ad1661e7ef13ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxNAnVtDS", "replyto": "HJxNAnVtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139351808, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper261/Reviewers"], "noninvitees": [], "tcdate": 1570237754687, "tmdate": 1576139351824, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper261/-/Official_Review"}}}], "count": 8}