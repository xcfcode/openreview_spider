{"notes": [{"id": "rkeZNREFDr", "original": "rkeJlfL_Pr", "number": 1064, "cdate": 1569439272774, "ddate": null, "tcdate": 1569439272774, "tmdate": 1577168265757, "tddate": null, "forum": "rkeZNREFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2FKyNMplY", "original": null, "number": 1, "cdate": 1576798713620, "ddate": null, "tcdate": 1576798713620, "tmdate": 1576800922851, "tddate": null, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1064/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to learn self-explaining neural networks using a feature leveling idea.  Unfortunately, the reviewers have raised several concerns on the paper, including insufficiency of novelty, weakness on experiments, etc. The authors did not provide rebuttal. We hope the authors can improve the paper in future submission based on the comments.  \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727802, "tmdate": 1576800280106, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1064/-/Decision"}}}, {"id": "S1xlfwiujS", "original": null, "number": 4, "cdate": 1573594888168, "ddate": null, "tcdate": 1573594888168, "tmdate": 1573594888168, "tddate": null, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #7", "review": "This paper proposes a categorization of inner layer weights to be linearly or non-linearly correlated with the output. The motivation on why this is important is somewhat weak in the paper. But I could see cases where this is important, if there is supporting evidence that it helps with interpretability. However, I did not see that in this draft unfortunately. \n\n\u201cGLMs \u2026 interactions of non-linear activations are not involved\u201d \u2013 the link function in all GLMs except linear regression is non-linear. So I am not sure what this statement means. \n\nThe definitions of \\mathbb{L_k} and \\mathbb{H_k} are not clear when they are introduced. The example given talks about l_1 and h_1, not about \\mathbb{L_k} and \\mathbb{H_k}.\n\nI didn\u2019t get the point of the function B(.). Why not directly use z \\in {0,1} ?  Similarly, I did not get the point of introducing g(.), why not just push the entire mapping into \\phi ? The notation is made unnecessarily complicated. \n\nThe sentences after eq 7 are unclear to me. What does \\epsilon \\sim p(\\epsilon) even mean ? Is \\epsilon the parameter or the random variable here ? What is \\epsilon used for ? What is m(.) ?\n\nThe toy example is simple. But it does not address why such a classification into h_k and l_k is helpful and how it can be used. Can the authors motivate the usecases where such an interpretation (using the toy example) is useful ?\n\nThe experiments are not convincing. The MNIST experiment regarding identification of contours is very vague. There are tons of other methods that give better interpretation. The identification of \u201clong\u201d feature to be linearly correlated with the output can be done in a much faster and easier way, by simply checking individual feature correlations. The strength of such a method that the authors are proposing would be to extract useful information for cases where the input features are /not/ linearly correlated, while there are some inner layer features which /are/ linearly correlated. This could help in understanding the landscape of the classification better. But I did not see that happening here. Happy to be proved wrong by the authors or other reviewers if I am missing something here. Similarly, there is lot of work on sparsification/pruning of NNs, in light of which I am not sure what Section 6 adds.  \n\nAppendix A.4 is redundant, softmax/sigmoid being a glm is well-known. Also, in the main text (e.g. last para page 1), the reference is to Appendix A.3, while it should be to A.4. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer7"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer7"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682917655, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1064/Reviewers"], "noninvitees": [], "tcdate": 1570237742887, "tmdate": 1575682917672, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review"}}}, {"id": "SkgEmy9zsS", "original": null, "number": 3, "cdate": 1573195548299, "ddate": null, "tcdate": 1573195548299, "tmdate": 1573195548299, "tddate": null, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #6", "review": "Summary: \n\nThis paper proposes a novel architecture that is able to separate different types of features learned at each layer of a neural network through a gating structure -- features that are sufficiently passed through the network are immediately sent to the final output layer. In addition, they provide reasonable definitions of levels of features, in contrast to the standard \"low\" to \"high\" descriptions. Lastly, in order to make the model more interpretable, they utilize an L0 loss on the gates of each layer to prioritize lower level features being used in the final layer.\n\nSignificance:\n\nAlthough gating is not novel, their use to send kth-level features to the final GLM layer is. Other than that, not much is contributed, as their differentiability trick, as mentioned, has already been done. The motivation to separate different types of features is interesting and definitely an issue that should be studied more. \n\nQuality:\n\nThe paper is easy to follow and nicely written, but with a few minor typo issues:\n\n1. Page 1, refers to appendix A.3 but should be for A.4\n2. Page 2, \"section\" is inconsistently capitalized \n3. Page 6, mentions three commonly used datasets but only mentions MNIST and California Housing. \n4. Page 8, mentions Appendix A.11 for CNN but this section is empty.\n\nIn regards to content quality, a few things stand out that could be improved:\n\n1. A major issue is that the interpretability of features with k > 1 are still not explained -- all we know is that they don't need to be sent further through the network. (i.e. solves the separation issue but leaves gaps in interpretability)\n2. Since the gates themselves can be studied, rather than finding gradients, wouldn't a simpler way to explain the network be to look at which features are passed to the GLM layer? This would especially be helpful in the first layer when looking at the original input features. \n3. Currently it is not clear if the architecture learns when features (l_k) are directly \"useful\" for classification, or if they are just not compatible with the features passed on to the next layer (h_k). \n4. In terms of interpretability, only a few other methods are tested, and gradients are the only way they compare. An exploration of other attribution methods could have further supplemented their claims.\n5. Claims are made about how many layers a certain dataset needs for sufficient classification through heuristic experiments; however they are not thorough enough in terms of ablation to fully make this claim. Width of layers are chosen but not analyzed; how is gating affected by the width of the network? For example, in MNIST, would only 3 layers be needed if the width is increased or decreased? This isn't immediately clear.\n6. Extensiveness of experiments -- I do like the toy dataset as an example, but to show effectiveness of this framework, a larger breadth of datasets could have been used. As an example, in the SENN paper, they utilize breast cancer and COMPAS but these were not tested on this architecture.  In addition, the results from convolutional layers would be much more preferred, since the best performing architectures on large vision datasets such as ImageNet primarily use convolutions.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer6"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer6"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682917655, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1064/Reviewers"], "noninvitees": [], "tcdate": 1570237742887, "tmdate": 1575682917672, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review"}}}, {"id": "rJgZ18JcFH", "original": null, "number": 1, "cdate": 1571579353356, "ddate": null, "tcdate": 1571579353356, "tmdate": 1572972517264, "tddate": null, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a feature leveling technique to improve the self-explaining of deep fully connected neural networks. The authors propose to learn a gated function for each feature dimension for whether to directly send the feature to the final linear layer. The gated function is trained with L0 regularization technique proposed in (Louizos et al., 2017) to encourage more low-level features passed to the final layer. Experimental results on MNIST, California Housing, CIFAR10 show that the proposed method can achieve comparable performance with existing algorithms on sparse neural network training.\n\nQuality:\n\nOverall, the paper is well written with some minor formatting errors. The toy example demonstrates the idea of this paper clearly. However, the novelty of this paper, when compared to NIT, is that the L0 regularization is used to pass the feature to last layer. Considering the self-explaining feature, this work can only explain that some of the input features are suited for final layer, while there is no explanation on the other features since they are used to construct higher level features.\n\nClaririty:\n\nSome parts of this paper are not clear:\n1.\tWhy l_k and h_k has to be disjoint? A feature suited for final classification does not suggest that it can\u2019t be used to construct higher-level feature.\n2.\tIn (4), should not B() be an inverse binary activation function: (1-z)?\n3.\tIs g(.) the Bernoulli distribution?\n4.\tIn section 5.2, why compare the gradients of a specific input example while one can directly look at z_k, the gated function?\n5.\tI assume the fully connected layers have bias term. If so, (4) suggests that the gated location will also be added with a learned bias, which is different than what the paper proposes.\n\nNovelty:\n\nThe novelty of this paper lies in the sparse training objective becomes passing as many lower-level features to final layer as possible instead of zeros out the intermediate weights. However, the key technique, L0 regularization, has been proposed and used as stated in the related work. While the authors state the application L0 to a novel context to select features is different from prior work, the novelty is rather incremental.  \n\nSignificance:\n\nThis work demonstrates that the L0 regularization technique for sparse neural network training can also be applied to learn a skip-layer connection. However, from both novelty, performance, and self-explaining perspectives, this work does not introduce much to the field.\n\n\nPros:\n\n1.\tThe paper is well written.\n2.\tThe toy example showcases the issue that this work tries to tackle.\n3.\tThe experimental results show the comparable performance to existing works.\n\nCons:\n1.\tThe novelty is not sufficient considering the prior works on sparse neural network training.\n2.\tThere are some clarification issues as mentioned before.\n3.\tThe performance is only comparable to existing works.\n4.\tThe self-explaining contribution is not clear since only a few input features can be explained if they are passed to the final layers.\n5.\tThere is no experiment on how \\lambda would affect the resulting network architecture.\n\n\nMinor corrections:\n1.\tFirst paragraph on sec. 5: three datasets: two datasets (or mention it\u2019s in appendix).\n2.\t5.2 compare to NIT: the citations are in wrong format. Also the reference for NIT is corrupted.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682917655, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1064/Reviewers"], "noninvitees": [], "tcdate": 1570237742887, "tmdate": 1575682917672, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review"}}}, {"id": "Skl_RbuoKH", "original": null, "number": 2, "cdate": 1571680720473, "ddate": null, "tcdate": 1571680720473, "tmdate": 1572972517229, "tddate": null, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a neural network architecture to separate low-level features and high-level features. At the k-th hidden layer, 1) the k-th level features, defined as the set of features that requires k-1 hidden layers to extract, are directly passed to the final GLM layers; 2) the remaining features are further processed in the subsequent layers. This separation is achieved by applying the gating mechanism. The model can be interpreted by the weights associated with each of those k-th level features in the final GLM layer. Experimental results on the MNIST classification dataset and the California Housing regression dataset demonstrate the proposed approach can 1) achieve competitive performance (in terms of classification and regression) compared to the FCNN baseline; and 2) has better interpretability performance.\n\nThis paper is clearly written. The description of the model architecture is easy to follow. The introduction of the related works and background material are well organized.\n\nMy major concern with this work is how to interpret those k-th level features. Since the weights of those k-th level features in the final GLM model are used to indicate the importance of those features, interpreting the meaning of those k-th level features seem necessary. In practice, how to interpret the meaning of those higher-level features? For example, for the California Housing dataset, how to interpret the meaning of those features learned at level 2?\n\nThe interpretation of the MNIST classification examples seems difficult to understand. Compared to inspecting the raw pixels, would it be easier to interpret through learning a few prototypes, similar to the approach described in Alvarez-Melis and Jaakkola NIPS 2018?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1064/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation", "authors": ["Yingjing Lu", "Runde Yang"], "authorids": ["yingjinl@andrew.cmu.edu", "ry82@cornell.edu"], "keywords": [], "abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.", "pdf": "/pdf/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "code": "https://drive.google.com/file/d/1yj4XdmQeyYyJuYn24lUVg2SlpZcDYTzi/view?usp=sharing", "paperhash": "lu|not_all_features_are_equal_feature_leveling_deep_neural_networks_for_better_interpretation", "original_pdf": "/attachment/e0dfbf1d56f8642c17a413de11c01e9da66f51db.pdf", "_bibtex": "@misc{\nlu2020not,\ntitle={Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation},\nauthor={Yingjing Lu and Runde Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeZNREFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeZNREFDr", "replyto": "rkeZNREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682917655, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1064/Reviewers"], "noninvitees": [], "tcdate": 1570237742887, "tmdate": 1575682917672, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1064/-/Official_Review"}}}], "count": 6}