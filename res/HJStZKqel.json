{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396602488, "tcdate": 1486396602488, "number": 1, "id": "BkGT3fIdg", "invitation": "ICLR.cc/2017/conference/-/paper470/acceptance", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers were quite divided on this submission, which proposes a method for lifelong learning in the context of program generation. While a novel idea, the experiments and baselines are simply not clear enough or convincing enough, and the method itself is not clearly conveyed. The paper makes strong claims that are not substantiated with more compelling or challenging experiments. Since this is an interesting, novel idea in a relevant area, however, I think it should be invited as a workshop paper.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396603070, "id": "ICLR.cc/2017/conference/-/paper470/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJStZKqel", "replyto": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396603070}}}, {"tddate": null, "tmdate": 1484949531679, "tcdate": 1481886351120, "number": 1, "id": "Hkw5qHb4x", "invitation": "ICLR.cc/2017/conference/-/paper470/official/review", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "Final Review: Fine idea but very basic tasks, weak baselines, and misleading presentation", "rating": "2: Strong rejection", "review": "The authors explore the idea of life-long learning in the context of program generation.\n\nThe main weakness of this paper is that it mixes a few issues without showing strong results on any of them. The test tasks are about program generation, but these are toy tasks even by the low standards of deep-learning for program generation (except for the MATH task, they are limited to 2x2 grid). Even on MATH, the authors train and discuss generalization from 2-digit expressions -- these are very short, so the conclusiveness of the experiment is unclear. The main point of the paper is supposed to be transfer learning though. Unluckily, the authors do not compare to other transfer learning models (e.g., \"Progressive Neural Networks\") nor do they test on tasks that were previously used by others. We find that only testing on a newly-created task with a weak baseline is not sufficient for ICLR acceptance.\n\nAfter clarifying comments from the authors and more experiments (see the discussion above), I'm now convinced that the authors mostly measure overfitting, which in their model is prevented because the model is hand-fitted to the task. While the idea might still be valid and interesting, many harder and much more diverse experiments are needed to verify it. I consider this paper a clear rejection at present.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575410, "id": "ICLR.cc/2017/conference/-/paper470/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper470/AnonReviewer3", "ICLR.cc/2017/conference/paper470/AnonReviewer1", "ICLR.cc/2017/conference/paper470/AnonReviewer5"], "reply": {"forum": "HJStZKqel", "replyto": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575410}}}, {"tddate": null, "tmdate": 1484949237882, "tcdate": 1484949237882, "number": 7, "id": "BJAewZewe", "invitation": "ICLR.cc/2017/conference/-/paper470/official/comment", "forum": "HJStZKqel", "replyto": "S1ceCcJwg", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "Possible misunderstanding", "comment": "It might be that you misunderstood my comments. I find your points very convincing in the discussion about \"how much the number of parameters contributes to overfitting\" and \"LSTM vs Neural GPU\", I agree with a lot of what you say. But this is about comparing your 32-parameter model with dozens-of-millions parameter baselines. It's simply not a fair comparisons when you train those baselines at length 5. I read the other papers more carefully now, for example the Neural GPU was trained on length 40 and evaluated on lengths 400 or so in the original paper -- did you try that?\n\nBut my point is bigger than a specific training procedure for a specific baseline. Deep learning works great because it is at the intersection of learnable and powerful models. Your model with 32 parameters does not look powerful. You only have 1 non-trivial task (MATH) and your baseline experiments are not convincing. If you want to convince the readers that a model with 32 parameters can really be powerful, you'll have to include a number of tasks, not just MATH, which your model can be hand-tuned for. How about including all the algorithmic tasks from other papers that considered them? And training baselines on a lot of long data for a fair comparison.\n\nAt present, I simply don't believe that a model with 32 parameters can be a powerful deep learning model, and your experiments are not sufficient to warrant that belief. Moreover, I find some of your claims (such as claims that your model outperforms multi-million parameter baselines) misleading. While true on a single task that you designed, I don't believe they will hold up otherwise. The misleading presentation is a strong argument for me that the paper should be rejected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563204, "id": "ICLR.cc/2017/conference/-/paper470/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563204}}}, {"tddate": null, "tmdate": 1484932819211, "tcdate": 1481981335439, "number": 3, "id": "BJJjahzEe", "invitation": "ICLR.cc/2017/conference/-/paper470/official/review", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer5"], "content": {"title": "Review and review update", "rating": "8: Top 50% of accepted papers, clear accept", "review": "\nI think the paper is a bit more solid now and I still stand by my positive review. I do however agree with other reviewers that the tasks are very simple. While NPI is trained with stronger supervision, it is able to learn quicksort perfectly as shown by Dawn Song and colleagues in this conference. Reed et al had already demonstrated it for bubblesort. If the programs are much shorter, it becomes easy to marginalise over latent variables (pointers) and solve the task end to end. The failure to attack much longer combinatorial problems is my main complaint about this paper, because it makes one feel that it is over-claiming.\n\nIn relation to the comments concerning NPI,  Reed et al freeze the weights of the core LSTM to then show that an LSTM with fixed weights can continue learning new programs that re-use the existing programs (ie the trained model can create new programs). \n\nHowever, despite this criticism, I still think this is an excellent paper, illustrating the power of combining traditional programming with neural networks. It is very promising and I would love to see it appear at ICLR.\n\n===========\nThis paper makes a valuable contribution to the emerging research area of learning programs from data.\n\nThe authors mix their TerpreT framework, which enables them to compile programs with finite integer variables to a (differentiable) TensorFlow graph,  and neural networks for perceiving simple images. This is made possible through the use of simple tapes and arithmetic tasks.  In these arithmetic tasks, two networks are re-used, one for digits and one for arithmetic operations.  This clean setup enables the authors to demonstrate not only the avoidance of catastrophic interference, but in fact some reverse transfer. \n\nOverall, this is a very elegant and potentially very useful way to combine symbolic programming with neural networks. As a full-fledged tool, it could become very useful. Thus far it has only been demonstrated on very simple examples. It would be nice for instance to see it demonstrated in all the tasks introduced in other approaches to neural programming and induction: sorting, image manipulation, semantic parsing, question answering. Hopefully, the authors will release neural TerpreT to further advance research in this domain.\n\n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575410, "id": "ICLR.cc/2017/conference/-/paper470/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper470/AnonReviewer3", "ICLR.cc/2017/conference/paper470/AnonReviewer1", "ICLR.cc/2017/conference/paper470/AnonReviewer5"], "reply": {"forum": "HJStZKqel", "replyto": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575410}}}, {"tddate": null, "tmdate": 1484923723928, "tcdate": 1484923723928, "number": 10, "id": "ByEUXjyPl", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "HysiJjZEe", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Response", "comment": "\nThanks for the thoughtful review.\n\nThe preliminary tasks are less about individual task complexity, and more about the ability to learn modular knowledge that can be transferred to more complex tasks. The MATH task is on a level of complexity comparable to a failure case of an existing system in the literature that aims to learn algorithms [1]. \n\nWe have now added Progressive Neural Network and Neural GPU baselines which are strong baselines for transfer learning and learning algorithms respectively. Our approach outperforms these baselines and the qualitative conclusions of the paper are unchanged.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1484922836337, "tcdate": 1484922836337, "number": 9, "id": "Bk30Jj1De", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "Hkw5qHb4x", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Response", "comment": "\nIn addition to responding in the general response comment above, we have now addressed many of the main issues raised by this review:\n- Added a Neural GPU baseline for the MATH task.\n- Modified baselines for MATH to train on up to 5-digit expressions, showing the conclusions are unchanged.\n- Add a comparison to Progressive Neural Networks (PNNs), showing that our method substantially outperforms PNNs.\n- Added a response (see the thread above under \"General Response\") to the new claim that we are \"mostly measuring overfitting.\" Our comment argues why we do not believe this to be the case."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1484922354146, "tcdate": 1484922354146, "number": 8, "id": "S1ceCcJwg", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "Sy5VLKT8x", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Clarifications", "comment": "\nThere are some errors in this comment:\n\n1. \"a powerful model with 40M parameters will overfit 5-digit training much easier than a less-powerful LSTM with half that many, and, clearly, a 32-parameter model will not overfit.\"\n\nIt is not generally valid to compare model capacity just by counting parameters, especially when dealing with very different model classes. It is also not valid in the specific case being discussed here.  In [1], the Neural GPU model has 200k parameters while the baseline LSTM has 30k parameters, but the LSTM still \"overfits\" (in the strong generalization sense) on the Long Binary Addition task while the Neural GPU does not. Further, [2] states \"It is not self-evident that larger [Neural GPU] model would generalize better, but the empirical results are clear.\"\n\nTo confirm this in our specific setting, we ran additional experiments reducing the number of parameters for the Neural GPU, and the trend is that generalization performance gets worse as the number of parameters decreases (see the generalization performance on 8-digit expressions below). Thus, the issue with the Neural GPU is not overfitting, and explaining generalization performance in terms of number of parameters is overly simplistic and does not match experimental results.\n\n# filters | 32 | 64 | 128 | 256 | 512\n# parameters | 169k | 671k | 2.7M | 10.7M | 43.8M\n8-digit accuracy | 22.5% | 40.1% | 62.7% | 63.3% | 68.6%\n\n\n\n2. \"it was clearly established that Neural GPU generalizes much better than a LSTM baseline on arithmetic tasks, while you show the opposite\"\n\nWe are aware of only two experiments where Neural GPU was compared to LSTM. These are the ones in [1] showing that Neural GPU outperforms LSTM+attention on long addition and long multiplication when:\n(a) data are represented in binary,\n(b) expressions are always of length 2, and\n(c) a separate model is trained for each operator (addition vs multiplication).\n\nFurther, it does not appear that the experimental protocol used in [1] is applicable to the case where there are only short data sequences available for training. Specifically, the results rely upon access to the longer test data in order to select which setting from the grid should be used, and the text reports that generalization performance is sensitive to the choice of seed.\n\nIn contrast, in our experiment:\n(a) data are represented in decimal\n(b) expressions are of variable length,\n(c) a single model is trained on expressions that have a mix of operators, and\n(d) we do not assume access to longer validation data.\n\nOur setting is more challenging and defines a stricter definition of strong generalization than [1] (namely, generalizing beyond the length of data used for training *or validation*). In our setting, there is no evidence that Neural GPU generalizes better than an LSTM baseline. \n\nRegarding (a), the literature [1, 2] reports that Neural GPU performance degrades when numbers are represented in decimal, but there are no experiments showing how the LSTM baseline is affected.\n\nRegarding (b) & (c), [2] experiments with evaluating variable-length expressions with mixed mathematical operators (albeit on binary numbers). They report that a Neural GPU is not able to generalize well to longer sequences, which is consistent with our experiments. There is no LSTM baseline in [2].\n\nIn summary, there is no contradiction between our experiments and existing work, and our experiments are not \"mostly measuring overfitting\".\n\nOur paper is about supplying a very different type of inductive bias than models like an LSTM or Neural GPU. There are different assumptions being made in NTPT (the key one being that there is a component of the model that is well described using source code). We don't claim that this is the right assumption for all tasks (e.g., image classification), but there is a large space of tasks (e.g., evaluating a mathematical expression, navigation, complex policies in RL) where this assumption is valid, and in these cases we believe NTPT-like models will generalize more strongly from less data than models like LSTM or Neural GPU.\n\n\n[1] \u0141ukasz Kaiser & Ilya Sutskever; Neural GPUs Learn Algorithms (ICLR 2016)\n[2] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1484785202307, "tcdate": 1484785202307, "number": 5, "id": "Sy5VLKT8x", "invitation": "ICLR.cc/2017/conference/-/paper470/official/comment", "forum": "HJStZKqel", "replyto": "SyqFvQaLx", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "Deep learning 32 values.", "comment": "Thank you for your explanation. It took me some time to ponder how your experiments can be true. For example, it was clearly established that Neural GPU generalizes much better than a LSTM baseline on arithmetic tasks, while you show the opposite. The number of parameters solved the riddle: a powerful model with 40000000 parameters will overfit 5-digit training much easier than a less-powerful LSTM with half that many, and, clearly, a 32-parameter model will not overfit. Thus, it seems like your experiments mostly measure overfitting, and it is a strong concern that NTPT only performs well because it is small, and the problems are hand-picked to make it work. While this might be false, much more extensive experiments are needed, including NTPTs with much more parameters and many more diverse tasks. If the results hold in the extensive experiments, it might become a great technique. But in the present version, the results are simply not sufficent."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563204, "id": "ICLR.cc/2017/conference/-/paper470/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563204}}}, {"tddate": null, "tmdate": 1484781981240, "tcdate": 1483876405626, "number": 3, "id": "S1RNuo1Lg", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Revision in response to reviewers", "comment": "\nWe thank the reviewers again for their suggestions. We have now\u00a0rewritten the experiment section, and believe that it is considerably clearer and more complete thanks to their suggestions. The main\u00a0changes that address\u00a0the reviewers' concerns are:\n\n(1) We have\u00a0improved the description of the baseline models and included a Progressive Neural Network baseline in the 2x2 tasks as requested. These updates can be seen in section 4, figure 6c and figure 7.\n\n(2) We additionally study a Neural GPU baseline for the MATH task as an example of a more modern neural architecture for learning algorithms. In addition, we have pointed out that the MATH task is similar in complexity to tasks in contemporary literature [1]. These updates can be seen in section 5.2 and figure 8a.\n\n(3) We have extended the training of the LSTM\u00a0baseline in the MATH\u00a0task beyond two digit expressions and ensured that we have trained all models to convergence. We\u00a0also now test on\u00a0longer expressions up to\u00a016 digits. These updates can be seen in section 5.2 and figure 8b.\n\nThe qualitative conclusions of the paper are unchanged: \n\n(1) We demonstrate simultaneous learning of source code and neural networks to solve algorithmic tasks with perceptual inputs.\n\n(2) In a lifetime of learning, NTPT outperforms all baselines, and is the only model to demonstrate reverse transfer.\n\n(3) The source code representation of the algorithm for the MATH task leads to better generalization performance than baseline models on long expressions.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1484760961963, "tcdate": 1484760961963, "number": 7, "id": "SyqFvQaLx", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "B1rWG728e", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Parameter count", "comment": "\nYes, the NTPT model for MATH contains 32 independent floats that define distributions over 19 discrete values. These discrete values define a combinatorially large space of around 10^6 different programs in the MATH model (and in other cases the space is larger, such as fig 3b having a program space of over 10^12 syntactically distinct programs). In addition to these parameters, there are 0.5M parameters in the \"perceptual\" neural networks that define the mappings from images to distributions over discrete values in the programs.\n\nWhile it may seem that 32 values is small, there are a few things to consider:\n- Each joint configuration of the discrete variables gives rise to a different program; a small number of variables can parameterize a very large space of functions\n- NTPT separates the parameterization of the \"algorithmic\" and \"perceptual\" components of the model. There are many tasks where the algorithmic component is relatively simple given a good mapping from perceptual to discrete representations (e.g., navigation). If we can compactly parameterize the algorithmic component and do a good job of jointly learning the perceptual and algorithmic components (as NTPT allows), then we expect stronger generalization, which is what we see in experiments.\n- Representationally it is easy to specify a Turing machine or assembly code-like model for the algorithmic component of a NTPT model (see [1]), so there is no ceiling on the representational capacity of the algorithmic component. The challenge currently holding us back from scaling up further is not the representational capacity of the model; it's in the optimization.\n\n[1] Alexander L. Gaunt et al. Terpret: A probabilistic programming language for program induction. (http://arxiv.org/abs/1608.04428)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1484694012553, "tcdate": 1484694012553, "number": 4, "id": "B1rWG728e", "invitation": "ICLR.cc/2017/conference/-/paper470/official/comment", "forum": "HJStZKqel", "replyto": "r1QtCYt8x", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "Thanks for the interesting results", "comment": "I'm still trying to understand the above results. One thing that stands out to me is the number of parameters:\n\nNTPT: 32\n\nIs this correct? Only 32 floats, that's the whole model? It's hard to believe it can train on harder tasks. Clearly I missed something, I'll re-think it. Could you confirm that the model has 32 parameters? Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563204, "id": "ICLR.cc/2017/conference/-/paper470/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563204}}}, {"tddate": null, "tmdate": 1484525179109, "tcdate": 1484525179109, "number": 6, "id": "r1QtCYt8x", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "SJlPfk7Ue", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "further improvements to LSTM and additional Neural GPU baseline", "comment": "\nThank you for your comment. In light of your questions we have made three further improvements to the generalization section (5.2) and Fig 8 (updates shown in red in the pdf). \n\n------------------------------------\n(1) Dropout \n\n_Summary_: We improved the performance of both the LSTM baseline and NTPT by training for longer. \n\n_Details_: We find that dropout significantly slows down learning, but ultimately, by using dropout we were able to improve the baseline performance by increasing the baseline model size to a 3 layer LSTM with 1024 units in each layer (details of this architecture are now included at the end of section 4). This configuration required 8M training instances to reach convergence.\n\nTo be fair, we also extended the training of the NTPT model to 8M instances. The source code learned by NTPT converges rapidly (after 30k instances), but we find that extended training on the MATH task continues to improve digit classifier (net_0) in the library of neural functions from 95% to 97.5% (converged) accuracy on a single digit classification task. The performance of the NTPT model is very sensitive to the performance of the classifiers for long arithmetic expressions (e.g. consider the ratio 0.975^16/0.95^16 = 1.5 for the 16 digit case). Therefore, this small boost in classifier performance has a significant effect on the generalization accuracy. \n\n_Numerical values_: Below we provide the requested numerical values for the accuracies plotted in Fig. 8b:\n\n# Digits | 2 | 3 | 4| 5| 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16\nLSTM (2-digit) | 96.2 | 19.6 | 18.2 | 19.0 | 18.0 | 17.6 | 18.3 | 17.7 | 18.4 | 18.9 | 18.9 | 18.5 | 18.7 | 17.8 | 19.1\nLSTM (5-digit) | 94.2 | 92.1 | 90.8 | 89.3 | 88.4 | 88.0 | 87.0 | 85.7 | 85.2 | 85.0 | 84.4 | 84.4 | 83.9 | 82.9 | 82.8\nNTPT (2-digit) | 96.2 | 94.3 | 93.3 | 92.1 | 91.2 | 91.3 | 90.1 | 89.6 | 88.8 | 88.8 | 88.7 | 88.1 | 88.3 | 87.3 | 87.1\n\nThe numbers in parentheses indicate the maximum expression length observed during training. The conclusion remains unchanged: NTPT trained on only 2-digit expressions outperforms the best LSTM baseline even when the LSTM is provided with up to 5-digit expressions during training.\n\n_Changes to paper_: We have updated Fig 8b with the results from this extended training (and we include the numerical values for the 16 digit test accuracy). We also include a comment on the sensitivity of NTPT to the classifier accuracies in section 5.2.\n\n------------------------------------\n(2) Neural GPU baseline\n\n_Summary_: We performed an investigation into a Neural GPU baseline on the MATH task as requested, but find that it shows worse generalization than the LSTM. Independent findings from [1] support our conclusion.\n\n_Details_: We obtained an implementation of the Neural GPU from the original authors [2] (which includes all training tricks highlighted in [3]). To assess this model, we first evaluated the relative performance of the Neural GPU, the LSTM and TerpreT models on a simpler, perception-free version of the MATH task where digits and operators are supplied as one-hot vectors and there is not the additional complication of decoding handwritten symbols. Following [1] we tried Neural GPUs with 128, 256, or 512 filters, and found best performance with the largest model. We find that the Neural GPU can perfectly fit the training data (expressions of length <=5 digits), but generalizes very poorly to longer expressions. The LSTM on the other hand is seen to be a very strong baseline (see the numerical values below and Fig. 8a).\n\nOur findings are supported by [1] where the authors consider a task similar to our MATH problem. There are two main differences between our experiment and [1] which affect the difficulty of the task in different ways \u2013 these are:\n(a) we use a decimal representation of numbers while [1] uses binary (both [3] and [1] suggest that decimal representation tends to increase the difficulty of the task for the Neural GPU)\n(b) we consider shorter test cases: up to 32 tokens (digits + operators) on the input vs. 201 tokens in [1] (this reduces the difficulty of our task).\n\nUsing a 512-filter Neural GPU we achieve a 25% accuracy on our 32 token decimal MATH task, and argue that this seems compatible with the results from [1] where a 512-filter Neural GPU achieves a similarly low ~30% accuracy on the 201 token binary MATH task. \n\nOur experimental findings (& [1]) indicate that the LSTM is a much stronger baseline than the Neural GPU for our task. Since the gap in performance is so large on this simplified, perception-free version of the MATH task, we drop the Neural GPU when we re-introduce the perceptual component into the MATH experiments (Fig 8b). \n\n_Numerical values_: Below are the numerical values for the accuracies of all models on the perception-free MATH task (plotted in Fig 8a).\n\n# Digits | 2 | 3 | 4| 5| 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16\nNeural GPU (5-digit) | 100 | 100 | 100 | 100 | 99.56 | 92.82 | 68.55 | 54.35 | 47.36 | 44.82 | 42.72 | 38.18 | 34.33 | 29.0 | 25.0\nLSTM (5-digit) | 100 | 100 | 100 | 99.9 | 99.2 | 98.7 | 98.2 | 96.4 | 96.2 | 95.6 | 94.7 | 93.2 | 93.0 | 91.1 | 92.8\nNTPT (2-digit) | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 \n\n_Changes to the paper_: To justify our belief that the LSTM is a strong baseline, we include the results for the performance of the Neural GPU, LSTM and TerpreT on the simplified, perception-free version of the MATH task in the paper (Fig 8a). A short description of the results in Fig 8a is also included in section 5.2.\n\n------------------------------------\n(3) Parameter counts \n\n_Summary_: The algorithmic part of NTPT has six orders of magnitude fewer parameters than the LSTM baseline. \n\n_Numerical values_: Below we report the number of parameters in each model excluding parameters in the perceptual networks (net_0/1):\nNTPT: 32\nLSTM: 21.1M\nNeural GPU (512 filters): 43.8M\nThe perceptual nets themselves contribute an additional 0.5M parameters to each model. \n\n_Changes to the paper_: These parameter counts are included in Fig 8a.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n[2] https://github.com/tensorflow/models/tree/master/neural_gpu\n[3] \u0141ukasz Kaiser & Ilya Sutskever; Neural GPUs Learn Algorithms (ICLR 2016)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1484524547216, "tcdate": 1478295956589, "number": 470, "id": "HJStZKqel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJStZKqel", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "content": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 19, "writable": false, "overwriting": ["rJNulIVtx"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484087896102, "tcdate": 1484087896102, "number": 3, "id": "SJlPfk7Ue", "invitation": "ICLR.cc/2017/conference/-/paper470/official/comment", "forum": "HJStZKqel", "replyto": "H1RMiiJUx", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "New LSTM baseline", "comment": "Thanks for the new LSTM baseline. I think it makes a good addition. I see it in Figure 8 now, but cannot find the numbers. From what I see:\n* the NTPT and LSTM perform the same on 5-digit test when trained on 5-digit training (is this true?)\n* the NTPT generalizes a bit better; it also has less parameters if I understood correctly?\n\nWas dropout used in the LSTM? NTM should be more effective at generalization, but indeed, it hasn't been applied to arithmetics too often. You could try the Neural GPU which generalizes much better than an LSTM. Could you also clarify how many parameters the LSTM/NTPT have, if the LSTM has dropout (to generalize well with a lot of parameters, it certainly should), and what the exact numbers are (it's hard to read them from Figure 8)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563204, "id": "ICLR.cc/2017/conference/-/paper470/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563204}}}, {"tddate": null, "tmdate": 1483877181734, "tcdate": 1483877141730, "number": 4, "id": "H1RMiiJUx", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "HySL-VEHx", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "Clarification on NTMs", "comment": "\nThank you for your comment. We have now included the PNN baseline in the current revision of the paper, and we have improved the LSTM baseline in the MATH task.\n\nWe believe that the new LSTM results now make for a convincing and natural baseline, but to ensure that we have satisfied your concerns, we would like to ask for clarification on the request for more complex fully supervised models for MATH (e.g. the Neural Turing Machine).\nspecifically: \n(i) We are not sure what is meant by 'fully supervised': All of our models are weakly supervised with input-output examples.\n(ii) The Neural Turing Machine\u00a0has been shown to be effective at learning\u00a0copy/recall/sorting algorithms\u00a0which require shifting patterns around in memory. It is not clear that NTMs are more effective than LSTMs\u00a0at learning to perform arithmetic operations, nor is it clear\u00a0how to include perceptual inputs to a NTM. We believe that solving these issues with NTMs might be considered a separate line of work rather than a baseline for the present work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1483125068651, "tcdate": 1483125068651, "number": 2, "id": "HySL-VEHx", "invitation": "ICLR.cc/2017/conference/-/paper470/official/comment", "forum": "HJStZKqel", "replyto": "ryNroAMre", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer3"], "content": {"title": "Comment on the reply", "comment": "I thank the authors for their reply. I find including PNNs and also more complex fully supervised models for MATH (e.g., the Neural Turing Machine) is essential. I would consider increasing my score if these were included, but of course only once this is done."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563204, "id": "ICLR.cc/2017/conference/-/paper470/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563204}}}, {"tddate": null, "tmdate": 1483037499862, "tcdate": 1483037499862, "number": 2, "id": "ryNroAMre", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["~Alexander_L_Gaunt1"], "readers": ["everyone"], "writers": ["~Alexander_L_Gaunt1"], "content": {"title": "General response", "comment": "We thank all the reviewers for their time and comments on our work. To summarize, two reviewers state that this is an interesting line of research, and that this paper could be a valuable contribution to the emerging field of learning programs from data. However the reviewers raise a concern that the tasks presented in the experimental section are too simple and the baselines may be weak.\n\nIn particular, AnonReviewer1 notes that if strong supervision were available the ADD2x2 task could be solved by a baseline consisting of 2 consecutive linear models. However, in this paper we are concerned entirely with the LPPBE paradigm, where weak supervision and lifelong learning are key. We argue that the ADD2x2 and APPLY2x2 tasks in the experimental section were not chosen for complexity, but instead chosen to cleanly illustrate different benefits of our method relative to a purely neural baseline subject to these LPPBE restrictions. AnonReviewer5 agrees that our choice of tasks allows for a clean illustration of the key qualitative conclusions: success with weak supervision (Figure 5) and avoidance of catastrophic forgetting / demonstration of reverse transfer (Figure 6).\n\nHaving progressed through the illustrative early tasks, the lifetime of learning culminates in the MATH task, which is a non-trivial task of some practical use (evaluation of handwritten arithmetic expressions of arbitrary length). We argue that AnonReviewer1\u2019s analysis concentrates on a linear model for our simplest ADD2x2 task and ignores this more complex MATH task (which even with strong supervision would require at least a recurrent non-linear model).\n\nWe thank the AnonReviewer3 for the suggestion to include a Progressive Neural Network (PNN) as a more modern baseline model. We will include results using PNNs in a final version of the paper. Note that this baseline does not affect Figure 5 (which shows the success of our model over all weakly supervised baselines on a single task), and while PNNs may prevent catastrophic forgetting in Figure 6, they will not exhibit reverse transfer. We therefore feel that the main qualitative messages of the paper are not significantly changed with the addition of this baseline.\n\nUltimately, as we mention in the paper, the complexity of the tasks that we consider is limited by the known difficulty in training differentiable interpreters. The tasks we present in this paper require search over program spaces of size up to 10^12 syntactically distinct programs. Solution to problems of this scale is comparable to the state of the art for methods using weakly supervised differentiable interpreters. It is an open area of research to improve the performance of these models, and this work aims to provide motivation for this line of research by producing the first demonstration of the value of differentiable interpreters as a glue for combining symbolic programming with neural networks.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1481908131108, "tcdate": 1481908131108, "number": 2, "id": "HysiJjZEe", "invitation": "ICLR.cc/2017/conference/-/paper470/official/review", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes an extension of TerpreT by adding a set of functions that can deal with inputs in the form of tensor with continuous values. This potentially allows TerpreT to learn programs over images or other \u201cnatural\u201d sources.\n\nTerpreT generates a source code from a set of input/output examples. The code is generate in the form of a TensorFlow computation graph based on a set of simple and elegant program representations. One of the limitation of TerpreT is the type of inputs it can work with, this work aim at enriching it by adding \u201clearnable functions\u201d that can deal with more complex input variables.\n\nWhile I really like this direction of research and the development of TerpreT, I find the contribution of this work to be a bit limited. This would have been fine if it was supported by a strong and convincing experimental section, but unfortunately, the experimental section is a bit weak: the tasks studied are relatively simple and the baselines are not very strong.\n\nFor example let us consider the SUM2x2 problem:  all the images of digits are from MNIST, which can be classify with an error of 8% with a linear model (and even better with neural networks), There is also a linear model that given 4 numbers will compute the 2x2sum of them that is: y=Ax where x is the vector containing the 4 numbers and A = [1 0 1 0;1 1 0 0;1 0 0 1; 0 1 0 1]. This means a succession of two linear models can solve the sum2x2 problems with little trouble. While I'm aware that this work aims at automatically finding the combination of simple models to achieve this task end-to-end, the fact that the solution is a set of 2 consecutive linear models makes it a bit too simple in my humble opinion. \n\nOverall, I think that this paper proposes a promising extension of TerpreT that is unfortunately not backed by experiments that are convincing enough.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512575410, "id": "ICLR.cc/2017/conference/-/paper470/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper470/AnonReviewer3", "ICLR.cc/2017/conference/paper470/AnonReviewer1", "ICLR.cc/2017/conference/paper470/AnonReviewer5"], "reply": {"forum": "HJStZKqel", "replyto": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512575410}}}, {"tddate": null, "tmdate": 1481874939081, "tcdate": 1481874840717, "number": 1, "id": "BJZjpf-4e", "invitation": "ICLR.cc/2017/conference/-/paper470/public/comment", "forum": "HJStZKqel", "replyto": "Hkb8KkxNl", "signatures": ["~Danny_Tarlow1"], "readers": ["everyone"], "writers": ["~Danny_Tarlow1"], "content": {"title": "It remains to be seen, but we think the future is bright", "comment": "Thank you for the question and the nice feedback.\n\nTo answer the last question first, we are unaware of any formal answer to the question. What is learnable depends not only on the supervision but also on the assumptions and priors that are encoded into the model (either explicitly or implicitly). What we infer based upon seeing examples [2, 1, 4]->[1, 2, 4] and [4, 3]->[3, 4] depends on the priors and inductive bias of the model. Thus in some sense, this question boils down to \"what inductive bias can/should be encoded over program space, and how powerful is it to disambiguate a 'correct' program (one that generalizes strongly) from an 'incorrect' program (one that does not)?\"\n\nWe don't have a clear answer to this right now and it is beyond the scope of this paper, but our long-term view is the following: (1) the explicit source code representation provides strong inductive bias towards learning an algorithm that generalizes in a strong way; (2) this inductive bias could be strengthened further by studying how people use programming languages in practice and incorporating this as a prior over source code representations; and (3) learning in a lifelong setting can achieve many of the benefits of a curriculum and thus help towards building up to more complicated programs from simpler ones.\n\nFinally, we believe that an ultimate lifelong learning system should be able to learn from many types of supervision, ranging from very strong to very weak, and I/O examples are one important point on that spectrum that we should be studying."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287563339, "id": "ICLR.cc/2017/conference/-/paper470/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJStZKqel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper470/reviewers", "ICLR.cc/2017/conference/paper470/areachairs"], "cdate": 1485287563339}}}, {"tddate": null, "tmdate": 1481795913459, "tcdate": 1481795913452, "number": 1, "id": "Hkb8KkxNl", "invitation": "ICLR.cc/2017/conference/-/paper470/pre-review/question", "forum": "HJStZKqel", "replyto": "HJStZKqel", "signatures": ["ICLR.cc/2017/conference/paper470/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper470/AnonReviewer5"], "content": {"title": "How far can one push the notion of weak supervision in program induction", "question": "\nThis is a wonderful paper on program induction, with a very nice framework for combining code with neural networks.\n\nI'd like to ask the authors a simple question, which in no way attempts to diminish the great value of this paper, but rather prompts for clarification.\n\nCan you learn more complex programs such as sorting with weak supervision? (and without curriculum, which amounts to strong supervision of hierarchy). Reed et al learn sorting with only 8 demonstrations, but imitation is of course a stronger form of supervision.\n\nCan the family of tasks that can be learned with weak supervision be easily characterised? Does classical complexity theory offer an answer?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "pdf": "/pdf/1af94cbdee9cb33e75af6d46adbbb6e4601abcb9.pdf", "TL;DR": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "paperhash": "gaunt|lifelong_perceptual_programming_by_example", "conflicts": ["microsoft.com"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481795914071, "id": "ICLR.cc/2017/conference/-/paper470/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper470/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper470/AnonReviewer5"], "reply": {"forum": "HJStZKqel", "replyto": "HJStZKqel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper470/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481795914071}}}], "count": 20}