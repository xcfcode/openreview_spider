{"notes": [{"id": "YxuYqHFjH9", "original": null, "number": 10, "cdate": 1583772119369, "ddate": null, "tcdate": 1583772119369, "tmdate": 1583772119369, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "YqfqyWENWa", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment", "content": {"title": "Thoughts", "comment": "We respectfully disagree with this assessment. The paper makes a variety of contributions, including a method with substantial novelty. Nearly all classifiers are discriminative. Even approaches that use a generator typically involve a discriminator in the pipeline. For example, sometimes one learns a generator on unlabelled data, then recycles the representation as part of a discriminative classifier. Generative models are compelling because we are trying to create an object of interest. The challenge in generative modelling is that standard approaches to density estimation are poor descriptions of high-dimensional natural signals. \n\nThe method proposed in this paper, FlowGMM, is arguably one of the only end-to-end fully generative approaches to classification with normalizing flows, which is a very significant point of novelty. Just because the model involves a Gaussian mixture, and Gaussian mixtures have been used in other contexts, does not take away from the novelty. The method also provides a coherent approach to handling both labelled and unlabelled data, which are often treated separately in deep semi-supervised methods. We also propose a new type of probabilistic consistency regularization that significantly improves FlowGMM on image classification problems. And the method is also relatively interpretable and broadly applicable.\n\nWe appreciate the feedback and have made several modifications to the paper, including a more visible presentation of the contributions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1957/Authors|ICLR.cc/2020/Conference/Paper1957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148407, "tmdate": 1576860542466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment"}}}, {"id": "BJg_2JHKvH", "original": "HkxkA-yKvS", "number": 1957, "cdate": 1569439663898, "ddate": null, "tcdate": 1569439663898, "tmdate": 1577168261945, "tddate": null, "forum": "BJg_2JHKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YqfqyWENWa", "original": null, "number": 1, "cdate": 1576798736840, "ddate": null, "tcdate": 1576798736840, "tmdate": 1576800899503, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Decision", "content": {"decision": "Reject", "comment": "This paper offers a novel method for semi-supervised learning using GMMs.  Unfortunately the novelty of the contribution is unclear, and the majority of the reviewers find the paper is not acceptable in present form.  The AC concurs.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711421, "tmdate": 1576800260620, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Decision"}}}, {"id": "SJlkOR-cFH", "original": null, "number": 1, "cdate": 1571589735153, "ddate": null, "tcdate": 1571589735153, "tmdate": 1574237766606, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "The paper describes how to use normalising flows for Semi Supervised Learning (SSL). Briefly, the method consists in finding a (bijective) map for transforming a mixture of Gaussians into a density approximating the empirical data-distribution -- as usual for flow methods, the parameters are found through likelihood maximisation. This is a elegant approach that naturally exploits the standard so-called cluster assumption in SSL. The papers also shows how to incorporate a consistency-based regularisation within the method.\n\n\nAlthough it is an elegant and simple approach, and the article is relatively well written, I think that the paper should be rejected because (1) on image classification tasks (and even with consistency regularisation), the performances are well-below the straightforward-to-implement \\Pi-model. (2) for tabular/NLP data, although the performances seems to be good, the comparison with standard methods could have been much better done -- I am still not convinced by the method. \n\nI agree with the authors that there are many situations where it is not possible to find good perturbation (eg. NLP / tabular / genomics / etc...). If the authors could demonstrate more carefully that their approach does lead to state-of-the-art performances in this type of situations, I do believe that the approach would be of great interest. Given that the methods does not work well at all for image classification, I think that he authors should have been much more careful with the comparisons with the standard methods when investigating the performances on NLP/tabular tasks.\n\n\n(1) basic k-NN benchmark?\n(2) basic dimension reduction (PCA / autoencoder / extract lower representation from a NN) associated with either k-NN or label-propagation?\n(3) it is *not* difficult at all to implement label propagation with fast nearest-neighbours (eg. FAISS library) and sparse linear algebra on the full datasets. In the current submission, it has not been done for the NLP datasets.\n(4) There are indeed several ways to compute distance / affinity within label-propagation-type approaches\n(5) Brief description of parameter tuning for label-prop should be added\n\nI think that the method has a lot of potential and the fact that it is not competitive for computer vision task is not important. I encourage the authors to carry out more convincing numerical comparisons ing tabular/NLP/etc.. settings in order to strengthen the message of the paper. If convincing results can be obtained, I believe that the method has a lot of potential.\n\n[Edit after rebuttal]\nI would like to thank the authors to have provided additional label propagation experiments and details -- the proposed method appears to be quite much better than this baseline approach, which is very reassuring and proves that it is worth exploring further this line of work.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917897953, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Reviewers"], "noninvitees": [], "tcdate": 1570237729836, "tmdate": 1575917897970, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review"}}}, {"id": "rkeEIJV3ir", "original": null, "number": 8, "cdate": 1573826379997, "ddate": null, "tcdate": 1573826379997, "tmdate": 1573826379997, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "rJg2gb6ooB", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment", "content": {"title": "Response to response", "comment": "Dear authors, thank you for updating the paper and addressing my concerns.\n\nI understand your points about examining the latent space and interpretability, but I am still not convinced that the emphasis of the paper is quite right. It is not so much about squeezing even more into the paper, it is more about changing the focus. Though I do appreciate that this is not possible for a rebuttal.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1957/Authors|ICLR.cc/2020/Conference/Paper1957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148407, "tmdate": 1576860542466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment"}}}, {"id": "rJg2gb6ooB", "original": null, "number": 7, "cdate": 1573798131980, "ddate": null, "tcdate": 1573798131980, "tmdate": 1573798131980, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BygksbcRtB", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for detailed comments. We addressed the clarity issues that the reviewer identified in the updated version of the paper. In particular,\nFlowGMM Sup and FlowGMM Supervised were both referring to the same method (we renamed the entries in the updates version), FlowGMM trained only using the labeled data. In Table 2, \u201cFlowGMM Sup (All labels)\u201d was trained in a fully-supervised setting, when labels are available for all data points (e.g. 50k labels in CIFAR-10), and \u201cFlowGMM Sup ($n_l$ labels)\u201d was trained only on $n_l$ labeled data (e.g. 4k data points for CIFAR-10).\nIn all Tables we use classification accuracy as the predictive metric.\n\nRegarding the novelty of our model interpretation experiments, while we agree that GMMs are not novel, we believe that the combination of normalizing flows with GMMs is novel. We argue that while we could expect some of the observed properties would hold, they are not trivial and verifying them is important. In particular,\nIf the data was generated from the FlowGMM model, we would indeed be sure a priori that the decision boundary between classes was passing through a low-density region. However, when we fit actual image data using FlowGMM the fit is not perfect, and there is no way of concluding that the same property would hold without experimentally verifying it. Further, the separation between classes in the latent space is of crucial importance for interpretation of FlowGMM, so we believe that it is important to study it explicitly.\nAnother important observation about the latent spaces is that including unlabeled data does indeed push the decision boundary away from unlabeled data. This property is desired, and we believe that explicitly demonstrating it helps interpreting FlowGMM.\n\nWe agree that using the Chinese Restaurant Process GMM to automatically determine the number of classes in the data is an exciting direction for future work. However, it would require a non-trivial amount of effort, and methodological advancement, and the reviewer pointed out that \u201cthe authors tried to squeeze too much into the paper\u201d even with the current content of the paper. We do not agree that not being able to infer the number of classes is a major shortcoming of FlowGMM, as the setting when the number of classes is unknown is not typically considered in semi-supervised literature, and most of the existing semi-supervised methods are also not directly applicable in this setting. We plan to explore inferring the number of unlabeled classes in future work. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1957/Authors|ICLR.cc/2020/Conference/Paper1957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148407, "tmdate": 1576860542466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment"}}}, {"id": "S1gJ3l6ioS", "original": null, "number": 6, "cdate": 1573798054858, "ddate": null, "tcdate": 1573798054858, "tmdate": 1573798054858, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "SJlkOR-cFH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We appreciate the reviewer\u2019s comments and advice. In response to (1), while it\u2019s true that our model does not perform as well as the Pi model (Tarvainen and Valpola, 2017), the base network architecture in that work is substantially more powerful owing to it not being constrained with invertibility. When trained using all of the labels on CIFAR10 and no unlabeled data, the CNN from Tarvainen and Valpola (2017) has an error rate of 5.56 and the RealNVP architecture we use gets an error rate of 11.55. \n\nThe second point made was that although FlowGMM performance on NLP/tabular tasks is promising, the experiments needed a more thorough and careful comparison to supervised and semi-supervised baselines. We agree and have added additional baselines to this section. The performance of k-NN is very similar to the other supervised only methods on the UCI datasets but on the two text classification datasets the performance is substantially worse. We suspect this has to do with the way the BERT embeddings were originally trained for separation type tasks. The label propagation baseline applied in the paper uses a dense affinity matrix, hence challenges with scaling but we thank the reviewer for their suggestion and updated the semi-supervised baselines to include sparse k-NN based label spreading approach that uses a larger fraction of unlabeled data. For tuning the hyper parameters of these label spreading methods, we perform an independent grid search for each method on each dataset.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1957/Authors|ICLR.cc/2020/Conference/Paper1957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148407, "tmdate": 1576860542466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment"}}}, {"id": "H1xq0J6siH", "original": null, "number": 5, "cdate": 1573797842142, "ddate": null, "tcdate": 1573797842142, "tmdate": 1573797842142, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "r1xTxK8CYr", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "0) We updated the caption of the Tables and specified the performance metrics.\n1) We found that the performance of the methods in Table 1 had high variance, so we decided to adopt the following strategy. We train each method three times, and pick up the run that attained the best accuracy on a validation set. We then report the performance of that run on the test data (different from validation). This procedure is still fair, and is attainable in practice. In an updated version of the paper we will report the mean and std over multiple repetitions of this process.\n\n2) The performance of a supervised model (which was trained with all labels) shows the general capacity of the model. For example, on CIFAR-10, FlowGMM Sup (All labels), 2nd row of Table 2, is trained on 50k labeled examples, while FlowGMM Sup ($n_l$ labels), 5th row of Table 2, is trained on 4k labeled examples (unlabeled data is not used); reporting both accuracies shows the gap which appears when using much less data, and this gap is significantly decreased when we add unlabeled data. The testing data for a fixed dataset is the standard test split, and is the same across all models and all settings (supervised and semi-supervised).\n\n3) Like other methods of feature visualization (regularized optimization and inversion by optimization) our novel feature visualization method gives insight into what kinds of features activate a given channel and spatial location, a tool for understanding the intermediate representations and what is learned by the network. Unlike other feature visualization methods, our method does not require optimization or hyperparameters and hence can be performed at real time rates for interactive feature exploration."}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1957/Authors|ICLR.cc/2020/Conference/Paper1957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148407, "tmdate": 1576860542466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Comment"}}}, {"id": "r1xTxK8CYr", "original": null, "number": 2, "cdate": 1571870964652, "ddate": null, "tcdate": 1571870964652, "tmdate": 1572972401733, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a semi-supervised learning model, named as Flow Gaussian Mixture Model (FlowGMM). The model is learnt by maximizing the join likelihood of the labeled and unlabeled data with a consistency regularization.\nThe authors demonstrate that the proposed model outperforms others on text classification; for image classification, the performance can be improved in future.\nAlso the authors demonstrate that the model is interpretable via feature visualizations.\nOverall the paper is fine written.\nYet, The conclusion is not fairly supported and the paper could be much stronger with the issues discussed already but I don\u2019t think its current form is ready yet.\n\nBelow are more detailed comments:\n0) It would be nice to add the definition of the performance metric; without the definitions, none of the numbers in the tables would make sense. \n1) The main result for text classification in Table 1 is reporting the best of 3 runs, which can\u2019t support the conclusion that the proposed method outperforms the other. In general, it\u2019s nice to provide statistical significance comparing two models or reporting the mean and std across multiple runs. \n2) In Table 2, it\u2019s not clear what conclusion could be drawn by comparing the performance of supervised and semi-supervised performance. Are the testing data points the same?\n3) The feature visualization as discussed in Section 6.3 is not explained clear. Specifically, \u201cgiving us insight into the workings of the model\u201d is not clear; what exactly insight can we get and what exactly are the workings can we get?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917897953, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Reviewers"], "noninvitees": [], "tcdate": 1570237729836, "tmdate": 1575917897970, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review"}}}, {"id": "BygksbcRtB", "original": null, "number": 3, "cdate": 1571885462873, "ddate": null, "tcdate": 1571885462873, "tmdate": 1572972401684, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a normalising flow with the prior distribution represented by a Gaussian mixture model (GMM). The method, FlowGMM, maps each class of the dataset to a Gaussian distribution in the latent space by optimising the joint likelihood of both labelled and unlabelled data, thus making the method useful for semi-supervised problems. Predictions are made using the maximum a posteriori estimate of the class label. To make the method robust to small perturbations of the inputs, the authors introduce a novel consistency regularisation term to the total loss function, which maximises the likelihood of predicting the same class after a perturbation.\nThe authors further examine the learnt latent space by considering two simple, synthetic datasets that can be easily visualised, showing that the latent space behaves in a way one would intuitively expect.\nThe method is evaluated on both tabular and image data, showing promising results in terms of accuracy (presumably, see below). As the model is found to be overconfident in its predictions, the authors introduce a calibration scheme and empirically verify that it improves the uncertainty estimates. Lastly, the authors introduce a feature visualisation scheme and use it to illustrate the effect of perturbing the activations of the invertible transformations.\n\nI generally like the proposed method, which seems useful and intuitive. I am particularly happy with the discussion on uncertainty calibration, where the authors suggest an elegant addition to the model to increase the variance of the mixture components. I do, however, have significant concerns about the novelty of the paper as well as its structure and clarity, as detailed below. I do, therefore, not recommend it for acceptance.\n\nThe paper reads well, although I feel that it lacks some details and explanations. For example, in table 2, it is never mentioned what \"FlowGMM Sup\" refers to and if it is different from \"FlowGMM Supervised\". It is also not clear what \"(All labels)\" refers to - does it mean that labels were provided for the entire dataset or that the models were trained only on the small subset with labels? Or something else? Which performance metric is used in the tables? The accuracy, presumably, but this is never specifically stated. Similarly, the number of datapoints and ratio of labelled to unlabelled data for the synthetic datasets are not reported. They are not crucial to know but should be included for completeness.\n\nWhile the first half of the paper is informative and well-structured, the second half appears a bit less so. From experimentally verifying that the method works, the paper goes on to discuss uncertainty calibration, examine the latent space representations, and visualising the effect of feature perturbations. While I greatly appreciate the focus on interpreting the trained model, I think it appears somewhat chaotic, as if the authors tried to squeeze too much into the paper. For example, the feature visualisation technique is quite neat, but it works for any flow and is not really used for anything in the paper. I would suggest saving it for a dedicated paper.\n\nI am not convinced by the novelty of this paper. The authors list two contributions: 1) the model itself, 2) an empirical analysis with much focus on the interpretability of the model. While the model is, to my knowledge, indeed novel, the analysis is quite standard, and the interpretability even appears to be oversold. GMMs are nice and intuitive, but not novel in any way, yet the authors seem to be describing the properties of GMMs as specific to their method.\nIn particular, the authors go to great lengths to show that the latent space representations cluster around the means of the mixture components and that the decision boundary lies in low-density regions of the latent space. I do not see why these properties should be so surprising since the method directly optimises the likelihood of the data under the mixture distribution. That this is also empirically observed is, of course, reassuring, but these observations are better suited for the appendix, in particular given that the paper went over the recommended page limit.\nI think that much of the claimed second contribution follows directly from the GMM aspect of the model. Instead of claiming the standard GMM properties as contributions, I think the proposed consistency loss term should be highlighted as a contribution on its own. I find it elegant, and I guess it would be particularly useful for NLP tasks where sentences can be phrased in different ways but still mean the same.\n\nInstead of discussing the latent space, I would have preferred to see extra evaluations of the method, like convergence rates of both FlowGMM and FlowGMM-cons compared to the competing models. Furthermore, a major limitation of the model is that knowledge of the correct number of classes in the data - even in the unsupervised setting. The authors hint at extensions to mitigate this in the discussion (using a Chinese Restaurant Process GMM or by adding extra Gaussians to the mixture during training), but these should have been investigated in the current paper.\n\nIn conclusion, I think that the paper lacks novelty and that it spends far too much space on \"trivial\" properties of the model instead of addressing shortcomings, like the prior specification of the number of classes, which the authors even point out in the discussion.\n\nMinor comments:\n- p 4, bottom: \"each with 1 hidden layers\" -> \"each with 1 hidden layer\"\n- p 5, middle: \"FlowGMM is able to leveraged\" -> \"FlowGMM is able to leverage\"\n- p 5, bottom: \"Table 5.1\" -> \"Table 1\"\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1957/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917897953, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1957/Reviewers"], "noninvitees": [], "tcdate": 1570237729836, "tmdate": 1575917897970, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Official_Review"}}}, {"id": "BkeV9headB", "original": null, "number": 1, "cdate": 1570733196057, "ddate": null, "tcdate": 1570733196057, "tmdate": 1570733196057, "tddate": null, "forum": "BJg_2JHKvH", "replyto": "BJg_2JHKvH", "invitation": "ICLR.cc/2020/Conference/Paper1957/-/Public_Comment", "content": {"comment": "Thank you for the interesting work. I suggest that the paper \"Semi-Conditional Normalizing Flows for Semi-Supervised Learning\" from ICML Workshop is relevant. The work also uses a class conditional prior in the form of Normalizing flow and GMM. The discussion of the difference between the methods will be useful.\n\nlink: https://invertibleworkshop.github.io/accepted_papers/pdfs/INNF_2019_paper_20.pdf", "title": "The relevant paper \"Semi-Conditional Normalizing Flows for Semi-Supervised Learning\""}, "signatures": ["~Arsenii_Ashukha1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Arsenii_Ashukha1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["izmailovpavel@gmail.com", "pk1822@nyu.edu", "maf820@nyu.edu", "andrew@cornell.edu"], "title": "Semi-Supervised Learning with Normalizing Flows", "authors": ["Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Wilson"], "pdf": "/pdf/19509e2eafbe499c8e9237ea96a9f792dab1cd1a.pdf", "TL;DR": "Probabilistic semi-supervised learning method based on normalizing flows", "abstract": "We propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. We approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. We train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. We evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Answers text data, MNIST, SVHN and CIFAR-10 image classification problems as well as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, we show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. ", "keywords": ["Semi-Supervised Learning", "Normalizing Flows"], "paperhash": "izmailov|semisupervised_learning_with_normalizing_flows", "original_pdf": "/attachment/26b7223cc068d508a09e247eaab44f690e1960b8.pdf", "_bibtex": "@misc{\nizmailov2020semisupervised,\ntitle={Semi-Supervised Learning with Normalizing Flows},\nauthor={Pavel Izmailov and Polina Kirichenko and Marc Finzi and Andrew Wilson},\nyear={2020},\nurl={https://openreview.net/forum?id=BJg_2JHKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJg_2JHKvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187157, "tmdate": 1576860575857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1957/Authors", "ICLR.cc/2020/Conference/Paper1957/Reviewers", "ICLR.cc/2020/Conference/Paper1957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1957/-/Public_Comment"}}}], "count": 11}