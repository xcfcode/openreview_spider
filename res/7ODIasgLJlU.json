{"notes": [{"id": "7ODIasgLJlU", "original": "Y1xmozwfrPu", "number": 2881, "cdate": 1601308319626, "ddate": null, "tcdate": 1601308319626, "tmdate": 1614985659540, "tddate": null, "forum": "7ODIasgLJlU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sr_Z49fVJpP", "original": null, "number": 1, "cdate": 1610040502257, "ddate": null, "tcdate": 1610040502257, "tmdate": 1610474109164, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies RL with low switching cost under the deep RL setting. It provides new heuristics for doing so. The reviewers are worrying about whether the problem is important in practice, whether the policies obtained can be used in practice, and the theories might not be strong enough. The paper can be strengthened if better theory and more experiments are provided.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040502243, "tmdate": 1610474109149, "id": "ICLR.cc/2021/Conference/Paper2881/-/Decision"}}}, {"id": "yEDt2Q8FmU-", "original": null, "number": 2, "cdate": 1603749758228, "ddate": null, "tcdate": 1603749758228, "tmdate": 1606424507217, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review", "content": {"title": "Proposes an adaptive heuristic for deciding when to update the simulation policy for deep RL", "review": "######################################\n\nSummary:\nIn many real world applications for RL such as medicine, there are limits on the number of policies from which we can simulate data. This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Experiments on a medical treatment environment and Atari show that the approach obtains similar performance to on-policy RL with fewer changes of the simulation policy.\n\n######################################\n\nPros:\n1. The proposed approach is straightforward, adaptive, and achieves results comparable to the classical on-policy setting with fewer policy switches on all environments shown. It is also applicable to both model-based and model-free RL.\n2. The paper is organized well, and the algorithms are clearly explained.\n\nCons:\n1. I did not find the theoretical justification for the proposed approach to be very convincing for RL, since it is based on a construction in a simplified linear regression case. However, I think this is ok since the paper is application-focused.\n2. Based on the results on GYMIC, the proposed approach seems to have much greater variance than the other algorithms, especially at the early stages of training. This is often detrimental for the applications considered, such as medicine, in which robustness is also desirable.\n\n######################################\n\nOverall:\nI would lean toward accepting this paper. I am not completely familiar with the literature on RL with low switching cost, but the proposed approach appears to be novel. The experiments show that when combined with Rainbow DQN,  it effectively reduces the switching cost on a range of environments and is based on the training path, requiring less environment-specific hand-tuning than fixed or adaptive interval switching.\n\n######################################\n\nFurther comments and questions:\n1. How were the six Atari games chosen? How do the different approaches compare in other games?\n2. There are a number of typos, e.g. \"deno\" in the first paragraph of section 3.1. \n\n######################################\n\nUpdate after reading other reviews and author response:\n\nI have decided to lower my score from 6 to 5, as I agree with Reviewer 3 that more experimental analysis of the method is needed (ablations, sensitivities, etc.) given that the theoretical backing is not convincing. The authors also did not directly answer our questions.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2881/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086757, "tmdate": 1606915800745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review"}}}, {"id": "mwSQh79xhv8", "original": null, "number": 6, "cdate": 1605523762311, "ddate": null, "tcdate": 1605523762311, "tmdate": 1605523762311, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Official_Comment", "content": {"title": "Thank all reviewers for your reviews and suggestions.", "comment": "Thank all reviewers for your reviews and suggestions.\n\nIn this paper, we take the first step towards designing a generic solution for reducing the switching cost while maintaining performance, which is an important problem when applying the RL algorithms to many real-world scenarios.\nWe sincerely appreciate all the suggestions and will continue working in this direction in future research. \n\nAlthough our method may be a little simple, there was no previous study on this important problem, and we empirically and theoretically verify the feasibility of doing so. We believe that there exists great potential in this direction, and further study would promote the application of RL algorithms.\n\nAnother thing worth noting is that in real-world applications such as the medical domain, we usually need a deterministic policy, and we also derive a deterministic Rainbow DQN. Although in deterministic Rainbow classic $\\epsilon$-greedy exploration is no longer feasible and it is not easy for the policy to explore in the environment, our proposed deterministic Rainbow also achieves empirical success in many scenarios, we believe this also serves as a contribution of our paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7ODIasgLJlU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2881/Authors|ICLR.cc/2021/Conference/Paper2881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843509, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2881/-/Official_Comment"}}}, {"id": "tmIpH-rYoMP", "original": null, "number": 1, "cdate": 1603212170914, "ddate": null, "tcdate": 1603212170914, "tmdate": 1605024111633, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review", "content": {"title": "Interesting setting. A few elements should be improved.", "review": "In the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance. This study is done in the context of deep reinforcement learning. A few generic baselines solutions are provided as well as a more complex solution that empirically outperforms the baselines.\n\nMotivation of the paper:\nThis paper studies an interesting question that is rarely studied in practice. The paper mentions a few applications domains but what would be a very concrete example where the switching cost is important? The paper would benefit from mentioning in what kind of specific application this can be useful to back up the sentences like \"Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc,\". \n\nThe approach\nThe paper investigates different simple switching criteria based on deep Q-networks that are used as baselines and it also proposes an adaptive approach based on the feature distance between Q-networks. I'm not fully convinced by the theoretical justification from a representation learning perspective. What are exactly the extracted feature denoted as f(x) beyond \"a representation function that maps the input to a k-dimension vector\" and that it is based on the Q-networks. Are these the features at the last layer of the Q-network? In fact, why are they referred to as representation at all in the case of DQN?\n\nExperiments\nIt seems that the paper does not mention the number of seeds (runs) that are used for the experiments. Otherwise, the experiments back up the claims.\n\nOther comments:\n- line 16 of the algorithm: do you mean \"h % H_{target}==0\" ?\n- The structure of the paper might be improved a bit and there are some typos (state encdoer, hyper-paramter, ...)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2881/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086757, "tmdate": 1606915800745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review"}}}, {"id": "SvziB7Jx5yo", "original": null, "number": 3, "cdate": 1604368330781, "ddate": null, "tcdate": 1604368330781, "tmdate": 1605024111556, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review", "content": {"title": "More analysis is required", "review": "Hi,\n\nFirst I want to thank authors for putting this manuscript together, and looking into an interesting issue. \n\n*Summary* : Authors proposed an algorithm for switching policies while using Deep Q-learning. The method is based on feature similarity (calculated by cosine similarity) between online and the deployed policy. In addition, they performed experiments in Atari and sepsis simulator.\n\n*Strength* : \n1. I believe the paper is well written and easy to follow. \n2. An important problem to tackle (well motivated)\n\n*Weakness* :\n1. Content : I believe the paper is a good first step, but for a publication analysis should go further, to expand the understanding of the method. For example, I'd like to see the effect of \"a\" for switching on the performance. It shows how stable/unstable the method will be. The effect of batch size \"|B|\" on how often we switch, and if we were to use smaller batches (higher variance in sim(B) score) are we going to see a large hit in the performance? \nIn addition, it is important to check (or discuss) other possible similarity metrics. For example, what if we look at inf norm of the two representation difference, or other similar metrics.\n\n2. Performance : It seems to me that FIX_10^3 has always lower switching cost, and also learns a good policy. I appreciate authors trying 3 different fix number, as mentioned above I will be curious to see effect of \"a\" as well.\n\nQuestions : I think that maybe another good criteria is to measure the distance between the deployed and the online policy, rather than feature representation. It may be the case that features distance determines the policy distance but that's not necessarily true. I was wondering what authors think about this?  (Of course it's hard to measure the distance between the two policy, but maybe using DRL we can measure the distribution distance of the two in a given batch?)\n\n\nAt the end, I would like to say that, the paper is a good a step, but for publication the analysis/ experiments should be more thorough and possibly give insights about how to \"formalize\" the problem. (which I believe should be the main focus, developing theoretical grounding for low-switching cost problems).\n\nHope it was helpful,\nThanks.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2881/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086757, "tmdate": 1606915800745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review"}}}, {"id": "mVi3Ck0SbD", "original": null, "number": 4, "cdate": 1604595473950, "ddate": null, "tcdate": 1604595473950, "tmdate": 1605024111496, "tddate": null, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "invitation": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review", "content": {"title": "An important problem but insufficient results", "review": "This paper studies RL with low switching cost under the deep RL setting. It points out several naive algorithms like switching after a certain number of steps and then propose a new heuristic. This heuristic learns a new policy offline using the experience replay the behavior collected and switches the behavior policy once the similarity of the feature embeddings of the current state by these two policies becomes large. The paper also makes an attempt to provide a theoretical justification for a better understanding of the heuristic. This method might outperform the naive algorithms by some margin, if any. It would be a more interesting manuscript if some stronger results could be provided from the perspective of any of theory, experiments, or applications.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2881/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2881/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q-Learning with Low Switching Cost", "authorids": ["~Shusheng_Xu1", "~Simon_Shaolei_Du1", "~Yi_Wu1"], "authors": ["Shusheng Xu", "Simon Shaolei Du", "Yi Wu"], "keywords": ["deep Q-network", "DQN", "switching cost", "deep Q-learning"], "abstract": "We initiate the study on deep reinforcement learning problems that require low switching cost, i.e., small number of policy switches during training.  Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc, where the deployed policy that actually interacts with the environment cannot change frequently. Our paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the  deployed  Q-network  and  the  underlying  learning  Q-network.   Through  extensive experiments on a medical treatment environment and a collection of the Atari games, we find our feature-switching criterion substantially decreases the switching cost while maintains a similar sample efficiency to the case without the low-switching-cost constraint.  We also complement this empirical finding with a theoretical justification from a representation learning perspective.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|deep_qlearning_with_low_switching_cost", "one-sentence_summary": "A systematic study on deep Q-learning that requires low switching cost.", "supplementary_material": "/attachment/05ea8287ef5a76378ef5bd667148706272f3fdd3.zip", "pdf": "/pdf/fded44de873f759c00a5d020334a61478bd001bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kb2EBV01zl", "_bibtex": "@misc{\nxu2021deep,\ntitle={Deep Q-Learning with Low Switching Cost},\nauthor={Shusheng Xu and Simon Shaolei Du and Yi Wu},\nyear={2021},\nurl={https://openreview.net/forum?id=7ODIasgLJlU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7ODIasgLJlU", "replyto": "7ODIasgLJlU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086757, "tmdate": 1606915800745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2881/-/Official_Review"}}}], "count": 7}