{"notes": [{"id": "_Tf6jEzbH9", "original": "tOj5AcsJBj", "number": 2071, "cdate": 1601308228151, "ddate": null, "tcdate": 1601308228151, "tmdate": 1614985735439, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KihYRJVgqE", "original": null, "number": 1, "cdate": 1610040402992, "ddate": null, "tcdate": 1610040402992, "tmdate": 1610473999165, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper addresses the task of context-agnostic learning and presents an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. It is reported a theoretical ground proposing to decompose factors contributing to the classification risk in context bias and object error. The method makes use of only one synthetic sample in training, still being able to generalize well.\n\nThe paper received contrasting reviews, 2 positive (7 and 6) and 2 below threshold (5 and 5). R2, R3 and R4 raised similar issues, especially regarding the experimental validation, which is the main shortcoming of the work: addressing \"simple\" datasets only, no comprehensive comparative analysis only in relation to baselines (vanilla SGD) but not in relation to state-of-the-art methods, possibly slightly revised to accomplish the experimental protocol proposed in this work (authors claim that the originality of the work do not allow a proper comparison with SoA method as-is). Indeed, I deem R3's rating (6) a bit overestimated given the provided comments. \nAC does not see an issue the use of only one sample and no info about target, rather, it'd be interesting to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair, while assessing performance in comparative terms with SoA, to give value to the method also in relation to performance. \n\nUnfortunately the rebuttal did not lead an increase of the ratings, nor to better comments. \nAfter the rebuttal, R2 and R4 still remained below threshold; R1 was also not changing idea, remaining positive, and R3 did not react after rebuttal.\n\nOverall, the AC deems this paper containing interesting contributions, but it is not sufficiently ready to be accepted at ICLR mainly because the experiental validation is not showing a fully convincing evaluation of the proposed approach (see above). \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040402978, "tmdate": 1610473999149, "id": "ICLR.cc/2021/Conference/Paper2071/-/Decision"}}}, {"id": "6Lm59l5MUY", "original": null, "number": 2, "cdate": 1603863380274, "ddate": null, "tcdate": 1603863380274, "tmdate": 1606816376176, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review", "content": {"title": "More comparison in experiments and motivation for this theoretical setting", "review": "The authors propose a theoretical setting for context-agnostic few-shot learning. While I do acknowledge that the background signals should not be involved or impact the predictions, I am concerned why this should have a new setting specific for this case? Can the authors elaborate more on the motivation for this theoretical setting and how this setting can benefit the application in the real world? To me, I appreciate that the synthetic data can provide more supervision for few-shot image classification tasks, while I don't understand why this should set up a new setting here.\n\nIn the experiments, the authors show the comparison to previous methods in Appendix Table 2, 3 , 4, and the proposed Context Agnostic method seems not outperforms previous baselines and sometimes even perform much worse than baselines. For example, 7% lower than GCR on Omniglot. Can the authors explain the  benefits of the proposed model please? Also, I notice the authors mainly use two simple types datasets, traffic signs and hand-written characters, while I am expecting to see the performance on more realistic datasets, such as ImageNet. Can the authors explain why they choose these two types of datasets? Are they easier to separate the context / background ?\n\nAs this paper seems more related to synthetic data, I am curious how synthetic data-based few-shot learning methods perform, such as [a,b]\n\n[a]Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar,\nRogerio Feris, Raja Giryes, and Alex Bronstein. Delta-encoder: an effective sample synthesis\nmethod for few-shot object recognition. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,\npp. 2845\u20132855. 2018.\n\n[b]Jian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu, and Xiaokang Yang. Variational few-shot\nlearning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2019.\n\nAfter rebuttal:\nThanks very much for the detailed response! I do agree with the AC's comment that *I don't see a drawback the use of only one sample and no info about the target, rather I'd like to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair and still be competitive or outperforming them.* Also,  I would recommend the authors to have more experimental validation and resubmission. Thus, I keep my score.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104742, "tmdate": 1606915773434, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2071/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review"}}}, {"id": "JZV1BKedr6E", "original": null, "number": 4, "cdate": 1603954995323, "ddate": null, "tcdate": 1603954995323, "tmdate": 1606769562983, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review", "content": {"title": "How to achieve \"context-agnostic\" characteristic in algorithm 1 and 2?", "review": "This paper proposed a context-agnostic learning approach that combines an object area and a context image (s.t. background image) to generate input synthetic image and train the model as context-independent. The proposed method is made more efficient by including this generation process in the training loop, compared to a exhaustive sampling method that randomly selects from a set of combinations of object areas and context images. When applying this method on the task of traffic sign recognition, character recognition, it provides enhanced performance in a setting where model is trained using synthetic data and evaluated on real-world dataset.\n\nMy concern is the presentation.\n\"Context\" and \"context-agnostic\" are properly defined. However, it is very difficult to understand how to achieve \"context-agnostic\" in algorithm 1 and 2. In my understanding, context bias is corrected by forcing a model trained using the opposite (or other) object class in the same context in the next training iteration. If it is correct, how can the bias be corrected just by training with other objects? Does it affect to reduce ||B(h,c)|| in Definition 3.3? Please clarify it.\n\nIn addition, the effectiveness of the proposed method can be verified with such simple tasks, but it is advisable to add a discussion about more complex tasks such as object detection that requires more diverse contexts. This will be more interesting for many.\n\nI can recognize some technical contributions that can appeal to many ICLR researchers.\n\nAs a minor correction,\nIn Section 3.2, the object error o^ is defined but not used anywhere.\n\n----------------------------------------------------------------------------------------------------------\nI have read the revised manuscript and the comments of all reviewers. For concerns that the experiments are not sufficient to validate the proposed method, I am leaning to the author's rebuttals that the datasets have been chosen to meet the heretical assumptions of context-agnostic learning. The given datasets seem to be sufficient to demonstrate the effectiveness of the proposed learning method.\n\nTherefore, I will not change the rating.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104742, "tmdate": 1606915773434, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2071/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review"}}}, {"id": "BJ9i5HFgSqx", "original": null, "number": 3, "cdate": 1603901676288, "ddate": null, "tcdate": 1603901676288, "tmdate": 1606753934643, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review", "content": {"title": "Interesting paper", "review": "The paper defines the task of context-agnostic learning and proposes an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. They propose to decompose factors contributing to the risk into two, context bias and object error. Based on this interpretation, an algorithm is designed to 'greedily correct bias' while employing adversarial training (or robustness training) for 'local refinement'. The method achieves high accuracy on two synthetic visual tasks, digits and traffic sign classification, when a model is trained using one sample per class from the source domain and tested on an unseen target domain.\n\n+) Theorem 3.1 provides a new view on risk. Risk is decomposed into two factors, context bias and object error. I think this gives new insight to consider the effect of context bias and object modeling on risk separately.\n\n+) The experimental results are impressive because the model is trained using a very limited number of samples (one sample per class from the source domain) but showed high generalization performance on an unseen domain. The proposed method achieves promising performance on two synthetic classification tasks compared to other existing methods for few-shot learning and domain adaptation, which requires more labeled or unlabeled data during training.\n\n-) Their underlying assumption for greedy bias correction is that a classifier learns a strong bias on recent training inputs when taken as contexts. However, if stochastic gradient descent is used for optimization, I think it is unlikely because the model changes continuously. Therefore, it is uncertain how effective this greedy selection strategy can sample contexts with large bias.\n\n-) Relating to the above point, I also have a concern about the experimental validation. In all the experiments, gamma is defined as a function that takes object and context images and outputs their overlap. It is not guaranteed that the proposed heuristic sampling strategy generalizes to other gamma functions.\n\n-) Also, all the experiments are performed for a relatively small number of classes (up to 50), and synthetic images are small iconic images with objects in the center. Although the method shows promising results under this specific setting, it is hard to conclude that the proposed heuristics will generalize other settings, such as when there are more classes, image resolutions are higher, and objects have a larger variation in their appearance. I think evaluation on additional datasets with different characteristics (such as CIFAR-100, Caltech-256, CUB-200) would be necessary.\n\n-) The assumption that one can sample objects and contexts independently may restrict its application.\n\nThough I found this paper proposes an interesting view on risk, I would recommend 'reject' due to concerns stated above.\n\n---\nThanks for the detailed response. I can agree that the observation function gamma in the form of addition can model many noisy signals. However, the argument in the paper that the proposed method works for an arbitrary gamma still lacks experimental validation. So I would like to keep my recommendation. My other concerns have been addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104742, "tmdate": 1606915773434, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2071/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review"}}}, {"id": "nsmWT4LDUKX", "original": null, "number": 8, "cdate": 1605406951218, "ddate": null, "tcdate": 1605406951218, "tmdate": 1606149646052, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "jRpdiZdG87P", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "1/1", "comment": "We thank the reviewer for the insightful questions and comments. Please let us know if there are any points that require further elaboration.\n\n>  The proposed method has a strict restriction: The training sets must have a single synthetic image for each object class with no additional information about the target domain. This may limit the training of the model in some hard conditions.\n\nIn fact, the only requirements for applying greedy bias correction are 1) a decomposition of the input space into object and context spaces, and 2) the ability to sample from the object and context spaces independently. In our heuristic, we suggest drawing samples from the object space uniformly at random; thus, it is straightforward to apply our techniques when there are multiple images (not necessarily synthetic) for each object class. Likewise, there is no restriction on using information from the target domain, though practically speaking, we argue that the ability of our approach to generalize even in absence of any target information is a significant strength of our approach.\n\n> The quality of the paper will be upgraded if the authors further investigate more data augmentation or style transfer methods in related work and ablation study. Since the context and object concepts are similar with the style and content in some GANs and transferring works\n\nThank you for the suggestion! Style transfer does seem conceptually related, though GANs still require a large amount of images to learn, so we don't think they can be applied in the setting of our experiments. Note that we already cite several GAN-based approaches for domain adaptation--if there are any specific suggestions we would be happy to include them in the discussion.\n\n> The experiments seem a little bit weak. The experiments are only conducted on two simple datasets, i.e. GTSRB and MINST.\n\nOur results actually include a third dataset, Omniglot, which is a standard dataset for few-shot learning. This task differs from MNIST in that there are 1623 character classes, each of which is drawn by the same 20 people; the general idea is that Omniglot is about learning many characters over a small set of handwritings, whereas MNIST is about learning a small set of characters over many different handwritings.\n\nThe datasets used for our experiments were selected because they very cleanly satisfy the theoretical assumptions of context-agnostic learning. In particular, we needed to be able to (1) produce uniform samples from the entire object space for training, and (2) have ground-truth segmentation of the object from the context for testing. These characteristics allow us to provide the results and ablation studies showing that, among other results:\n- classifiers trained using our method on synthetic datasets achieve both context-agnostic performance as well as generalization to the corresponding real datasets (Table 1)\n- classifiers trained using vanilla SGD on realistic datasets do not achieve context-agnostic performance (Figure 3)\n\nWe would to emphasize that this second point is the second main contribution of this work, independent of the proposed Greedy Bias Correction. Specifically, we formalize the objective for context agnostic learning (and prove Theorem 3.1 as motivation for our setting). We believe this view on risk delivers a theoretically rigorous framework for analyzing the extent to which classifiers depend on background signals. Indeed, our ablation results show that image classifiers trained using vanilla SGD on real-world images are very brittle to small changes in the background. We note that the images generated when measuring the context agnostic performance are, qualitatively, extremely reasonable (see Figure 7 in Appendix E.2). This is not a trivial result, and we believe our datasets are a good setting for our theoretical contributions.\n\n> The comparison with existing methods is insufficient. There is no representative baseline comparison. Though this setting is novel, we can conduct task by slighting changing existing methods.\n\nWe believe representative baseline comparisons are provided in our ablation studies. Our results include 1) standard data augmentation on our datasets, 2) the bias heuristic without local refinement, 3) local refinement without the bias heuristic.\n\nAppendix D, Full Experimental Results, also reports results from existing methods on the same test sets. They use varying types and amounts of additional data for training; we were unable to identify any existing methods that be applied to our training set. To be explicit, for GTSRB we train using 43 images in total (whereas the standard training set contains 39,209 images). Perhaps the reviewer could elaborate slightly on how to adapt existing methods to this setting?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "xjeqcHvZ8F", "original": null, "number": 9, "cdate": 1605891527828, "ddate": null, "tcdate": 1605891527828, "tmdate": 1605891527828, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "Please, check rebuttals and start discussion if needed", "comment": "Dear Reviewers and Authors,\nThanks for starting the discussion.\n\nReviewers: please, check the rebuttals provided by the authors, verify if they replied properly and you are satisfied.\nPossibly, give further feedback or make questions, only if needed and important for your final evaluation.\nPlease, be accurate and precise in your further requests, so that authors can understand and reply properly and focused.\nEventually, you need to revise your review and report final comments and rating.\n\nAuthors: please, check if there are further clarifications needed by the Reviewers.\nPlease, be focused in your final answers and avoid to ask questions to Reviewers, if not absolutely necessary. \n\nFor All: please, I would avoid a long chat-like discussion, a couple of iterations are affordable on a few specific points to be clarified, but no more.\n\nThanks and best regards\n\nAC\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "-C-5--Y42Op", "original": null, "number": 7, "cdate": 1605406350248, "ddate": null, "tcdate": 1605406350248, "tmdate": 1605406920367, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "NRKBSLkp98a", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "2/2", "comment": "> As this paper seems more related to synthetic data, I am curious how synthetic data-based few-shot learning methods perform, such as [a,b]\n\nThank you for the additional references, which we will be sure to include. [a] does not test on any of our benchmarks, but we note that their method requires large amounts of data for training the GAN. [b] benchmarks using the 20-way 1-shot task for Omniglot and achieves 98% accuracy vs 92% using our approach. However, they use 24000 images to learn the prior distribution, while we train our models from scratch using only a single example of each character in the test set.\n\n[a]Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, and Alex Bronstein. Delta-encoder: an effective sample synthesis method for few-shot object recognition. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2845\u20132855. 2018. \n\n[b]Jian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu, and Xiaokang Yang. Variational few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "NRKBSLkp98a", "original": null, "number": 6, "cdate": 1605406339247, "ddate": null, "tcdate": 1605406339247, "tmdate": 1605406811161, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "6Lm59l5MUY", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "1/2", "comment": "We thank the reviewer for the detailed questions and suggestions. If anything remains unclear, we would be happy to continue the discussion.\n\n> The authors propose a theoretical setting for context-agnostic few-shot learning. While I do acknowledge that the background signals should not be involved or impact the predictions, I am concerned why this should have a new setting specific for this case? \n\nThe notion of a \"background signal\", and the question of whether the background is \"involved or impact the predictions\", are not well-defined. We develop a new setting so that we may speak precisely on these matters.\n\n> Can the authors elaborate more on the motivation for this theoretical setting...\n\nOur theoretical setting is one way to rigorously define the notion of a \"background\" signal by splitting the input space into separate object and context spaces. Theorem 3.1 justifies our chosen formalisms by drawing a link between our setting and the standard notion of risk using the context bias. This permits us to reason about ways to achieve this learning objective, and design experiments to evaluate algorithms for this objective.\n\n> ... and how this setting can benefit the application in the real world? \n\nFor instance, the ablation studies show that classifiers trained using vanilla SGD on realistic datasets are surprisingly brittle to background signals (Table 3). This evaluation is well-defined, because we can measure against the objective of context-agnostic learning. We note that the images generated when measuring the context agnostic performance are, qualitatively, extremely reasonable (see Figure 7 in Appendix E.2). This is not a trivial observation, and we believe this result has significance for any computer vision systems that is deployed in the real world.\n\n> To me, I appreciate that the synthetic data can provide more supervision for few-shot image classification tasks, while I don't understand why this should set up a new setting here.\n\nFirst, our proposed approach (greedy bias correction) is an independent contribution from the theoretical setting (and Theorem 3.1), which defines the learning objective independently of any solution. Second, though our experiments use synthetic data (and also can be seen as a type of extremely limited few-shot learning), these are not inherent aspects of the algorithm.\n\n> In the experiments, the authors show the comparison to previous methods in Appendix Table 2, 3 , 4, and the proposed Context Agnostic method seems not outperforms previous baselines and sometimes even perform much worse than baselines. For example, 7% lower than GCR on Omniglot. Can the authors explain the benefits of the proposed model please? \n\nThere are several benefits to our approach. The first is that we use significantly less data: only one sample of each class. For instance, our dataset for GTSRB consists of 43 images in total (whereas the standard training set contains 39,209 images). For Omniglot, we train using only the 20 (or 5) images in the test set. To the best of our knowledge, no other approach can be applied to this setting: both domain adaptation and few-shot learning (including GAN-based approaches) still require large amounts of training data. Thus, while a handful of other approaches achieve better test performance, our results are not directly comparable as the baseline approaches do so using significantly more data. \n\nSecond, we note in the ablation studies that our models exhibit much higher resilience to background signals than models trained using standard supervised methods (Figure 3). As the existing baselines for few-shot learning and domain adaptation are not trained for the context agnostic objective, we believe they would suffer similarly.\n\n> Also, I notice the authors mainly use two simple types datasets, traffic signs and hand-written characters, while I am expecting to see the performance on more realistic datasets, such as ImageNet. Can the authors explain why they choose these two types of datasets?\n\nWe selected datasets for which we could produce \"ground truth\" samples from the object space. This allows us a clean setting for both training our models, as well as measuring the context-agnostic performance. Note that none of the experiments in the paper use foregrounds separated from pre-existing training data; rather, we start with a single synthetic example of each class (e.g., for MNIST, we use a digit font; for GTSRB, we use the official, high-resolution images of each sign). \n\n> Are they easier to separate the context / background ?\n\nAs far as we know, automatic segmentation is an open problem and current approaches still suffer from inaccuracies and artifacts. Thus we chose to avoid this approach for our experiments."}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "zoDr5PfzNM", "original": null, "number": 4, "cdate": 1605406187283, "ddate": null, "tcdate": 1605406187283, "tmdate": 1605406266464, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "BJ9i5HFgSqx", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "1/2", "comment": "We thank the reviewer for the detailed questions. Please see our responses below, and we would appreciate follow up discussions if anything remains unclear.\n\n> Their underlying assumption for greedy bias correction is that a classifier learns a strong bias on recent training inputs when taken as contexts. However, if stochastic gradient descent is used for optimization, I think it is unlikely because the model changes continuously. Therefore, it is uncertain how effective this greedy selection strategy can sample contexts with large bias.\n\nThe greedy bias correction heuristic aims to solve an optimization objective which is computational intractable in general, namely, identifying the context with the highest bias. We do not claim that our heuristic successfully identifies contexts with the \"largest\" bias, but we believe our experimental results support that our heuristic identifies contexts of \"sufficiently large\" bias. First, as noted we achieve good performance on real-world datasets, and the ablation studies show our heuristic gives much better generalization than randomly sampling from the context space. Second, in Figure 3 we validate the empirical context bias of various classifiers by fixing the foreground and varying the background. In particular, initialization using our bias heuristic gives a much stronger attack than a random initialization, even when the attack is unbounded, suggesting that the contexts returned by the bias heuristic are indeed closer to the optimum. \n\nRegarding the interaction of our heuristic with SGD, the contexts are continuously updated along with the model, so we're not sure why SGD should present a particular challenge (perhaps the reviewer can clarify this point?)\n\n> Relating to the above point, I also have a concern about the experimental validation. In all the experiments, gamma is defined as a function that takes object and context images and outputs their overlap. It is not guaranteed that the proposed heuristic sampling strategy generalizes to other gamma functions.\n\nThe reviewer raises a good point about the observation function being similar across experiments. We do not think this is a significant weakness however, as many types of noise manifest additively in the signal, particularly noise which arises from natural observations (e.g., the background of images, noise in audio clips, measurement noise for sensors, etc.). For instance, the Kalman filter is another example of a model that assumes additive noise, but this has not prevented it from being widely used (perhaps most famously in the navigation systems of manned missions to the moon).\n\n> Also, all the experiments are performed for a relatively small number of classes (up to 50), and synthetic images are small iconic images with objects in the center. Although the method shows promising results under this specific setting, it is hard to conclude that the proposed heuristics will generalize other settings, such as when there are more classes, image resolutions are higher, and objects have a larger variation in their appearance. I think evaluation on additional datasets with different characteristics (such as CIFAR-100, Caltech-256, CUB-200) would be necessary.\n\nThe datasets used for our experiments were selected because they very cleanly satisfy the theoretical assumptions of context-agnostic learning. In particular, we needed to be able to (1) produce uniform samples from the entire object space for training, and (2) have ground-truth segmentation of the object from the context for testing. These characteristics allow us to provide the results and ablation studies showing that, among other results:\n- classifiers trained using our method on synthetic datasets achieve both context-agnostic performance as well as generalization to the corresponding real datasets (Table 1)\n- classifiers trained using vanilla SGD on realistic datasets do not achieve context-agnostic performance (Figure 3)\n\nWe would to emphasize that this second point is the second main contribution of this work, independent of the proposed Greedy Bias Correction. Specifically, we formalize the objective for context agnostic learning (and prove Theorem 3.1 as motivation for our setting). We believe this view on risk delivers a theoretically rigorous framework for analyzing the extent to which classifiers depend on background signals. Indeed, our ablation results show that image classifiers trained using vanilla SGD on real-world images are very brittle to small changes in the background. We note that the images generated when measuring the context agnostic performance are, qualitatively, extremely reasonable (see Figure 7 in Appendix E.2). This is not a trivial result, and we believe our datasets are a good setting for our theoretical contributions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "0b-wjCDwvTa", "original": null, "number": 5, "cdate": 1605406198927, "ddate": null, "tcdate": 1605406198927, "tmdate": 1605406198927, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "zoDr5PfzNM", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "2/2", "comment": "> The assumption that one can sample objects and contexts independently may restrict its application.\n\nOur technique for context-agnostic learning is immediately applicable to synthetic data pipelines, which is widely used in, e.g., autonomous driving and robotics (see [1] for a survey).\n\n[1] Sergey I. Nikolenko, Synthetic Data for Deep Learning. https://arxiv.org/abs/1909.11512"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "gK4bX2wDFrC", "original": null, "number": 3, "cdate": 1605406100219, "ddate": null, "tcdate": 1605406100219, "tmdate": 1605406100219, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "JZV1BKedr6E", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment", "content": {"title": "1/1", "comment": "We thank the reviewer for the suggestions. Please let us know if the following responses leave any concerns unaddressed.\n\n> My concern is the presentation. \"Context\" and \"context-agnostic\" are properly defined. However, it is very difficult to understand how to achieve \"context-agnostic\" in algorithm 1 and 2. In my understanding, context bias is corrected by forcing a model trained using the opposite (or other) object class in the same context in the next training iteration. If it is correct, how can the bias be corrected just by training with other objects? Does it affect to reduce ||B(h,c)|| in Definition 3.3? Please clarify it.\n\nThe rationale behind our approach is covered at the end of Section 3.1. The naive method for controlling the bias with respect to Theorem 3.1 would be: for each training sample use the context of the current maximum bias, i.e., argmax_c ||B(h,c)||. Unfortunately, as we note, this optimization problem is computationally intractable in general. Our heuristic is based on the observation that the most recent training sample can be repurposed as a context of large bias for the next training sample. \n\nWe believe our experiments support this heuristic. First, we are able to generalize from synthetic to several real-world datasets, and our ablation studies show that this heuristic contributes significantly to achieving such performance (Figure 1). We also compare our classifiers against training directly on real-world data, and show that our classifiers are far more robust to background perturbations than the classifiers trained using real-world data, i.e., our method does indeed succeed in producing classifiers which are empirically context-agnostic (Figure 3). This evaluation further demonstrates that initialization using our bias heuristic gives a much stronger attack than a random initialization, even when the attack is unbounded, suggesting that the contexts returned by the bias heuristic are indeed closer to the optimum. \n\n> In addition, the effectiveness of the proposed method can be verified with such simple tasks, but it is advisable to add a discussion about more complex tasks such as object detection that requires more diverse contexts. This will be more interesting for many.\n\nObject detection is an interesting extension. For now, we have focused on classification because it makes the theoretical development cleaner, and we felt it was important that both the setting and results be easily interpretable. However the suggestion to include a section discussing more complex tasks is well-received and we will use a portion of the additional page limit to this.\n\n> As a minor correction, In Section 3.2, the object error o^ is defined but not used anywhere.\n\nThe object error is used in the statement of Theorem 3.1--we will address this in our revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_Tf6jEzbH9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2071/Authors|ICLR.cc/2021/Conference/Paper2071/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852627, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Comment"}}}, {"id": "jRpdiZdG87P", "original": null, "number": 1, "cdate": 1603816315500, "ddate": null, "tcdate": 1603816315500, "tmdate": 1605024296392, "tddate": null, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "invitation": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review", "content": {"title": "The paper proposes an interesting setting for learning, which relies on the ability to sample from the object and context spaces independently. This paper introduces a novel idea showing promising results in several benchmarks. The authors give a theoretical analysis for their method which is convincing. Extensive empirical results show that the method produces good results both in domain adaptation and few-shot learning settings.", "review": "Strengths:\n\n1. This paper studies a theoretical setting for learning models whose predictions are independent of background signals. There are indeed some practical applications of this setting.\n\n2. The proposed approach is very simple, yet effective. This method is able to learn a context-agnostic model by minimizing a formally defined notion of context bias. \n\n3. On the theorical side, the authors try to explain their goal of classification as learning to extract reliable signals.\n\n4. The paper is well written and easy to read. Also, the figures in supplementary materials help understand the paper, especially the context learning part.\n\nWeaknesses/concerns:\n\n1. The proposed method has a strict restriction: The training sets must have a single synthetic image for each object class with no additional information about the target domain. This may limit the training of the model in some hard conditions.\n\n2. The quality of the paper will be upgraded if the authors further investigate more data augmentation or style transfer methods in related work and ablation study. Since the context and object concepts are similar with the style and content in some GANs and transferring works.\n\n3. The experiments seem a little bit weak. The experiments are only conducted on two simple datasets, i.e. GTSRB and MINST.\n\n4. The comparison with existing methods is insufficient. There is no representative baseline comparison. Though this setting is novel, we can conduct task by slighting changing existing methods.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2071/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2071/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-Agnostic Learning Using Synthetic Data", "authorids": ["~Charles_Jin1", "~Martin_Rinard1"], "authors": ["Charles Jin", "Martin Rinard"], "keywords": ["machine learning", "synthetic data", "few-shot learning", "domain adaptation"], "abstract": "We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. Given the ability to sample from each set independently, we present an algorithm that learns a classifier over the input domain more efficiently than sampling from the input domain directly. We apply this setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single example of each class. On several standard benchmarks for real-world image classification, our approach achieves performance competitive with state-of-the-art results from the few-shot learning and domain transfer literature, while using significantly less data.", "one-sentence_summary": "We develop a new setting for learning in which we train image classifiers from scratch using only a single synthetic example per class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|contextagnostic_learning_using_synthetic_data", "pdf": "/pdf/e3b4ee799f4319181ba867ebfe10ec2f6f90017d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LK8fpSwv2o", "_bibtex": "@misc{\njin2021contextagnostic,\ntitle={Context-Agnostic Learning Using Synthetic Data},\nauthor={Charles Jin and Martin Rinard},\nyear={2021},\nurl={https://openreview.net/forum?id=_Tf6jEzbH9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_Tf6jEzbH9", "replyto": "_Tf6jEzbH9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2071/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104742, "tmdate": 1606915773434, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2071/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2071/-/Official_Review"}}}], "count": 13}