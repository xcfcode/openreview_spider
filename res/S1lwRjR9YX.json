{"notes": [{"id": "BkggqZzYdN", "original": null, "number": 1, "cdate": 1553699208400, "ddate": null, "tcdate": 1553699208400, "tmdate": 1553699208400, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Public_Comment", "content": {"comment": "Collectible cars have always been and remain an excellent gift for both a child and an adult. For example, if your friend dreams of a particular make or model of car, or perhaps he has one, then why not give him a scale model of his favorite car?\n\nMany collectible models https://www.bestadvisor.com/car-model-kits have a metal case and even prefab models are no exception. Most models open doors, luggage compartment, hood, and even turns the steering wheel in the cabin. All these possibilities depend on the specific model of the souvenir machine and its features. Large-scale collectibles such as 1:18 or 1:24 always have a lot of extra features. And for 1:43 or 1:34 scale models, only the doors are usually opened.", "title": "carmodel"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311725449, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1lwRjR9YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311725449}}}, {"id": "S1lwRjR9YX", "original": "rkeAAJ0tFm", "number": 901, "cdate": 1538087886690, "ddate": null, "tcdate": 1538087886690, "tmdate": 1545355412946, "tddate": null, "forum": "S1lwRjR9YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gJUIiRy4", "original": null, "number": 1, "cdate": 1544627783202, "ddate": null, "tcdate": 1544627783202, "tmdate": 1545354501468, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Meta_Review", "content": {"metareview": "The paper according to Reviewers needs more work for publication and significantly more clarifications. The Reviewers are not convinced on publishing even after intensive discussion that the AC read in full. The AC recommends further improvements on the paper to address better Reviewer's concerns.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Paper needs more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper901/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353043027, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353043027}}}, {"id": "rJxdSh34yV", "original": null, "number": 20, "cdate": 1543978048398, "ddate": null, "tcdate": 1543978048398, "tmdate": 1543978170916, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "SJet7-5TC7", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer3 Comments ", "comment": "Dear reviewer,\n\nRegarding the comparison with (Jain et al., 2018), we should clarify that only linear regression problem with quadratic loss function has been studied in (Jain et al., 2018), while we consider a general strongly-convex loss function. In addition, our generalization bound is based on uniform stability, which is not the case in (Jain et al., 2018). Hence, we do not find a strong connection between the results of (Jain et al., 2018) and those of our paper. \n\nWe emphasize that the claim that \"for problems with larger condition number, the momentum should approach to one\" is indeed based on *convergence* analysis of GD with momentum. It does not account for *generalization*. In addition, those recommended parameters are not necessarily optimal for SGD. In our view, what mainly matters is extending the results of (Hardt et al., 2016) to SGMM and showing that there exists some mu for which SGMM satisfies uniform stability, i.e., our machine learning model can be trained for multiple epochs and still generalizes. We have verified the trends predicted by our stability bounds using experimental evaluations. Those evaluations are based on common machine learning models leading to a smooth and strongly convex loss function.\n\nBy asymptotic, we mean that the problem structure imposes very large *kappa*. As explained before, the gamma parameter can be tuned by adjusting a weight decay regularization parameter in a typical machine learning model. We used 3.5 merely as an example to show that there exists mathematical problems for which the suggested momentum based on the convergence analysis of GD falls within the interval specified by Theorem 2 in our paper. We do not claim that kappa=3.5 necessarily represents practical problems in machine learning. Please note that even for the original work of  (Hardt et al., 2016), which analyzes the stability of SGD without momentum, there are some conditions on the learning rate in the theorem statements to satisfy the uniform stability, i.e., unlike the convergence analysis, the stability bounds typically involve limitations on the range of hyper-parameters. We further emphasize that our theorem for convergence analysis (Theorem 3) does not have any limitation on mu.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HJlGj7UR0Q", "original": null, "number": 19, "cdate": 1543558041976, "ddate": null, "tcdate": 1543558041976, "tmdate": 1543558041976, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "Skl6ykATA7", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Thank you for your response.", "comment": "I am convinced that proofs are correct and assumptions are also reasonable."}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "Skl6ykATA7", "original": null, "number": 18, "cdate": 1543524068591, "ddate": null, "tcdate": 1543524068591, "tmdate": 1543524068591, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "HkgEmbtaA7", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer1 Comments ", "comment": "Dear reviewer,\n\nThank you for your comments and pointing out the typos.\n\nRegarding Proposition 1, please note that in our proof (Page 2 in the \nsupplementary document of the original submission, which can be accessed \nby clicking \"show revisions\"), we have first shown (as stated in \nLemma 2) that the stability bound holds for the average of ${w_t}$. \nTherefore, we believe that Proposition 1 is correct.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HkgEmbtaA7", "original": null, "number": 16, "cdate": 1543504155750, "ddate": null, "tcdate": 1543504155750, "tmdate": 1543512368696, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "Skl1MikYnX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Additional comments", "comment": "As for the convergence theorem, I think the proof of Theorem 3 for projected SGMM seems correct, but I found another small bug that I did not notice when I read for the first time.\nProposition 1 cannot be obtained by Theorem 2 and 3 directly because the stability bound is given for the latest parameter $w_t$ while the convergence is guaranteed for the average of ${w_t}$.\nThus, it would be nice if the authors could fix it.\n\n[Minor typos]\n- \"Since \\|.\\| is a convex function\" -> \"Since \\|.\\|^2 is a convex function\".\n\n- RHS of Equation after \"Note that the LHS of (25) ...\" in p. 6:\n1-\\mu -> (1-\\mu)^2,\n\\| ... \\| -> \\| ... \\|^2."}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "SJet7-5TC7", "original": null, "number": 17, "cdate": 1543508256698, "ddate": null, "tcdate": 1543508256698, "tmdate": 1543508256698, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "SyenUHNcA7", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "response", "comment": "Thank you for your response. \n\nFirstly, please note that in Kidambi et al (2018), the paper presents the fact that SGD + momentum (for any value of the learning rate + momentum tuple) does not outperform vanilla SGD (with mu=0) by more than a constant factor (i.e. there is no asymptotic running time improvements in the big-Omega/big-Oh notation). That said, momentum + SGD can never be worse than vanilla SGD since we can always set mu=0 in momentum methods to recreate SGD's behavior.\n\nWith regards to values of momentum for generalization: There are generalization results for acceleration with stochastic gradient methods. In particular, Jain et al. \"accelerating stochastic gradient descent for least squares regression\" 2017 present (for the strongly convex least squares problem) a result that admits a similar flavor - where, momentum approaches 1 for harder problems (large condition number) than ones for easier problems (low condition number). \n\nAt the end of the day, the trend offered by both deterministic and/or stochastic accelerated methods for easy (low condition number) vs. hard (large condition number) problems is what matters: for harder problems with large condition number, the momentum approaches 1, but, for the bounds admitted by the paper, the momentum approaches zero, which is rather worrying. This basically implies that this paper's theory, while beginning to make progress on this problem, does not provide a reasonable guarantee to characterize what happens when SGD is used with momentum.\n\nAnd a condition number of 3.5? A typical (easy) machine learning problem has a condition number around O(10^3) or more. Harder (and more typical) ones that have many correlated features have a condition number that is roughly 10^5 or more with greater correlations across features leading to worsening of the condition number. A condition number of 3.5 is trivial (from an optimization standpoint) - gradient descent requires roughly 10-20 steps to converge on this problem. A condition number of 3.5 implies that there is close to little advantage of kappa versus sqrt(kappa), which is the advantage offered by acceleration. Finally, in order to make the condition number 3.5, one would have to regularize the problem too strongly in a way that the solution will very well generalize far worse than solving the erm problem.\n\nAgain, I don't understand the claim of why a smaller kappa resembles \"non-asymptotics\". Precisely put, there is *no* relation of kappa versus asymptotics.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "B1e21iAhA7", "original": null, "number": 14, "cdate": 1543461604237, "ddate": null, "tcdate": 1543461604237, "tmdate": 1543461687716, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "HJlL1YjoCm", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer2 Comment", "comment": "Our problem setting involves constrained optimization where we seek the optimal solution within a compact set. This constraint is assumed to be given apriori in the problem definition.  Such setting have been widely considered in the literature. See for example:  (Hardt et al., 2016)[Section 3.4]).  \n\nPlease note that we do not address the unconstrained optimization problem that you mention in your response. Thus we do not need to design the compact set that increases the chance of the compact set containing the optimal solution.\n\nWe hope this clarifies our problem setup and you are convinced by the technical soundness of our work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HJlL1YjoCm", "original": null, "number": 13, "cdate": 1543383262164, "ddate": null, "tcdate": 1543383262164, "tmdate": 1543383262164, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "ryemHZssRm", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response", "comment": "Yes, it should hold for projection steps in (A2) and (A3). But even so, your constant L must depend on the radius of the compact set of the parameters.  How could you guarantee the optimal solution of the strongly convex problem is always in that compact set? In order to increase \"the chance\" of the compact set containing the optimal solution, you need to \"increase\" the size of the compact set, which implicitly pushes L be arbitrarily large. Therefore, it is only possible if you just find the solution from the small compact set (from which the optimal solution may be very far).  \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "ryemHZssRm", "original": null, "number": 11, "cdate": 1543381307101, "ddate": null, "tcdate": 1543381307101, "tmdate": 1543381307101, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "HyeCCsdoRm", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer2 Comment", "comment": "Dear reviewer,\n\nThe supplementary document was submitted along with the original \nsubmission. It can be found by clicking the \"Show revisions\" link below \nthe paper title.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HyeCCsdoRm", "original": null, "number": 9, "cdate": 1543371733983, "ddate": null, "tcdate": 1543371733983, "tmdate": 1543371733983, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "rJxsFK_iRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "response", "comment": "I am not sure where I could find the supplementary document as you said. The pdf file only contains 9 pages. "}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "rJxsFK_iRQ", "original": null, "number": 8, "cdate": 1543371139316, "ddate": null, "tcdate": 1543371139316, "tmdate": 1543371584619, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "ryeWFpGiCX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer2 Comment", "comment": "Dear reviewer,\n\nPlease note that inequalities (A.2) and (A.3) (shown in the proof of  Lemma 1 in the supplementary document) hold for the projected SGMM update (5) because Euclidean projection does not increase the distance  between projected points. We are quite certain that our proof is  correct, since our approach to handle projection is a commonly used  technique in existing work. For example, it is used in (Hardt et al., 2016)[Section 3.4]. "}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "ryeWFpGiCX", "original": null, "number": 7, "cdate": 1543347576545, "ddate": null, "tcdate": 1543347576545, "tmdate": 1543347576545, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "HkxhFKzs07", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response", "comment": "Dear author(s), \n\nYou are using a stochastic algorithm. There is nothing guarantee that all of your updated iterations are in compact set. It is true that you consider projected step as in (5). However, your proof in Lemma 1 from the beginning to (14), you are using without projection. It would damage your bound in (14) as I mentioned before that L could be arbitrarily large. Your explanation in the last paragraph of Lemma 1 is not convincing. In order to fix this issue, you may need to consider your derivation with projected step from the beginning of the proof. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HkxhFKzs07", "original": null, "number": 6, "cdate": 1543346563723, "ddate": null, "tcdate": 1543346563723, "tmdate": 1543346563723, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "SkglZ4I90Q", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer2 Comments ", "comment": "Dear reviewer, \n\n- To clarify the correctness of our proof, please note that L in the theorem statement and the proof is bounded due to compactness of the parameter space. We have shown that (19) holds for projected SGMM. Before (19), we have not used L-Lipschitz. \n\n- Regarding convergence analysis, please note that our goal is to find a *global* convergence bound not a \"local\" one. We know that classical results show that heavy ball momentum achieves linear convergence rate locally. However, those results are for batch gradient descent not stochastic gradient descent for a general strongly-convex function. "}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "SkglZ4I90Q", "original": null, "number": 5, "cdate": 1543295992115, "ddate": null, "tcdate": 1543295992115, "tmdate": 1543295992115, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "SkewoU49R7", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to authors", "comment": "Dear author(s), \n\nThank you for your response! \n\n1. The set of assumptions (smooth, Lipschitz and strongly convex) is not valid on the whole set R^d,  for example quadratic function. Your L may be arbitrarily large and your bound in (14) could be damaged. I do not think you can properly apply the projection step here after deriving (14) in case of L -> \\infty. \n\n2. I was asking about the linear convergence to \"neighborhood\", not to the \"optimal solution\". Your theory seems not able to cover this case. "}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "SkewoU49R7", "original": null, "number": 4, "cdate": 1543288478886, "ddate": null, "tcdate": 1543288478886, "tmdate": 1543288478886, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "SJx-bmHbnm", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer2 Comments ", "comment": "R2C1: We believe that our results are substantial and important. Our analysis involves some subtle but important steps in dealing with the momentum term in the recursion in Section 4. This method was not conceived in prior attempts on this problem. We reproduce the following statement from [Hardt et al., Section 7]: ``One very important technique that we did not discuss is momentum. However, it is not clear that momentum adds stability. It is possible that momentum speeds up training but adversely impacts generalization.'' Our work is the first successful attempt that establishes that SGMM generalizes, for the practically important class of strongly convex loss functions.\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nR2C2: Please note that we first discuss the proofs without projection to keep the notation uncluttered. We then explain how the proofs can be modified to accommodate projection. We believe this approach is technically sound, and it helps the readers to better understand the insights in our proofs. We respectfully request that the reviewer point out any specific issue in our proofs such that we can fix it.\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nR2C3: To the best of our understanding, linear convergence happens under a very stringent condition: $\\Pr\\{\\nabla f_i(x^*)=0\\}=1$, \\ie $x^*$ is a simultaneous minimizer of (almost) all $f_i(x^*)$ [Needell et al. , 2014] . Such a condition would artificially force that the loss function be simultaneously minimized on each training example. In absence of this condition, SGD appears to exhibit similar convergence rate as our paper, albeit under somewhat different assumptions on the loss function.\n\nMoreover, in terms of convergence, we cannot claim that SGMM always outperforms SGM without momentum. For example, in [Kidambi et al. , 2018], the authors show that there exists linear regression problems for which SGM outperforms SGMM in terms of convergence for any learning rate and momentum parameter. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "HJxr0SV90m", "original": null, "number": 3, "cdate": 1543288269255, "ddate": null, "tcdate": 1543288269255, "tmdate": 1543288269255, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "Skl1MikYnX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer1 Comments", "comment": "R1C1: Our analysis involves some subtle but important steps in dealing with the momentum term in the recursion in Section 4. This method was not conceived in prior attempts on this problem. To convince you, we reproduce the following statement from [Hardt et al., Section 7]: ``One very important technique that we did not discuss is momentum. However, it is not clear that momentum adds stability. It is possible that momentum speeds up training but adversely impacts generalization.'' Our work is the first successful attempt that establishes that SGMM generalizes, for the practically important class of strongly convex loss functions."}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "SyenUHNcA7", "original": null, "number": 2, "cdate": 1543288148187, "ddate": null, "tcdate": 1543288148187, "tmdate": 1543288148187, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "r1lvWTHc2X", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "content": {"title": "Response to AnonReviewer3 Comments", "comment": "R3C1: Please note that the theoretically advocated momentum parameters mu = (sqrt(kappa)-1)/(sqrt(kappa)+1) [Nesterov, 1983] or mu = ( (sqrt(kappa)-1)/(sqrt(kappa)+1) )^2 [Polyak,1964] are based on the *convergence* analysis of GD with momentum -- they do not account for *generalization*. Therefore, these values are not necessarily optimal for SGD with momentum (SGMM), in terms of our objective of true risk. In Theorem 2, our focus is to find a bound on stability, i.e., the condition on generalization. Our theorem for convergence analysis (Theorem 3) does not have any limitation on mu.\n\nOur goal in Theorem 2 is to find the tightest possible bound that shows why machine learning models can be trained for multiple epochs of SGMM while their generalization errors are limited. In order to satisfy uniform stability for SGMM (with constant momentum), we need to have a recursion with coeff<1. Even ignoring the third term in the RHS of (12), we still have to assume mu<1/kappa in order to have such a recursion. \n\nWe agree theoretically suggested momentum parameters for GD approach 1 \u2013 1/sqrt(kappa), which grow close to one as kappa grows large. On the other hand, as our concern in this work is on gamma-strongly convex loss functions, where the gamma parameter can be tuned by adjusting a weight decay regularization parameter in a typical machine learning model, it is indeed important and interesting to study the generalization bound when kappa is not too large, i.e., the non-asymptotic regime. When kappa is not too large, the range specified in our theorem captures a typical value of the suggested momentum based on convergence analysis of GD. As an example, if we set kappa = 3.5, then ((sqrt(kappa)-1)/(sqrt(kappa)+1))^2\u22480.1, which is approximately 1/(3*kappa). \n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nR3C2: In the original submission, we specified the condition mu*(beta+gamma)<<alpha*beta*gamma in the supplementary document. In the revision, we have explicitly provided this condition it in the proposition statement. Please note that this condition is used only to make tractable the optimization of the expected true risk over alpha. In practice, we can still use alpha as specified in Proposition 1. However, it will not necessarily optimize the upper-bound on the expected true risk.\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nR3C3: Please note that although [1]-[3] study first-order methods with noisy (imperfect) gradients, none of these works study generalization of SGD for a strongly convex loss function using algorithmic stability. We note that both [1] and [2] are cited in [3]. In the revision, we have added the following sentence to our introduction: \"First-order methods with noisy gradient are studied in [Kidambi et al. , 2018] and references therein. In [Kidambi et al. , 2018], the authors show that there exists linear regression problems for which SGM outperforms SGMM in terms of convergence.\"\n\nRegarding comparison with [Loizou et al. , 2018], please note that [Loizou et al. , 2018] considers the special case of a convex *quadratic* loss function of a least-squares type, while we consider the general case of strongly convex loss functions. Furthermore, we emphasize that we do not limit our analysis of SGMM to super large batch sizes. Our analysis indeed works even for a batch size of one.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608448, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lwRjR9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper901/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper901/Authors|ICLR.cc/2019/Conference/Paper901/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers", "ICLR.cc/2019/Conference/Paper901/Authors", "ICLR.cc/2019/Conference/Paper901/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608448}}}, {"id": "r1lvWTHc2X", "original": null, "number": 3, "cdate": 1541197054879, "ddate": null, "tcdate": 1541197054879, "tmdate": 1541533594665, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "content": {"title": "Interesting question and direction", "review": "This paper presents an analysis of generalization error of SGD with multiple passes for strongly convex objectives using the framework of algorithmic stability [Bousquet and Elisseef, 2002] and its recent use to analyze generalization error of SGD based methods [Hardt, Recht and Singer 2016].\n\nThe problem considered by this work is interesting and raises the possibility of understanding generalization related questions of SGD style methods when augmented with momentum, which is common practice in Deep Learning [Sutskever et al. 2013]. That said, there are some concerns about the results as presented in this paper, which I will elaborate below:\n\n- Consider the stability bound admitted by theorem 2: The special case (similar to theorem 3.9 of Hardt et al 2016) when the learning rate alpha = 1/beta (which is the typical learning rate that theory advocates), and setting kappa = beta/gamma where kappa is the condition number of the problem, leads to the following bound on momentum allowed by theorem 2, which is:\n\n(something non-positive) <= mu < 1/(3*kappa). \n\nThis is basically the regime where momentum does not make any difference towards accelerating optimization. Referring to the standard value of momentum for strongly convex functions, we see that the momentum is set as mu = (sqrt(kappa)-1)/(sqrt(kappa)+1) [Nesterov, 1983], or, mu = ( (sqrt(kappa)-1)/(sqrt(kappa)+1) )^2  [Polyak,1964]. Upon simplification of this standard momentum values, we see that mu \\approx 1 - 1/sqrt(kappa) which grows close to one as kappa grows large. On the other hand, the momentum values admitted by the paper for their bound is super tiny (which gets to zero as the condition number kappa grows large). This essentially implies there is not much about momentum that is captured by the bound of theorem 2 since there is no characterization of the provided bound for theoretically advocated and practically used parameters for momentum.\n\n- In proposition 1, there is no quantitative description of what \"sufficiently small\" mu (momentum parameter) is - this statement is imprecise. As mentioned in the previous point, sufficiently small mu really is not descriptive of momentum parameters employed in practice (mu in practice typically is >= 0.9). For strongly convex objectives, this should be closer to 1- (1/sqrt(kappa)). Sufficiently small mu parameter essentially does not yield quantitatively different behaviors compared to standard SGD. \n\n\nIn summary, while this paper attempts to make progress on an interesting question, but falls short and doesn't really capture the behavior of these methods that is even mildly reflective of practice (even in terms of the parameter regimes admitted by the bounds proven in the theorems).\n\n- This paper does not perform a thorough literature survey of published results. Furthermore, this paper does not present a precise treatment of assumptions (and implications) amongst other works cited in the paper (see for e.g. [4] below). \n\n[1] Polyak (1987) presents (generalization) behavior of Heavy Ball momentum with noisy (inexact) gradients.\n[2] Several efforts in Signal Processing literature do consider the similar setting as one considered by this paper, which is that of Heavy Ball (called accelerated LMS) method with noisy gradients: refer to Proakis (1974), Roy and Shynk (1990), Sharma et al. (1998). \n[3] Kidambi et al (2018) estimate the \"optimization\" power (which is a part of characterization of generalization error [Bach and Moulines 2011], since this dominates at the start of optimization) of HB method with Stochastic Gradients and prove that HB+stochastic gradients does not offer any speedup over vanilla SGD.\n[4] Loizou and Richtarik provide an analysis of stochastic heavy ball with super large batch sizes (so they end up showing accelerated rates) under similar assumptions as considered by this paper, such as assuming the function is smooth and strongly convex. However, the paper dismisses the work of Loizou and Richtarik to be working with a different set of assumptions - this is not really true.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "cdate": 1542234350957, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335828920, "tmdate": 1552335828920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Skl1MikYnX", "original": null, "number": 2, "cdate": 1541106438792, "ddate": null, "tcdate": 1541106438792, "tmdate": 1541533594425, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "content": {"title": "Review", "review": "This paper studies the algorithmic stability of SGD with momentum and provides an upper-bound on true risk through convergence analysis.\nThis bound clarifies dependencies of convergence speed on the size of dataset and the momentum parameter.\n\nThe presentation is easy to follow and technically sounds good.\nSGD with momentum is heavily used for learning linear models and deep neural networks, hence to analyze its convergence behavior is quite important.\nThis paper achieves this goal well by extending a previous result on vanilla SGD in a straightforward manner, although it is not technically difficult.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "cdate": 1542234350957, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335828920, "tmdate": 1552335828920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJx-bmHbnm", "original": null, "number": 1, "cdate": 1540604665033, "ddate": null, "tcdate": 1540604665033, "tmdate": 1541533594203, "tddate": null, "forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "content": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "review": "Comments: \n\nThe author(s) provide stability and generalization bounds for SGD with momentum for strongly convex, smooth, and Lipschitz losses. \n\nThis paper basically follows and extends the results from (Hardt, Recht, and Singer, 2016). Section 2 is quite identical but without mentioning the overlap from Section 2 in (Hardt et al, 2016). The analysis closely follows the approach from there. \n\nThe proof of Theorem 2 has some issues. The set of assumptions (smooth, Lipschitz and strongly convex) is not valid on the whole set R^d, for example quadratic function. In this case, your Lipschitz constant L would be arbitrarily large and could be damaged your theoretical result. To consider projected step is true, but the proof without projection (and then explaining in the end) should have troubles. \n\nFrom the theoretical results, it is not clear that momentum parameter affects positively or negatively. In Theorem 3, what is the advantage of this convergence compared to SGD? It seems that it is not better than SGD. Moreover, if \\mu = 0 and \\gamma > 0, it seems not able to recover the linear convergence to neighborhood of SGD. Please also notice that, in this situation, L also could be large. \n\nThe topic could be interesting but the contributions are very incremental. At the current state, I do not support the publications of this paper. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper901/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions", "abstract": "While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work, we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk,  in terms of the number of training steps, the size of the training set, and the momentum parameter.", "keywords": ["Generalization Error", "Stochastic Gradient Descent", "Uniform Stability"], "authorids": ["aramezani@ece.utoronto.ca", "akhisti@ece.utoronto.ca", "liang@ece.utoronto.ca"], "authors": ["Ali Ramezani-Kebrya", "Ashish Khisti", "and Ben Liang"], "TL;DR": "Stochastic gradient method with momentum generalizes.", "pdf": "/pdf/5c803bc14ce64346e8cddcb134f7f8c0f18a3578.pdf", "paperhash": "ramezanikebrya|stability_of_stochastic_gradient_method_with_momentum_for_strongly_convex_loss_functions", "_bibtex": "@misc{\nramezani-kebrya2019stability,\ntitle={Stability of Stochastic Gradient Method with Momentum for Strongly Convex Loss Functions},\nauthor={Ali Ramezani-Kebrya and Ashish Khisti and and Ben Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lwRjR9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper901/Official_Review", "cdate": 1542234350957, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lwRjR9YX", "replyto": "S1lwRjR9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper901/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335828920, "tmdate": 1552335828920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper901/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 22}