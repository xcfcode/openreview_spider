{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124427979, "tcdate": 1518471174151, "number": 293, "cdate": 1518471174151, "id": "Hk0ZOFkwf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hk0ZOFkwf", "signatures": ["~Kirill_Struminsky1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582963697, "tcdate": 1520112884816, "number": 1, "cdate": 1520112884816, "id": "By6lHq__f", "invitation": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "signatures": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer2"], "content": {"title": "Claim of GANs producing higher entropy is likely wrong", "rating": "3: Clear rejection", "review": "Summary:\nThis paper proposes an estimator for the differential entropy of decoder-based generative models as a means for evaluation. Using their estimator, the authors find that GANs and WGANs have higher entropy than a VAE when evaluated on MNIST and Fashion MNIST (which is likely due to a sign flip).\n\nReview:\nAt a conceptual level, I wonder how useful differential entropy is as a metric for evaluating decoder-based generative models like GANs. Given that GANs are used to learn degenerate distributions (with undefined/negative infinite differential entropy), it is safe to assume that having large differential entropy is not a concern in this line of research.\n\nAlthough I did not follow all the steps of the derivation, the estimator (Equation 3) seems to have a wrong sign, since:\n\u2013\u00a0the entropy seems to decrease with increasing s and increasing gradient norms\n\u2013\u00a0the extremely surprising finding that according to this estimator, GANs produce higher entropy estimates than VAEs\n\u2013\u00a0based on a comparison with the ELBO representation in Equation 2 (which is also missing log)\n\nWhat do you mean by \"differential entropy of a uniform distribution on a half-sphere is equal to log(2pi)\"? I'd expect the differential entropy of a degenerate distribution such as a distribution on a sphere to be undefined/negative infinity.\n\nSimilarly, in 2.2 you provide a formula for a density \"where the image x can lie in a higher-dimensional space [than z].\" Please clarify what you mean by density here, since degenerate distributions don't have probability densities. Are you referring to a more general notion such as Radon-Nikodym derivatives?\n\nMinor:\n\"improper\" -> \"degenerate\" in first paragraph on page 2", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582963458, "id": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper293/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer1"], "reply": {"forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582963458}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582877661, "tcdate": 1520538589568, "number": 2, "cdate": 1520538589568, "id": "ryBJEGytf", "invitation": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "signatures": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer3"], "content": {"title": "I don't think this is accurate", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose a method for estimating the entropy of a generative model from its samples. The resulting estimates do not seem to make sense. The authors claim that VAEs have lower entropy than GANs, which contradicts all earlier literature on this topic. My guess is that this is due to the Taylor approximation used to approximate the posterior of the latents in their method: this will work if the decoder is approximately one to one, which tends to be true for VAEs to a larger extent than for GANs.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582963458, "id": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper293/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer1"], "reply": {"forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582963458}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582582873, "tcdate": 1521325092444, "number": 3, "cdate": 1521325092444, "id": "S13m4MoYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "signatures": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer1"], "content": {"title": "Not confident entropy can be accurately estimated in implicit models", "rating": "5: Marginally below acceptance threshold", "review": "Evaluating generative models, and especially implicit models where the likelihood is intractable, an outstanding problem. The authors propose using entropy as a way to measure sample diversity. Entropy requires calculating an expectation with repect to the distribution's likelihood: they lower bound this quantity using typical variational techniques, and in the case of implicit models, assume a Gaussian likelihood.\n\nThe statistical desiderata is very nice, but unfortunately, practical evaluation is complicated by computational considerations. Like Wu et al. (2016), I'm not confident where this method is applicable because it compounds onto the same problems as likelihood evaluation overall. Namely, GANs do not have such tractable likelihoods, and any evaluation metrics which rely on them should carefully note limitations in assumptions that do in fact assume they're tractable. The scale parameter assumption is verified only on a toy problem two-dimensional half-sphere.\n\nIt's also unclear how the variational lower bound plays a role: model comparison with lower bounds can be highly suspect. Is AIS applicable as in Wu et al. (2016) for tighter bounds?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582963458, "id": "ICLR.cc/2018/Workshop/-/Paper293/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper293/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper293/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper293/AnonReviewer1"], "reply": {"forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper293/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582963458}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573604259, "tcdate": 1521573604259, "number": 257, "cdate": 1521573603918, "id": "rknkkyk9f", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518871901558, "tcdate": 1518871901558, "number": 1, "cdate": 1518871901558, "id": "ryrwrirvz", "invitation": "ICLR.cc/2018/Workshop/-/Paper293/Public_Comment", "forum": "Hk0ZOFkwf", "replyto": "Hk0ZOFkwf", "signatures": ["~Oriol_Vinyals1"], "readers": ["everyone"], "writers": ["~Oriol_Vinyals1"], "content": {"title": "Please Fix Length", "comment": "Your paper violates by a few lines the 3 page limit (see https://iclr.cc/Conferences/2018/CallForWorkshops). Please send us a fixed version of your PDF at iclr2018.programchairs@gmail.com by the end of Monday, February 19th, or else we will reject your paper.\n\nThanks,\nICLR2018 Program Chairs"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy Estimates for Generative Models", "abstract": "Different approaches to generative modeling entail different approaches to evaluation. While some models admit test likelihood estimation, for others only proxy metrics for visual quality are being reported. In this paper, we propose a simple method to compute differential entropy of an arbitrary decoder-based generative model. Using this approach, we found that models with qualitatively different samples are distinguishable in terms of entropy. In particular, adversarially trained generative models typically have higher entropy than variational autoencoders. Additionally, we provide support for the application of entropy as a measure of sample diversity.", "paperhash": "struminsky|entropy_estimates_for_generative_models", "keywords": ["generative modeling", "differential entropy", "variational autoencoder", "variational inference"], "_bibtex": "@misc{\n  struminsky2018entropy,\n  title={Entropy Estimates for Generative Models},\n  author={Kirill Struminsky and Michael Figurnov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0ZOFkwf}\n}", "authorids": ["k.struminsky@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "authors": ["Kirill Struminsky", "Michael Figurnov", "Dmitry Vetrov"], "TL;DR": "A simple differential entropy estimator applied to comparison of generative models", "pdf": "/pdf/66515a55ff5da57b73b2ea09f3791f725ae4bfbf.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623456, "id": "ICLR.cc/2018/Workshop/-/Paper293/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper293/Reviewers"], "reply": {"replyto": null, "forum": "Hk0ZOFkwf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623456}}}], "count": 6}