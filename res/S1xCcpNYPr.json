{"notes": [{"id": "S1xCcpNYPr", "original": "HylJ18RDvB", "number": 726, "cdate": 1569439125999, "ddate": null, "tcdate": 1569439125999, "tmdate": 1577168228151, "tddate": null, "forum": "S1xCcpNYPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DPVrpJe7d", "original": null, "number": 1, "cdate": 1576798704357, "ddate": null, "tcdate": 1576798704357, "tmdate": 1576800931694, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training. The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations. The algorithm has two-phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold. \nThis approach is incremental in nature -- the resulting multi-objective optimisation problem is not a significant improvement over BOT. After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications. Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant. Hence, I will recommend the rejection of this paper. To merit acceptance to ICLR the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721845, "tmdate": 1576800273014, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper726/-/Decision"}}}, {"id": "SklONH5IoH", "original": null, "number": 2, "cdate": 1573459248442, "ddate": null, "tcdate": 1573459248442, "tmdate": 1573549244613, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "BJl6tE9IoB", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment", "content": {"title": "Author Response to Reviewer #3 : part 2", "comment": "(continued reponses)\n\n3. \u201dFurthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work.\u201d\nResponse: Our approach is significantly better than the compared work and presents a strong success. Our reasons are as follows:\n(1)  The compared work is BOT (Li et al., 2019), which is the state-of-the-art approach in DL testing. As described in Section 4.2 \u201cExperimental Design\u201d, different from DeepReduce, BOT requires users to specify the number of data instances to be selected, whose optimal value is unknown to users before using BOT. To enable comparison, we actually implemented a variant of BOT, called BOTv, by changing the termination criteria to the one used in DeepReduce. That is, the state-of-the-art approach BOT itself cannot be directly used to compare with our work. The results of BOTv shown in Table 2 already incorporate the knowledge (i.e., the KL value) of our work.  \n\n(2) Note that our work tries to obtain a representative subset of testing data, thus we expect the accuracy of the selected subset to be close to the original one. Our accuracy (the error of output distribution) is only slightly worse than that of BOTv. The results in Table 2 show that the absolute differences between our accuracy (the error of output distribution) and the original one are only slightly worse than those of BOTv (ranging from 0.0018 to 0.0255, with an average of 0.0105). In four out of six studied models, the differences are less than 0.01 and only in one studied model the difference is more than 0.02 (0.0255).\n\n(3) Although BOTv performs slightly better than DeepReduce in accuracy,  it does not consider satisfying structural coverage in data selection. Therefore, testing completeness cannot be ensured by BOTv. For example,  the average neuron coverage (when the threshold of NC is 0.75) achieved by BOTv is 92.7% of the coverage achieved by the original testing set. However, our proposed approach can ensure both testing completeness (always achieving 100% coverage) and testing effectiveness. \n\n\n(4) As described in Section 4.3.1, our approach is more stable than BOTv, which adopts a random sampling strategy. We cannot guarantee that the average effectiveness can be achieved every time we use BOTv. For example, on NIN, the accuracy achieved by BOTv ranges from 0.7925 to 0.88, which varies a lot. In comparison, DeepReduce is more stable than BOTv as it does not use random sampling. Therefore, our approach is more useful in practice.\n\n(5) Although our approach is slightly worse than BOTv in terms of overall accuracy, it is better than BOTv in terms of size, indicating that our approach can save more testing costs in DL testing.  \n\nIn summary, we believe that our approach is significantly better than the compared work (BOT/BOTv) and presents a strong success.  Reviewer #1 also confirms that the performance of our approach is good.\n\n\n4. About the change of definition to f(T,M)\u2248f(T\u2019M) and g(T,M)\u2248g(T\u2019M) without any explanation.\nResponse: According to the third paragraph of Section 3.1, we do not change the sub-goal f(T,M)=f(T\u2019,M) to f(T,M)\u2248f(T\u2019,M). We only replace one sub-goal g(T,M)=g(T',M) with g(T,M)\u2248g(T',M). This is because the defined input reduction problem (with the two sub-goals) is NP-complete. We relax the requirement on the output distribution g(T,M) so that our algorithm is more likely to produce a subset of input in an acceptable time. A brief explanation is given in this paragraph, including the representative meaning of '\u2248'. We will add more explanations in this paragraph. \n\n\n5. Regarding the comment on \u201cthe metrics of distribution or the accuracy of each class\u201d\nResponse: Thanks for your suggestion! In Table 2, we showed the overall accuracy of all classes. The difference between the proposed subset and the original testing set (|accuracy of proposed subset - accuracy of ran testing set|) is 0.0284 on average (ranging from 0.0008 to 0.1057), which is similar to the observation on the overall accuracy of all classes. The results indicate that the proposed subset can be used to represent the raw testing data.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCcpNYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper726/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper726/Authors|ICLR.cc/2020/Conference/Paper726/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167149, "tmdate": 1576860555988, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment"}}}, {"id": "SJlCuBcIoS", "original": null, "number": 4, "cdate": 1573459318209, "ddate": null, "tcdate": 1573459318209, "tmdate": 1573459318209, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "Bylyz7Jstr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment", "content": {"title": "Author Response to Reviewer #1", "comment": "We thank the reviewer for your helpful and encouraging comments. We have addressed the comments and revised the paper.  Please see the detailed replies below:\n\n1. \u2019'...the accuracy is lower with KL < 0.001 than with KL < 0.005. I would expect monotonicity here.'\nResponse: We agree that there is monotonicity among the results with different KL values. The monotonicity can be generally observed in Table 3. The only exception is on VGG19 when KL<0.005 and NC=0.25. This is because our approach is a greedy heuristic, the monotonicity may not be very strict when the absolute difference in accuracy is small. For VGG19, when KL<0.005 and NC=0.25, the absolute difference in accuracy is less than 0.004, which could be caused by just one or two test data points. \n\n\n2. About the runtime to obtain the subsets of the test data of Table 2\nResponse: The runtime to obtain the subsets varies for different models. For example, the runtime for three LeNet models ranges from 8.93 to 16.27 seconds. More details will be given on our website.\n\n\n3. About the presentation and typos:\nResponse: Thanks for your suggestions! We have improved the presentation of the paper and fixed the typos. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCcpNYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper726/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper726/Authors|ICLR.cc/2020/Conference/Paper726/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167149, "tmdate": 1576860555988, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment"}}}, {"id": "HJlcLH98iS", "original": null, "number": 3, "cdate": 1573459282055, "ddate": null, "tcdate": 1573459282055, "tmdate": 1573459282055, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "HkgaYdV6Fr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment", "content": {"title": "Author Response to Reviewer #2", "comment": "We thank the reviewer for your helpful and encouraging comments. Please see the detailed replies below:\n\n1. Regarding the comment on theoretical framework. \nResponse: Thanks for your valuable suggestion! As you point out, our paper investigates the input reduction problem from a  practical perspective. It would be interesting to investigate the theoretical framework in our future work.\t"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCcpNYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper726/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper726/Authors|ICLR.cc/2020/Conference/Paper726/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167149, "tmdate": 1576860555988, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment"}}}, {"id": "BJl6tE9IoB", "original": null, "number": 1, "cdate": 1573459076571, "ddate": null, "tcdate": 1573459076571, "tmdate": 1573459076571, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "HkxAIib0Fr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment", "content": {"title": "Author Response to Reviewer #3 : part 1", "comment": "We thank the reviewer for your valuable comments. The details of our response are as follows:\n\n1. Regarding the comment on the novelty of the proposed work.\nResponse: We believe that our work is significant and novel. The novelty can be summarized as follows:\n(1) Our work is the first work to define multi-objective input reduction problem in DL testing, and the first work to reduce the cost of DL testing by satisfying three objectives (efficiency, completeness, and effectiveness). Related work such as BOT focuses on single-objective (effectiveness) and does not ensure testing completeness. \n\n(2) We propose a two-phase reduction algorithm, which is a new approach to input reduction. The two-phase algorithm combines three objectives by using the HGS algorithm and a carefully designed heuristic. Our approach is a meaningful approach in practice, which is also accepted by another reviewer (#2).\n\n(3) An extensive evaluation of the proposed approach, including non-regression and regression scenarios. The latter is a more practical scenario in the development process. Our approach can work well in both scenarios.\n\n2. Regarding the applicability of the proposed algorithms to other related works\nResponse: Our algorithm is general and can be easily adapted to other related work (e.g., other criteria and DL models). In our work, we use neuron coverage and the outputs of the last hidden layer as the two inputs. However, our algorithm is not specific to the neuron coverage criterion, and can support various other coverage criteria. That is, if the coverage of a DL model can be obtained, the first phase (HGS) of our algorithm can also be applied to it. Besides, for most of the DL models, the outputs of the last hidden layer (or the last few hidden layers) are numerical values, indicating that the latter phase of our algorithm can be applied to other DL models too. To sum up, our algorithm can be easily applied to other work as well. \n\nNote that there are some other related works targeting the cost problem in DL (e.g., those described in Section 2). Our approach is proposed to solve the input reduction problem in DL testing, and we do not claim that our approach can be applied to the work described in Section 2, as these work are quite different from our work. However, our approach and these work can complement each other in order to reduce DL costs. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCcpNYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper726/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper726/Authors|ICLR.cc/2020/Conference/Paper726/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167149, "tmdate": 1576860555988, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper726/Authors", "ICLR.cc/2020/Conference/Paper726/Reviewers", "ICLR.cc/2020/Conference/Paper726/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Comment"}}}, {"id": "Bylyz7Jstr", "original": null, "number": 1, "cdate": 1571644167343, "ddate": null, "tcdate": 1571644167343, "tmdate": 1572972560018, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity.\n\nThe key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold.\n\nIn particular, the output distribution is approximated by dividing the output range of each neuron into K intervals. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data.\n\nThe whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution.\n\nThe paper also presents a good experimental campaign that shows good performances.\n\nThe paper is really well written and enjoyable, clear in its description and in the objectives it aims to achieve. To improve the readability a bit further, I would suggest trying to move equation 1 after or close to its reference, or at least to describe before it what KL is.\nMoreover, a similar problem is present for TI, which is introduced in Algorithm 1 and described later. I would add, for example in the input section of the algorithm, a sentence stating that TI is used in getCandidate and described later.\n\nThe first sentence on page 5 seems to be incomplete as it is written. I would suggest rephrasing the sentence.\n\nIn Algorithm 2, about the two \"for\" cycles for i in 1,m and foreach k in 2,K, I suggest unifying them and use the same cycle (only for reasons of readability). Moreover, I think that using braces instead of parentheses would be more correct in these cycles.\n\nIt is interesting to note that in the first line for VGG19, last column of Table 3, the accuracy is lower with KL < 0.001 than with KL < 0.005. I would expect monotonicity here. Do the authors have any idea of the reasons for this?\n\nFinally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems.\n\nTypos:\nOn page 6, penultimate paragraph, the word \"the\" is repeated twice in the parentheses.\nOn page 8, at the beginning of the first sentence after Table 3, there is a comma that seems to be useless after the word \"that\".\nOn page 8, \"When the termination crite gets stricter\", crite should be corrected in criterion.\nThere is a typo in the README of the github project linked in the paper: \"coveraeg data\" instead of \"coverage data\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917976142, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper726/Reviewers"], "noninvitees": [], "tcdate": 1570237747982, "tmdate": 1575917976157, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Review"}}}, {"id": "HkgaYdV6Fr", "original": null, "number": 2, "cdate": 1571797124522, "ddate": null, "tcdate": 1571797124522, "tmdate": 1572972559983, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper develops methods to reduce the test data size while maintaining the coverage and the effectiveness on large test data. The paper proposes  a two-phase reduction approach to select representative samples based on heuristics. Extensive experiments have shown the proposed methods can reduce 95% test samples while still obtaining similar measurements. \n\nThe paper targets a very important problem in practice. Effectively selecting small, representative test sets can save many computational resources and greatly accelerate the research and development. Although the developed technique is quite simple, they are meaningful in practice. Overall, the work can be much improved if a theoretical framework is proposed. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917976142, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper726/Reviewers"], "noninvitees": [], "tcdate": 1570237747982, "tmdate": 1575917976157, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Review"}}}, {"id": "HkxAIib0Fr", "original": null, "number": 3, "cdate": 1571851093812, "ddate": null, "tcdate": 1571851093812, "tmdate": 1572972559926, "tddate": null, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "invitation": "ICLR.cc/2020/Conference/Paper726/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work tries to build a sub-pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution. In this paper, the work is done by adding a test-sample search algorithm on top of the HGS algorithm to balance the output distribution.\n\nHowever, the novelty of the proposed work is limited, and there is no evidence to show the proposed algorithms can be applied to other related works. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work.\n\nOther comments to the proposed manuscript are:\n\n1. In Definition 1 the authors declare that the goal is to satisfy f(T,M)=f(T\u2019M) and g(T,M)=g(T\u2019M), and then in the following paragraphs they change it to f(T,M)\u2248f(T\u2019M) and g(T,M)\u2248g(T\u2019M) with no justification. More explanation is needed.\n\n2. In Table 2, the authors try to compare the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper726/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhoujianyi@pku.edu.cn", "lifeng2014@pku.edu.cn", "xdu_jhdong@163.com", "hongyu.zhang@newcastle.edu.au", "haod@sei.pku.edu.cn"], "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "authors": ["Jianyi Zhou", "Feng Li", "Jinhao Dong", "Hongyu Zhang", "Dan Hao"], "pdf": "/pdf/35e7ab9240d2b4e5d0476dd7e7c0db9b27c66345.pdf", "TL;DR": "we propose DeepReduce, a software engineering approach to cost-effective testing of Deep Learning models.", "abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.", "code": "https://github.com/DeepReduce/DeepReduce", "keywords": ["Software Testing", "Deep Learning", "Input Data Reduction"], "paperhash": "zhou|costeffective_testing_of_a_deep_learning_model_through_input_reduction", "original_pdf": "/attachment/1129891a9edc2a3204ee510863202b617307ebdc.pdf", "_bibtex": "@misc{\nzhou2020costeffective,\ntitle={Cost-Effective Testing of a Deep Learning Model through Input Reduction},\nauthor={Jianyi Zhou and Feng Li and Jinhao Dong and Hongyu Zhang and Dan Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCcpNYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCcpNYPr", "replyto": "S1xCcpNYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper726/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917976142, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper726/Reviewers"], "noninvitees": [], "tcdate": 1570237747982, "tmdate": 1575917976157, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper726/-/Official_Review"}}}], "count": 9}