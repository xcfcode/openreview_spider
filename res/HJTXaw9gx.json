{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396586727, "tcdate": 1486396586727, "number": 1, "id": "ry72nfLOl", "invitation": "ICLR.cc/2017/conference/-/paper441/acceptance", "forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The basic approach of this paper is to use a neural net to sequentially generate points that can be used as the basis points in a PDE solver. The idea is definitely an interesting one, and all three reviewers are in agreement that the approach does seem to have a lot of potential.\n \n The main drawback of the paper, simply, is that it's unclear whether this result would be of sufficient interest for the ICLR audience. Ultimately, it seems as though the authors are simply training a neural network to generate this points, and the interesting contribution here comes from the application to PDE solving, not really from any advance in the NN/ML side itself. As such, it seems like the paper would be better appreciated (as a full conference paper or journal paper), within the control community, rather than ICLR. However, I do think that as an application, many at ICLR would be interested in seeing this work, even if its likely to have relatively low impact on the community. Thus, I think the best avenue for this paper is probably as a workshop post at ICLR, hopefully with further submission and exposure in the controls community.\n \n Pros:\n + Nice application of ML to a fun problem, generating sample points for PDE solutions\n + Overall well-written and clearly presented\n \n Cons:\n - Unclear contribution to the actual ML side of things\n - Probably better suited to controls conferences", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396587238, "id": "ICLR.cc/2017/conference/-/paper441/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396587238}}}, {"tddate": null, "tmdate": 1485454722003, "tcdate": 1482268814745, "number": 3, "id": "r1D5x7vNx", "invitation": "ICLR.cc/2017/conference/-/paper441/official/review", "forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "signatures": ["ICLR.cc/2017/conference/paper441/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper441/AnonReviewer2"], "content": {"title": "Good Work, Preliminary Results", "rating": "7: Good paper, accept", "review": "This paper presents an algorithm for approximating the solution of certain time-evolution PDEs. The paper presents an interesting learning-based approach to solve such PDEs. The idea is to alternate between:\n1. sampling points in space-time\n2. generating solution to PDE at \"those\" sampled points\n3. regressing a space-time function to satisfy the latter solutions at the sampled points (and hopefully generalize beyond those points).\n\nI actually find the proposed algorithm interesting, and potentially useful in practice. The classic grid-based simulation of PDEs is often too expensive to be practical, due to the curse of dimensionality. Hence, learning the solution of PDEs makes a lot of sense for practical settings. On the other hand, as the authors point out, simply running gradient descent on the regression loss function does not work, because of the non-differentiablity of the \"min\" that shows up in the studied PDEs.\n\nTherefore, I think the proposed idea is actually very interesting approach to learning the PDE solution in presence of non-differentability, which is indeed a \"challenging\" setup for numerically solving PDEs.\n\nThe paper motivates the problem (time-evolution PDE with \"min\" operator applied to the spatial derivatives) by applications in control thery, but I think there is more direct interest in such problems for the machine learning community, and even deep learning community. For example http://link.springer.com/chapter/10.1007/978-3-319-14612-6_4 studies approximate solution to PDEs with very similar properties (evolution+\"min\") to develop new optimization algorithms. The latter is indeed used to training deep networks: https://arxiv.org/abs/1601.04114\n\nI think this work would catch even more attention if the authors could show some experiments with higher-dimensional problems (where grid-based methods are absolutely inapplicable).\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584500, "id": "ICLR.cc/2017/conference/-/paper441/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper441/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper441/AnonReviewer1", "ICLR.cc/2017/conference/paper441/AnonReviewer3", "ICLR.cc/2017/conference/paper441/AnonReviewer2"], "reply": {"forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584500}}}, {"tddate": null, "tmdate": 1484312913114, "tcdate": 1484312913114, "number": 4, "id": "SkKUW8IIg", "invitation": "ICLR.cc/2017/conference/-/paper441/public/comment", "forum": "HJTXaw9gx", "replyto": "r1D5x7vNx", "signatures": ["~Vicen\u00e7_Rubies_Royo1"], "readers": ["everyone"], "writers": ["~Vicen\u00e7_Rubies_Royo1"], "content": {"title": "Addressing the Reviewer", "comment": "Thank you very much for taking the time to review the paper and for the attached URLs, we look forward to reading their contents. \n\nAlso, we added an extra experiment in 3D at the end."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575214, "id": "ICLR.cc/2017/conference/-/paper441/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJTXaw9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper441/reviewers", "ICLR.cc/2017/conference/paper441/areachairs"], "cdate": 1485287575214}}}, {"tddate": null, "tmdate": 1484312603573, "tcdate": 1484312603573, "number": 3, "id": "Hk47l8U8x", "invitation": "ICLR.cc/2017/conference/-/paper441/public/comment", "forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "signatures": ["~Vicen\u00e7_Rubies_Royo1"], "readers": ["everyone"], "writers": ["~Vicen\u00e7_Rubies_Royo1"], "content": {"title": "Extra Experiment Appended", "comment": "We appended a 3D experiment in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575214, "id": "ICLR.cc/2017/conference/-/paper441/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJTXaw9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper441/reviewers", "ICLR.cc/2017/conference/paper441/areachairs"], "cdate": 1485287575214}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484312468983, "tcdate": 1478290724977, "number": 441, "id": "HJTXaw9gx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJTXaw9gx", "signatures": ["~Vicen\u00e7_Rubies_Royo1"], "readers": ["everyone"], "content": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484312392276, "tcdate": 1484312392276, "number": 2, "id": "H1xLyUILe", "invitation": "ICLR.cc/2017/conference/-/paper441/public/comment", "forum": "HJTXaw9gx", "replyto": "r1INGfVNl", "signatures": ["~Vicen\u00e7_Rubies_Royo1"], "readers": ["everyone"], "writers": ["~Vicen\u00e7_Rubies_Royo1"], "content": {"title": "Addressing the reviewer's questions II", "comment": "Thank you for taking the time to review our work.\n\n1.) We are aware that the PDE residual does not ultimately reflect how good an approximation is. One can have an approximation where the function has a \"discontinuous jump\" in an arbitrarily small time interval and the residual might remain small, even though the approximation is clearly not correct. This is in part why we did not use a single error metric, but used two error metrics instead. Also, since the PDE residual is still being used in some papers to get a \"feeling\" for the learning progression, we decided to use it as well.\n\n2.) We used a set of 2D examples to check whether the algorithm would work in the first place. If it does not work in 2D it probably won't work in 3D, 4D etc. This being said, we know that interesting problems are those in high dimensions, and that is what we are currently working towards. To that end, we appended an extra experiment in 3D at the end of the paper.\n\n3.) Finally, we believe that this work is in fact suited for this conference just like other submissions, particularly those in RL, which make use of NNs and solely focus on algorithmic performance and make no mention of learned representations/features. Moreover, the topic of safety is gaining more importance in the AI community, and thus it seems appropriate to present our work here.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575214, "id": "ICLR.cc/2017/conference/-/paper441/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJTXaw9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper441/reviewers", "ICLR.cc/2017/conference/paper441/areachairs"], "cdate": 1485287575214}}}, {"tddate": null, "tmdate": 1482068526295, "tcdate": 1482068526295, "number": 2, "id": "r1INGfVNl", "invitation": "ICLR.cc/2017/conference/-/paper441/official/review", "forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "signatures": ["ICLR.cc/2017/conference/paper441/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper441/AnonReviewer3"], "content": {"title": "somewhat interesting paper, wrong conference", "rating": "3: Clear rejection", "review": "Approximating solutions to PDEs with NN approximators is very hard. In particular the HJB and HJI eqs have in general discontinuous and non-differentiable solutions making them particularly tricky (unless the underlying process is a diffusion in which case the Ito term makes everything smooth, but this paper doesn't do that). What's worse, there is no direct correlation between a small PDE residual and a well performing-policy [tsitsiklis? beard? todorov?, I forget]. There's been lots of work on this which is not properly cited. \n\nThe 2D toy examples are inadequate. What reason is there to think this will scale to do anything useful? \n\nThere are a bunch of typos (\"Range-Kutta\"?) .\n\nMore than anything, this paper is submitted to the wrong venue. There are no learned representations here. You're just using a NN. That's not what ICLR is about. Resubmit to ACC, ADPRL or CDC.\n\nSorry for terseness. Despite rough review, I absolutely love this direction of research. More than anything, you have to solve harder control problems for people to take notice...", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584500, "id": "ICLR.cc/2017/conference/-/paper441/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper441/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper441/AnonReviewer1", "ICLR.cc/2017/conference/paper441/AnonReviewer3", "ICLR.cc/2017/conference/paper441/AnonReviewer2"], "reply": {"forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584500}}}, {"tddate": null, "tmdate": 1481768436020, "tcdate": 1481768337298, "number": 1, "id": "B1FqTuk4x", "invitation": "ICLR.cc/2017/conference/-/paper441/public/comment", "forum": "HJTXaw9gx", "replyto": "rku9KeJ4x", "signatures": ["~Vicen\u00e7_Rubies_Royo1"], "readers": ["everyone"], "writers": ["~Vicen\u00e7_Rubies_Royo1"], "content": {"title": "Addressing the reviewer's questions", "comment": "Thank you for the review. Below I will address the points mentioned by the reviewer.\n\n1.) & 3.)\nThis method indeed requires retraining if the domain bounds, the dynamics f(x)/f(x,a)/f(x,a,b) or the boundary condition change. However, to the best of our knowledge, this is also the case for all existing gridding/finite element techniques. This, I am afraid, seems unavoidable for most (if not all) PDE numerical methods.\n\nThis approach is meant to offer a different way to compute the approximation to the HJI PDE with some of the advantages mentioned in the \"Advantages and Disadvantages\" section, including curse of dimensionality alleviation for memory, differentiability of the approximation (finite/gridding methods yield the gradient approximation using the grid, which is itself an approximation) and approximation over the entire domain (not just grid points). Some very recent experiments with the algorithm also hint that the method may scale to state dimensions higher than 4, which is something promising given that gridding methods become intractable beyond state dimension of 4 (again, due to the curse of dimensionality). \n\nIn terms of a \"unifying approach\", one of the core ideas was to discretize the partial derivative for time in such a way that values would \"flow\" away from the boundary; in that context, we believe this could also work for other PDEs where the left hand-side of the PDE is the only place where the partial derivative with respect to time appears (this includes the diffusion PDE). That would essentially require changing lines 11 and 12 of the algorithm. Last but not least, in comparison to other neural network approaches to approximating PDE solutions, ours does not require the right-hand side to be differentiable.\n\n2.)\nThe set S \\in [-5,5]^2 was used for all experiments. This work has a lot of different parts that can be tweaked to push the algorithm to its limits, and changing the size of S is one of them; given the time and space constraints we have not yet been able to explore all of these limits, but we will certainly be exploring them in the near future. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575214, "id": "ICLR.cc/2017/conference/-/paper441/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJTXaw9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper441/reviewers", "ICLR.cc/2017/conference/paper441/areachairs"], "cdate": 1485287575214}}}, {"tddate": null, "tmdate": 1481734543894, "tcdate": 1481734543886, "number": 1, "id": "rku9KeJ4x", "invitation": "ICLR.cc/2017/conference/-/paper441/official/review", "forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "signatures": ["ICLR.cc/2017/conference/paper441/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper441/AnonReviewer1"], "content": {"title": "Hard to follow; unclear about contribution", "rating": "5: Marginally below acceptance threshold", "review": "I have no familiarity with the HJI PDE (I've only dealt with parabolic PDE's such as diffusion in the past). So the details of transforming this problem into a supervised loss escape me. Therefore, as indicated below, my review should be taken as an \"educated guess\". I imagine that many readers of ICLR will face a similar problem as me, and so, if this paper is accepted, at the least the authors should prepare an appendix that provides an introduction to the HJI PDE. At a high level, my comments are:\n\n1. It seems that another disadvantage of this approach is that a new network must be trained for each new domain (including domain size), system function f(x) or boundary condition. If that is correct, I wonder if it's worth the trouble when existing tools already solve these PDE's. Can the authors shed light on a more \"unifying approach\" that would require minimal changes to generalize across PDE's?\n\n2. How sensitive is the network's result to domains of different sizes? It seems only a single size 51 x 51 was tested. Do errors increase with domain size?\n\n3. How general is this approach to PDE's of other types e.g. diffusion? \n", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recursive Regression with Neural Networks: Approximating the HJI PDE Solution", "abstract": "Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a regression problem. Beyond this family of problems, we find many cases where the unavailability of data pairs makes this approach unfeasible. However, similar to what we find in the reinforcement learning literature, if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this context, we present an algorithm that approximates the solution to a partial differential equation known as the Hamilton-Jacobi-Isaacs PDE and compare it to current state of the art tools. This PDE, which is found in the fields of control theory and robotics, is of particular importance in safety critical systems where guarantees of performance are a must.", "pdf": "/pdf/3a69364c022247e1d0a905f593408784f7a72f41.pdf", "TL;DR": "A neural network that learns an approximation to a function by generating its own regression points", "paperhash": "royo|recursive_regression_with_neural_networks_approximating_the_hji_pde_solution", "conflicts": ["berkeley.edu"], "keywords": ["Supervised Learning", "Games", "Theory"], "authors": ["Vicen\u00e7 Rubies Royo", "Claire Tomlin"], "authorids": ["vrubies@berkeley.edu", "tomlin@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584500, "id": "ICLR.cc/2017/conference/-/paper441/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper441/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper441/AnonReviewer1", "ICLR.cc/2017/conference/paper441/AnonReviewer3", "ICLR.cc/2017/conference/paper441/AnonReviewer2"], "reply": {"forum": "HJTXaw9gx", "replyto": "HJTXaw9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper441/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584500}}}], "count": 9}