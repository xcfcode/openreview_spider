{"notes": [{"id": "yHeg4PbFHh", "original": "1wWMUbyzUW8", "number": 1310, "cdate": 1601308146321, "ddate": null, "tcdate": 1601308146321, "tmdate": 1616051612417, "tddate": null, "forum": "yHeg4PbFHh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rF3us7ESVAx", "original": null, "number": 1, "cdate": 1610040469127, "ddate": null, "tcdate": 1610040469127, "tmdate": 1610474072996, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The reviewers have supported the acceptance of this paper (R3 and R5 were particularly excited) so I recommend to accept this paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040469114, "tmdate": 1610474072980, "id": "ICLR.cc/2021/Conference/Paper1310/-/Decision"}}}, {"id": "NihPCWlhfwP", "original": null, "number": 10, "cdate": 1606248860645, "ddate": null, "tcdate": 1606248860645, "tmdate": 1606248860645, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "aY3WZflYeeW", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Clarification", "comment": "Thank you for your response.\n\nI believe the algorithm is much more clear now.\n\nI would also appreaciate if the authors include the results for the CVC4 (and Z3) solver in the next revision of the paper as it should not be that the readers need to find it in the comments of reviews and the paper did some selection of tasks, so the results are readily available elsewhere."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "IaA4KPN1o7", "original": null, "number": 9, "cdate": 1606134947193, "ddate": null, "tcdate": 1606134947193, "tmdate": 1606134947193, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "rDV96Z02p1", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Re: Response", "comment": "Thank you very much for the clarification and for your detailed response."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "l1elOIePLTa", "original": null, "number": 8, "cdate": 1605897939288, "ddate": null, "tcdate": 1605897939288, "tmdate": 1605897939288, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "mR8taxNJFsG", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Regarding your pushback, I would like to push back too: model without heuristics only outperforms heuristics 31/29 on your benchmarks and underperforms heuristics on SyGuS. So if model by itself is a major contribution, then the performance is not great.\nIf you believe that you can \"claim that our search technique coupled with the heuristics is itself a new contribution of this paper\", then you should do so. That claim - in addition to your existing claims - would strengthen the paper. Especially, since ultimately it's the model+heuristics which performs best.\n \nRegarding \"we didn't retrain the neural network for SyGuS problems\" - If you have to retrain the neural network for any new set of problems/benchmarks, then there is a risk that it is overfitted. I realize that you may have to retrain for a substantially different domain, but there's a line than needs to be treaded carefully. I think with the training challenges you described, it may be difficult to analyze potential overfitting, so let's leave it as it is.\n\nThank you."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "W2sUDoD2k8", "original": null, "number": 7, "cdate": 1605837907806, "ddate": null, "tcdate": 1605837907806, "tmdate": 1605837907806, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "pdIZkQY4SI9", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Review score", "comment": "You're obviously free not to do this, of course, but we wonder if -- in light of this comment -- you'd consider updating your score to 6 instead of 5? We feel that this more closely matches a position of \"I am fine with accept\" than does 5, though of course we realize that different people may have different interpretations of the numbers. We have indeed found your other comments useful - see our response below."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "mR8taxNJFsG", "original": null, "number": 6, "cdate": 1605837615979, "ddate": null, "tcdate": 1605837615979, "tmdate": 1605837615979, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "dyENazeehj6", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thanks very much for the review. Responses inline below:\n\n> In experiments, the algorithm barely outperforms a baseline with heuristics only.\n\nWe would like to push back on this a bit.\n\nFirst, we think the improvement is reasonably large. The improvement in our benchmarks for \"Successes by Time Elapsed\" of \"using model and heuristics\" over \"using heuristics only\" is over 10% (32 / 29). It's true that the improvement is closer to 4% for SyGuS, but this leaves out a crucial point: we didn't retrain the neural network for SyGuS problems! Furthermore, the benchmark tasks have widely varying difficulty, and since the search space grows exponentially with the solution size, solving even a few more problems shows a significant improvement in the synthesizer\u2019s ability to scale to find larger solutions.\n\nSecond, we put a lot of effort into crafting those strong heuristics. Many other synthesis papers do not bother comparing their method to well-engineered heuristics. If we had excluded the heuristics, our improvements would have been 31 / 26 (19%) for our 38 benchmarks and 75 / 65 (15%) for SyGuS. In our opinion, authors should not be penalized for engineering strong baselines for comparison.\n\nThird, the heuristics are only possible to implement because of the way we set up the search (bottom-up where we can evaluate every intermediate expression to obtain concrete results).\nWe could reasonably claim that our search technique coupled with the heuristics is itself a new contribution of this paper.\n\n> I would say that the idea to use NN for this is not significantly novel\n\nWe have made the specific contribution of noticing that when you do bottom-up search in which all intermediate values can be evaluated, you can train a model to predict whether those values will appear in a solution. This is not something that has been done before. Moreover, we have actually made it efficient enough to yield a wall-clock speedup. We believe it is important to recognize empirical contributions that demonstrate the practical utility of new techniques."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "yrqHP6IuBis", "original": null, "number": 5, "cdate": 1605837586890, "ddate": null, "tcdate": 1605837586890, "tmdate": 1605837586890, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "5OBxRtUyW-j", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your in-depth review! We are glad to hear that our expanded experiments have greatly improved the paper. We also agree that large-scale success of program synthesis likely depends on reasoning about high-level concepts, and property signatures is an important step toward that direction.\n\nWe\u2019ve made many changes to the paper, following your helpful suggestions, including adding a diagram (Figure 2 in the updated paper).\n\n> I got a little confused by the comments about removing restrictions of Concat() in the second paragraph in section 2.2.\n\nCertain prior works (e.g., FlashFill, RobustFill) impose a restriction of having Concat as the top-level operation, i.e., they only handle tasks with this property. They can then use version space algebras, dynamic programming, or other pruning strategies to exploit the property that partial programs must form substrings of the output (or, a prefix of the code solution must produce a prefix of the output). Our DSL lifts this constraint, allowing the synthesizer to handle more kinds of tasks than those previous works.\n\n> Can you provide some intuition on the rationale behind keeping \u201c100 positive and 100 negative values\u201d as explained in the last sentence in section 3.1?\n\nEach training point consists of random inputs, a value encountered during the search which we treat as the output O, and an \u201cintermediate value\u201d V that is either chosen to be a sub-expression of the output, or some other randomly-chosen value from the search that is not a sub-expression of the output. We want to construct a balanced training dataset, where half of the datapoints should be predicted as positive (V is a sub-expression of O), and half are negative (V is not a sub-expression of O). Hence, from each of 1000 synthesis runs using random inputs, we extract 100 positive data points and 100 negative data points for the training set."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "aY3WZflYeeW", "original": null, "number": 4, "cdate": 1605837545804, "ddate": null, "tcdate": 1605837545804, "tmdate": 1605837545804, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "AcyLvDE5bCI", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your review!\n\n> It makes it difficult to compare based on the numbers, but it looks like the result is not competitive with existing solvers for the CyGuS PBE competitions. The paper should mention where it stands here.\n\nThe CVC4 solver is able to solve more SyGuS tasks than other methods, but its solutions are often very long, difficult to understand, and overfit to the I/O examples. For example, CVC4 generates a solution with more than 5000 AST nodes for one of the string benchmarks, mostly consisting of a case split style program. In contrast, enumerative searches like BUSTLE prioritize shorter programs over longer ones, and thus the synthesized programs are more interpretable and more likely to generalize. We instead compare against DeepCoder and RobustFill, which are state-of-the-art synthesis systems using machine learning. Additionally, the DSL used in our paper is different and more general than the DSL used in SyGuS tasks, e.g., BUSTLE handles changing case of strings while this is not supported by SyGuS.\n\n> The actual contribution of the work is mostly in the implementation and combining known techniques.\n\nOur main contribution is applying machine learning in the loop for bottom-up synthesis, and showing that it can be done quickly enough for wall-clock speedups overall. To our knowledge, no other work has used machine learning within a bottom-up search in this manner; previous works like DeepCoder use machine learning only at the beginning and hence do not guide the search as it happens, or use an end-to-end ML approach like RobustFill. As AnonReviewer5 mentioned, the simplicity of our approach (which is novel nonetheless) is one of its strengths.\n\n> It is not completely clear what ExtractConstants does.\n\nExtractConstants uses heuristics to select constants from a set of I/O examples, including common symbols/delimiters that appear in the example strings, and long substrings that appear multiple times in the example strings. We added this clarification to the paper.\n\n> The inputs I and outputs O should be vectors, but they are used as sets. The algorithm actually is unclear here. Does the set E[1] include the entire input vector for input examples as an element and each constant as a vector of the constants with this length (In this case it should be E[1] = {I} \\cup C)?\n\nYou are right, I and O are confusing in the algorithm. Suppose there are M separate input variables (such that the program we wish to synthesize is a function with arity M), and there are N different I/O examples. Every \u201cvalue\u201d in the algorithm represents a code expression and internally contains the result of that code expression when applied to each of the N examples. The output value, O, is a single value (as usual, containing the desired outputs for all N examples). The input values, I, is a set of M values, each of which contains N strings (the example input strings for that input variable). When we compare values in the algorithm (\u201cif V = O then: return Expression(V)\u201d), we are comparing whether all N strings match, so in this case, a code expression will be returned if it evaluates to the desired output for all N examples. With this clarification, \u201cE[1]\u2190I\u222aC\u201d is correct -- each of the M input variables, as well as each extracted constant, becomes a value with weight 1. We\u2019ve updated the paper to make this more clear.\n\n> There is also no intuition for what concrete property signatures make sense.\n\nRecall that the model is conditioned on two property signatures: one from the inputs to the output, and one from the intermediate value to the output. Listing 1 shows 3 example property signatures, which compare whether an input is a substring of the output, a suffix of the output, or a substring of the output when ignoring case. These are all properties that are useful for string manipulation:\n\n* If the input is a substring of the output, then the output is likely to be constructed by concatenating other strings to the input.\n* If the input is only a substring of the output when ignoring case, then the desired program likely includes some case-changing functionality.\n* If the intermediate value is a substring of the output, then it is more likely to be useful than another value that has no relation to the output.\n\nAlso, listings 5-8 contain all of the properties we use."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "rDV96Z02p1", "original": null, "number": 3, "cdate": 1605837494575, "ddate": null, "tcdate": 1605837494575, "tmdate": 1605837494575, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "pSdFuuNKS-T", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "Response to Reviewer 5", "comment": "Thank you for your review! In particular we agree that BUSTLE\u2019s simplicity is a strength of the method. In our updated paper, we have fixed all the typos you pointed out.\n\n> Out of curiosity: If the order is the same for all the benchmarks for all methods, it seems to be the case the method being fastest to find a solution for a benchmark task varies from task to task, and the here proposed method does not always seem to be the fastest for any instance. Given the demanding real-time setting, do you think an algorithm selection approach would work here, choosing a method on a per task basis?\n\nThe benchmarks are run in a consistent order in our experiments, but actually the order does not matter for the plots. We ran each method for the full timeout of 30 seconds on _each_ benchmark task, and the plot shows the number of tasks where the method succeeded (Y-axis) before a specific amount of time had elapsed for that task (X-axis), and similarly for the number-of-expressions plots. Your observation is still correct though -- no method is consistently the fastest for all tasks. We believe that an algorithm selection approach could lead to improvements, but doing so would require different training algorithms (possibly reinforcement learning). Similarly we might be able to improve the BUSTLE approach as well, training the model by using real data from the synthesizer instead of synthetic data."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "pdIZkQY4SI9", "original": null, "number": 2, "cdate": 1605456031944, "ddate": null, "tcdate": 1605456031944, "tmdate": 1605456031944, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment", "content": {"title": "I am fine with accept", "comment": "Although I stand behind my thoughts and comments in my review, I am fine with \"accept\" decision based on the arguments of other reviewers. Hopefully my input will still be useful for authors."}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yHeg4PbFHh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1310/Authors|ICLR.cc/2021/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861204, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Comment"}}}, {"id": "dyENazeehj6", "original": null, "number": 1, "cdate": 1603393001637, "ddate": null, "tcdate": 1603393001637, "tmdate": 1605024476926, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review", "content": {"title": "Well written paper but questions about contribution significance. Possible reject.", "review": "The paper proposes to add machine learning component to the bottom-up program synthesis algorithm. Machine learning component uses information from property signatures to prioritize the candidate expressions for further expression space exploration. Authors show that their algorithm outperforms other approaches on the benchmark sets.\n\nPositives:\n- The paper presents incremental improvement to the bottom-up program synthesis algorithm.\n- Authors have considered the efficiency issues of adding machine learning model into the bottom-up search and came up with optimizations that result in model cost being lower than efficiency gains.\n- Authors figured out how to train the NN for their task absent actual training dataset.\n- Algorithm outperforms baseline approaches.\n- All sections of work are well presented, understandable, and easy to follow. The paper was a pleasure to read.\n\nConcerns:\n- My concerns can be summarized in short question: are contributions significant enough?\n- In experiments, the algorithm barely outperforms a baseline with heuristics only. This is the case for both authors' benchmark set and SyGuS benchmark set.\n- Using property signatures is not new. The novel part is adding NN to prioritize search based on property signatures. I would say that the idea to use NN for this is not significantly novel. There is some novelty in setting up training dataset as I observed above. \n\nPossible mitigation of concerns:\n- Choose more complicated problem (benchmark) set. This may show where baseline approaches do not work well. Since authors created first benchmark set themselves, I am surprised this was not done already.\n- Another variation would be a useful problem set that is not more complicated, but where for some reason baseline(s) do not work.\n- Provide results for a baseline that uses property signatures without NN. This is a minor mitigation: it is possible that property signatures without NN will not outperform the baseline with heuristics. It still would be interesting to see the comparison.\n\nIn summary: I am not convinced that contribution is significant enough for acceptance.\n\nReproducibility: It would be nice if authors presented actual NN architecture used for ease of reproducibility.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121626, "tmdate": 1606915791821, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1310/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review"}}}, {"id": "5OBxRtUyW-j", "original": null, "number": 2, "cdate": 1603843634798, "ddate": null, "tcdate": 1603843634798, "tmdate": 1605024476857, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review", "content": {"title": "Review of BUSTLE - a program synthesis technique using program signatures (fusing formal methods with machine learning)", "review": "Note about NeurIPS \u201920 version: \n\nI was on the NeurIPS \u201920 program committee and I was assigned an earlier version of this paper. As such, I\u2019m deeply familiar with it. In its NeurIPS form, I felt it wasn\u2019t ready for tier-1 publication. However, my concerns were principally around the lack of experimental results. I strongly supported of the ideas presented in the paper, but I fought to ensure it wasn\u2019t accepted to NeurIPS because I felt it would have been a disservice to both NeurIPS and the authors, resulting in a rather mediocre paper that could have been exceptional if the proper experiments were run. I attempted to make this clear in my review and encouraged the authors to address this weakness.\n\nIt appears the authors have addressed my primary concern. This version of the paper resolves my critical reservation of weak empirical results \u2013 the results, I believe, are now satisfactory of tier-1 publication, which include four different synthesis systems and over 100 different synthesized programs of two different domains. As such, I now support its acceptance at ICLR.\n\nSummary:\n\nThis paper appears to be the first work to use program signatures (ICLR \u201920, Odena & Sutton) for program synthesis. The high-level concepts the authors present, as I understand them, is that by using program signatures for program synthesis, they can more closely replicate the process of program synthesis the way programmers develop programs. That is, by breaking one large program into many small sub-programs. Once enough of these small sub-programs have been generated, they can be composed together to solve the larger program \u2013 the actual goal.\n\nAssuming this hypothesis holds, the end result might be that such an approach would result in a program synthesizer that could generate both (i) more correct programs, (ii) faster than prior systems. For their experimental evaluation, this hypothesis seems to hold and BUSTLE does in fact generate more correct program, more quickly than prior state-of-the-art. The authors compare BUSTLE to three other variant systems: a baseline (less sophisticated BUSTLE system), RobustFill (ICML 2017), and DeepCoder (ICLR 2017). RobustFill and DeepCoder have demonstrated state-of-the-art performance, historically, so I believe these comparisons are sufficient for this paper\u2019s acceptance.\n\nOverall, I think the paper provides a truly novel approach to program synthesis with its fusion of program signatures. I am admittedly biased in favor of program signatures, because I believe the future of machine programming / software 2.0 / neural programming / program synthesis with both stochastic and deterministic approaches (whatever we want to call it) is going to be heavily reliant on our ability to lift concepts from code (the \u201cwhat\u201d) which is notably more challenging than lifting the implementation (the \u201chow\u201d). This is because the \u201cwhat\u201d tends to not necessarily be obvious from the code, whereas the \u201chow\u201d almost always is \u2013 it\u2019s the implementation. With that in mind, this paper presents what I believe is the first demonstrable evidence that program signatures can be used in this fashion. I suspect this is just the beginning of exploration with program signatures \u2013 I expect a flurry of follow-up research to emerge that uses them.\n\nWhen taken holistically, I strongly support accepting this paper, but I do have some minor nits I\u2019d like the authors to address.\n\nMinor suggestions:\n\n1. There appears to be no system diagram of BUSTLE. While an expert in the space of program synthesis and property signatures can likely understand what is going on, non-experts I think will really struggle without some kind of visual diagram showing how BUSTLE works. It should be relatively easy to add this diagram to the paper and I believe it will make the paper more widely accessible. If only one of my recommendations is addressed by the authors, I would request it be this one.\n\n2. There appears to be multiple locations where the authors seem to deem neural network inference is \u201ctoo slow\u201d without qualification. I think this is a mistake and is a bit of a turn off and it\u2019s a bit of a confusing one given that BUSTLE uses neural network inference. Yes, I do agree with the authors that inference with large neural networks could make the problem of program synthesis slower, but I don\u2019t believe this is a universal truth. I think it\u2019s proportional to the computational complexity of the neural network. I would request the authors find all such \u201cinference is too slow\u201d cases in the paper and properly qualify them. I suspect this will encourage future work to consider other neural network architectures that may be competitive or even outperform BUSTLE.\n\n3. Can you label the different variants of BUSTLE from something like \u201cUsing model and heuristics\u201d to just BUSTLE, \u201cUsing model only\u201d to BUSTLE (model only), etc. Right now it\u2019s a bit confusing at first glance on Figure 2 to see which is the full BUSTLE system because there isn\u2019t actually any legend item that is called \u201cBUSTLE\u201d. Should be an easy fix and will likely make the figure easier for the audience to understand.\n\n4. Can you please drop the word \u201cvery\u201d from the paper everywhere it appears? I do not have a mathematical representation of what that word means (nor does anybody I think) and, as such, I believe it introduces unnecessary ambiguity and also wastes paper space.\n\n5. I couldn\u2019t tell how the BUSTLE training time was factored into the analysis. Can you find a way to explain that more clearly? I realize that it\u2019s a potential one-time only penalty, but it doesn\u2019t come for free (to my understanding) while some traditional program synthesis systems using formal methods can simply generate programs without any learning overhead. I think this needs to be captured somewhere so people don\u2019t forget about this cost.\n\n6. I got a little confused by the comments about removing restrictions of Concat() in the second paragraph in section 2.2. Can you try to explain that more clearly?\n\n7. Can you provide some intuition on the rationale behind keeping \u201c100 positive and 100 negative values\u201d as explained in the last sentence in section 3.1?\n\n8. Can you double check to ensure all of your acronyms are fully spelled out first? I\u2019m familiar with all of them, but others might not. I don\u2019t think I saw the spelled out versions of DSL, AST, JVM, etc. While these terms are generally widely known in the programming languages community, I\u2019m not sure if the machine learning community is as deeply aware of them. Regardless, it seems to me that it\u2019s usually a good idea to spell out all acronyms first.\n\n\nFuture work:\n\nDo you really think an abstract syntax tree (AST) representation is the right representation for this approach? \n\nI\u2019m not so sure. I recommend taking a look at the Aroma\u2019s simplified parse tree in the paper by FAIR, Berkeley, and Riverside (OOPSLA \u201919) and, more comprehensively, MISIM\u2019s context-aware semantics structure from Intel, Georgia Tech, MIT (arxiv). I suspect a next iteration of BUSTLE using either of these structures might result in even better performance than what you\u2019ve currently achieved with an AST representation. But, that\u2019s just a guess. :)\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121626, "tmdate": 1606915791821, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1310/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review"}}}, {"id": "AcyLvDE5bCI", "original": null, "number": 3, "cdate": 1603918042924, "ddate": null, "tcdate": 1603918042924, "tmdate": 1605024476790, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review", "content": {"title": "A well written and evaluated paper combining synthesis techniques with machine learning.", "review": "The paper proposes to combine bottom-up program synthesis from input/output examples with a machine learning model. The machine learning model determines for each candidate intermediate value (coming bottom-up from the inputs) if it may be considered at the next round of candidate expression generation or it will be deferred for at least K rounds (with K up to 5 in the evaluation).\n\nPros:\n\n- The paper is evaluated on a good range of string manipulating programs. The presentation and the evaluation is showing the advantages of the reweighting for the number of solved programs.\n\n- The advantages of the method are not only in the number of candidates explored, but in actual wall-time. This is a relatively rare result - many prior synthesis with machine learning tasks completely actual running time.\n\n- The paper and its implementation address the engineering side of the work - batching the requests for effective machine learning and interestingly, the results are good even when the machine learning model was trained on random data.\n\nCons:\n\n- It makes it difficult to compare based on the numbers, but it looks like the result is not competitive with existing solvers for the CyGuS PBE competitions. The paper should mention where it stands here.\n\n- The actual contribution of the work is mostly in the implementation and combining known techniques.\n\nIn terms of writing, there are some improvements that are possible:\n\n- The algorithm description is not self-contained. It is not completely clear what ExtractConstants does. The inputs I and outputs O should be vectors, but they are used as sets. The algorithm actually is unclear here. Does the set E[1] include the entire input vector for input examples as an element and each constant as a vector of the constants with this length (In this case it should be E[1] = {I} \\cup C)?\n- There is also no intuition for what concrete property signatures make sense.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121626, "tmdate": 1606915791821, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1310/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review"}}}, {"id": "pSdFuuNKS-T", "original": null, "number": 4, "cdate": 1604330089183, "ddate": null, "tcdate": 1604330089183, "tmdate": 1605024476730, "tddate": null, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "invitation": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review", "content": {"title": "Compelling approach to bottom-up program synthesis", "review": "# Summary\n\nThe paper proposes an approach to program synthesis which is done in a bottom-up fashion.\nIn order to guide the search more effectively the bottom-up search algorithm is accompanied with a model predicting whether particular sub-expressions of a program are promising directions.\nAs the system for program synthesis needs to be real-time capable, the proposed approach heavily relies on property signatures as introduced in (Odena & Sutton, 2020) for featurizing program inputs.\nThe here proposed system called BUSTLE is shown to perform favorably to a set of baselines including state-of-the-art approaches to program synthesis.\n\n# Resons for Score\nGenerally speaking, I really enjoyed the reading of the paper. The paper is well structured and written as well as easy to follow.\nEverything is explained in sufficient detail, i.e., thorough but concise.\nThe contribution of the paper is significant and the approach is technically sound.\n\n# Pros\n\nAlthough the methods might not be feasible to be used in practice yet, it performs reasonably well with only a few examples for describing the desired behavior of the programs to be synthesized.\nHowever, the programs that are synthesized are still quite smallish. It would be interesting if the authors could also highlight some future directions in order to make a step towards more complex scenarios.\n\nThe paper provides compelling experiments which are set up in a thoughtful and rational manner rather than throwing various methods on some benchmarks.\nThe experiments are conducted in a very systematic way, facilitating insights into the performance of the proposed method as well as how it compares to the baselines.\nHowever, a taxonomy of problems (e.g. how to quantify problem severity etc.) would maybe help to better spot differences in performance.\n\nThe method itself is quite simple. The authors almost seem to apologize or vindicate. I rather consider it an advantage.\n\nThe proposed approach is well placed in the context of the existing literature.\n\n# Cons\n\nSome information about the experiment setup is missing: Are all benchmark tasks of one set (either on the proposed set or SyGuS) run in a row?\nIs the order kept consistent? What hardware is used for conducting the experiments?\n\n# Questions during rebuttal period\n\n- Out of curiosity: If the order is the same for all the benchmarks for all methods, it seems to be the case the method being fastest to find a solution for a benchmark task varies from task to task,\nand the here proposed method does not always seem to be the fastest for any instance. Given the demanding real-time setting, do you think an algorithm selection approach would work here, choosing a method\non a per task basis?\n\n\n\n# Typos\n- p.3: \"during the search, [...]\" => During the search\n- p.6: \"fixed length representation\" => fixed-length representation (this is occurring at least twice)\n- p.8: \"This provides further evidence the our model [...]\" => This provides further evidence that our model [...] (occurs also in the appendix)\n- p.8 last but one paragraph in Section 5: \"[...], while like BUSTLE, uses values produced by intermediate programs [...]\" => The comma after BUSTLE is kind of irritating. Please, consider removing it or adding another comma before \"like\".\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1310/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1310/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "authorids": ["~Augustus_Odena1", "~Kensen_Shi1", "~David_Bieber1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Hanjun_Dai1"], "authors": ["Augustus Odena", "Kensen Shi", "David Bieber", "Rishabh Singh", "Charles Sutton", "Hanjun Dai"], "keywords": ["Program Synthesis"], "abstract": "Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "one-sentence_summary": "We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "odena|bustle_bottomup_program_synthesis_through_learningguided_exploration", "pdf": "/pdf/2a5e6446f3e44243b64f41369e186a582fb55a63.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nodena2021bustle,\ntitle={{\\{}BUSTLE{\\}}: Bottom-Up Program Synthesis Through Learning-Guided Exploration},\nauthor={Augustus Odena and Kensen Shi and David Bieber and Rishabh Singh and Charles Sutton and Hanjun Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=yHeg4PbFHh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yHeg4PbFHh", "replyto": "yHeg4PbFHh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121626, "tmdate": 1606915791821, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1310/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1310/-/Official_Review"}}}], "count": 15}