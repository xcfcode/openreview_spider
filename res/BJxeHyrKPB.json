{"notes": [{"id": "BJxeHyrKPB", "original": "ryeVde6OwS", "number": 1678, "cdate": 1569439543897, "ddate": null, "tcdate": 1569439543897, "tmdate": 1577168252826, "tddate": null, "forum": "BJxeHyrKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XgeyUMBMJm", "original": null, "number": 1, "cdate": 1576798729611, "ddate": null, "tcdate": 1576798729611, "tmdate": 1576800906893, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Decision", "content": {"decision": "Reject", "comment": "Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication.  \n\nThis can be repaired, and the authors should try again after a thorough revision and rewrite.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710143, "tmdate": 1576800259078, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Decision"}}}, {"id": "BkeYHLbnsr", "original": null, "number": 5, "cdate": 1573815873274, "ddate": null, "tcdate": 1573815873274, "tmdate": 1573829643234, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "SylJiIjpqr", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 1)", "comment": "Thank you for your time and valuable comments. \nFrom your comments, we found that our work would be closely related to a practical method of isometric embedding of Riemannian manifold. \nBecause our background is not only deep autoencoders but also image compression, we have overlooked that there is a gap between image compression and VAE. \nPlease understand that we revised to some extent in order to respectfully deal with your comments and to make our claim more persuasive.\n\n>>Regarding our motivation and its connection with prior works \nWe added an explicit discussion about our motivation, idea, and connection among prior works. Figures 1 and 2 give an overview. Please find the following discussion are added.\nThe term \u201cRate-distortion optimization\u201d or \u201cRDO\u201d is a method to improve quality in image compression with orthonormal transform coding. \nhttps://en.wikipedia.org/wiki/Rate-distortion_optimization\nPrior works of deep image compression such as Balle et al., 2018 also used RDO. We added an overview of RDO and our motivation.  The derivation of our idea is based on the analogy of orthonormal transform coding.\nWe also added the analogy and difference between VAE and our idea. The summary is as follows. \n\nAccording to RDO theory, the condition of optimization in transform coding is that: (i) transform data deterministically using orthonormal basis (orthogonal is not enough) such as DCT, KLT, and so on (ii) quantize by uniform quantizer for all channels which cause uniform noise (iii) assign the optimum entropy code.\nOur intuition is that if the equivalent noise is added to latent variables z and rate-distortion is optimized, z should have orthonormality. Consequently, Jacobian becomes constant automatically.\nObeying this flow, z is obtained deterministically and the entropy is used rather than KL divergence between an encoder and a prior.\n\nRate-distortion optimization condition for VAE and our model is contrasted as follows. In VAE, because PDF is fixed as prior, noise should be variable and scaling between data and latent spaces is also variable, meaning Jacobian is inconstant. In ours, because noise is uniform, PDF should be variable(parametric) and scaling is constant.  Thus, in our model, there is not fixed prior.\n\nAbout the loss function, The second and third terms in eq (5) are an approximate decomposition of D(x, x_\\breve) as shown in Rolinek et al., 2019. By this decomposition, we can independently control the reconstruction loss and scaling Jacobian and lead to better performance.\n\n>> Relation to [1] http://proceedings.mlr.press/v84/chen18e.html\nThank you for introducing an interesting paper to us. Thanks to your comment, we found that our work, especially Eqs. (10) and (11), would be related to an isometric embedding of Riemannian manifold where A(x) is a Riemannian tensor.  In this paper, the authors discussed the distance between two points is the shortest path on a Riemannian manifold induced by the transformation. Then, the impact on the domain data caused by the variance of latent variables is measured. This is related to the discussion of Fig. 6 (c) and (d). While VAE needs to find a winding road in the latent space that corresponds to the shortest path, in our model, a linear path in the latent space expected to be connected to that.  While we did not include this discussion this time because of page limitation though, this will be our future work.\n\n\n>> Do your parameters need to be in the optimum for your analysis to hold true?\nStrictly speaking, yes. Although, as experiment result showed, when parameters are optimized decently, it works almost as in theory even though there remains the left behind margin.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "SylLp4W3jB", "original": null, "number": 4, "cdate": 1573815486337, "ddate": null, "tcdate": 1573815486337, "tmdate": 1573827721530, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "SylJiIjpqr", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 2) ", "comment": ">>Regarding the experimental result.\nFirst of all, actually, our model does not increase model complexity even though you concerned about this point. \nWhen we compared with our model and DAGMM (baseline model), the number of network parameters is completely the same. We added this point explicitly.\nNevertheless, our model provides a significant performance boost in the anomaly detection task.\n\nExperiment with toy data is executed to confirm our model\u2019s property though, it also supports the result of anomaly detection. In DAGMM, the relation PDF of x and z is unclear. On the other hand, in our model, the PDFs of x and z are close to proportional. That means, our model can capture the probability of real data methodically in the latent space. This fact should be very intuitive to explain the performance boost in the anomaly detection task in which PDF estimation is a critical issue. Other comparison methods also essentially lead the disorder in the density estimation like DAGMM because the Jacobian is not controlled.\n\nIn the analysis of the latent state in CelebA, we assume that since we could not tell you the purpose of the experiment enough, it was not convincing for you.\nThis is an experiment to confirm that the latent variable in our model works as PCA components, and the influence of each component can be evaluated quantitatively as in theory while (beta-)VAE does not have this property. We revised this sections and captions for easy following.\nThe two on the right of Fig. 6 in the revised version show the scaling between the latent and metric dependent data space. (c) shows the scaling in VAE is anisometric, and (d) shows the scaling in ours is isometric.\nThe two on the left of Fig. 6 in the revised version is the variance of the latent space. Since the scaling of z in our model is isometric, the variance shows the importance of each latent variable like PCA.  \n\nConsequently, we believe our experimental results demonstrate the validity of our method decently. \n\nPCA can simultaneously disentangle the data and estimate the importance of latent variables by variance. We believe this trait is very helpful to the interpretation of the latent variable of deep models. \n\n\n>>minor issues\nWe fixed the minor issues you pointed (we would appreciate if you could be indulgent of a bit long model acronym). \nWe also promise to request a grammatical check by a native no later than the camera-ready version. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "rJeBEtbnsB", "original": null, "number": 7, "cdate": 1573816620737, "ddate": null, "tcdate": 1573816620737, "tmdate": 1573826909431, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "HJewScvRYH", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 1)", "comment": "Thank you for your time and valuable comments. Please understand that we revised to some extent in order to respectfully deal with your comments and to make our claim more persuasive.\n\nFirst of all, we added an explicit discussion about our motivation, idea, and connection among prior works which we overlooked before. Figures 1 and 2 give an overview. \n\n>>Why the introduced method is better than VAE as a generative model for capturing the latest representation is not explained well. It is not also used as a baseline in most of the experiments.\n\nWe added section 3 to make the relation and difference between VAE and our method much explicit.\nBy this section, we believe the following two points stated repeatedly in the entire text became easy to follow. These are difference not only from VAE but also from other autoencoders without Jacobian control. Since you seem not to care about the second point very much, we would appreciate your attention to it. We insist the second point promotes the interpretation of latent variables which has been discussed as one of the most important problems of deep learning.  \n\n(i) the probability distribution of the latent space obtained by this model is proportional to the\nprobability distribution of the real space because Jacobian between two spaces is constant;\n(ii) our model behaves as non-linear PCA, where the energy of acquired latent space is concentrated on several principal components and the influence of each component can be evaluated quantitatively\n\nThe experiment in section 5.1(in the revised version) demonstrates the first feature. Furthermore, an experiment in section 5.2 shows the validity of the practical task. The second feature is examined in the experiment in section 5.3. Figure. 4 (Fig. 6 in revised ver.) demonstrates the second property. \n\nDAGMM is known as a model to estimate the density better than (beta-)VAE and suitable for baseline though, we added the result of the experiment in section 5.1 (toy data task) in Appendix G. As it is mentioned before, in VAE, Jacobian is not constant and Px(x) and Pz_\\psi(z) have no correlation. We can also move this to the main text if it is necessary.\n\nIn the anomaly detection task, we added the score of VAE cited from Liao et al. (2018). Actually GMVAE is also a VAE based method. Unfortunately, we couldn\u2019t reproduce the result by ourselves though, our model performs significantly better compared with that. Since GMVAE does not care about Jacobian and maximizing ELBO as well, it essentially includes disorder in the density estimation.\n\n>>The motivation for having the third term in Equation (4) needs to be explained. Also, what is h() in the second term? The authors only describe briefly both terms together after they used it here but failed to describe what each term is.  Why there is an h for the second term but not for the third term. h() becomes more clear much later in the paper but when it is used the first time, it not defined.\n\nWe added the explanation the third term and h() when it is used the first time. The second and third terms are actually decomposition of D(x, x_\\breve) as shown in Rolinek et al., 2019. By this decomposition, we can independently control the reconstruction loss and scaling Jacobian and lead to better performance.\n\n>> I believe A in Equation (5) should be also positive-definite.\nYes, it is. We added the description.\n\n>>What is L(x) in Equation (8). It needs to be defined.\nWe added the definition of L(x). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "r1lHzzZ3iH", "original": null, "number": 2, "cdate": 1573814797119, "ddate": null, "tcdate": 1573814797119, "tmdate": 1573825837311, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "r1xu3tgSYH", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 2)", "comment": ">>Regarding experimental results\n>>In the experiment 4.1. the proposed method seems to achieve matching densities, although the distributions are wrongly normalized. How does the density matching improve? All three methods seem to have equally good scatters.\n\nAs written in the result section, even though the baseline method (DAGMM) also captured good scatter, the density is not estimated adequately. Figure 5 (in revised version) depicts plot of Px(x) (x-axis) and Pz\u03c8(z) (y-axis).  It is obvious that we can see the proportionality between Px(x) and Pz\u03c8(z), while we cannot see the tendency in the baseline. This is also quantitatively evaluated. The correlation coefficients are 0.691 (baseline) vs 0.997 or more (ours). (Originally we showed residual of linear regression though, for intuitive understanding, we replaced it by correlation coefficients.)\nFor the easy following, we also updated the caption and description in the main text. \n\n>>The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so. If one would visualize the z_100 there would be no variance in RADO and possibly some variance in VAE. \n\nYes, what you said happens. This means that our model correctly works as PCA as in theory. The point is the variance of latent variables directly correlated \nThis is the experiment to confirm that latent variables of our model work as PCA components. To make it further explicit, we added this statement at the beginning of the section.\n\nLet\u2019s say if we want to find an important latent variable in terms of the influence on the metrics function, what should we do?\nIn our model, we can find that latent variable easily, since the variance of z is directly related to the visual. In (beta-)VAE, as we described, variance and impact to visual are uncorrelated. Thus, for example, we need to visualize all latent variables to find the important ones. Moreover, even if you come up with you run PCA for the latent variables of VAE, you need to set the number of PCA components and may struggle to decide how many components are appropriate. In our model, it is automatically optimized in terms of minimizing entropy.\nConsequently, the latent variable in our model is quantitatively understandable. We believe this character is helpful for the interpretation of latent variables and meta-prior of data.\n\n>>The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches (such as VAE), and how does the proposed method alleviate them.\n\nAn Experiment with MNIST could be worth-doing though, we demonstrated the above characteristic clearly with CelebA dataset.\n\nIn terms of the interpretation of latent variables, some of the standard approaches are visualizing or evaluate independencies of variables as in  (Lopez et al., 2018; Chen et al., 2018; Kim & Mnih, 2018; Chen et al., 2016). They do not directly evaluate the importance of latent variables on the metric function(such as MSE or SSIM). In our method, this can be quantitatively measured like PCA. \nNote that, we do not intend to claim this way of analysis is always better than previous ways. We argue that making use of PCA like analysis as an option and incorporating them will promote further interpretation of latent variables. \n\n>>For minor comments\no The point of eq 5 is unclear, it seems unnecessary. It also does not contain h(), which is claimed after eq6\nEquation 5 (6 in the revised version) is a condition for function D(\u30fb,\u30fb). As long as D(\u30fb,\u30fb) can be approximated as eq 5, it can be applied. We added this explanation.\n\no The log pz(z) in eq 4 is not entropy\nWe fixed it.\n\no eq 8 is unclear, is the dx a derivative, distance or change?\nIt is derivative. We added the notation.\n\no the t prefix notation is confusing, what does it mean?\nThe t denotes the transpose of a matrix. We added the notation.\n\no what is the \\sim and line notation in eq 5\n\\simeq denotes approximation.\n\n o what are the products in eq9, are these inner products?\nIt is a multiplication. We removed dots.\n\noi n eq 13 pz, pxd or hat(pxd) have not been introduced or defined\nhat(pxd) is defined as \u201clet hat(pxd) be estimated probability of xd.\u201d\u3000We added definition pz, pxd as the true PDF of z and xd\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "rkg7tMbnsS", "original": null, "number": 3, "cdate": 1573814906574, "ddate": null, "tcdate": 1573814906574, "tmdate": 1573824534837, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "r1xu3tgSYH", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 1) ", "comment": "Thank you for your time and comments. Thanks to your comments, we could improve the paper a lot! We hope the revised version and these rebuttal comments will solve your confusion. Please understand that we revised to some extent in order to respectfully deal with your comments and to make our claim more persuasive. \n\n>>Regarding our motivation and connection to prior works.\nBecause we have a background not only about deep autoencoder but also about image compression, we have overlooked a gap between image compression and VAE as you pointed out. To make our motivation and the difference from previous work clear, we added section 3. Figures 1 and 2 describe the overview. Please find the following points are described.\n\nOur method is based on rate-distortion optimization(RDO) of transform coding for image compression. RDO is a method to improve quality in image compression with orthonormal transform coding. \n(https://en.wikipedia.org/wiki/Rate-distortion_optimization)\nFor the readers who are not familiar with RDO, we added the overview of RDO, its connection to VAE, and our motivation to introduce RDO to autoencoder (not VAE). \n\nHere is a summary of the added section.\nAccording to RDO theory, the condition of optimization in transform coding is that: (i) transform data using orthonormal basis (orthogonal is not enough) such as DCT, KLT, and so on (ii) quantize by uniform quantizer for all channels which cause uniform noise (iii) assign the optimum entropy code.\nOur intuition is that if the equivalent noise is added to latent variables z and rate-distortion is optimized, z should have orthonormality. Consequently, Jacobian becomes constant automatically.\nAccordingly, z is obtained deterministically. The reason to add equivalent noise is based on this idea. Since our model minimizes entropy of z, there is no prior for p(z).\n\nActually, RDO can be analogously discussed in VAE and there are works considering rate-distortion trade-off into VAE. But in the way to assume fixed distribution as prior like VAE, even if rate-distortion is optimized, orthonormality is not guaranteed, and Jacobian is not constant.\n\nAs we mentioned in section 2., Flow-based models take Jacobian of into account (we assume this model would be \u2018elsewhere\u2019). Although, in Flow method, the encoder and the decoder need to be a bijection, which means the dimension of the data space and the latent space is the same. On the other hand, our model can compress data into a lower dimension with a constant Jacobian. \n\n>> Regarding theory part\n>>It seems that eq 14 is the final result\nThe final cost function is eq. (5) and it is substantially the same as eq. (14) ((15) in revised version). This equation is related to rather the needlessness of ELBO than orthonormality. \nTo avoid confusion, we split the section into the method part and theoretical part. Also, we enhanced the purpose of each equation.\n\n>>orthogonal argument\nAlthough we gave full proof in Appendix A,  in the main text, we rely on  Rolinek et al., (2019) for the argument of orthogonality since it is already proved. We show the proof of constant Jacobian and orthonormality, combining the orthogonality.\n\n>>Optimizing eq 14 seems trivial since we can always match Pz and Pz_\\varphi easily with neural networks, or similarly the two x-distributions.\n\nYou seem to imagine matching the KL-divergence of Pz as prior and pz\\varphi or something like that. As it is mentioned, Jacobian J is not constant in the most previous methods. Thus, even if just Pz and Pz_\\varphi were matched, it does not mean estimating p(x), which is our goal, is achieved."}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "BklKgdWnsS", "original": null, "number": 6, "cdate": 1573816305024, "ddate": null, "tcdate": 1573816305024, "tmdate": 1573822745878, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "HJewScvRYH", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment", "content": {"title": "Response to the comments (part 2)", "comment": "Regarding experiments: \n>>1- It is useful to also plot the original data in space s to see how the results in Figure 2 make sense. \nThanks for your point. We added the plot of the original data source.\n\n>>2- Figure 3 is not clear. \nFigure 3 (Figure 5 in revised version) depicts plot of Px(x) (x-axis) and Pz_\\psi (z) (y-axis). A linear plot means that the probability density of Px(x) is tidily mapped into the latent space. Thanks to this property, Px(x) can be estimated by Pz_\\psi (z) in our model. It is obvious that DAGMM does not have this trait.  This is also quantitatively evaluated. The correlation coefficients are 0.691 (baseline) vs 0.997 or more (ours).\nFor the easy following, we revised the caption and description in the main text.\n\n>>3- In the Anomaly detection experiments, the authors make two assumptions that usually do not exist in real-worlds: (1) they assume that they have access to a training set that only contains normal cases. (2) They assume that they know the correct rate of anomaly. I think both these assumptions are very restrictive and unreal. While these assumptions are used for all the comparing methods, it is not obvious how different algorithms behave in a real scenario. \n\nI understand that there is an unrealistic assumption, but this setting is established and widely admitted in this anomaly detection task.\nRegardless of this assumption is realistic or not, density estimation remains a critical issue, and better estimation provides better performance. Although investigating the performance in a truly real scenario might be future work, we argue this point is not a defect to show the validity of our method.  \n\n>>4- Figure 4 and what it represents is not clear.\nThis is caused because we could not tell you the purpose of this experiment sufficiently. This is an experiment to show an important property of our model: our model behaves as PCA, where the energy of acquired latent space is concentrated on several principal components and the influence of each component can be evaluated quantitatively\nThe two on the left of Fig. 4 (Fig. 6 in the revised version) is the variance of the latent space. Since our model works as PCA, the variance is concentrated in a few dimensions. Two on the right shows that the influence of minute displacement of each z to the real image is the almost constant in our model while it is varied in beta-VAE. Thus, we can evaluate the importance of latent variables by variance like PCA. We added the caption and enhanced the purpose of the experiment.\n\n>>Regarding writing issue\nThank you for pointing. We fixed them."}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxeHyrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1678/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1678/Authors|ICLR.cc/2020/Conference/Paper1678/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152478, "tmdate": 1576860546234, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Authors", "ICLR.cc/2020/Conference/Paper1678/Reviewers", "ICLR.cc/2020/Conference/Paper1678/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Comment"}}}, {"id": "r1xu3tgSYH", "original": null, "number": 1, "cdate": 1571256752302, "ddate": null, "tcdate": 1571256752302, "tmdate": 1572972437439, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper propose a noisy autoencoder that considers the jacobian between data and latent spaces to match the corresponding densities. This idea has already been proposed elsewhere, and here it is applied to autoencoders. Overall I had hard time understanding the paper, the motivation, the main contribution or the claim, the model definition and the jacobian method. The paper is poorly written, with lots of issues in math notation and poor motivation and explication of what the sections are introducing, and what parts of the presentation is novel and what is already known. Lots of the math machinery is too vague to follow.\n\nThe distribution p(z) is unclear, and whether z is random variable or not. It seems that \u201cz\" is a non-random variable, and then adding noise \\eps makes it stochastic. However, then p(z) without \\eps does not make any sense since z is not random. It seems that p(z) is maybe a prior distribution instead (or maybe the variational posterior?), but then adding \\eps noise to an already stochastic variable is strange. Overall I have hard time understanding the motivation of the two discrepancies in eq 4, what is the point of adding more noise to \u201cz\u201d? This seems some kind of noisy or perhaps robust AE variant, but the paper does not explicate this. I have hard time following the eqs 8-15. I am not convinced of the orthogonality argument, and I fail to see what this section tries to show or demonstrate. It seems that eq 14 is the final result, but its difficult to follow due to most terms in eq 14 being undefined. Optimizing eq 14 seems trivial since we can always match pz and pz_\\varphi easily with neural networks, or similarly the two x-distributions. \n\nIn the experiment 4.1. the proposed method seems to achieve matching densities, although the distributions are wrongly normalized. How does the density matching improve? All three methods seem to have equally good scatters. The benchmarks on table 1 show clear improvement with the method. The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so. If one would visualise the z_100 there would be no variance in RADO and possibly some variance in VAE. The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches (such as VAE), and how does the proposed method alleviate them.\n\nOverall the paper is poorly presented and difficult to follow. Despite this the method does seem to work remarkably well, and the Jacobian idea is clearly very promising. Nevertheless in its current form the paper is badly premature for ICLR, and needs a lot more work and polish to be made understandable for wider ML audience.\n\nMinor comments\no Px(x), x1, x2 are probably missing subscripts\no The point of eq 5 is unclear, it seems unnecessary. It also does not contain h(), which is claimed after eq6\no The log pz(z) in eq 4 is not entropy\no eq 8 is unclear, is the dx a derivative, distance or change?\no the $^t$ prefix notation is confusing, what does it mean?\no what is the \\sim and line notation in eq 5?\no what are the products in eq9, are these inner products?\no in eq 13 pz, pxd or hat(pxd) have not been introduced or defined\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667553086, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Reviewers"], "noninvitees": [], "tcdate": 1570237733888, "tmdate": 1575667553100, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review"}}}, {"id": "HJewScvRYH", "original": null, "number": 2, "cdate": 1571875391468, "ddate": null, "tcdate": 1571875391468, "tmdate": 1572972437401, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to obtain latent representation of data such that probability density for the real space can be calculated correctly from that in the latent space. The authors optimize a loss function that has components related to parametric probabilistic distribution and auto encoder simultaneously. While this might be an important problem (I am not sure), the paper is not written and organized well which makes a through evaluation very difficult. I provide below some of the problems with this the paper:\n\nWhy the introduced method is better than VAE as a generative model for capturing the latest representation is not explained well. It is not also used as a baseline in most of the experiments.\n\nThe motivation for having the third term in Equation (4) needs to be explained. Also what is h() in the second term. The authors only describe briefly both terms together after they used it here but failed to describe what each term is.  Why there is an h for the second term but not for the third term. h() becomes more clear much later in the paper but when it is used the first time, it not defined. \n\nI believe A in Equation (5) should be also positive-definite. \n\nWhat is L(x) in Equation (8).  It needs to be defined.\n\n\nExperiments:\n1-\tIt is useful to also plot the original data in space s to see how the results in Figure 2 make sense. \n2-\tFigure 3 is not clear.\n3-\tIn the Anomaly detection experiments, the authors make two assumptions that usually do not exist in real-worlds: (1) they assume that they have access to training set that only contains normal cases. (2) They assume that they know the correct rate of anomaly. I think both these assumptions are very restrictive and unreal. While these assumptions are used for all the comparing methods, it is not obvious how different algorithms behave in real scenario. \n4-\tFigure 4 and what it represents is not clear. \n\nWriting Problems:\n1-\tIn the text of paragraph before Figure 1, Eq. (5) in \u201cin the second term of Eq. (5)\u201d is a typo and should be Eq. (4). \n2-\tIn the paragraph before Figure 1, the following sentence is not complete: \u201cThen, averaging Eq. (4) according to distribution, x~P_x(x) and epsilon~ P(epsilon).\u201d\n3-\tSection 4.2.1: \u201cthere is a difference is PDF \u2192 \u201cthere is a difference in PDF\u201d"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667553086, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Reviewers"], "noninvitees": [], "tcdate": 1570237733888, "tmdate": 1575667553100, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review"}}}, {"id": "SylJiIjpqr", "original": null, "number": 3, "cdate": 1572873879129, "ddate": null, "tcdate": 1572873879129, "tmdate": 1572972437355, "tddate": null, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "invitation": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary of the paper: The authors propose a latent variable model RaDOGAGA, a generative autoencoding model. The model is trained via a tradeoff between distortion (the reconstruction error) and the rate (the capacity of the latent space, measured by entropy). The paper provides an analysis of theoretical properties of their approach, and presents supporting experimental results.\n\nReview tl;dr: weak reject, for three main reasons:\n(i) While the existing literature around VAEs, beta-VAEs,  and Rate-Distortion theory is mentioned in the related work, the connections are not nearly discussed sufficiently. \n(ii) On top of (i), the derivation of their loss function and architecture is not sufficiently motivated. This is in astonishing contrast to 1.5 pages of main text and 8 pages of (much appreciated!) analysis of properties.\n(iii) Given the paper is clearly related to existing approaches in the literature, the experiments would require a much more careful comparison to existing models. It remains unclear why an interested user should favor your model over conceptually simpler generative models with fewer hyperparameters.\n\nDetailed review:\n\nNota bene: This review is a late reassignment. While I reviewed the paper to the best of my ability, time constraints did not allow me to review parts of the paper in depth.  I am open to reassess my review during the second stage.\n\nConnection to prior art: As a probabilistic, neural autoencoding model, the connections to the family of VAE models are obvious. The loss function (eq. (4)) still looks very much like the ELBO, where the typical conditional log-likelihood was split into two distortion terms. How is this different from e.g. a beta-VAE? Particularly, what is the connection between the rate-distortion analysis of beta-VAE by Alemi et al. and yours? These things need to be discussed explicitly, with more than a sentence or two in the related work section.\nA lesser, but still important omission in your discussion of prior work: The Jacobian of the generator has also been studied, even for the VAE, cf. e.g. [1]. I believe this deserves more attention in your assessment of prior art.\n\nMotivation: You use two distortion terms: actual sample vs. undistorted reconstruction. Why is that? What is the interpretation of the multipliers? How do I choose them? Why is a large part of your architecture (the pipeline from x to \\hat(x)) actually deterministic? Why are you using the entropy of the prior over the latents, rather than the KL divergence between encoder and a prior? I think an interested reader could learn much more from your paper if you discussed your model embedded in th related work rather than in isolation.\n\nTheory: Due to aforementioned time constraints, I was not able to review the extensive theoretical analysis in depth. Still, I would strongly recommend structuring the respective sections more clearly. Separate model and architecture description from the theoretical analysis; precisely formulate your claims. In particular, state your assumptions clearly. For instance, you assume \"that each function's parameter is rich enough to fit ideally\" (and similar e.g. in Appendix A). Does this only mean that the true distributions are part of the parametric family? What if this is not the case? Do your parameters need to be in the optimum for your analysis to hold true? \n\nGiven that the full 20-page manuscript spends 10 pages on theory, I think this contribution is not given appropriate space in the main text.\n\nExperiments: There are three experiments: a simple 3D proof of concept; anomaly detection; analysis of the latent state in CelebA. As mentioned in my review of the methods section, I believe the approach to be very similar to established models. None of the experiments provides convincing evidence why I should prefer the new, arguably more complex model.\nFor instance, I would have much preferred that you investigate properties of your model against alternatives over the anomaly detection experiments, which did not further my understanding of the proposed model. \n\nSummary: The paper tackles an important problem, namely the lack of control over the latent embedding in autoencoding generative models. I believe the author's contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper.\n\nMinor comments:\n1. While I appreciate a pun, I would recommend to rename the model along with the acronym to a more concise name.\n2. Please revise your notation and typsetting. Examples: x1 instead of x_1, f of f(\\cdot) instead of f(), \\log instead of log.\n3. Introduce acronyms before using them (e.g. VAE, MSE, SSIM), even when they seem obvious to you.\n4. Please carefully check the manuscript for typos, missing articles, missing spaces etc.\n5. Your citations are inconsistent, in that they sometimes use first names, sometimes first name initials, and sometimes no first names.\n6. To my knowledge, the term scale function does not have an obvious definition. I think you are simply referring to monotonically increasing functions. Please clarify!\n7. Your figures should be understandable without too much context, they need more detailed captions.\n\n[1] http://proceedings.mlr.press/v84/chen18e.html"}, "signatures": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1678/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kato.keizo@jp.fujitsu.com", "zhoujing@cn.fujitsu.com", "anaka@jp.fujitsu.com"], "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "authors": ["Keizo Kato", "Jing Zhou", "Akira Nakagawa"], "pdf": "/pdf/bbd1aa96a619d0bf499e0f2aebb4235e6bac37d5.pdf", "TL;DR": "We propose an autoencoder based on Rate-Distortion Optimization.  With our model, log-likelihood maximization is possible without ELBO.", "abstract": "In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose a method to learn parametric probability distribution and autoencoder simultaneously based on Rate-Distortion Optimization to support scaling control. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant: (ii) our model behaves as non-linear PCA, which enables to evaluate the influence of latent variables on data. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and outperform current state-of-the-art methods.", "keywords": ["Autoencoder", "Rate-distortion optimization", "Generative model", "Unsupervised learning", "Jacobian"], "paperhash": "kato|ratedistortion_optimization_guided_autoencoder_for_generative_approach", "original_pdf": "/attachment/f828885de9e0bfd2585040becbc84cb39f186043.pdf", "_bibtex": "@misc{\nkato2020ratedistortion,\ntitle={{\\{}RATE{\\}}-{\\{}DISTORTION{\\}} {\\{}OPTIMIZATION{\\}} {\\{}GUIDED{\\}} {\\{}AUTOENCODER{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}APPROACH{\\}}},\nauthor={Keizo Kato and Jing Zhou and Akira Nakagawa},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxeHyrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxeHyrKPB", "replyto": "BJxeHyrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1678/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667553086, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1678/Reviewers"], "noninvitees": [], "tcdate": 1570237733888, "tmdate": 1575667553100, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1678/-/Official_Review"}}}], "count": 11}