{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457641776538, "tcdate": 1457641776538, "id": "L7VjyN649TRNGwArs4jA", "invitation": "ICLR.cc/2016/workshop/-/paper/187/review/12", "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "signatures": ["ICLR.cc/2016/workshop/paper/187/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/187/reviewer/12"], "content": {"title": "Interesting idea", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a layers wise adaptive depth quantization of DCNs, giving an better tradeoff of error rate/ memory requirement than the fixed bit width across layers.\nSome points are not clear in the paper:\n- how do you fine tune after quantization? The statement \"... 6.78% with floating point weights and 8-bit fixed point activations \" is not clear.\n-Seems that the convolutional layers are the only one quantized and they have less parameters than the  the fully connected layers, how does the number of parameters growing up impact the performance?\n- How do you choose kappa?  \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "pdf": "/pdf/yovBjmpo1ur682gwszM7.pdf", "paperhash": "lin|fixed_point_quantization_of_deep_convolutional_networks", "conflicts": ["qualcomm.com"], "authors": ["Darryl D. Lin", "Sachin S. Talathi", "V. Sreekanth Annapureddy"], "authorids": ["dexul@qti.qualcomm.com", "stalathi@qti.qualcomm.com", "sreekanthav@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580174369, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580174369, "id": "ICLR.cc/2016/workshop/-/paper/187/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/187/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457638894367, "tcdate": 1457638894367, "id": "0YrwzL9g8uGJ7gK5tROW", "invitation": "ICLR.cc/2016/workshop/-/paper/187/review/11", "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "signatures": ["ICLR.cc/2016/workshop/paper/187/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/187/reviewer/11"], "content": {"title": "interesting idea but more experimental results needed", "rating": "6: Marginally above acceptance threshold", "review": "The authors describe an optimization problem for determining the bit-width for different layers of DCNs for reducing model size and required computation.\n\nThe described method is interesting and seems easy to implement. Experimental results on CIFAR-10 illustrate the benefits of approach with modest reduction in model size.\n\nHowever, more experiments and details should be given.\nWhat about testing networks with just fully-connected layers? Is it the case that the ImageNet issue holds for CIFAR-100 as well? Details of how they fine tune after quantization? What is the previous state-of-the-art classification error on CIFAR-10 with fixed point? Plot w/ FLOPS required after quantization?\nThese would help give better understanding of the significance of the work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "pdf": "/pdf/yovBjmpo1ur682gwszM7.pdf", "paperhash": "lin|fixed_point_quantization_of_deep_convolutional_networks", "conflicts": ["qualcomm.com"], "authors": ["Darryl D. Lin", "Sachin S. Talathi", "V. Sreekanth Annapureddy"], "authorids": ["dexul@qti.qualcomm.com", "stalathi@qti.qualcomm.com", "sreekanthav@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580174395, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580174395, "id": "ICLR.cc/2016/workshop/-/paper/187/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/187/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457636072992, "tcdate": 1457636072992, "id": "BNYVJpgqwU7PwR1riXWz", "invitation": "ICLR.cc/2016/workshop/-/paper/187/review/10", "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "signatures": ["ICLR.cc/2016/workshop/paper/187/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/187/reviewer/10"], "content": {"title": "Good paper, accept", "rating": "7: Good paper, accept", "review": "This paper builds further upon the line of research that tries to represent neural network weights and outputs with lower bit-depths. This way, NN weights will take less memory/space and can speed up implementations of NNs (on GPUs or more specialized hardware).\n\nIn this paper they present a new heuristic for choosing the bit-depth of every layer differently, so as to trade-off speed, accuracy and model size more optimally then when using an equal bit depth for every layer.\n\nPro: The paper is well written and the results are interesting and new.\nCons: More experiments would be helpful. E.g., numbers on ImageNet, better comparison with previous art where they focus on fixed point networks during training.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "pdf": "/pdf/yovBjmpo1ur682gwszM7.pdf", "paperhash": "lin|fixed_point_quantization_of_deep_convolutional_networks", "conflicts": ["qualcomm.com"], "authors": ["Darryl D. Lin", "Sachin S. Talathi", "V. Sreekanth Annapureddy"], "authorids": ["dexul@qti.qualcomm.com", "stalathi@qti.qualcomm.com", "sreekanthav@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580175259, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580175259, "id": "ICLR.cc/2016/workshop/-/paper/187/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "yovBjmpo1ur682gwszM7", "replyto": "yovBjmpo1ur682gwszM7", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/187/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455832401275, "tcdate": 1455832401275, "id": "yovBjmpo1ur682gwszM7", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "yovBjmpo1ur682gwszM7", "signatures": ["~Darryl_D_Lin1"], "readers": ["everyone"], "writers": ["~Darryl_D_Lin1"], "content": {"CMT_id": "", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "pdf": "/pdf/yovBjmpo1ur682gwszM7.pdf", "paperhash": "lin|fixed_point_quantization_of_deep_convolutional_networks", "conflicts": ["qualcomm.com"], "authors": ["Darryl D. Lin", "Sachin S. Talathi", "V. Sreekanth Annapureddy"], "authorids": ["dexul@qti.qualcomm.com", "stalathi@qti.qualcomm.com", "sreekanthav@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}