{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396459746, "tcdate": 1486396459746, "number": 1, "id": "rkVN3zUdx", "invitation": "ICLR.cc/2017/conference/-/paper250/acceptance", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers raise several important questions about modeling and methodology that should be answered in later versions of the paper. The paper also overstates its findings."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396460227, "id": "ICLR.cc/2017/conference/-/paper250/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396460227}}}, {"tddate": null, "tmdate": 1482083890836, "tcdate": 1482083890836, "number": 3, "id": "rkoNCSV4e", "invitation": "ICLR.cc/2017/conference/-/paper250/official/review", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer1"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "This paper proposes an extension of neural network language (NLM) models to better handle large vocabularies. The main idea is to obtain word embeddings by combining character-level embeddings with a convolutional network.\n\nThe authors compare word embeddings (WE),character embeddings (CE) as well a combined character and word embeddings (CWE). It's quite obvious how CE or CWE embeddings can be used at the input of an NLM, but this is more tricky at the output layer. The authors propose to use NCE to handle this problem.  NCE allows to speed-up training, but has no impact on inference during testing: the full softmax output layer must be calculated and normalized (which can be very costly).\n\nIt was not clear to me how the network is used during TESTING with an open-vocabulary. Since the NLM is only used during reranking, the unnormalized probability of the requested word could be obtained at the output. However, when reranking n-best lists with the NLM feature, different sentences are compared and I wonder whether this does work well without proper normalization.\n\nIn addition, the authors provide perplexities in Table 2 and Figures 2 and 3.  This needs normalization, but it is not clear to me how this was performed.  The authors mention a 250k output vocabulary. I doubt that the softmax was calculated over 250k values. Please explain.\n\nThe model is evaluated by reranking n-best lists of an SMT systems for the IWSLT 2016 EN/CZ task.  In the abstract, the authors mention a gain of 0.7 BLEU. I do not agree with this claim. A vanilla word-based NLM, i.e. a well-known model, achieves already a gain of 0.6 BLEU. Therefore, the new model proposed in this paper brings only an additional improvement of 0.1 BLEU. This is not statistically significant. I conjecture that a similar variation could be obtained by just training several models with different initializations, etc.\n\nUnfortunately, the NLM models which use a character representation at the output do not work well. There are already several works which use some form of character-level representations at the input.\n\nCould you please discuss the computational complexity during training and inference.\n\nMinor comments\n - Figure 2 and 3 have the caption \"Figure 4\". This is misleading.\n - the format of the citations is unusual, eg.\n   \"While the use of subword units Botha & Blunsom (2014)\"\n   -> \"While the use of subword units (Botha & Blunsom, 2014)\"", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512649766, "id": "ICLR.cc/2017/conference/-/paper250/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper250/AnonReviewer2", "ICLR.cc/2017/conference/paper250/AnonReviewer3", "ICLR.cc/2017/conference/paper250/AnonReviewer1"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512649766}}}, {"tddate": null, "tmdate": 1482014662649, "tcdate": 1482014662649, "number": 2, "id": "rJ1RyB7Ng", "invitation": "ICLR.cc/2017/conference/-/paper250/official/review", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer3"], "content": {"title": "lacks experimental evidence", "rating": "2: Strong rejection", "review": "this paper proposes a model for representing unseen words in a neural language model. the proposed model achieves poor results in LM and a slight improvement over a baseline model. \n\nthis work needs a more comprehensive analysis:\n- there's no comparison with related work trying to address the same problem\n- an intrinsic evaluation and investigation of why/how their work should be better are missing.\n- to make a bolder claim, more investigation should be done with other morphologically rich languages. Especially for MT, in addition to going from En-> Language_X, MRL_X -> En or MRL_X -> MRL_Y should be done.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512649766, "id": "ICLR.cc/2017/conference/-/paper250/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper250/AnonReviewer2", "ICLR.cc/2017/conference/paper250/AnonReviewer3", "ICLR.cc/2017/conference/paper250/AnonReviewer1"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512649766}}}, {"tddate": null, "tmdate": 1481826384600, "tcdate": 1481826384594, "number": 1, "id": "S1F8xwxNx", "invitation": "ICLR.cc/2017/conference/-/paper250/official/review", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512649766, "id": "ICLR.cc/2017/conference/-/paper250/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper250/AnonReviewer2", "ICLR.cc/2017/conference/paper250/AnonReviewer3", "ICLR.cc/2017/conference/paper250/AnonReviewer1"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512649766}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481503393441, "tcdate": 1478280682172, "number": 250, "id": "SygGlIBcel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SygGlIBcel", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481501891197, "tcdate": 1481501891190, "number": 8, "id": "rysp3woQg", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "Sywz2_yXx", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "perplexity evaluation and recommendations", "comment": "To compute perplexity, we simply consider words not in the vocabulary labelled as unknown. Their word representations are the unknown tokens in\nthe word and character vocabulary.\n\nThank you for your suggestions.\n- We made the choice not to have a related work section, to keep our paper at the recommended length. We will add one.\n- While we didn't priotirize experiments with bilstm given results previously obtained on a different task, we agree that we should compare these models.\n- MT was our first choice, but we will indeed focus on more intrisic tasks in the future."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499431835, "tcdate": 1481499431830, "number": 7, "id": "HJeV7Pjme", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "HJIcFATzg", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "PPL for open vocabulary", "comment": "We didn't consider such an approach, which could by adapted to our situation. Thank you for your suggestion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499294488, "tcdate": 1481499294331, "number": 6, "id": "HJIjfPiXx", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "ByDSOR6Gx", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "Results using character-level representations", "comment": "We indeed considered practictal suggestions describded in Kim et al AAAI 2015. The idea of using different convolution window sizes would however be difficult to experiment with, because of increased computation time.\nOur main problem, the poor performance of the output character-based based representation, is only briefly discussed in the first point of the section \"Further observation\". However, using NCE, we don't need to run our character layer on all the vocabulary at each batch, which gives us a reasonnable training time."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499266781, "tcdate": 1481499266776, "number": 5, "id": "SyiKMwiXx", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "SyB_D0TMg", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "NCE tuning", "comment": "We used the unigram distribution of words in the training data as our noise distribution, and sampled 25 noise examples for each sampled example from the data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499242018, "tcdate": 1481499242015, "number": 4, "id": "SJfOfwj7x", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "BkHmH0aze", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "Use of feed-forward NN", "comment": "We indeed use a feed-forward Neural Network."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499216567, "tcdate": 1481499216564, "number": 3, "id": "H1YIGviXl", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "rJnT406Me", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "Sec. 2.1: Character-level word embedding", "comment": "Indeed, the structure is the same (though we use only one size of character n-gram). The paper will be updated."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499135382, "tcdate": 1481499135376, "number": 2, "id": "SywZMDjXg", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "r1pTeCaGx", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "Notation", "comment": "Thank you for your remarks. \n\nThe use of the colon in the first equation of Sec 2.1 indicates concatenation of the embeddings.\n\nThere is indeed a mistake in the notations before the second equation of Sec 2.3:\n\"P^H((w:H)\\in D) [and not P_d^H((w:H)\\in D)], denotes the posterior probability ...\"\nWhen considering a sample example (context H, word w) coming from a mixture of the data distribution P_d and noise distribution P_n, this is the posterior probability for the example to come from the data. The sentence in the article indeed lacks clarity and will be updated.\n\ne^out and e^char-out denote respectively the output word embeddings and the output word embedding build from characters using the procedure describes in sec 2.1. We will also update our notation. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1481499076708, "tcdate": 1481499076702, "number": 1, "id": "r1p6WPo7g", "invitation": "ICLR.cc/2017/conference/-/paper250/public/comment", "forum": "SygGlIBcel", "replyto": "BJu9yRTGg", "signatures": ["~Matthieu_Labeau1"], "readers": ["everyone"], "writers": ["~Matthieu_Labeau1"], "content": {"title": "Objective function and vocabulary size", "comment": "In classical neural networks language models, as defined in Bengio et al(2003), words belong to a finite vocabulary. Given a network input (a context of several words), the model computes a multinomial distribution (of the vocabulary size) which describes what the next word will be. The objective defined at the beginning of section 2.3 aims at the maximum likelihood estimation of the model parameters (for the output multinomial distributions to be as close as possible to n-grams frequencies in the training data). The P_{theta} in the objective function LL(\\theta) is explicited in section 2.2, and it implies a sum over all term of the output distribution (in order to normalize it), which bounds the vocabulary that defines it to be finite. Moving to predicting individual character targets would indeed allow us to generate any word while using a finite number of character, but it would imply a very different network structure."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287666070, "id": "ICLR.cc/2017/conference/-/paper250/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287666070}}}, {"tddate": null, "tmdate": 1480719375493, "tcdate": 1480719375487, "number": 2, "id": "Sywz2_yXx", "invitation": "ICLR.cc/2017/conference/-/paper250/pre-review/question", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer3"], "content": {"title": "perplexity evaluation and recommendations", "question": "questions:\n- can you explain how do you calculate perplexity with an open vocabulary? that part is not clear.\n\n\nsuggestions:\n\n- related work : there's a fine amount of literature on addressing this problem. to list a few : https://arxiv.org/pdf/1508.02096.pdf,  https://arxiv.org/pdf/1603.08148.pdf\n I recommend having a related work section. \n\n- to provide a more thorough analysis I recommend comparing with lstm/bilstm char-level model.\n- MT adds another layer to your investigation. I think language modelling for morphologically rich languages is more of an intrinsic task to test your hypotheses compared to MT.\n\nMINOR issues:\n- there is some issues in using \\cite . for instance on page 5, \"The use of Adagrad Duchi et al. (2010) \" or \"a different task Labeau et al. (2015)\"\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959380766, "id": "ICLR.cc/2017/conference/-/paper250/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper250/AnonReviewer2", "ICLR.cc/2017/conference/paper250/AnonReviewer3"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959380766}}}, {"tddate": null, "tmdate": 1480612238439, "tcdate": 1480612238435, "number": 6, "id": "HJIcFATzg", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "PPL for open vocabulary", "comment": "In Sec. 4.2 you mention that perplexity is hard to interpret for models not using an explicit output vocabulary. When analysing open vocabulary approaches, perplexity can also be renormalized to character level, cf. e.g. Shaik et al. IWLST 2013. Did you consider this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480611903046, "tcdate": 1480611903041, "number": 5, "id": "ByDSOR6Gx", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Results using character-level representations", "comment": "Also Kim et al. AAAI 2015 got the similar conclusions w.r.t. the performance of character-level embeddings and also provided a discussion with suggestions for improvements. Did you consider these?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480611692794, "tcdate": 1480611692789, "number": 4, "id": "SyB_D0TMg", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "NCE tuning", "comment": "Can you provide more details on the configuration of the NCE training?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480611101501, "tcdate": 1480611101498, "number": 3, "id": "BkHmH0aze", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Use of  feed-forward NN", "comment": "From your notation I get that you used a feed-forward NN, can you confirm?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480611012151, "tcdate": 1480611012146, "number": 2, "id": "rJnT406Me", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Sec. 2.1: Character-level word embedding", "comment": "Can you confirm that the character-level word embedding used here is the same as in the google paper by Kim et al. AAAI 2015? It is not cited in Sec. 2.1.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480610231905, "tcdate": 1480609989163, "number": 1, "id": "r1pTeCaGx", "invitation": "ICLR.cc/2017/conference/-/paper250/official/comment", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Notation", "comment": "Pls. define the use of the colon in the first equation of Sec. 2.1\n\nP^H((w:H)\\in D) is not defined before the second equation in Sec. 2.3. Also, in the sentence introducing this equation to refer to \"this probability\" - please provide an explicit reference to what probability is meant here.\n\nPls. define e^{out} and e^{char-out} in Sec. 2.4 - are they the same as e^{out}_w in Sec. 2.2 and e^{char} in Sec. 2.1?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287665926, "id": "ICLR.cc/2017/conference/-/paper250/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers", "ICLR.cc/2017/conference/paper250/areachairs"], "cdate": 1485287665926}}}, {"tddate": null, "tmdate": 1480609680466, "tcdate": 1480609680461, "number": 1, "id": "BJu9yRTGg", "invitation": "ICLR.cc/2017/conference/-/paper250/pre-review/question", "forum": "SygGlIBcel", "replyto": "SygGlIBcel", "signatures": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper250/AnonReviewer2"], "content": {"title": "Objective function and vocabulary size", "question": "Sec. 2.3: please explain more explicitly why/in what circumstances the criterion implies a finite vocabulary. Especially, if e.g. moving to (individual) character targets, the word-level vocabulary would not be limited."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "pdf": "/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf", "paperhash": "labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations", "conflicts": ["limsi.fr"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Matthieu Labeau", "Alexandre Allauzen"], "authorids": ["labeau@limsi.fr", "allauzen@limsi.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959380766, "id": "ICLR.cc/2017/conference/-/paper250/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper250/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper250/AnonReviewer2", "ICLR.cc/2017/conference/paper250/AnonReviewer3"], "reply": {"forum": "SygGlIBcel", "replyto": "SygGlIBcel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper250/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959380766}}}], "count": 21}