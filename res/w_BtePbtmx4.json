{"notes": [{"id": "w_BtePbtmx4", "original": "Bac0lY_Dw0u", "number": 2641, "cdate": 1601308292441, "ddate": null, "tcdate": 1601308292441, "tmdate": 1614985753579, "tddate": null, "forum": "w_BtePbtmx4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4Nhb-4xKKsU", "original": null, "number": 1, "cdate": 1610040381362, "ddate": null, "tcdate": 1610040381362, "tmdate": 1610473974398, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "In this paper, the authors proposed a new approach by the name of LoCal + SGD (Localized Updates) to replace the traditional Backpropagation method. The key idea is to selectively update some layers\u2019 weights using localized learning rules, so as to reduce the computational complexity of training these layers so as to achieve a better tradeoff between overall speed and accuracy.\nThe paper received quite mixed reviewers. Some reviewers criticized the incremental nature of the proposed technology, while some other reviewers thought that this is one of the very early papers that demonstrates the practical effectiveness of localized learning. \n\nThe reviewers have made several rounds of discussions, and as a result of that, we think while this direction (localized learning) is very important and promising, this particular paper might not have provided a sufficiently novel and good solution to it.  Specifically, in terms of localized learning, this paper has not proposed brand new concepts or methodologies, instead it adopts existing methods in selective layers. In this sense, it does not really resolve the accuracy issue of localized learning, rather, it achieves the tradeoff by only applying localized learning in some layers. In other words, the current results still heavily rely on BP and has not brought a real breakthrough to localized learning.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040381347, "tmdate": 1610473974379, "id": "ICLR.cc/2021/Conference/Paper2641/-/Decision"}}}, {"id": "ZaXIt7ZDPhl", "original": null, "number": 2, "cdate": 1603864934180, "ddate": null, "tcdate": 1603864934180, "tmdate": 1606725948630, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review", "content": {"title": "A possible useful training practice, but some important experiments are missing", "review": "This paper try to leverage the benefit of Hebb learning to reduce CNN training time cost. In order to achieve this,  a learning mode selection algorithm is proposed to progressively increase  number of layers using Hebb learning. The writing of this paper is good and the idea is also interesting, however, the experimental part should be improved:\n\n1. The criterion used in learning mode selection algorithm is the model-update norm of current epoch. If the norm is small enough, the transition layer index will be increased. A small model-update norm also means that current layer is nearly convergent. Could you just fix these layers to accelerate training? Yes, freezing layer exps are tried but the comparison is not fair in my opinion. When Hebb-Learning-Layers are frozen, the final accuracy drops, but the training speedup is improved. So if you freeze less layers to make training speedups of freezing-layer-training and Hebb-Learning same, what will the accuracy relationship be? Does the proposed method still outperforms freezing strategy?\n\n2. A weak supervision scheme is proposed in this paper, but I did not find any experiments to evaluate its effect, could you add this part? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091770, "tmdate": 1606915767185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review"}}}, {"id": "xd5iQX7dhT", "original": null, "number": 1, "cdate": 1603477832720, "ddate": null, "tcdate": 1603477832720, "tmdate": 1606516115783, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review", "content": {"title": "First competitive results for a (partial) localized learning procedure. Has wide applications across many fields.", "review": "EDIT: 2020-11-27: Updated my score. Further explanations at the end of this post.\n\nSummary: \nThe authors propose an algorithm that separates a network into two distinct regions where one is trained with SGD while the other is trained with a local Hebbian learning rule. A weak supervision rule is proposed that improves localized learning. The authors demonstrate close to baseline performance on CIFAR-100 and ImageNet for diverse networks while speeding up training.\n\nStrong points:\n- Very creative use of multiple learning rules during training.\n- People could never get localized learning to work well enough to compete with SGD. This is the first work that uses some localized learning and manages to come close. This is a big success.\n- These findings have broad applicability, from brain-like learning algorithms, efficient training on one GPU, and efficient learning algorithms for model parallelism for massive networks where communication is the bottleneck.\n\nWeak points:\n- Missing ablation for the weak supervision algorithm.\n- This work is very impactful in many different ways, but it only mentions the computational efficiency perspective. The related work needs to be expanded to make readers aware of the impact of this work.\n- The heuristic for learning mode selection is complicated. Initial experiments suggested that a simple learning-rate-schedule-like way to select the learning mode would be possible.\n- Some algorithmic details in the weak supervision algorithm not clear.\n\nRecommendation (short):\nThis is very important work with results that will significantly impact many fields (efficient training, parallelization, brain-like algorithms). It is a creative solution to a significant problem in localized learning. I strongly recommend accepting this work. If this work is rejected, I will no longer review for future ICLR conferences. I recommend this work to be accepted as an oral presentation. Selecting this work for a best paper award would be appropriate.\n\nRecommendation (long):\nThis work is impactful for multiple reasons:\n- Localized learning is difficult. There has not been a work that uses localized learning and makes it work close to SGD performance on large datasets/models.\n- The brain does not use SGD, and it is difficult to think about algorithms that work in the brain and yield good performance. It might be that initial learning in the brain is done differently until local learning rules are used. This view is mostly ignored, but this paper yields evidence that such learning might be possible and efficient.\n- Layers updated with localized rules can be updated independently of other layers (if the weakly supervised rule is not used). This enables fully asynchronous training for early layers. There will be a synchronization point at the SGD layers, but through pipeline parallelism, the communication overhead can be hidden in Hebbian layers. This enables the training of massive neural networks with trillions of parameters. With current parallelism tools becoming more and more limited, this is a crucial innovation since previous asynchronous parallelism procedures always decrease predictive performance. This is the first work that shows a way to do asynchronous parallel training without performance degradation.\n\nBeyond this, the paper also yields some speedups for tasks while decreasing predictive performance only slightly. This is also an impressive feat, but the overall broad insights this paper yields make it much more impactful than this result. As such, I do not view the experimental results as the main contribution, but overall, the papers' main contribution is that it shows a way to include (gradual) localized learning in a neural network while not impacting performance.\n\n\nComments for authors:\nThis is excellent work \u2014 well done! I think the main weakness is currently a missing ablation on the effect of the weak supervision rule. You note that improves performance but by how much would be an important detail. If you do not include these ablations, I would still accept the paper, but I might rescind my oral presentation recommendation.\n\nAnother issue is that your work is relevant in many different domains, but you keep it confined to the idea that your method is only useful for faster training. I think making the reader aware that local training has many advantages across many domains could be very valuable. You do not need to elaborate on this, but I would like to see some of these connections in the paper because not everyone has the background to see these connections. I think you can do this mostly by mentioning it in the conclusion since you already mention a little bit of work in parallelism, and you mention previous results about local learning that failed to obtain good performance. Another line of work that I would mention in the related work section is that of neuroscientific faithful learning rules. The most relevant line of research is the work on various forms of feedback alignment and other algorithms. For a summary of past research and results on large datasets, see Bartunov et al., 2018[1]. Beyond this, you might want to add \"sparse training\" to the related work on efficient deep learning. Sparse training differentiates from pruning during training by initializing the neural network sparsely during initialization (not densely and then prune to sparse). See work on a mixture of experts[2,3] and sparse dynamic training[4,5,6]. I do not require you to include these references, but they might improve the related work section. \n\nOn the algorithmic and experimental side, it seems that a simple learning-rule-like schedule might be sufficient for selecting the learning mode. While I do not require you to add these experiments, it would make the algorithm simpler and more appealing if you can show that a simple learning rule works a la \"warmup with SGD for 5 epochs, then shift by 1 layer (block) every 5 epochs\" etc.\n\n[1] Sartunov et al., 2018. Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures\n[2] Shazeer et al., 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n[3] Lepikhin et al., 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n[4] Mostafa & Wang et al., 2019. Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization.\n[5] Dettmers & Zettlemoyer et al., 2019. Sparse Networks from Scratch: Faster Training without Losing Performance.\n[6] Evci et al., 2019. Rigging the Lottery: Making All Tickets Winners.\n\n\nUpdate:\n==================================\n\nComments for Area Chair and Reviewers: \n\nIf I view this work merely by the story conveyed in the paper, my assessment would be more in line with the other reviewers. I am not quite sure if this is the right way to evaluate this paper since I view it as having a broader impact that goes beyond the story in the paper, but other reviewers disagree with my view on its broader impacts. I see this as a sign that the paper is currently not in a good enough state to really convey its potential impact.\n\nComments for authors:\n\nI believe you still did good work here on the merits of the \"speedup training\" story that you convey in your paper. I believe that you have much more than this in your hands though. I think you could go two ways from here: (1) get this paper accepted in this form and work closely on the other angles that this work offers in a new paper, e.g. learning which is in line with biological or efficient parallelization of large networks. The second way (2) would be to rewrite this paper more in line with that view and resubmit. I think (1) might be better for you. I do not think many reviewers would understand a paper that comes from the process in (2). Good luck!", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091770, "tmdate": 1606915767185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review"}}}, {"id": "RUqcSUX3_yk", "original": null, "number": 13, "cdate": 1606290157373, "ddate": null, "tcdate": 1606290157373, "tmdate": 1606290221345, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "Qk5T8hfQNfX", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Thanks so much for your efforts", "comment": "I carefully read the rebuttal and really appreciate the efforts of the authors.\n\nPros: \n\nThis is an interesting idea. It may inspire researchers to create fundamentally new methods to replace Backpropagation.\n\nCons:\n\nThe generalization performance is not good, especially for large-batch optimization and LoCal+Adam.\n\nThe trade-off is accuracy and speed. It may hurt the convergence for some applications.\n\nThe limitation of the proposed method may be a concern.\n\nTherefore, I'd like to keep my current rating.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "UCL9azocUz2", "original": null, "number": 6, "cdate": 1605914790824, "ddate": null, "tcdate": 1605914790824, "tmdate": 1606272233980, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "xd5iQX7dhT", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We sincerely thank the reviewer for the positive feedback! We have incorporated the reviewer\u2019s suggestions and have provided our response as follows.\n\n\n1.\tWeak Supervision Ablation Analysis:\n\nWe have included an analysis of the impact of weak supervision for the ImageNet benchmarks in Table 5 (page 13) of the revised paper. As can be seen, the impact of the weak supervision technique on accuracy is 0.06-0.17%. Removing weak supervision leads to only a minimal degradation in accuracy, allowing us to successfully realize model parallelism during training as pointed out by the reviewer.\n\n2.\tExpanding Related Work:\n\nWe have updated the related work sections to include bio-plausible learning rules such as Target Propagation and Feedback Alignment. Due to space limitations, we are unfortunately unable to include references to sparse training efforts. It is definitely an interesting suggestion and will certainly be included with the increased page limit if the paper is accepted.\n\n3.    Static learning rate like schedule for learning mode selection :\n\nAs suggested by the reviewer, we have implemented a simple static schedule to determine the position of the Localized->SGD transition layer every epoch. We have updated the paper to include such an analysis in Section 6.5. The static schedule favors aggressive application of the localized learning rule in initial layers, and gradually decreases the number of epochs localized learning is applied in the deeper layers. \n\nN= [max\u2061(0,$c_{1}$- $c_{2}$*(E- $E_{max}$ )^2 ]\n\nWe opt for a quadratic scheduling function, as we empirically observe they perform better compared to the linear functions studied. Here N determines the position of the transition layer every epoch, $E_{max}$ is the maximum number of training epochs, and $c_{1}$ and $c_{2}$  are constants obtained using grid search.\n\nWe report the results using this static schedule in Table 8 (page 14) of the revised paper, on the ImageNet-ResNet50 and MobileNetV2 benchmarks. Compared to the existing approach used in the paper (Table 1,page 6), we find that the static schedule achieves slightly higher runtime benefits, for marginally lower accuracies. However, static schedules suffer from some drawbacks \u2013 several static scheduling functions are feasible, e.g. exponential, quadratic, etc., and identifying the best scheduling function for each network requires extensive empirical analysis. The learning mode selection algorithm utilized in the paper helps alleviate this by automatically identifying the position of the transition layer every epoch, leveraging the gradient information at the boundary between localized updates and SGD.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "vspWjV_NsDW", "original": null, "number": 5, "cdate": 1605914317007, "ddate": null, "tcdate": 1605914317007, "tmdate": 1606272170026, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "6YizFfZAPun", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the suggestions, please find our response below.\n\n1. Effort to configure hyper-parameters\n\nAs pointed out to Reviewer 5, we first tune the hyper-parameters $\\alpha$, $t_{shift}$, $L_{max}$   using simple grid search on a single benchmark for a particular dataset, and utilize the same hyper-parameters across all networks trained on that dataset. This vastly reduces time spent in hyper-parameter tuning, rendering it as merely a one-time cost. Moreover, to justify our use of a common set of hyper-parameters, we have listed the accuracy and runtime benefits accrued by tuning the $\\alpha$, $t_{shift}$, $L_{max}$ parameters for each individual network in Table 4, page 14 of the revised paper\u2013 what we observe is only a marginal improvement in accuracy and runtime benefits, over using a common set of hyper-parameters. We have updated the paper to include this analysis in Section 6.2.\n\nIn Figure 6 of Section 3.3 of the revised paper, we have conducted ablation studies for the  $t_{shift}$ and $L_{max}$ parameters for the ResNet50 network trained on the ImageNet dataset. The ablation analysis performed for $t_{shift}$  indicates that the accuracy benefits are highest for $t_{shift}$  in the range of 5-10% of total network depth, and is largely constant within this range. Further, the runtime benefits are largely insensitive to $t_{shift}$  as the average number of layers updated locally remain the same. We hence set $t_{shift}$ to a size of a recurring block for MobileNets and ResNets (For ResNet50 and MobileNetV2, $t_{shift}$\u22486% of network depth) . For VGG-like networks, it is set to a single convolutional layer. \n\nFor $L_{max}$, it is clear from the same figure that accuracy begins to drop steeply when localized updates are applied beyond 75-80% of the network layers. We hence set to $L_{max}$ to 75% in all our experiments. As mentioned previously, we find that these values of $t_{shift}$ and $L_{max}$ translate well to other benchmarks such as MobileNetV2 and ResNet34. \n\n\n2.   Weak Supervision Technique:\n\nThe weak supervision technique introduced is a coarse-grained, hyper-parameter free, low-cost supervision mechanism that modulates the learning rate of the locally updated layers, depending on the changes in the global classification loss. As mentioned in the paper, whenever the global classification loss increases, weak supervision encourages the weight updates of the locally updated layers to proceed in the reverse direction. The additional accuracy achieved by the weak supervision technique, on top of the learning mode selection algorithm, is around 0.06-0.17% across all our benchmarks- this boost in accuracy brings the LoCal+SGD training technique closer to the baseline SGD in terms of accuracy.  \n\nFurther, we note that we studied different weak supervision techniques, which for example rewarded/penalized the weight updates by different magnitudes when the classification loss decreased/increased. Such techniques provided a higher boost in accuracy (~1%) compared to the existing technique in the paper. However, due to the added hyper-parameter complexity, we opted for the simple, hyper-parameter free technique presented in the paper.\n\nFinally, we point out that the weak supervision technique developed will not be beneficial for optimizers such as SGD/Adam, as the errors are already calculated in a fine-grained manner for each neuron, based on overall classification loss.\n\n3. Incremental Benefits\n\nWe assume that the reviewer is referring to the incremental benefits of the two techniques, i.e., learning mode selection algorithm and weak supervision. We have thus listed the accuracy with and without using weak supervision, in Table 5 (page 13) of the revised paper. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "eFf6KAQstUq", "original": null, "number": 2, "cdate": 1605913176802, "ddate": null, "tcdate": 1605913176802, "tmdate": 1606272037252, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "xYbpldQCHMC", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 5, Part 1/2", "comment": "We thank the reviewer for the helpful comments, please find our response below.\n\n1.\tApplication of LoCal+SGD to large batch training: \n\nTo demonstrate the applicability of LoCal+SGD to large batch training, we \u2018simulate\u2019 a batch size of 32k on 4 GPUs, by using gradient accumulation [1]. The results are as follows in Table A, for the ResNet50 network trained using ImageNet:\n\n $\\hspace{3in}$  Table A\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++        \nBenchmark           $\\hspace{1.3in}$                                          Top-1 Error % \t$\\hspace{0.7in}$\t\t                                 Speed-Up        \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++     \nSGD baseline \t$\\hspace{1.3in}$       \t                                     26.8\t$\\hspace{1.3in}$\t\t\t\t                                 1x                                                                     \nLoCal+SGD             $\\hspace{1.4in}$                                             27.24`      $\\hspace{1.3in}$\t\t\t\t\t                                1.33x\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n                                         \nAs can be seen, LoCal+SGD remains scalable in a massively distributed scenario.\n[1]: https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html\n\n\n2.   Overhead of hyper-parameter tuning:  \n\nTo realize LoCal+SGD, we introduce three hyper-parameters: $\\alpha$, $t_{shift}$ and $L_{max}$. $t_{shift}$ controls the number of layers that switch to SGD-based updates every epoch, $L_{max}$ is the maximum number of layers that can be use localized learning, and $\\alpha$ defines a threshold that is used to determine whether the transition layer is shifted deeper into the network.\n\nTo obtain optimized values for these hyper-parameters, we first perform a grid search using a single network for a particular dataset (for example, we choose the ResNet50 network for ImageNet). We transfer the same hyper-parameter values to all other networks for the same dataset. We justify our use of common hyper-parameter values by the following experiment. In Table 4 (page 13) we depict the results on other ImageNet benchmarks (ResNet34 and MobileNetV2) when hyper-parameter tuning is performed for each benchmark individually. As can be seen, the accuracy and runtime benefits are only marginally better than those obtained using a common set of hyper-parameters obtained by tuning on the ResNet50 benchmark. We thus utilize common values for a dataset, effectively rendering them constants. The time taken to obtain these constants is thus a one-time cost, and does not impact the speedups obtained by LoCal+SGD.\n\nWe have included the above analysis in Section 6.2 of the paper.\n\n3. Number of epochs used for training:  \n\nThe number of epochs used for LoCal+SGD is the same as that of the baseline. We have clarified this in Section 6.1 (Appendix) of the revised manuscript.\n\n4. Absolute speed, implementation details : \n\nWe compare the LoCal+SGD implementation against optimized CUBLAS libraries (CUDA version is 9.0). The absolute speed i.e., average time taken by LoCal+SGD per epoch, to process a minibatch of size 64 on a single Nvidia GTX 1080 Ti GPU for the ImageNet benchmarks are reported in Table B below.\n\n $\\hspace{3in}$  Table B\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++        \nBenchmark           $\\hspace{1.3in}$                                          Total Time Taken to Process 1 minibatch=64 images        \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++     \nResNet34 \t$\\hspace{2in}$       \t                                    0.0128 sec\t\t\t                                                                                                      \nResNet50           $\\hspace{1.95in}$                                             0.0247 sec    \t\t\t\t                            \nMobileNetV2      $\\hspace{1.8in}$                                            0.0121 sec`      \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n5.  Limitations of proposed efforts : \n\nWe have discussed the limitations of our work in Section 3.3 of the revised manuscript. LoCal+SGD is difficult to apply to the deeper layers of the network \u2013 aggressive application of localized learning rules to more than 75-80% of the network layers often leads to severe losses in accuracy. Further, the applicability of such an approach to LSTMs, Transformers, and memory-augmented neural networks is also an open challenge that would be interesting to explore as part of future efforts.\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "SVflQWKVKiF", "original": null, "number": 4, "cdate": 1605913724269, "ddate": null, "tcdate": 1605913724269, "tmdate": 1606271809453, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "ZaXIt7ZDPhl", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the suggestions, please find our response below.\n\n1. Comparison against freezing layers during training :\n\nAs requested by the reviewer, we perform a more thorough comparison of LoCal+SGD against freezing layers during training. Specifically, we perform this comparison at iso-runtime, and compare the resulting accuracies of the approaches. We have updated the paper to include this analysis in Section 6.4.2.\n\nWe first identify the best LoCal+SGD configuration that can reach within 0.05%, 0.1%, 0.25%, 0.5% and 1% of the baseline SGD accuracy. Then, for the runtimes taken by each LoCal+SGD configuration, we identify the number of layers that must be frozen to match the training time, and report the resulting accuracy. Our results for the Cifar10 Resnet18 benchmark can be found in Table 7, page 14 of the revised paper. LoCal+SGD performs superior to freezing layers during training on 3 out of the 5 configurations studied, i.e., is a superior technique when the loss compared to SGD is allowed to exceed 0.1%.\n\n\n2. Weak Supervision Evaluation :\n\nWe have included the requested analysis in Table 5, page 13 of the revised paper. As seen, the weak supervision technique adds around 0.06-0.17% increase in Top-1 accuracy. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "rjD_VHlsjoz", "original": null, "number": 7, "cdate": 1605915306746, "ddate": null, "tcdate": 1605915306746, "tmdate": 1606260209770, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "NY4HU8Sy-tZ", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "1.  Applicability to other domains\n\nAs the reviewer pointed out, the primary focus of this work is on feed-forward classification models. While applying localized learning rules to smaller LSTM/RNN networks may be feasible, some additional thought is required in developing the hybrid training strategy, i.e., learning mode selection algorithm, in a manner that achieves a good trade-off between accuracy and training runtime. It is an interesting suggestion, and we will investigate it further as part of future efforts.\n\nHowever, extending LoCal+SGD to U-Nets is not difficult. The long-range connections are handled similar to the shortcut connections in ResNets. Consider a Layer K, whose input and output activations are $A_{K-1}$ and $A_{K}$. Further, let us assume Layer K receives activation input $A_{J}$ from a preceding layer J. The weight update for Layer K is performed by convolving the summed activation $A_{K}$ + $A_{J}$, with $A_{K-1}$. Based on the reviewer\u2019s suggestion, we have conducted experiments that demonstrate the feasibility of LoCal+SGD to U-Net training. The results are demonstrated in Table A below. We utilize the U-Net architecture from [1], and the isbi challenge dataset from [2]. \n\n$\\hspace{3in}$  Table A\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++        \nBenchmark           $\\hspace{1.3in}$                                          Dice Coefficient [3] \t$\\hspace{0.7in}$\t\t                                 Speed-Up        \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++     \nSGD baseline \t$\\hspace{1.3in}$       \t                                     0.948\t$\\hspace{1.3in}$\t\t\t\t                                 1x                                                                     \nLoCal+SGD             $\\hspace{1.4in}$                                             0.944      $\\hspace{1.3in}$\t\t\t\t\t                                1.28x\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n[1] U-Net: Convolutional Networks for Biomedical Image Segmentation: Olaf Ronneberger, Philipp Fischer and Thomas Brox \n\n[2] http://brainiac2.mit.edu/isbi_challenge \n\n[3] https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n\n2. Comparison against SGD baseline with fewer epochs:\n\nAs requested by the reviewer, we compare LoCal+SGD against an SGD baseline that is trained with fewer epochs, i.e., at iso-accuracy. We have updated the paper with this analysis in Section 6.4.1 for the ImageNet benchmarks. Clearly, LoCal+SGD achieves runtime benefits even when compared against a baseline with fewer training epochs.\n\n\n3.  Ablation analysis\n\nWe thank the reviewer for the suggestion. We have conducted an ablation analysis for $t_{shift}$ and $L_{max}$ and updated the paper in Section 3.3.\n\nThe ablation analysis performed for  $t_{shift}$  indicates that the accuracy is highest for $t_{shift}$  in the range of 5-10% of total network depth, and is largely constant within this range. The impact of $t_{shift}$ on runtime benefits is quite straightforward \u2013 the runtime benefits are largely insensitive to $t_{shift}$, as the average number of layers updated locally remains nearly the same across the training period. In all our experiments, we thus set $t_{shift}$ to 5-10% of total network depth.  \n\nFor $L_{max}$, it is clear from the same figure that accuracy begins to drop steeply when localized updates are applied beyond 80% of the network layers. Additionally, the runtime benefits naturally scale with increasing values of $L_{max}$.  Thus, $L_{max}$ is set to 75% in all our experiments. \n\nFurthermore, as pointed out to Reviewer 5, the above hyper-parameters are first identified using grid-search on one network, and the same values are used across all networks of a particular dataset. The impact of using such common hyper-parameter values, instead of tuning on each individual network is minimal, as shown in Table 4 in the revised version of the paper. We thus reduce hyper-parameter tuning to a one-time cost.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "Qk5T8hfQNfX", "original": null, "number": 3, "cdate": 1605913498869, "ddate": null, "tcdate": 1605913498869, "tmdate": 1606194886023, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "eFf6KAQstUq", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment", "content": {"title": "Response to Reviewer 5, Part (2/2)", "comment": "6. Applicability to other methods:\n\nWe are not sure what the reviewer is referring to as \u201cother methods\u201d. We assume the reviewer is referring to other optimization algorithms, such as Adam. For the paper, we use SGD with Nesterov momentum as the optimization method. To address the reviewer\u2019s question, we also implemented hybrid LoCal+Adam training for the Cifar10-ResNet benchmark, and present our results in Table C below. The results suggest that the approach can also work with other optimizers.\n\n $\\hspace{3in}$  Table C\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++        \nTraining Technique           $\\hspace{1.3in}$                                       Top-1 Error % \t$\\hspace{0.7in}$\t\t                                 Speed-Up        \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++     \nAdam \t$\\hspace{2.3in}$       \t                                     7.7\t$\\hspace{1.3in}$\t\t\t\t                                 1x                                                                     \nLoCal+Adam             $\\hspace{1.9in}$                                            7.96`    $\\hspace{1.2in}$\t\t\t\t\t                                1.48x\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\n\n7. Convergence Analysis:\n\nWe thank the reviewer for the suggestion. While a convergence analysis is a valuable suggestion, we would like to note that LoCal+SGD falls into the bucket of empirical techniques that accelerate training such as [1] and [2]. Theoretical analysis is out of the scope of this paper, but we will certainly consider it as part of future efforts.\n\n[1] Deep Networks with Stochastic Depth: Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra and Kilian Q. Weinberger\n[2] E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings: Yue Wang, Ziyu Jiang, Xiaohan Chen, Pengfei Xu, Yang Zhao, Yingyan Lin, Zhangyang Wan\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w_BtePbtmx4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2641/Authors|ICLR.cc/2021/Conference/Paper2641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Comment"}}}, {"id": "6YizFfZAPun", "original": null, "number": 3, "cdate": 1603903452127, "ddate": null, "tcdate": 1603903452127, "tmdate": 1605024163255, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review", "content": {"title": "Needs more work", "review": "This work demonstrates that localized learning can improve DNN training efficiency by reducing computation and memory requirements. The effectiveness of this approach is shown by the experimental results that report a nice trade-off of minimal accuracy loss and good throughput improvement compared to baseline SGD and competing efficient training techniques. \nWhile the experimental results are indeed appealing, a major flaw is that the paper does not sufficiently explain why the underlying techniques (learning mode selection and weak supervision) work. Since these techniques are configured with a number of hyper-parameters, and I could not gain an intuition of how/why/whether they work in general. For example, $t\\_{shift}$ is set to recurring block size of residual nets, but what is not given is the justification for this choice, or how to set for non-residual nets. In other words, the hyperparameter settings for these techniques appear ad-hoc (I suspect that it is not), and so it is not clear to me how much exploration is required. What could have helped is to include a study of the incremental benefits of these techniques in the experiment section. In summary, while the results are good, the writing and presentation could be greatly improved to help readers learn and use the proposal.\n`\n**Pros**\n1. Tackles an important problem of reducing time and resource requirements of DNN training.\n2. The general approach of computing layers differently over the course of training is quite intuitive. \n3. Presents two techniques that appear to make localized learning practical and effective for DNN training.  \n\n**Cons**\n1. The proposed techniques are parameterized (e.g., $t\\_{shift}$, $\\alpha$, etc.), but how, and effort required to configure them is not clear. \n2.  The proposed techniques are not sufficiently explained to help build intuition. For example, the weak supervision suggests that reversing the weight update direction can effectively reverse increase in classification loss (i.e., divergence), but it is not clear why this is the case or where this observation applies to SGD and other optimizers.  \n3. Incremental benefits of the techniques is not provided in evaluation. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091770, "tmdate": 1606915767185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review"}}}, {"id": "NY4HU8Sy-tZ", "original": null, "number": 4, "cdate": 1603988805758, "ddate": null, "tcdate": 1603988805758, "tmdate": 1605024163190, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes a combination of SGD with selective application of a non-backprop learning rule (Hebbian). The two learning rules are not applied together, but rather a boundary is determined where layers prior use SGD, and the ones after use the Hebbian approach. A selection algorithm dynamically adjusts the boundary over training. For accuracy reasons, they include weak supervision by using the overall classification loss to control the sign of the update. \n\nFrom computational efficiency perspective, the contributions reduce the need for a backprop calculation, and also leads to a smaller memory footprint, since the activation values need not be stored. On ImageNet benchmark models, they show <0.5% Top1 drop in exchange for ~1.3x runtime speed compared to vanilla SGD.\n\nStrengths\n+ Focus on run-time improvements brings practical significant to their proposed method\n+ Algorithm is relatively simple to implement\n+ Convergence results only show a small degradation from SOTA\n+ The tuning results for alpha (Figure 5) are useful for practitioners that need to balance accuracy and compute\n\nWeaknesses\n- The 'meta' boundary selection and weak supervision approaches add additional hyperparameter complexity to the tuning process. These are also empirically but not theoretically motivated, and unclear if they generalize to other domains. I understand that non-classification models are out of scope for this paper, but his paper's impact would be improved by some comment on transferability. For example, U-net models have long range skip connections that span the model. \n- While the focus on runtime is welcome, what is relevant to the practitioner is time-to-train to a particular accuracy target, similar to the metric adopted by MLPerf. Since this method does introduce an accuracy degradation (e.g. for RN-34, 27.04% versus 26.6%; Table 1), the more fair comparison would account for the fewer epochs the baseline SGD needs to hit 26.6%. To make a more convincing argument to practitioners, I would compare either the wall-clock time, or the total FLOPS needed, to hit the target accuracy.\n- Several techniques are introduced without ablations to measure their effectiveness and justify the added complexity. This is particularly important as these techniques add additional burden on the practitioner in terms of tuning the new hyperparameters. \n\nThe work combines existing learning rules (SGD, Hebbian) with some novelty in how they are employed, and with a weak supervisory signal, to achieve reasonable results. These contributions were not foundational improvements, so the paper's main merit is in the potential practical impacts of this method. The significance to practitioners however, is greatly reduced by the weaknesses described above.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091770, "tmdate": 1606915767185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review"}}}, {"id": "xYbpldQCHMC", "original": null, "number": 5, "cdate": 1604501148213, "ddate": null, "tcdate": 1604501148213, "tmdate": 1605024163127, "tddate": null, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "invitation": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review", "content": {"title": "interesting work but needs more details", "review": "Accelerating DNN Training through Selective Localized Learning\n\nIn this paper, the authors proposed a new approach by the name of LoCal + SGD (Localized Updates) to replace the traditional Backpropagation method. The key idea is to selectively update some layers\u2019 weights using localized learning rules. For these layers, the computation cost is reduced from two matrix multiply operations to one matrix multiply operation. The authors also proposed the Learning Mode Selection Algorithm to maintain the accuracy and convergence.\n\nThe authors provided some experimental results on common deep learning benchmarks such as ImageNet/ResNet and CIFAR/VGG. Overall, the authors reported that this approach can achieve around 1.36x speedup for a 0.4% loss in accuracy for ResNet-50. The authors also reported that they can achieve a higher speed than recent methods such as Structured Pruning and stochastic depth.\n\nThis work is a trade-off between computation and accuracy (details in Figure 5). I have some questions for the authors:\n\n(1) How stable is the proposed method (LoCal + SGD)? Does it still work for large-batch optimization and asynchronous training?\n\n(2) What is the overhead of hyper-parameter tuning?\n\n(3) Did the authors use the same number of epochs as the baseline to finish the training?\n\n(4) What is the absolute speed (e.g. in GFlops or TFlops)? Can the proposed method beat a well-optimized NVIDIA implementation?\n\n(5) What is the limit of the proposed method?\n\n(6) Can the Learning Mode Selection Algorithm work with other methods?\n\nSince this work fundamentally changes the way of learning, it is probably necessary to do a convergence analysis for the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2641/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2641/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating DNN Training through Selective Localized Learning ", "authorids": ["~Sarada_Krithivasan1", "~Sanchari_Sen1", "swagath.venkataramani@us.ibm.com", "~Anand_Raghunathan1"], "authors": ["Sarada Krithivasan", "Sanchari Sen", "Swagath Venkataramani", "Anand Raghunathan"], "keywords": ["Efficient DNN Training"], "abstract": "Training Deep Neural Networks (DNNs) places immense compute requirements on the underlying hardware platforms, expending large amounts of time and energy. We proposeLoCal+SGD, a new algorithmic approach to accelerate DNN train-ing by selectively combining localized or Hebbian learning within a StochasticGradient Descent (SGD) based training framework. Back-propagation is a computationally expensive process that requires 2 Generalized Matrix Multiply (GEMM)operations to compute the error and weight gradients for each layer. We alleviate this by selectively updating some layers\u2019 weights using localized learning rules that require only 1 GEMM operation per layer. Further, since the weight update is performed during the forward pass itself, the layer activations for the mini-batch do not need to be stored until the backward pass, resulting in a reduced memory footprint. Localized updates can substantially boost training speed, but need to be used selectively and judiciously in order to preserve accuracy and convergence. We address this challenge through the design of a Learning Mode Selection Algorithm, where all layers start with SGD, and as epochs progress, layers gradually transition to localized learning. Specifically, for each epoch, the algorithm identifies a Localized\u2192SGDtransition layer, which delineates the network into two regions. Layers before the transition layer use localized updates, while the transition layer and later layers use gradient-based updates. The trend in the weight updates made to the transition layer across epochs is used to determine how the boundary betweenSGD and localized updates is shifted in future epochs. We also propose a low-cost weak supervision mechanism by controlling the learning rate of localized updates based on the overall training loss. We applied LoCal+SGDto 8 image recognition CNNs (including ResNet50 and MobileNetV2) across 3 datasets (Cifar10, Cifar100and ImageNet). Our measurements on a Nvidia GTX 1080Ti GPU demonstrate upto 1.5\u00d7improvement in end-to-end training time with\u223c0.5% loss in Top-1classification accuracy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krithivasan|accelerating_dnn_training_through_selective_localized_learning", "pdf": "/pdf/926351e27cb4089d69321ebca1dd73e07c22d83f.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pfI7FeqGBe", "_bibtex": "@misc{\nkrithivasan2021accelerating,\ntitle={Accelerating {\\{}DNN{\\}} Training through Selective Localized Learning },\nauthor={Sarada Krithivasan and Sanchari Sen and Swagath Venkataramani and Anand Raghunathan},\nyear={2021},\nurl={https://openreview.net/forum?id=w_BtePbtmx4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w_BtePbtmx4", "replyto": "w_BtePbtmx4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091770, "tmdate": 1606915767185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2641/-/Official_Review"}}}], "count": 14}