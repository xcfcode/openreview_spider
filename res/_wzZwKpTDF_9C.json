{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392845640000, "tcdate": 1392845640000, "number": 6, "id": "dDibL51ebyDSr", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["Andrew Saxe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for the careful reviews! We've incorporated the suggestions made into a new revised draft, which will appear on arxiv soon (by Thu Feb 20th ).  We've split up our responses below by reviewer.\r\n\r\nAll reviewers:\r\n\r\nFormatting issues: Thank you for pointing these out, we have corrected these issues in the revised draft.\r\n\r\nLength/lack of space:  We realize this paper is a little long.  We tried to make our presentation as clear as possible in lieu of adhering strictly to the page limit, since the ICLR call for papers said it would not be too strict about space.  However, if it is necessary to get it down to size it will be straightforward to do so by judiciously moving a bunch of information to the supplementary material.  The fact that the paper will have large amounts of supplementary material should not on the other hand be interpreted as a weakness - but rather as a strength.   Moreover, if the paper is chosen as one of the ones to be published in JMLR, the supplementary material can be brought back in to make a substantive longer paper we feel.   \r\n\r\n\r\nReviewer a044:\r\n-'It should be noted that the authors' learning rule is not the batch version of the standard backpropagation algorithm'\r\n\r\nWe note, as mentioned in our earlier response, that we do in fact treat standard backpropagation. Our equations may look unfamiliar but they are simply a rearrangement of the terms in standard backpropagation (and the rearrangement uses the linearity of the network).\r\n\r\n\r\nReviewer 733d:\r\n\r\n-Special initial conditions: \u201cThe central issue I have with this paper is that the special initial conditions in which in analysis is done are essentially a partial solution to an SVD problem, which if it were solved fully, would give the optimal weights of the linear network in one step. In particular, the assumptions used require that the weight matrices share singular vectors in a way that basically turns the optimization across all layers into a decoupled set of scalar optimizations. In non-linear networks, where the optimal solution isn't given by an SVD, it is harder to believe that this kind of partial SVD would help as much as it does in the linear case (or perhaps at all).\r\n\r\nOur main goal is to understand the dynamics of learning in deep linear networks. In linear networks, the SVD is crucial to the solution--and hence it should not be surprising that it plays a critical role in understanding the dynamics as well. Finding a change of variables in which the dynamics decouple is a central contribution of this paper. We also note that the dynamics starting from small random weights are very well described by our solutions (see our Fig. 3)--hence our special initial conditions are not too special, in the sense that they behave similarly to starting from small random weights. We certainly agree that the change of variables we use will not necessarily help decouple the dynamics in nonlinear networks when they are operating in their nonlinear regime, but we believe our results shed light on certain behaviors of nonlinear networks for two reasons. First, even nonlinear networks, when they contain nonlinearities such as tanh and are initialized with small random values, may start off in a roughly linear regime that is well described by our results (see our Fig. 3 for an empirical example). Thus early in training, nonlinear networks may behave similarly, though the quality of the linear approximation is likely to break down as training proceeds and the network enters its nonlinear regime. Second, our deep linear networks exhibit a number of features seen in nonlinear nets, such as long plateaus during training with relatively little performance improvement followed by rapid learning; a slowing of training speeds with depth; and faster convergence from pretrained initial conditions than from small random initial conditions. Fully understanding these phenomena in the linear case is an important goal in its own right, and may lead to further avenues of analysis for certain nonlinearities (such as piecewise linear activation functions like rectified linear units).\r\n\r\nFurthermore, in the newer version of the manuscript, we note that we have analyzed not only pre-trained initial conditions on weights, but also a new class of random orthogonal initial conditions on weights, and shown that they both yield depth independent learning times, in contrast to random Gaussian (Glorot-style) initialization.   Thus our newer version yields an initialization method (random orthogonal) that does not involve solving any part of the optimization problem (not that pre-training either involves solving the supervised learning problem). \r\n\r\n-Learning speed analysis in continuous and discrete time\r\nWe've been thinking about your suggestion to bound the error one gets by taking a continuous time approximation and choosing lambda based on when this error can be argued to be negligible. At the moment, we see no clear criterion to use to argue that error will be negligible. Our curvature condition, by contrast, has been widely used in theoretical analyses in the field, and our empirical experiments back up the results we obtain using this method. For this reason we have confidence that our analysis gives an accurate picture of learning speeds in deep linear networks.  In particular, the main qualitative result of the analytic theory that pre-trained and random orthogonal initialization, but not random Gaussian, achieve depth independent training times was confirmed by numerical simulations of gradient descent in discrete time.  This gives us confidence that our analytic methods describe well what happens in discrete time.\r\n\r\n-'edge of chaos': Thank you, we have edited the paper to introduce this term before using it, since it may be unfamiliar.  We give an intuitive description of it in the introduction.\r\n\r\n-'the input-output correlation matrix contains all of the information about the dataset used in learning' is indeed only true for linear networks. We do not feel it is necessary to reiterate our use of linear networks here, given that this restriction is clearly stated in our title, abstract, introduction, first sentence of the section, and all equations and derivations leading up to this point in the text.\r\n\r\n-Page 3: 'column of W^{21}' This should have a bar over it, thank you. We have corrected this in our new draft.\r\n\r\n-Page 4: 'What kinds of networks did ref [19] analyze?' Thank you for the comment, we have edited the draft to indicate that the analysis was for linear neural networks. ([19] presents an analysis of three layer networks).\r\n\r\n-Page 6: 'caveat' We've changed the wording in the draft.\r\n\r\n-Page 7: Clarification of classification task. We've changed the wording to make it clear that details of the task, not just the learning rate, can be found in appendix C.\r\n\r\n-Page 8: 'Optimization performance is only improved in the short term and maybe medium terms by pre-training vs standard carefully scaled inits. In the longer term (which is what really matters in the end, as it dominates most of the run-time) the difference is less significant, if it is present at all.' \r\n\r\nWhile this may indeed be a feature of nonlinear neural network learning, our goal in this paper is to understand the linear case. We feel this is an important prerequisite for understanding more complex nonlinear networks (we agree that it is hard to imagine saying much about arbitrary nonlinearities, but rectified linear units and other recent piecewise linear activation functions might be amenable to future analysis building on our own). Your comment raises an important point that we have included in our revised draft: Our analysis is likely, to the extent that it does also illuminate the behavior of nonlinear networks, to be most accurate in the early epochs of learning when the nonlinear network is operating in a roughly linear regime (assuming it starts from small weights, with a nonlinearity that is roughly linear around the origin such as tanh). We make no claims that this initial period represents the bottleneck in training deep nonlinear networks. Our goal is to study the linear case, and show which pieces of nonlinear behavior can be explained in this way. We have edited the draft to make it clear that our results, depending on details of initializations and nonlinearities, might describe the early portion of training a nonlinear network; but cannot be expected to apply to later portions of training, when the nonlinear network is in a more nonlinear regime. We also note that linear networks serve as a sort of lower bound on training times. Thus the fact that very deep linear networks, such as the 100 layer networks we train, can require 150 epochs to reach the error obtained in just a few epochs by a pretrained net, means that a nonlinear network of this depth would almost certainly suffer at least as severely, and hence pretraining will provide a speed boost early in training.\r\n\r\n-'it seems reasonable to suspect that in later stages of optimization that the network may behave in a much less linear fashion as the units move out of their linear regimes, and so the optimization advantages of pre-training will diminish or even disappear' \r\nWe definitely agree that as neurons move out of their linear regime our analysis should not be expected to apply. We have edited our draft to emphasize this. We note that, unless random initial conditions somehow speed up late in training relative to pretrained initial conditions (i.e., pretraining actually hurts you once you\u2019re in the nonlinear regime), the benefits accrued early in training are likely to persist (though they may be small relative to the overall training time). It therefore seems unlikely that the optimization advantage will \u201cdisappear\u201d entirely.\r\n\r\n-'In Appendix D you cite some evidence from other papers. But the results with Hessian-free optimization and other methods are consistent with what is written above (believe me, I know this work very well). Look for example at the results for SGD in Figure 3 of the Chapelle & Erhan paper that you cite. SGD reaches nearly identical KLs on MNIST after 700 dataset passes when you compare random initializations vs RBM pre-trained initializations. The Curves results seem to favor pre-training a small bit with SGD, but the difference is small (maybe 1.5x faster), and would likely become insignificant in the longer term as random-init SGD appears to be catching up before the graph cuts off. My experience, along with that of many other people who have studies these methods is that if you run these kinds of experiments longer, the approximate 1.5-2x speed increase with pre-training wanes significantly over time, eventually reaching near parity with well chosen random initialization methods, so that overall not much time is saved, if any.' \r\nBased on the published evidence, we believe we are justified in concluding that pretraining confers an optimization advantage. To take your example of Chapelle & Erhan, Fig. 3, the KL divergence of RBM-pretrained nets on MNIST using SGD reaches about 4.8 by epoch 700. The random initialization takes until approximately epoch 1000 to reach this level (in making these comparisons, note that both the x and y axes differ between subplots). Hence pretraining does confer an optimization advantage in this instance of about 300 epochs. Furthermore, the plot you identified (Fig. 3 MNIST) is the one in which this advantage is least evident; their Fig. 1 shows Hessian free methods converging more quickly on both MNIST and Curves datasets (Curves: pretrained initializations reach a KL divergence of 10 on iteration 10 or so, compared to iteration 120 for random initializations. MNIST: pretrained drops below 10 by iteration 30, while for random none have dropped below 10 by iteration 70). The other dataset used in Fig. 3 (Curves) shows a robust optimization advantage due to pretraining (pretrained KL drops below 2 around iteration 400; random drops below 2 around iteration 1400). Although the current evidence available in the literature does not appear to document it, we certainly agree it is possible that this speed advantage would wane if these experiments were run for a longer time. Our analysis is only for linear networks, and hence cannot be expected to accurately apply to networks in their nonlinear regime. Thus we would expect our analysis to be most relevant early in training. Nevertheless, the empirical results we have cited show a very consistent optimization advantage due to pretraining, even in nonlinear nets, that persists and remains significant to a reasonable length of training time (hundred to a thousand iterations). We also emphasize that the magnitude of the speed increase may not always be large enough in practice to warrant the extra trouble (and time) of pretraining a network. Our goal is not to say that pretraining should always be used by practitioners, but rather, to understand why it confers an optimization advantage.\r\n   Also, we now give a random (orthogonal) initialization scheme that does as well as pre-training.  We are fundamentally not interested in advocating one initialization scheme over another; we just want to gain an understanding of *why* an initialization scheme confers an advantage when it does. \r\n\r\n-Page 8: 'The discussion about what pre-training should do didn't make sense to me in multiple places.'\r\nWe're sorry this section was not clear enough--we've rewritten it completely in the new draft. \r\n\r\n-'For example, previously you said that you were assuming Sigma^11 = I, but I guess this is no longer the case.'\r\nThat is correct. In fact our analytic results for the learning dynamics can be generalized to allow Sigma^11 = VDV^T where V are the right singular values of the input-output correlation matrix. Then each mode still completely decouples. We excluded this result from our main exposition in the interest of brevity and simplicity, but we've now seen that it's necessary to explain the advantage of pretraining. The details of this will be added to the supplementary appendix.\r\n\r\n\r\n-'You then say that the product of the weights converges to Sigma^31(Sigma^31)^-1. But how can this be true unless N2 >= N1,N3? When N2 < N1,N3, the product of the weight matrices has rank at most N2, while Sigma^31(Sigma^31)^-1 will be full-rank in general.'\r\n\r\nYes this is true, we were taking the full rank case to simplify the presentation and focus on intuitions. Details of the extension to the rank constrained case are very straightforward (take the best rank N2 approximation).\r\n\r\n\r\n-'You defined a^alpha to be a vector before, but now it is a 'strength'?'\r\nThank you for catching the typo, we meant 'a', not 'a^alpha'. 'a' is the scalar strength of the mode.\r\n\r\n\r\n-When you consider taking W21 = R2Q^T where R2 is 'an arbitrary orthogonal matrix', what does this actually mean? Orthogonal matrices are square, and so is Q, so does that make W21 square (i.e. N1 = N2)? And if R2 is truly arbitrary, then it is meaningless, as any orthogonal matrix M can be written as R2 Q^T for some R2 (take R2 = MQ). \r\n\r\nThank you for the comment, we have abused notation a bit here: R2 will not always be orthogonal (since it may not be square) depending on the size of N2 and N1. What we require for a decoupled initialization is that R2^TR2=I. For square R2, this means it must be orthogonal and, as you rightly point out, arbitrary--W21 might converge to any orthogonal matrix. Interestingly, in this case pretraining reduces simply to establishing the orthogonal initial conditions that we discuss in the subsequent section. For N2 < N1, R2 is size N2 x N1 and R2^TR2 cannot equal the identity. We require that it equal a diagonal matrix D of size N1 that is only nonzero for N2 elements on the diagonal, where it must be equal to 1. I.e., it must ignore some input directions, and learn the rest with unity scaling. With pretraining, the directions with the most input variance will be learned, and the directions with smaller variance will be the ones ignored. Hence, compared to random orthogonal initializations, pretraining biases the initial solution to input directions with high variance. Thus in this case we obtain a slightly modified consistency condition: input-analyzing singular vectors must match input principal components for the first N2 highest input variance directions, and the set of the N2 largest input variance components must be the same set as the N2 largest singular values of the task. Clearly for this setting, R2 is not arbitrary. Finally for N2 > N1, we do require R2^TR2=I, but this does not mean R2R2^T=I. We can write R2 as AB where A is size N2 x N2 orthogonal, and B is a matrix of size N2 x N1 with C = [R; 0] where R is N1 x N1 orthogonal. We have edited the draft to clarify this.\r\n\r\n-'When you say 'Now consider fine-tuning on...' are you now taking the input and output not to be equal. This is confusing especially since this sentence appears in a paragraph which starts by saying that we are now talking about auto-encoding with input = output.'\r\n\r\nYes. In the standard pretraining/finetuning paradigm, one first pretrains a network using an unsupervised method, in our case an autoencoder, and subsequently finetunes on a task of interest, in our case by minimizing squared error on a set of input-output pairs. This is the setting we analyze. We are only talking about autoencoding for the pretraining period.\r\n\r\n\r\n-Page 9: 'What is the error being plotted in the figure?'\r\n\r\nAll throughout the paper, our linear networks are trained to minimize squared error as defined in the first section. Details of the input and output data are in the supplementary material.\r\n\r\n\r\n-Page 9: 'That initializing a linear network with part of the optimal SVD solution (which if you take the full solution, will instantly give the optimal error) isn't too surprising. It would be much more interesting to see if these initialization schemes work well in the nonlinear case.'\r\n\r\nWe believe there's a misunderstanding here--the network is not initialized with the full solution. It is initialized with the results of unsupervised pretraining using an autoencoder. This is an unsupervised initialization that clearly cannot know the full solution to the final supervised task. Indeed, the pretrained initial condition can in theory be very different from what is required to learn the task. It will be relevant to the solution to the degree that our consistency condition is satisfied. We hope that our revision of this section, which was not originally clear enough, will have made this evident.\r\n\r\n-Page 9: \u201cThe Glorot initialization scheme uses uniform random numbers, and I think there is a factor of 6 somewhere in there. Gaussians with 1/sqrt(N) is definitely not what they end up recommending. Also, they did their calculations for tanh units, although it should probably work for linear ones. In my experience, getting the scale constants and other details right matters a lot in practice!\u201d\r\n\r\nThe recommendation of Glorot et al. is an initialization that allows the variance of the gradient to remain constant with depth. To do this, the variance of the distribution each weight is drawn from, var(W), must equal 1/N. Using uniform random numbers, this requires initializing to U[-sqrt(6)/sqrt(2N), sqrt(6)/sqrt(2N)]. For Gaussian random variables, this requires a variance of 1/sqrt(N) as we have used. Glorot et al. makes no claim that uniform is better than Gaussian. In fact, our empirical results in Fig. 6 are for the uniform initialization (the exact initialization used in Glorot et al.), not a Gaussian initialization--we have edited the text to note this fact. We certainly agree that getting scale constants right matters greatly in practice--this is the effect we're analyzing. Using a uniform distribution without the factor of sqrt(6), or using a Gaussian with an extra factor of sqrt(6) will change the results a great deal.\r\n\r\n-Page 9: How can you have the weight matrices be random orthogonal matrices? Orthogonal matrices are square and the weight matrices don't have to be. \r\n\r\nThe generalization of orthogonal matrix to the rectangular case is any matrix whose singular values are all 1.  Moreover one can maintain perfect dynamical isometry in products of such matrices in a subspace of dimension as large as the number of neurons in the smallest layer. To do so, one need only have the column space of one layer feed into the row space of the next layer, so that no further dimensions of activity get killed.  We have added a note to the document. \r\n\r\n\r\n-Page 11: Merely scaling g very large won't be any good. The 'activity' will propagate, yes, but the units will completely saturate. \r\n\r\nYes we agree, a very large g might accomplish the goal of allowing activity to propagate through a deep network, but might hinder other important goals such as keeping units desaturated. Nowhere do we claim that scaling g very large is optimal, or that activity propagation is the only relevant concern in deep networks.  In fact we show that g near 1 guarantees the nice property of dynamical isometry for random scaled orthogonal initializations with the tanh nonlinearity. \r\n\r\n-Page 12: 'Variance' isn't really what eqn 21 is measuring. That implies deviation from the mean, which doesn't have to be 0 here. The units could all be completely saturating (this is bad), but eqn 21 would still have a non-zero finite value. \r\n\r\nWe neglected to mention an implicit assumption:  if the nonlinearity is odd, and the weights are random orthogonal, then the distribution of activations x_i across neurons i in a layer will be symmetric about the origin, and therefore this distribution will have zero mean.  Therefore Eqn 21 is measuring variance.  We have added this note to the text.\r\n\r\n-Page 12: The analysis done in section seems interesting. It does sound similar to work done on Echo State Networks (ESNs) and initializations. Have you compared to that? \r\n\r\nWe're glad you found the analysis interesting, yes, this is similar to echo state networks among others, which is why we used the term 'edge of chaos, in analogy with terminology for recurrent networks' (p12). In ESNs, however, the results are for a fixed recurrent matrix that is often not altered during learning.   More generally, this analysis is essentially the method of mean field theory from statistical physics, which predates the advent of echo-state networks.\r\n\r\n -What is missing for me from this section is an application of these ideas to real networks. Do these ideas improve on optimization performance? \r\n\r\nAs you can imagine, we are very interested in this question! However we think that to do a proper, careful comparison is too much for this already long submission.\r\n\r\n\r\nReviewer 9c88:\r\n\r\nThank you for the kind comments, we are very encouraged that you found the analysis insightful!\r\n\r\n-Ambiguity of 'learning time': We agree, this is an important point. We have changed the text to indicate that we are considering only iterations and not computation time.\r\n\r\n- p 9:  'mode strength was chosen to be small (u=0.001)' because ... ? \r\n\r\nA small initial mode strength is an accurate approximation for learning that starts from small random initial conditions on the weights. If a large initial mode strength is used, this corresponds to embedding knowledge about the task before learning starts--in the extreme, if the initial mode strengths are chosen to be the corresponding singular values, then the input-output map is initialized at its optimum and no learning is necessary. Greedy layer wise pretraining on MNIST, because MNIST satisfies the consistency condition, provides an unsupervised way to initialize mode strengths to greater values than can be achieved using small random weights. This correspondingly improves learning speeds. We included this note as an attempt to explain the difference between the two plots--although both start on the decoupled manifold, pretrained solutions start with a larger mode strength and hence learn much more quickly.\r\n\r\nWe've corrected the typos below in the new draft, thanks for catching them.\r\n-'We show (Fig 6A, left, red curve)' -> 'We empirically show (\u2026'\r\n- Figure 6: 'A Histograms of the singular values' should be 'B Histograms of the singular values':\r\n- p 11:  'netoworks' -> 'networks'  \r\n- Discussion: 'propery' -> 'property'  \r\n\r\n- Section 4: 'and phi(x) is any nonlinearity that saturates as ...' Any nonlinearity, really? This doesn't sound right, since any nonlinearity could incorporate some scaling that could override the role of gain factor g. You should more specifically characterize the properties of the nonlinearity, and state here which one(s) you will actually consider.  \r\n\r\nAt the level of generality that we first introduce phi, it can indeed be any nonlinearity. We say in general that there exists a g_c such that if the gain g < g_c activity decays, and if g > g_c activity does not decay (and does not blow up due to saturation).  g_c of course depends on the nature of the nonlinearity (and could be 0 or infinity).  We have not specified how in general it depends on the nonlinearity (one has to solve the fixed point equation in the appendix).   When the nonlinearity is tanh (or any monotonically increasing nonlinearity with slope 1 at the origin), g_c = 1.  \r\n\r\n-Appendix D.2: 'An analysis of this effect in deep linear networks' I believe you meant to write 'in deep nonlinear networks' since linear networks would all converge to equivalent solutions (at different paces) and thus could not exhibit any difference in generalization.\r\n\r\nWe did actually mean deep linear networks--by 'this effect' we meant the generalization advantage due to pretraining, which we believe can exist in deep linear networks, though it may require reformulating the learning problem. The ref [20] we cite provides an example of how generalization performance can be analyzed in deep linear networks (there a three layer network), and our suggestion is that it may be possible to build on this work to analyze the case of using unlabeled data to do unsupervised pretraining, followed by supervised fine tuning. Their analysis uses overrealizeable input-output maps and adds measurement noise to the training data. Whether analysis based on these assumptions would shed light on what happens in typical deep nonlinear networks is unclear to us, but it seems worth pursuing. We agree that, if training is run to completion, all linear nets would converge to the same solution, and thus some early stopping criterion might be necessary. Working through these issues is an interesting goal for future work.   Interestingly - our work reveals analytically the effects of early stopping - it kills the components of the small singular values/vectors of the input-output correlation matrix in the product of weights across layers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392173880000, "tcdate": 1392173880000, "number": 5, "id": "eeQ8Ai9IvaA-o", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["anonymous reviewer 9c88"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "review": "* Brief summary of the paper:\r\n\r\nThis work is mostly a mathematical analysis of the dynamics of gradient descent learning in *linear* multi-layer networks, which surprisingly displays a complex non-linear dynamic. It delivers analytic results and novel insights regarding learning speed, how it is affected by depth, and regarding optimal initialization regimes, drawing connections to the benefits of pre-training strategies. In particular it shows that orthogonal initialization is a better strategy than the appropriately scaled random Gaussian initialization proposed in Glorot & Bengio AISTATS 2010. Mathematical results are empirically validated with linear networks trained on MNIST. Some empirical results using non-linear tanh networks are also provided, convincingly showing that some of the insights gained from the analysis of linear networks can apply to the non-linear case. \r\n\r\n* Quality assessment:\r\n\r\nThis is a very well written paper. I believe it offers a novel, thorough, and enlightening analysis of the dynamics of learning in linear networks. This is a worthy enterprise, and succeeds in uncovering novel insights that will likely also have an impact on practical approaches to training non-linear deep networks. It also paves the way to a more formal analysis of the dynamics of learning in non-linear deep nets.\r\nI found the analysis of initialization strategies in sections 3 and 4 particularly interesting due to their practical relevance. \r\n\r\nMy only concern, since this paper literally offers a lot, has to do with the ICLR policy on page limits (and text squeezing), that I am unsure about. To be checked.\r\n\r\n\r\n* Pros and Cons:\r\n\r\n+ Thorough mathematical analysis, conveying valuable novel insights on the dynamics of learning in deep linear networks\r\n+ Evidence that these insights can have practical relevance for improving approaches to training deep non-linear networks\r\n- TODO: Check ICLR policy on page limits??\r\n\r\n\r\n* Detailed suggestions for improvements:\r\n\r\n- Top of Figure 3 hides part of the above paragraph (a part that seemed important for understanding!)\r\n- Bottom of caption of figure 3 too close to main text (almost overlaps)\r\n\r\n- I find the term 'learning time' ambiguous or ill-chosen, as you do not take into account the fact that deeper networks usually involve more computation per epoch. I suggest you clearlry define it as meaning number of training iterations (epochs) upon first introducing it.\r\n\r\n- Top of page 5: there is an ill-placed newline between 'logarithmically spaced' and the rest of the sentence.\r\n\r\n- p 9: \r\n'mode strength was chosen to be small (u=0.001)'  ... because ... ? \r\n\r\n'We show (Fig 6A, left, red curve)' -> 'We empirically show (...'\r\n\r\n- Figure 6: 'A Histograms of the singular values' should be 'B Histograms of the singular values'\r\n\r\n- p 11: \r\n'netoworks' -> 'networks'\r\n\r\n- Section 4:\r\n'and phi(x) is any nonlinearity that saturates as ...' Any nonlinearity, really? This doesn't sound right, since any nonlinearity could incorporate some scaling that could override the role of gain factor g. You should more specifically characterize the properties of the nonlinearity, and state here which one(s) you will actually consider.\r\n\r\n- Discussion:\r\n'propery' -> 'property'\r\n\r\nAppendix D.2:\r\n'An analysis of this effect in deep linear networks' I believe you meant to write 'in deep nonlinear networks' since linear networks would all converge to equivalent solutions (at different paces) and thus could not exhibit any difference in generalization."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392167340000, "tcdate": 1392167340000, "number": 4, "id": "Du1surK3sxC6t", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I'm not an official reviewer of this paper but I want to state for the record that I think it should be accepted. In the interest of full disclosure, I should make it clear that I'm a former co-author of the the lead author of this paper, but I don't think that has compromised my objectivity in this case. I also reviewed an earlier version of this work for NIPS 2013 without knowledge of its authorship and I also recommended that it be accepted then.\r\n\r\nI don't intend for this comment to be a complete review, but I do want to argue against one of the main criticisms of this paper that I've seen made by the official reviewers, which is that deep linear models are uninteresting and that the algorithms studied are not used in practice.\r\n\r\nLinear networks formed by composing matrix multiplies are an interesting model class in my opinion. They allow us to study some of the properties of deep networks without needing to commit to analyzing the most difficult cases.\r\n\r\nOther papers have previously been published at leading conferences about using deep linear models to study deep learning. See for example this ICML 2012 paper from Geoffrey Hinton's group:\r\nhttp://www.cs.toronto.edu/~tang/papers/deep_mfa.pdf\r\nThis paper chooses to study deep mixtures of factor analyzers (DMFAs) 'even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels'.\r\n\r\nWhile it is true that the state of the art deep nets are not linear, so technically the exact method studied here is not used in practice, I think that the current state of the art deep nets are similar enough for this case to be interesting. For a wide variety of problems, rectified linear hidden units feeding into a linear classifier such as a softmax or SVM are the state of the art. I myself have also had a lot of success with my 'maxout' units, which are also locally linear. So quite a lot of the current neural net literature deals with networks whose functions are linear on domains of non-negligible size.\r\n\r\nIf you train a deep rectifier net and monitor how many hidden units change which linear piece they are are for each gradient step, this 'piece change rate' is quite small. I ran this experiment on a state of the art feedforward net on MNIST (trained with dropout and rectified linear units). I found that by the end of the first epoch, the piece change rate is less than 1%, and by the end of training it has fallen to less than 0.04%. This suggests that on short timescales it is actually quite reasonable to model training of state of the art deep nets as being linear. I suspect the fidelity of such a model breaks down over longer time scales, but if our model of short term learning dynamics allows us to improve the short term behavior of our learning algorithms, I suspect improving the short term behavior at every time step will result in an improvement in the long term behavior, even if we are not able to specifically predict that behavior accurately.\r\n\r\nFor these reasons, I suspect that insights gained on the model family that this paper studies will lead to improvements in training of non-linear deep nets."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392137340000, "tcdate": 1392137340000, "number": 3, "id": "e4N34lBE3RWox", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["anonymous reviewer 733d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "review": "This paper analyzes gradient descent learning in deep networks of linear units.  While such networks are trivially equivalent to 1 layer linear transforms, the authors argue that their learning dynamics should act as a tractable analogy for learning in non-linear networks.  Under certain simplifying assumptions they are able to produce some analytic expressions for how gradient descent learning acts in the idealized continuous time case.  \r\n\r\nThere is also an attempt to justify pre-training in terms of this analysis that I find not particularly convincing, as here the 'pre-training' amounts to solving the problem almost in the linear case but certainly not in the linear case.  \r\n\r\nA new addition to this paper is a discussion non-linear networks and the role of initialization, which includes some interesting numerical simulations demonstrating that certain choices of scaling constant under a particular 'orthogonal' initialization scheme.  I like this part of the paper the best.  However, it is not the focus of the paper and should be developed further (perhaps in another paper?).\r\n\r\nI should say that I reviewed this paper before.   It is in many ways improved from the original version.  A few problems still remain, and there are also many new ones (page 8 in particular). However, I'm mostly satisfied with it and would recommend acceptance.  I'm hoping the authors can answer my various question, as some parts of the paper either confused me and possibly contain mistakes.\r\n\r\n\r\nIn terms of the main content, the central issue I have with this paper is that the special initial conditions in which in analysis is done are essentially a partial solution to an SVD problem, which if it were solved fully, would give the optimal weights of the linear network in one step.  In particular, the assumptions used require that the weight matrices share singular vectors in a way that basically turns the optimization across all layers into a decoupled set of scalar optimizations.  In non-linear networks, where the optimal solution isn't given by an SVD, it is harder to believe that this kind of partial SVD would help as much as it does in the linear case (or perhaps at all).\r\n\r\nI appreciate the attempt to make the analysis more rigorous in terms of 'learning speed'.  Although perhaps a better way to work out a realistic lambda would be to bound the error one gets by taking the continuous time approximation of the original difference equations and choose lambda based on when this error can be argued to have a negligible effect.  A lambda which has been chosen via some kind of curvature criterion will generally give a more stable iteration, but there is no guarantee that this will imply a reasonable correspondence between the discrete and continuous time versions, where mere stability isn't enough.\r\n\r\n\r\n\r\n\r\n\r\nAbs/Intro:  'edge of chaos'?  This is only explained late into the paper, and sounds bizarre without any context.\r\n\r\nPage 3:  'the input-output correlation matrix contains all of the information about the dataset used in learning'.  You mean just for learning linear networks/functions right?  Surely this isn't true in general...\r\n\r\nPage 3: 'column of W^{21}'  Should this have a bar over it?\r\n\r\nPage 4:  When you say that the fixed point structure of gradient descent learning was worked out in [19], you don't mention on what kinds of networks this analysis was done.\r\n\r\nPage 5:  There are formatting issues at the top of the page.  Some text is cut off.\r\n\r\nPage 6:  The word 'caveat' (used twice) doesn't seem appropriate here.  I usually don't think of special cases of definitions, or simplifying assumptions, as 'caveats'.\r\n\r\nPage 7:  The bottom of the page contains formatting issues with the figure.\r\n\r\nPage 7:  You should mention this is a classification task and also that the details are in appendix C.  The way it is written now it sounds like only the details of the choice of learning rate are in the appendix.\r\n\r\nPage 8:  My experience is that optimization performance is only improved in the short term and maybe medium terms by pre-training vs standard carefully scaled inits.  In the longer term (which is what really matters in the end, as it dominates most of the run-time) the difference is less significant, if it is present at all. Improvement in generalization performance is less controversial, and in my opinion remains the best reason to try unsupervised pre-training.\r\n\r\nI think it is quite reasonable to suspect that, insofar as the networks are approximately linear, perhaps unsupervised pre-training is having the boosting effect that your paper predicts that it might have.  But it seems reasonable to suspect that in later stages of optimization that the network may behave in a much less linear fashion as the units move out of their linear regimes, and so the optimization advantages of pre-training will diminish or even disappear.  This is consistent with the evidence from the literature.  \r\n\r\nIn Appendix D you cite some evidence from other papers.  But the results with Hessian-free optimization and other methods are consistent with what is written above (believe me, I know this work very well).  Look for example at the results for SGD in Figure 3 of the Chapelle & Erhan paper that you cite.  SGD reaches nearly identical KLs on MNIST after 700 dataset passes when you compare random initializations vs RBM pre-trained initializations.  The Curves results seem to favor pre-training a small bit with SGD, but the difference is small (maybe 1.5x faster), and would likely become insignificant in the longer term as random-init SGD appears to be catching up before the graph cuts off.    My experience, along with that of many other people who have studies these methods is that if you run these kinds of experiments longer, the approximate 1.5-2x speed increase with pre-training wanes significantly over time, eventually reaching near parity with well chosen random initialization methods, so that overall not much time is saved, if any.\r\n\r\nPage 8:  The discussion about what pre-training should do didn't make sense to me in multiple places.\r\n\r\nFor example, previously you said that you were assuming Sigma^11 = I, but I guess this is no longer the case.  You then say that the product of the weights converges to Sigma^31(Sigma^31)^-1.   But how can this be true unless N2 >= N1,N3?  When N2 < N1,N3, the product of the weight matrices has rank at most N2, while Sigma^31(Sigma^31)^-1 will be full-rank in general.\r\n\r\nYou defined a^alpha to be a vector before, but now it is a 'strength'?\r\n\r\nWhen you consider taking W21 = R2Q^T where R2 is 'an arbitrary orthogonal matrix', what does this actually mean?   Orthogonal matrices are square, and so is Q, so does that make W21 square (i.e. N1 = N2)?  And if R2 is truly arbitrary, then it is meaningless, as any orthogonal matrix M can be written as R2 Q^T for some R2 (take R2 = MQ).\r\n\r\nWhen you say 'Now consider fine-tuning on...' are you now taking the input and output not to be equal.  This is confusing especially since this sentence appears in a paragraph which starts by saying that we are now talking about auto-encoding with input = output.\r\n\r\nIn what sense is 'W^21 = R2D1V11' a 'task'?   I suspect you meant something else here but this sentence is very awkwardly phrased.\r\n\r\nI gave up trying to understand what this part was actually trying to say.\r\n\r\n\r\nPage 9:  What is the error being plotted in the figure?\r\n\r\nPage 9:  That initializing a linear network with part of the optimal SVD solution (which if you take the full solution, will instantly give the optimal error) isn't too surprising.  It would be much more interesting to see if these initialization schemes work well in the nonlinear case.\r\n\r\nPage 9:  The Glorot initialization scheme uses uniform random numbers, and I think there is a factor of 6 somewhere in there.  Gaussians with 1/sqrt(N) is definitely not what they end up recommending.  Also, they did their calculations for tanh units, although it should probably work for linear ones.   In my experience, getting the scale constants and other details right matters a lot in practice!\r\n\r\nPage 9:  How can you have the weight matrices be random orthogonal matrices?  Orthogonal matrices are square and the weight matrices don't have to be.\r\n\r\nPage 11:  Merely scaling g very large won't be any good.  The 'activity' will propagate, yes, but the units will completely saturate.\r\n\r\nPage 12:  'Variance' isn't really what eqn 21 is measuring.  That implies deviation from the mean, which doesn't have to be 0 here.  The units could all be completely saturating (this is bad), but eqn 21 would still have a non-zero finite value.\r\n\r\nPage 12:  The analysis done in section seems interesting.  It does sound similar to work done on Echo State Networks (ESNs) and initializations.  Have you compared to that?  Sutskever et al. (2013) also report finding empirically that there was a critical range of scale parameters around which learning seemed to work best.  These scale parameters were also applied to weight matrices which had their spectral radius initially normalized to 1, which is similar again to the ESN papers. \r\n\r\nWhat is missing for me from this section is an application of these ideas to real networks.  Do these ideas improve on optimization performance?  A good comparison would be against a *properly implemented* Glorot-style initialization (see my above comments re. this), and/or something like the sparse initialization used in the HF papers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391520480000, "tcdate": 1391520480000, "number": 2, "id": "IIFDI_qHXBNmK", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["Andrew Saxe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for the review, we have uploaded a new draft with expanded material on a new property, called dynamic isometry, that allows gradients to propagate from input to output in a deep linear net. We have added a comparison to and analysis of other random initialization schemes in both linear and nonlinear nets.\r\n\r\nAlso we'd like to point out that we do in fact study standard backpropagation. In the linear case, backpropagation gives exactly the equations used in the paper. Our form is simply a rearrangement of terms from what one usually sees (and the rearrangement relies on the linearity of the network). For example, the hidden unit activity h in standard back propagation can be written as h=W_1x in the linear case. The error is delta=(y-hat y), and in the linear case, hat y=W^{32}W^{21}x. Hence the change in W^{32} = delta h^T =(y-W^{32}W^{21}x)x^TW^{21}=(yx^T-W^{32}W^{21}xx^T)W^{21} which is our expression. A similar argument will yield our expression for the change in W^{21}. See our citation [16] for a similar derivation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390429260000, "tcdate": 1390429260000, "number": 1, "id": "TTDzYRnZ-UTo_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "_wzZwKpTDF_9C", "replyto": "_wzZwKpTDF_9C", "signatures": ["anonymous reviewer a044"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "review": "In this paper, the authors try to analyze theoretically the dynamics of learning in deep neural networks. They consider the quite restricted case of deep linear neural networks. Such linear deep networks are not of practical interest, because the mapping y = Wx between the input vector x and output vector y in them can always be realized using a single weight matrix W which is the product of weight matrices of different layers, and therefore adding hidden layers does not improve their performance.\r\n\r\nAs the performance criterion the authors use the standard mean-square error between the output vectors of the networks and their target values (desired responses), which is in practice approximated by the respective squared error over all the training pairs. The gradient descent method applied to this criterion provides the batch learning rule (1) used in the paper. It should be noted that the authors' learning rule is not the batch version of the standard backpropagation algorithm. This is because already in the case of a single hidden layer it depends on the two weight matrices between the input and hidden layer, and between the hidden and output layer.\r\n\r\nDespite the linearity of the studied deep networks, this learning rule has nonlinear dynamics on weights that change with the addition of each new hidden layer. The authors show by simplifying further their analyses that such deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initialization.\r\n\r\nIt is good that the authors know and refer to the paper by Baldi and Hornik from the year 1989, reference [19]. In this paper Baldi and Hornik showed that in a linear feedforward (multilayer perceptron) network with one hidden layer the optimal solution minimizing the mean-square error is given by PCA, more accurately by the subspace spanned by the principal eigenvectors of the correlation matrix of the input data, coincides with the data covariance matrix for zero mean data. However, Baldi and Hornik did not analyze the learning dynamics.\r\n\r\nThe paper suffers from lack of space. It extends to 10th page and with supplementary material to 15th page. Obviously because of space limitations figures are too small and there is no Section 1. Introduction, only plain text after the abstract.\r\n\r\nPros and cons:\r\n--------------\r\n\r\n+ The analysis on learning dynamics is novel and interesting. I am not aware of this kind of analyses on deep networks before this paper.\r\n+ With their mathematical analyses, the authors are able to explain some phenomena observed experimentally in learning deep networks.\r\n- The deep network analyzed is linear, with no practical interest.\r\n- The learning algorithm(s) that the authors study are not used in practice."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387947600000, "tcdate": 1387947600000, "number": 54, "id": "_wzZwKpTDF_9C", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "_wzZwKpTDF_9C", "signatures": ["asaximus@gmail.com"], "readers": ["everyone"], "content": {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "decision": "submitted, no decision", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.", "pdf": "https://arxiv.org/abs/1312.6120", "paperhash": "saxe|exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Saxe", "James L. McClelland", "Surya Ganguli"], "authorids": ["asaximus@gmail.com", "mcclelland@stanford.edu", "sganguli@stanford.edu"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}