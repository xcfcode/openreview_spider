{"notes": [{"id": "Bkl8YR4YDB", "original": "B1xAXEduvB", "number": 1249, "cdate": 1569439358347, "ddate": null, "tcdate": 1569439358347, "tmdate": 1577168224942, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DCcY4ZUKAE", "original": null, "number": 1, "cdate": 1576798718575, "ddate": null, "tcdate": 1576798718575, "tmdate": 1576800917991, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Decision", "content": {"decision": "Reject", "comment": "The authors address the problem of training an NMT model on a really massive parallel data set of 40 billion Chinese-English sentence pairs, an order of magnitude bigger than other cz-en experiments. To address noise and training time problems they propose pretraining + a couple of different ways of creating a fine-tuning data set. Two of the reviewers assert that the technical contribution is thin, and the results are SOTA but not really as good as you might hope with this amount of data. This combined with the fact that the data set is not released, makes me think that this paper is not a good fit with ICLR and would more appropriate for an application focussed conference. The authors engaged strongly with the reviewers, adding more backtranslation results. The reviewers took their responses into account but did not change their scores. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709570, "tmdate": 1576800258363, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Decision"}}}, {"id": "rJxOid9SjH", "original": null, "number": 4, "cdate": 1573394592292, "ddate": null, "tcdate": 1573394592292, "tmdate": 1573817009259, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "SklbY_5rjB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment", "content": {"title": "response to Review #1 (second half) ", "comment": "Re: did you try the approach of \"Hassan et al (2018)\" suggested in Section 4.1, where only in-domain sentences are selected? It is true that \"every time we encounter a new domain, we have to retrain the model\", but I think this is still a more viable approach than pre-training on the full 40B sentences. Why not trying to train on the top-100M sentence pairs that are the most in-domain?\n\nThank you for the comment. We actually did try as you suggested and followed Hassan et al (2018) in the first place and found it did not work well. Results from vanilla transformer are shown as follows:\ntop-100M+20M       single           joint training                                            25.4\ntop-100M+20M       single           pretrain(100M)+finetune(20M)             26.2\ntop-100M+20M       ensemble    joint training                                            26.9\ntop-100M+20M       ensemble    pretrain(100M)+finetune(20M)             27.6\n\nWe will add these results to the updated version. \nAdditionally, \n1) Hassan et al (2018) select different training instances for different target domains. This means every time we encounter a new domain, we have to retrain the model. \n2) The value of data filtering threshold is crucial but hard to decide. \n\nIn summary, it is widely accepted that the large-scale training corpus often introduces performance boost. We do believe that using 40B properly will definitely leads to better performance.  The intuition of our work is to leverage the pretraining and fine-tuning pipeline for domain specific translation task. The 40B training corpus is to train a universal machine translator and then fine-tune the model on the training set of the target domain. We believe that \n\n\nRe: In the back-translation (BT) experiments, did you select 100M monolingual sentences randomly? If that is the case, this is expected to see a drop in performance, BT is usually a very effective, but not so much when the monolingual data is noisy or out of domain. Although it is critical to work with cleaned data in NLP (especially in the context of generation), dataset cleaning is not really addressed in the paper.\n\n-- It is expected that BT does not help our setting. BT works when we don't have enough parallel data, and we generate significantly larger augmented dataset using BT. But in our setting where we have huge parallel dataset, we need even large BT-generated data to make it help, and this might not be practical.  \n-- As mentioned in Section 3, actually we spent significant amount of time in data cleaning. \nfollowed Uszkoreit et al., 2010 `https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36580.pdf` and used a standard dynamic programming approach for data cleaning. \n\nRe: The technical contributions seem quite thin to me for a ML conference. The dataset used in the paper is not available to the research community, which prevents reproducibility. As mentioned above, I think several important experiments are missing.\n\n-- In this paper, we provide a general mechanism to handle massively large training dataset in NMT. We propose the dynamic data split strategy for pretraining of MT models. This proposed strategy is effective and easy to implement, and can also be applied to any other large-scale pretraining language models as well as other tasks in the field of NLP. We believe such methods can make great contributions to the research community.\n-- Since the 40B corpus is brought from about 60 commercial translation agencies, we are now actively negotiating with the legal department for the allowance to release the dataset for non-commercial uses.\n\n\n[1] Hassan et al. Achieving Human Parity on Automatic Chinese to English News Translation. 2018\n[2] He et al. Hard but robust, easy but sensitive: How encoder and decoder perform in neural machine translation. 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1249/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1249/Authors|ICLR.cc/2020/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158918, "tmdate": 1576860557260, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment"}}}, {"id": "rkx1OUbhoS", "original": null, "number": 5, "cdate": 1573815911206, "ddate": null, "tcdate": 1573815911206, "tmdate": 1573816490403, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "S1lNNanTYS", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment", "content": {"title": "response to Review #3", "comment": "thank you for the useful comments. \n\n1. regarding WMT19\nThank you for the sensible comment. We added results on both WMT18 and WMT19, and the model achieves SOTA results on both: a bleu score of 34.4, +2.6 over the best previous system on WMT18 and 42.2, +2.9 over the best previous system on WMT19. Please refer the paper for details. \n\n2. Large-scale back translation\nThanks for the comment. We think it is a good idea. We will add more experiments regarding large-scale back translation in the updated version. \n\n3. data and model publication\n the corpus is brought from about 60 commercial translation agencies. We are now actively negotiating with the legal department for the allowance to release the dataset for non-commercial uses. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1249/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1249/Authors|ICLR.cc/2020/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158918, "tmdate": 1576860557260, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment"}}}, {"id": "SklbY_5rjB", "original": null, "number": 3, "cdate": 1573394552851, "ddate": null, "tcdate": 1573394552851, "tmdate": 1573394552851, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "r1gPaCZ9KB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment", "content": {"title": "response to Review #1 (first half)", "comment": "Re: I'm curious about why only 6 layers are used in the encoder and decoder? Large scale pretraining like in BERT usually benefits from very large datasets, but also very large models. 6 layers seems very small given the size of the training set. \n\n-- Thank you for the comment. For model structure, we followed the previously SOTA structure of the Microsoft MT system in \"Achieving Human Parity on Automatic Chinese to English News Translation\". The adopted structured (6 blocks, 16 heads, 1,024 embedding dimension and 2,048 inner-layer dimension) is a commonly used setup in MT. Given fixed memory, the number of layers trades the number of heads, embedding dimension, and batch size. Additionally, the structure for BERT cannot readily be transferred to MT (BERT training uses TPU. And here we use V100). \n\n\nRe: Table 2 reports results with \"Small\" and \"Large\" models. What does this correspond to? Only Section 4.3 discusses the size of the model, but it does not mention different architectural choices.\n\n-- Sorry for the confusion. \"Small\" and \"Large\" are discussed in the \"Model Size and Data Size\" paragraph of section 5.  \"Small\" refers to the model with 6 blocks, 8 heads, 512 embedding dimension and 512 inner-layer dimension and large refers to the model with 6 blocks, 16 heads, 1,024 embedding dimension and 2,048 inner-layer dimension.\n\n\nRe: In Section 5.1 the paper mentions \"the single-model achieves a BLEU score of 29.7, already outperforming the current best system\", but in Tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7.\n\n-- Nice catch. 29.7 is a typo. All should be 28.7. We will correct it. \n\n\nRe: Table 2 suggests that a single model, even trained with 40B sentence pairs, does not outperform a single model trained with 20M sentence pairs as in \"He et al., 2019\", while being significantly more expensive to train.\n\n-- Thanks for the comment. We think it is not fair to compare with our result with He et al., 2019 because\na) He et al., 2019 is an ensemble result\nb) The model that achieves 29.1 involves sophisticated structure design. From their reported results, a vanilla transformer achieves a bleu score of 23.20. Since our result is obtained based on vanilla transformer, it is fair to compare our result with their bleu score  of 23.2, rather than 29.1. \nc) our single model pretrained on 40B ran only 2 epochs (which already took 3 months) and was far from convergence,\n\n\nRe: The comparisons in Table 1 are done between single and ensemble models, which is not a fair comparison. The model with a BLEU score of 32.0 uses an ensemble of 10 models. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs?\n\n-- Sorry for confusion. These results are listed in comparison to our single and ensemble models, respectively. \n\nWe choose the following models for comparison to our single models:\n--- Hassan et al., 2018 with a BLEU of 24.2 refers to a single model.\n--- Hassan et al., 2018 with a BLEU of 27.4 in Table 1 refer to a single model\n\nWe choose the following models for comparison to our ensemble models:\n--- He et al., 2019 with a BLEU of 29.1 in Table 1 refers to an ensemble model. \n--- Hassan et al., 2018 with a BLEU of 28.4 in Table 1 refers to an ensemble system. \n--- Wang et al., 2017 refers to an ensemble system. \n\nAdditionally, \nOur implementation of an ensemble of 10 vanilla transformer models on 20M obtains a bleu score of 26.7, and we will add it to the next version. \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1249/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1249/Authors|ICLR.cc/2020/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158918, "tmdate": 1576860557260, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment"}}}, {"id": "B1xQw39Qsr", "original": null, "number": 2, "cdate": 1573264475429, "ddate": null, "tcdate": 1573264475429, "tmdate": 1573264513914, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "SJeI_UPxcB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment", "content": {"title": "response to the Review #2", "comment": "thank you for the sensible comments. We do agree that reproducibility is important regarding this paper. \n\nThe origin of the dataset: the dataset was purchased from about 60 translation agencies. We do agree that releasing the dataset is of great importance and we are now actively negotiating with the legal department for the allowance to release the dataset for non-commercial uses. \n\nAdditionally, this paper for the first time provides a general mechanism to handle massively large dataset in NMT, and this strategy can be easily extended to any large-scale MT datasets. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1249/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1249/Authors|ICLR.cc/2020/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158918, "tmdate": 1576860557260, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment"}}}, {"id": "r1gPaCZ9KB", "original": null, "number": 1, "cdate": 1571589823181, "ddate": null, "tcdate": 1571589823181, "tmdate": 1572972493364, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes an approach to train NMT models on extremely large parallel corpora. Because of the dataset size, training several epochs on the full dataset with a single model is too expensive. As a result, the dataset is split into several chunks, on which different models are trained independently. The different models are combined to form an ensemble model. Different strategies are proposed to split the dataset effectively. The resulting model achieves a performance of 32.3, outperforming the previous SOTA by 3.2 BLEU.\n\nIn Section 4.3, I'm curious about why only 6 layers are used in the encoder and decoder? Large scale pretraining like in BERT usually benefits from very large datasets, but also very large models. 6 layers seems very small given the size of the training set. Table 2 reports results with \"Small\" and \"Large\" models. What does this correspond to? Only Section 4.3 discusses the size of the model, but it does not mention different architectural choices.\n\nIn Section 5.1 the paper mentions \"the single-model achieves a BLEU score of 29.7, already outperforming the current best system\", but in Tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7.\n\nI feel that the results are a bit disappointing given the scale of the experiments. Table 2 suggests that a single model, even trained with 40B sentence pairs, does not outperform a single model trained with 20M sentence pairs as in \"He et al., 2019\", while being significantly more expensive to train. Also, the comparisons in Table 1 are done between single and ensemble models, which is not a fair comparison. The model with a BLEU score of 32.0 uses an ensemble of 10 models. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs?\n\nAlso, did you try the approach of \"Hassan et al (2018)\" suggested in Section 4.1, where only in-domain sentences are selected? It is true that  \"every time we encounter a new domain, we have to retrain the model\", but I think this is still a more viable approach than pretraining on the full 40B sentences. Why not trying to train on the top-100M sentence pairs that are the most in-domain?\n\nIn the back-translation (BT) experiments, did you select 100M monolingual sentences randomly? If that is the case, this is expected to see a drop in performance, BT is usually a very effective, but not so much when the monolingual data is noisy or out of domain. Although it is critical to work with cleaned data in NLP (especially in the context of generation), dataset cleaning is not really addressed in the paper.\n\nOverall, the experimental setup is impressive, but the improvements in terms of BLEU are relatively small, and the technical contributions seem quite thin to me for a ML conference. Moreover, the dataset used in the paper is not available to the research community, which prevents reproducibility. Also, as mentioned above, I think several important experiments are missing."}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574979371149, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Reviewers"], "noninvitees": [], "tcdate": 1570237740134, "tmdate": 1574979371167, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review"}}}, {"id": "S1lNNanTYS", "original": null, "number": 2, "cdate": 1571831084044, "ddate": null, "tcdate": 1571831084044, "tmdate": 1572972493328, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis work conducts a large scale study on pretraining for neural machine translation. Overall, this work makes good contributions to the community, but the experiments need improvements. \n\nPros: \n\t1. The data scale is huge, with 40 billion sentence pairs.\n\t2. The results are promising, with 3.2 BLEU improvement over STOA results.\n\nCons:\n\t1. It is pity that the trained model is only evaluated on one test set and experiments are conducted on one language pair. Thus, it is not clear to me whether the improvement is general across datasets and language pairs. I understand that it is costly to conduct such a large scale study on another language pair. At least, it is easy to test the models on other datasets for the same language pair. For example, I'd like to see the results on WMT 2019 Chinese->English translation dataset.\n\t2. I'm curious how large-scale pretraining compare with large-scale back translation. The following paper shows that large-scale back translation can also significantly improve the final translation accuracy. Note that back translation only needs monolingual data, while the pretraining in this work needs bilingual sentence pairs. \nEdunov, Sergey, Myle Ott, Michael Auli, and David Grangier. \"Understanding back-translation at scale.\" arXiv preprint arXiv:1808.09381 (2018).\n\nBesides, will the model be shared to the public?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574979371149, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Reviewers"], "noninvitees": [], "tcdate": 1570237740134, "tmdate": 1574979371167, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review"}}}, {"id": "SJeI_UPxcB", "original": null, "number": 3, "cdate": 1572005485765, "ddate": null, "tcdate": 1572005485765, "tmdate": 1572972493284, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\nThis paper investigates the effectiveness of a massively large parallel corpus in NMT training, which consists of more than 40 billion En-Zh parallel sentences.\nTo the best of my knowledge, the 40 billion parallel corpus for the NMT training is the largest reported in the paper published so far.\n \nFor preventing long training time, this paper proposes a practical data split and utilization method, which the authors call \u201cdynamic-data-split.\u201d\nThe key idea of their method is to dynamically assign training instances to different model components and update different components according to the assigned instances.\n \nThis paper reports the BLEU score of WMT17 Chinese-English dataset for 32.3, which significantly outperformed the best score, and improved the performance of existing state-of-the-art results.\nThey also provide several deeper analyses of the proposed method by changing the model training strategy (pretrain only, pretrain+finetune), data split strategy, data size, and tokenization (word, BPE, character).\n \n\n\n\nThe main concern of this paper is the reproducibility of the experiments.\n \nTheir main focus is to investigate the effectiveness of 40B massive parallel data.\nHowever, the origin and how the authors correct the data is fully unknown; in the paper, they only say, \u201cThe data comes from diverse sources such as web pages (\u223c2 billion), digitized books (\u223c1 billion) and private purchase from translation agencies (\u223c46 billion).\u201d\nWhat is the \u201cprivate purchase from translation agencies.\u201d\nNo one knows how they were collected except the authors.\nIt is impossible to reproduce the results of the experiments conducted in this paper in future validation.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574979371149, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Reviewers"], "noninvitees": [], "tcdate": 1570237740134, "tmdate": 1574979371167, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Review"}}}, {"id": "SyxaX6OQ_r", "original": null, "number": 1, "cdate": 1570110756683, "ddate": null, "tcdate": 1570110756683, "tmdate": 1570110756683, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Hkgi3h5GuS", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment", "content": {"comment": "thanks for the comment. We will correct the incorrect reference. \n\nAlso, we will list more example output and do the qualitative analysis in the updated version. ", "title": "reply"}, "signatures": ["ICLR.cc/2020/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1249/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1249/Authors|ICLR.cc/2020/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158918, "tmdate": 1576860557260, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Official_Comment"}}}, {"id": "Hkgi3h5GuS", "original": null, "number": 2, "cdate": 1570053298737, "ddate": null, "tcdate": 1570053298737, "tmdate": 1570053298737, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Public_Comment", "content": {"comment": "Neat! Two minor comments:\n\n- The paper says that \"It is widely accepted that large-scale text learning can automatically learn and encode\nlinguistic structures (Hinoshita et al., 2011; Williams et al., 2018)\": I largely agree with your claim, but the citation to our work (Williams et al.) is wrong. In that paper, we argue that a specific class of models *fails* to learn any of the structure that it's meant to learn.\n\n- I'd love to see some example output and qualitative discussion of where you see improvements.", "title": "Minor comments"}, "signatures": ["~Samuel_R._Bowman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Samuel_R._Bowman1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504197476, "tmdate": 1576860590379, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Public_Comment"}}}, {"id": "BJla-C1fuB", "original": null, "number": 1, "cdate": 1570008580799, "ddate": null, "tcdate": 1570008580799, "tmdate": 1570008580799, "tddate": null, "forum": "Bkl8YR4YDB", "replyto": "Bkl8YR4YDB", "invitation": "ICLR.cc/2020/Conference/Paper1249/-/Public_Comment", "content": {"comment": "These are strong results, and it's a pity to invest so much in machines and then so little in eval.\n\nFor the benefit of the research community, I'm happy to offer a free and anonymous ModelFront evaluation.\n\nIt would include a break down by error types, actual examples of each error type and the estimated precision and recall against human evaluation.\n\nAll you have to do is send a link to system inputs and outputs to eval+Bkl8YR4YDB@modelfront.com.\n\nIf you want a comparison between ModelFront eval, human eval and BLEU, then send the reference translations too.", "title": "Better evaluation"}, "signatures": ["~Adam_Bittlingmayer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Adam_Bittlingmayer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs", "authors": ["Yuxian Meng", "Xiangyuan Ren", "Zijun Sun", "Xiaoya Li", "Arianna Yuan", "Fei Wu", "Jiwei Li"], "authorids": ["yuxian_meng@shannonai.com", "xiangyuan_re@shannonai.com", "zijun_sun@shannonai.com", "xiaoya_li@shannonai.com", "xfyuan@stanford.edu", "wufei@zju.edu.cn", "jiwei_li@shannonai.com"], "keywords": [], "abstract": "In this paper, we investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. We propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. We are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.", "pdf": "/pdf/6724d4ec62d32a09cb3696264cd244d06a6ec9b8.pdf", "paperhash": "meng|largescale_pretraining_for_neural_machine_translation_with_tens_of_billions_of_sentence_pairs", "original_pdf": "/attachment/ccf50a6850f6dbca10fea35d9a784383958256c9.pdf", "_bibtex": "@misc{\nmeng2020largescale,\ntitle={Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs},\nauthor={Yuxian Meng and Xiangyuan Ren and Zijun Sun and Xiaoya Li and Arianna Yuan and Fei Wu and Jiwei Li},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkl8YR4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkl8YR4YDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504197476, "tmdate": 1576860590379, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1249/Authors", "ICLR.cc/2020/Conference/Paper1249/Reviewers", "ICLR.cc/2020/Conference/Paper1249/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1249/-/Public_Comment"}}}], "count": 12}