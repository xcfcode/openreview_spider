{"notes": [{"id": "S1gNc3NtvB", "original": "HygivGKAIH", "number": 111, "cdate": 1569438859667, "ddate": null, "tcdate": 1569438859667, "tmdate": 1577168233892, "tddate": null, "forum": "S1gNc3NtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GoLydDLnVP", "original": null, "number": 1, "cdate": 1576798687694, "ddate": null, "tcdate": 1576798687694, "tmdate": 1576800947398, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Decision", "content": {"decision": "Reject", "comment": "The authors present a method that optimizes a differentiable neural computer with evolutionary search, and which can transfer abstract strategies to novel problems.  The reviewers all agreed that the approach is interesting, though were concerned about the magnitude of the contribution / novelty compared to existing work, clarity of contributions, impact of pretraining, and simplicity of examples.  While the reviewers felt that the authors resolved the many of their concerns in the rebuttal, there was remaining concern about the significance of the contribution.  Thus, I recommend this paper for rejection at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718169, "tmdate": 1576800268606, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper111/-/Decision"}}}, {"id": "rygiuvKJ9B", "original": null, "number": 3, "cdate": 1571948402686, "ddate": null, "tcdate": 1571948402686, "tmdate": 1574564935624, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper presents an approach called a neural computer, which has a Differential Neural Computer (DNC) at its core that is optimised with an evolutionary strategy. In addition to the typical DNC architecture, the system proposed in this paper has different modules that transfer different domain representations into the same representation, which allows the system to generalise to different and unseen tasks.\n\nThe paper is interesting and well written but I found that the contributions of this paper could be made more clear. \n\nFirst, the idea of evolving a Neural Turing machine was first proposed in Greve et al. 2016, which the authors cite, but only in passing in the conclusion. Greve et al. paper introduced the idea of hard attention mechanisms in an NTM through evolution and the benefits of having a memory structure that does not have to be differentiable. However, if the reader of this paper is not careful, they would miss this fact. I, therefore, suggest featuring Greve\u2019s paper more prominently and highlighting the differences/similarities to the current paper earlier in the introduction.\n\nSecond, the idea of learned modules to allow the approach to work across different domains is interesting, but I\u2019m wondering how novel it really is? Isn\u2019t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time? Also, the domains that this approach can be applied to seem potentially limited in that the two problems have to already be very similar; In fact, I probably wouldn\u2019t call them different tasks but the same task with a different visual representation. \n\nI also had a question about the DNC training. Is the DNC version also trained with NES? It would be good to know how much of the difference between the proposed approach and the DNC is because of the training method (NES vs SGD) or other factors.  \n\nOnce the points raised above and the specific contributions of this paper are made more clear, I would suggest accepting it. \n\n####After rebuttal####\nThe authors' response and the revised paper address most of my concerns now. Since I do believe the approach could be more novel, I'm keeping my weak accept, but do think it is very interesting work. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575426682248, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper111/Reviewers"], "noninvitees": [], "tcdate": 1570237756910, "tmdate": 1575426682262, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Review"}}}, {"id": "SyllmIQAtB", "original": null, "number": 2, "cdate": 1571857943942, "ddate": null, "tcdate": 1571857943942, "tmdate": 1574467275311, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper introduces a neural controller architecture for learning abstract algorithmic solutions to search and planning problems. By combining abstract and domain-specific components, the model is able to mimic two classical algorithms quite closely across several domains. The precision of the learning is very high; verified by generalization to substantially larger problem sizes and different domains. One notable conclusion is that Evolutionary Strategies is able to learn algorithmic solutions whose precision is on par with deterministic algorithms. The method of triggering learning based on curriculum level performance is a notable feature that nicely couples generalization progress with learning, and yields insightful learning curves.\n\nAlthough the paper shows that learning abstract algorithmic solutions is possible, it is not clear that the framework is able to learn such solutions when the training signals have not already been broken down into their smallest constituent parts. In other words, it is not clear what this framework could be used for, since it appears the experimenter must already possess a complete specification of the target algorithm and breakdown of the domain. Is there a setting where this framework could be used to learn something the experimenter is not already aware of? Or is the main point that it is technically possible to get an NN to learn this behavior?\n\nAlthough it is clear that models are achieved that satisfy R1-R3, it is not clear exactly what problem formulation is being considered. It would be very helpful if the paper included a formal problem definition so that the purpose of each framework component and differences w.r.t. prior work are clear. \n\nSimilarly, the motivation for each of the data dependent modules is not clear. Is there something fundamental about this particular decomposition into modules? Or are these just the modules that were necessary given the specifics of the algorithms that were learned in experiments? How does this framework generalize to other kinds of algorithms?\n\nAre the comparison methods (DNC and stackRNN) unable to generalize to larger problem sizes? Including the full comparisons on generalization would give a more complete picture. Similarly, the figures are missing the comparisons for Learning to Plan for DNC and stackRNN.\n\nIs the comparison w.r.t. training time in Figure 3c fair, since the proposed framework pretrains the submodules?\n\nIs there a fundamental problem of DNC being addressed here? E.g., are there some critical types of submodules where making them differentiable is not an option?\n\nIs the algorithm limited to cases where the number of actions at each state is equal? I.e., could it be applied to algorithms like shortest path in the DNC paper?\n\nFinally, the last line talks about intriguing applications to the real world, but the running example in the paper is sorting. Is there some hypothetical but concrete example of how this framework could help in the real world, and do something better than a hard-coded classical algorithm? Or discover a new algorithm?\n\n----------\n\nAfter the rebuttal and discussion, I am convinced the paper is a useful contribution, and have increased my rating. However, I still think the presentation can be much improved to improve the clarity of the contribution. I would hope to see the following addressed in the final version:\n\n1. More detailed and precise discussion of how the approach relates to prior work. As is, this discussion is informal and scattered throughout the paper. Enumerating the distinctions in one place would make the contribution much more clear.\n\n2. The above would also help make the explicit contribution more clear. E.g., the \"Contribution\" paragraph currently does not contain the fact that the machine has only been applied to \"planning\" problems. This should be included to avoid making the contribution seem overly general.\n\n3. More formal description of what each module does. Right now, they are described informally. Actually seeing the equations of what each computes would make it easier to understand how and why they all fit together.\n\nI still see the main contribution as \"It is technically possible to train the abstract controller of a neural computer for planning using NES, so that R1-R3 are satisfied.\" Ideally, the above clarifications would make it more clear that this is the main contribution, or that there are some other key contributions beyond this.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575426682248, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper111/Reviewers"], "noninvitees": [], "tcdate": 1570237756910, "tmdate": 1575426682262, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Review"}}}, {"id": "SkxklVWdiH", "original": null, "number": 5, "cdate": 1573553126893, "ddate": null, "tcdate": 1573553126893, "tmdate": 1573553126893, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "BJeb1Oawjr", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment", "content": {"title": "Thank you for the discussion.", "comment": "\u201cThis paper also showed the benefits of having a memory that is non-differentiable and using evolution in general to optimize the weights (not just structure) of a NTM; it is not only the hard attention mechanism that is related to this work. I hope the authors can update the paper before the discussion deadline to reflect this.\u201c\nWe agree that in addition to the non-differentiable memory that is due to the hard attention mechanism, using an evolution strategy to learn the model is another connection. We apologize for not clarifying this in our answer. Both links will be updated in the paper.\n\n\u201cThis response does not really answer the question of \"Isn\u2019t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time?\"\u201d\nThe focus of the paper is to show how to represent and learn algorithmic solutions fulfilling the R1-R3. Especially for enabling the abstract features R2 and R3, the information flow has to be divided into only data relevant and control/algorithmic signals. The learned mapping to the control signals can be seen as extracting abstract features for the algorithmic solution, we agree. Nevertheless, to the best of our knowledge this has not been implemented in an unified architecture and been used in the context of learning such abstract strategies fulfilling all three requirements. To represent and learn solutions on this abstraction level, such a mapping is crucial, whether it is learned or hardcoded.\n\n\u201c\"The learned algorithmic solution is building trees/graphs and using backtracking to extract the planning solution. Thus, any problem that can be framed as a search or symbolic planning problem can be solved by the learned solution. \" -> what problems would this exclude and is this aspect highlighted in the paper? \u201c\nIn this paper, we focused on symbolic planning tasks. As long as the problem can be framed as such, e.g., having the symbols and their manipulations, and having an initial and desired goal state, the solution can be applied. But to clarify, this applies to the learned solution, not the architecture itself. Using a different learning problem, a different fitness calculation, other problems than such symbolic planning can be investigated. Again, here we focus on such problems as they are not trivial to be learned (see comparisons) and to show the benefit of investigating algorithmic solutions with R1-R3. We\u2019ll clarify this in the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gNc3NtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper111/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper111/Authors|ICLR.cc/2020/Conference/Paper111/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176243, "tmdate": 1576860553683, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment"}}}, {"id": "BJeb1Oawjr", "original": null, "number": 4, "cdate": 1573537753178, "ddate": null, "tcdate": 1573537753178, "tmdate": 1573537753178, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "ryeQhz3LiB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment", "content": {"title": "More clarification needed", "comment": "Thank you for clarifying but I feel like my questions could have been addressed in more detail. \n\n\"Thank you for pointing out the missing hard attention mechanism reference in the beginning, we will update the paper accordingly. Beside the hard attention, Greve et al. use evolution strategies to evolve the structure of the NTM\" -> This paper also showed the benefits of having a memory that is non-differentiable and using evolution in general to optimize the weights (not just structure) of a NTM; it is not only the hard attention mechanism that is related to this work. I hope the authors can update the paper before the discussion deadline to reflect this.  \n\n\"The presented architecture can be seen as a combination of symbolic manipulation and deep learning, whereas the data modules provide the symbols on which the algorithmic modules learn the abstract strategies\" -> This response does not really answer the question of \"Isn\u2019t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time?\" \n\n\"The learned algorithmic solution is building trees/graphs and using backtracking to extract the planning solution. Thus, any problem that can be framed as a search or symbolic planning problem can be solved by the learned solution. \" -> what problems would this exclude and is this aspect highlighted in the paper? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gNc3NtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper111/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper111/Authors|ICLR.cc/2020/Conference/Paper111/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176243, "tmdate": 1576860553683, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment"}}}, {"id": "ryeQhz3LiB", "original": null, "number": 3, "cdate": 1573466794615, "ddate": null, "tcdate": 1573466794615, "tmdate": 1573466794615, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "rygiuvKJ9B", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment", "content": {"title": "We thank you for your valuable feedback.", "comment": "The contribution of this paper is a novel modular architecture building on a memory-augmented neural network that can represent and learn algorithmic solutions in a reinforcement learning setting using NES. The learned solutions fulfill the three specified requirements for algorithmic solutions: R1 \u2013 generalization to different and unseen task configurations and task complexities, R2 \u2013 independence of the data representation, and R3 \u2013 independence of the task domain. By fulfilling these requirements, the solutions learned by our architecture generalize to complexities far beyond the ones encounter during training, and can be transferred directly to other representations and domains.\n\n\u201cFirst, the idea of evolving a Neural Turing machine was first proposed in Greve et al. 2016, which the authors cite, but only in passing in the conclusion. .. \u201c\nThank you for pointing out the missing hard attention mechanism reference in the beginning, we will update the paper accordingly. Beside the hard attention, Greve et al. use evolution strategies to evolve the structure of the NTM. In contrast, our focus is to show how abstract strategies fulfilling R1-R3 can be represented and learned in a memory-augmented based neural architecture. To incorporate the approach of evolving the structure, or parts of it, in addition, is an interesting next step and therefore discussed in the conclusion.\n\n\u201cSecond, the idea of learned modules to allow the approach to work across different domains is interesting, but I\u2019m wondering how novel it really is? Isn\u2019t this basically just like feature engineering and changing the underlying representation, something that we have been doing for a long time?\u201d\nThe presented architecture can be seen as a combination of symbolic manipulation and deep learning, whereas the data modules provide the symbols on which the algorithmic modules learn the abstract strategies.\n\n\u201cAlso, the domains that this approach can be applied to seem potentially limited in that the two problems have to already be very similar\u201d\nThe learned algorithmic solution is building trees/graphs and using backtracking to extract the planning solution. Thus, any problem that can be framed as a search or symbolic planning problem can be solved by the learned solution. \n\n\u201cI also had a question about the DNC training. Is the DNC version also trained with NES?\u201d\nNo, the DNC and the stackRNN are trained in a supervised fashion with gradient descent (see Figure 3c caption, Section 3.3 and Appendix B), and thus, they use a richer learning signal, giving localized feedback in every computational step. In contrast, using NES on our architecture, only a single scalar value is used to score all computational steps and all samples in the mini-batch.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gNc3NtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper111/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper111/Authors|ICLR.cc/2020/Conference/Paper111/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176243, "tmdate": 1576860553683, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment"}}}, {"id": "BJgMFG2UjB", "original": null, "number": 2, "cdate": 1573466746061, "ddate": null, "tcdate": 1573466746061, "tmdate": 1573466746061, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "SyllmIQAtB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment", "content": {"title": "We thank you for your nice summary and  detailed comments.", "comment": "\u201c.. could be used to learn something the experimenter is not already aware of? Or is the main point that it is technically possible to get an NN to learn this behavior?\u201d\nThe main point is to show that such abstract strategies in form of algorithmic solutions fulfilling the three specified requirements (R1-R3) can be represented and learned from data sequences instead of program traces in a general memory-augmented network based architecture, and that these abstract solutions can be transferred directly to novel representations and domains. While it is correct that we use the data sequences of the target algorithms (instead of program traces as widely done) for learning, it is only used to calculate one single scalar value over all computational steps and samples in the mini-batch. By using a different strategy for calculating the score that does not use the data sequences but, e.g., intrinsic motivation signals, the architecture could potentially discover other solutions. This is part of future research, building on this work here, that has shown the general capability of the architecture first.\n\n\u201cIt would be very helpful if the paper included a formal problem definition so that the purpose of each framework component ..\u201d\nThe purpose of the individual modules is described in general and formally in Section 2.1 for the algorithmic modules and in Section 2.2 for the data dependent modules. In Section 3.1.1 the instantiations of the data  modules for the sokoban domain are explained. \n\n\u201cIs there something fundamental about this particular decomposition into modules? Or are these just the modules that were necessary given the specifics of the algorithms that were learned in experiments? How does this framework generalize to other kinds of algorithms?\u201d\nThe main idea behind this decomposition comes from the design of modern computer architectures and due to the split into data and control information flow. The framework was designed with these principles and to be generous. For different problem instances, the data modules can be instantiated as necessary, or not used at all by just implementing an identity function, as described in Section 2.2. The application to other kinds of algorithms is part of ongoing research, building on the first successful learning and representing of algorithmic solutions fulfilling R1-R3 presented here.\n\n\u201cAre the comparison methods (DNC and stackRNN) unable to generalize to larger problem sizes? .. Similarly, the figures are missing the comparisons for Learning to Plan for DNC and stackRNN.\u201d\nAs shown in Figure 3c, the comparison methods did not solve curriculum level 1 and therefore failed to generalize here. Learning to Plan is missing for these methods, as they failed on the Learning to Search task, which is a subtask of the Learning to Plan.\n\n\u201cIs the comparison w.r.t. training time in Figure 3c fair, since the proposed framework pretrains the submodules?\u201d\nThe comparisons methods were trained up to 500.000 iterations to compensate the pretraining of the data modules (they were trained for 300.000 iterations). As shown in Figure 3c, even with this budget and a richer learning signal (they were trained in a supervised fashion with ground truth feedback on every computational step), they did not solve level 1 successfully, while our architecture generalized to all complexities in about 3000 iterations.\n\n\u201c..problem of DNC being addressed here? E.g., are there some critical types of submodules where making them differentiable is not an option?\u201d\nThe main \u2018problem\u2019 of the DNC to fulfill R1-R3 is the missing separation between data and control information, i.e., between data and algorithmic modules. For efficient learning and representing such deterministic behavior, hard attention was a crucial change that is not differentiable, please see Figure 9 for that comparison.\n\n\u201c.. limited to cases where the number of actions at each state is equal? I.e., could it be applied to algorithms like shortest path in the DNC paper?\u201d\nThe Input module could output the available actions per state such that the algorithmic modules could learn to apply these actions. As the algorithmic solution learned by our architecture is performing breadth-first-search, it does find the shortest path (assuming equal edge costs). In contrast to the DNC paper, the graph is not presented to the model, but rather it has to build it itself, and the DNC papers shortests paths were of length 5, whereas our solution solved shortest paths up to 9, see Figure 2 and Section 3.1.\n\n\u201c.. and do something better than a hard-coded classical algorithm? Or discover a new algorithm?\u201d\nDiscover a new algorithm is a very interesting point, indeed. As mentioned before, by using, for example, an intrinsic motivation based calculation of the fitness function, the architecture may explore unknown solutions to the given problem. This is part of future research that can now be started building on the presented foundation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gNc3NtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper111/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper111/Authors|ICLR.cc/2020/Conference/Paper111/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176243, "tmdate": 1576860553683, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment"}}}, {"id": "rJlKJG2UsH", "original": null, "number": 1, "cdate": 1573466592769, "ddate": null, "tcdate": 1573466592769, "tmdate": 1573466592769, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "BylAVtWwFr", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment", "content": {"title": "We thank you for your feedback and helpful comments.", "comment": "\u201cNES clearly does not scale to higher dimensional spaces\u201d \nPopulation based blackbox optimization is currently challenging \u2018traditional\u2019 gradient based training for deep learning, and was shown to scale to millions of parameters [1-4].\nFurthermore, the modular design of the architecture enables the use of smaller models, and the complexity of the data modules is not affected by that.\n\n[1] Conti et al., Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents, NeurIPS 2018\n[2] Chrabaszcz et al., Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari, IJCAI 2018\n[3] Such et al., Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for\nTraining Deep Neural Networks for Reinforcement Learning, arXiv 2017\n[4] Salimans et al., Evolution strategies as a scalable alternative to reinforcement learning. arXiv 2017\n\n\u201cone does not just learn the vision module and freeze it.\u201d \nThe problem of learning robust and adaptive vision modules is an open question itself, that is beyond the scope of this paper. Here, we focus on learning the algorithmic modules, i.e., the algorithmic solution, focusing on achieving generalization and abstraction while assuming the data modules are working correctly. \n\n\u201cIn fact, some authors have shown theoretically provable generalisation via verification approaches: https://openreview.net/forum?id=BkbY4psgg\u201d\nWhile this paper indeed shows the benefits of recursive solutions for generalization of neural programming architectures, it is learned from program traces that already include the recursive structure. In contrast, our architecture is learned from the data sequences the algorithm produces, and discovers the recursive solution from that (see Figure 5 for the repetitive pattern of the learned solution). Additionally, using NES and a single scalar value to grade the all computational steps and all samples in a minibatch, leads to sparse and challenging learning setup. Moreover and in contrast, we show the direct transfer of the learned solutions to novel task representations and task domains.\n\n\u201cIn summary, this paper demonstrated the advantages of modular neural systems, but fails to address the important issue of making sure the modules scale to real problems.\u201d\nBeside the advantages of modular systems, we show that abstract strategies in form of algorithmic solutions fulfilling the three specified requirements (R1-R3), can be represented and learned from data sequences instead of program traces in a general memory-augmented network based architecture. This fundamental research enables further investigation on more and challenging problems. Additionally, the solution learned in a gridworld computer game is transferred amongst others to a robotic manipulation task, in simulation and on the real robot."}, "signatures": ["ICLR.cc/2020/Conference/Paper111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gNc3NtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper111/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper111/Authors|ICLR.cc/2020/Conference/Paper111/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176243, "tmdate": 1576860553683, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper111/Authors", "ICLR.cc/2020/Conference/Paper111/Reviewers", "ICLR.cc/2020/Conference/Paper111/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Comment"}}}, {"id": "BylAVtWwFr", "original": null, "number": 1, "cdate": 1571391798203, "ddate": null, "tcdate": 1571391798203, "tmdate": 1572972637408, "tddate": null, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "invitation": "ICLR.cc/2020/Conference/Paper111/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes modifications and modular extensions to the differential neural computer (DNC). The approach is nicely modular, decoupling the data modules from algorithmic modules. This enables the authors to pretrain the data modules with supervised learning and to train the small algorithmic modules with neural evolution strategies (NES). NES is a global optimization method (which may be understood as policy gradients where the parameters of the neural policy are the actions) and consequently this enables the authors to use discrete selection mechanisms instead of the soft attention mechanisms of DNC.\n\nThe authors correctly argue for generalization to different and unseen tasks (R1), independence of data representation (R2) and independence of task environment (R3). These are important points, and the authors are able to achieve these by using a modular approach where the interface modules are pretrained.\n\nWhile what can be achieved with modularity is important, the modules themselves are rather simple and leave open the question of scalability. NES clearly does not scale to higher dimensional spaces, and the complexity or real say vision modules is the problem for many people --- one does not just learn the vision module and freeze it. \n\nThe idea that modularity can be used to attain greater generality and domain independence has already been explored at ICLR to some extent. In fact, some authors have shown theoretically provable generalisation via verification approaches: https://openreview.net/forum?id=BkbY4psgg\n\nIn summary, this paper demonstrated the advantages of modular neural systems, but fails to address the important issue of making sure the modules scale to real problems.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper111/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer", "keywords": [], "pdf": "/pdf/555ab1e683223122e4550d507915c6704faddd46.pdf", "authors": ["Daniel Tanneberg", "Elmar Rueckert", "Jan Peters"], "TL;DR": "A novel neural computer architecture that learns transferable abstract strategies to symbolic planning tasks as algorithmic solutions with evolution strategies.", "authorids": ["daniel@robot-learning.de", "rueckert@rob.uni-luebeck.de", "mail@jan-peters.net"], "paperhash": "tanneberg|learning_algorithmic_solutions_to_symbolic_planning_tasks_with_a_neural_computer", "original_pdf": "/attachment/581dcbffb387b8ce2d7b006cf5602af8096bf696.pdf", "_bibtex": "@misc{\ntanneberg2020learning,\ntitle={Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer},\nauthor={Daniel Tanneberg and Elmar Rueckert and Jan Peters},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gNc3NtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gNc3NtvB", "replyto": "S1gNc3NtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575426682248, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper111/Reviewers"], "noninvitees": [], "tcdate": 1570237756910, "tmdate": 1575426682262, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper111/-/Official_Review"}}}], "count": 10}