{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396381093, "tcdate": 1486396381093, "number": 1, "id": "rkSynGL_e", "invitation": "ICLR.cc/2017/conference/-/paper150/acceptance", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The main strengths and weaknesses pointed out by the reviewers were:\n \n Strengths\n -Domain is interesting, problem is important (R2, R1)\n -Discretization of continuous domain may enable leveraging of advanced tools developed for discrete domains, e.g. NLP (R1)\n \n Weaknesses\n -Issues with experiments: models under-capacity, omission of obvious baselines (R1, R3)\n -Unclear conclusion: is quantization and embedding superior to working with the raw data? (R1)\n -Fair amount of relevant work omitted (R1, R3)\n \n While the authors engaged in the pre-review discussion, they did not respond to the official reviews. Therefore I have decided to align with the reviewers who are in consensus that the paper does not meet the acceptance bar."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396381733, "id": "ICLR.cc/2017/conference/-/paper150/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396381733}}}, {"tddate": null, "tmdate": 1485579486341, "tcdate": 1485579486341, "number": 6, "id": "HJ8JBjKwg", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "HkVGxmGVx", "signatures": ["~Shengdong_Zhang1"], "readers": ["everyone"], "writers": ["~Shengdong_Zhang1"], "content": {"title": "Re: Unique angle for modeling heterogeneous sequences", "comment": "- The LSTMs (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound *very* under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences).\n\nFor these three experiments, we tried LSTMs with more cells (16, 32, 64,128 cells), and more cells did not seem to give performance improvement but overfitting. For hard-drive dataset, we found 8 cells in LSTM was enough by cross-validation. It is reasonable because a logistic regression classifier, which is only able to learn linear decision boundaries, gives good performance already. For the seizure dataset, we used a CNN with 16 filters and an iRNN with 32 cells, not 16 cells LSTM. We tried to apply an iRNN or LSTM with 16, 32, 64, 128 cells directly on raw data or symbolized data and they all had poor performance, so we added a CNN layer to help model extract features. The final model architecture was selected by cross-validation. For the heat system dataset, we used an LSTM of 15 nodes. We observed that LSTMs with 32 cells or more resulted in overfitting, and the one with 15 cells gave the best performance.\n\n\n- That might explain both the relatively small gap between the LSTMs and logistic regression *and* the improvement of the embedding-based LSTMs. Hypothetically, if quantizing the inputs is really useful, the raw data LSTMs should be able to learn this transformation, but if they are under capacity, they might not be able to do so.\n\nKindly let us know which experimental results you are referring to regarding \u201csmall gap\u201d. If they are referring to the one on hard-drive dataset, the gap between LR and LSTM on raw data, and LSTM+ICE is not small in terms of AUC (0.851, 0.865 vs 0.893). Of course, selecting which number as the performance measure of a classifier is subjective and application dependent. If this is regarding the result on heating system dataset, then the fact that the LR and LSTM models are trained on hand-engineered features has been overlooked. We should emphasize that hand-engineering features is a time-consuming procedure and achieving the same level of performance but without doing feature engineering is still a big advantage. The inputs are different (119 vs 20 input size) and the models (~100 cells vs 15 cells in LSTMs) have totally different capacity. Small gap, in this case, indicates the success of symbolization methods in effectively capturing the domain-dependent features from raw data. Note that we also tried using raw data, instead of using hand-engineered features, on this application but we couldn't get good results with raw data and thus its performance is not reported. This will be further emphasized in the revised version of the paper.\n\n- What is more, using the same architecture (# layers, # units, etc.) for very different kinds of inputs (raw, WdE, SCE, ICE, hand-engineered features) is poor methodology. Obviously, hyperparameters should be tuned independently for each type of input.\n\nThanks for your comments. The main contribution of our paper is not to show that symbolization results in state-of-the-art performance, but to show symbolization and embedding layers provides a unified framework to deal with heterogeneous inputs, instead of hand-engineering features or feeding raw data. Moreover, we wanted to control the capacity of the models to be similar. However, we will include the result using individually optimized hyper-parameters in the next version of our paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}, {"tddate": null, "tmdate": 1485579399371, "tcdate": 1485579399371, "number": 5, "id": "HyJqVsKDe", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "rk8oEfM4e", "signatures": ["~Shengdong_Zhang1"], "readers": ["everyone"], "writers": ["~Shengdong_Zhang1"], "content": {"title": "Re: Is the ad hoc symbolization step necessary?", "comment": "- did you investigate how sensitive model architecture (# hidden layers, # units in each layer, etc.) was to choice of partition size? If your claim is \"not very sensitive,\" can you offer empirical evidence to back this up?\n\nYes, we investigated the sensitivity of model architecture. In particular, the ICE algorithm is insensitive to the partition sizes which are reasonably large. The table below summarizes the model performance given different symbol sizes for the seizure prediction task.\n\nModel Architecture:  ICE + CNN(16 filters, 5 filter length) + iRNN(32 hidden nodes)\n>> Symbol Size :      10      20      30      40      50\n>> AUC            :    0.808    0.811   0.807   0.801   0.803\n>> Balanced Acc:    0.759    0.765   0.757   0.755   0.753\n\nWe will include more experimental results in the next version of our paper.\n\n- for your non-symbolic LSTM baselines, how did you preprocess the continuous channels? Did you do any kind of normalization or rescaling? If not, could the inferior performance of the non-symbolic LSTM be explained by poor conditioning of the inputs?\n\nWe did 5 fold cross-validation and standardization as preprocessing, i.e. we transformed each channel of data to have zero mean and unit variance with the mean and variance calculated over the training set, and used them to normalize the training, validation and test sets.\n\n- did you try using an embedding layer for the raw continuous values, rather than feeding them directly into the LSTM?\n\nThank you for the suggestion. We applied a shared fully-connected layer on raw continuous values of each timestep and then LSTM, and the model performance did not improve compared to the baseline model which directly fed raw values to LSTM. We will include the experimental results in the next version of our paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}, {"tddate": null, "tmdate": 1485042270141, "tcdate": 1481941004689, "number": 2, "id": "HkVGxmGVx", "invitation": "ICLR.cc/2017/conference/-/paper150/official/review", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer1"], "content": {"title": "Unique angle for modeling heterogeneous sequences", "rating": "3: Clear rejection", "review": "Because the authors provided no further responses to reviewer feedback, I maintained my original review score.\n\n-----\n\nThis paper takes a unique approach to the modeling of heterogeneous sequence data. They first symbolize continuous inputs using a previously described approach (histograms or maximum entropy), the result being a multichannel discrete sequence (of symbolized time series or originally categorical data) of \"characters.\" They then investigate three different approaches to learning an embedding of the characters at each time step (which can be thought of as a \"word\"):\n1) Concatenate characters into a \"word\" and then apply standard lookup-based embeddings from language modeling (WDE)\n2) Embed each character independently and then sum over the embeddings (SCE)\n3) Embed each character as a scalar and concatenate the scalar embeddings (ICE)\nThe resulting embeddings can be used as inputs to any architecture, e.g., LSTM. The paper applies these methods primarily to event detection tasks, such as hard drive failures and seizures in EEG data. Empirical results largely suggest the a recurrent model combined with symbolization/embedding outperforms a comparable recurrent model applied to raw data. Results are inconclusive as to which embedding layer works best.\n\nStrengths:\n- The different embedding approaches, while simple, are designed to tackle a very interesting problem where the input consists of multivariate discrete sequences, which makes it different from standard language modeling and related domains. The proposed approaches offer several different interesting perspectives on how to approach this problem.\n- The empirical results suggest that symbolizing the continuous input space can improve results for some problems. This is an interesting possibility as it enables the direct application of a variety of language modeling tools (e.g., embeddings).\n\nWeaknesses:\n- The LSTMs (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound *very* under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences). That might explain both the relatively small gap between the LSTMs and logistic regression *and* the improvement of the embedding-based LSTMs. Hypothetically, if quantizing the inputs is really useful, the raw data LSTMs should be able to learn this transformation, but if they are under capacity, they might not be able to dos. What is more, using the same architecture (# layers, # units, etc.) for very different kinds of inputs (raw, WdE, SCE, ICE, hand-engineered features) is poor methodology. Obviously, hyperparameters should be tuned independently for each type of input.\n- The experiments omit obvious baselines, such as trying to directly learn an embedding of the continuous inputs.\n- The experimental results offer an incomplete, mixed conclusion. First, no one embedding approach performs best across all tasks and metrics, and the authors offer no insights into why this might be. Second, the current set of experiments are not sufficiently thorough to conclude that quantization and embedding is superior to working with the raw data.\n- The temporal weighting section appears out of place: it is unrelated to the core of the paper (quantizing and embedding continuous inputs), and there are no experiments to demonstrate its impact on performance.\n- The paper omits a large number of related works: anything by Eamonn Keogh's lab (e.g., Symbolic Aggregate approXimation or SAX), work on modifying loss functions for RNN classifiers (Dai and Le. Semi-supervised sequence learning. NIPS 2015; Lipton and Kale, et al. Learning to Diagnose with LSTM Recurrent Neural Networks. ICLR 2016), work on embedding non-traditional discrete sequential inputs (Choi, et al. Multi-layer Representation Learning for Medical Concepts. KDD 2016).\n\nThis is an interesting direction for research on time series modeling with neural nets, and the current work is a good first step. The authors need to perform more thorough experiments to test their hypotheses (i.e., that embedding helps performance). My intuition is that a continuous embedding layer + proper hyperparameter tuning will work just as well. If quantization proves to be beneficial, then I encourage them to pursue some direction that eliminates the need for ad hoc quantization, perhaps some kind of differentiable clustering layer?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512682869, "id": "ICLR.cc/2017/conference/-/paper150/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer1", "ICLR.cc/2017/conference/paper150/AnonReviewer2"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512682869}}}, {"tddate": null, "tmdate": 1484951686926, "tcdate": 1481902198865, "number": 1, "id": "HJ1KdtWVe", "invitation": "ICLR.cc/2017/conference/-/paper150/official/review", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer3"], "content": {"title": "Interesting approach for sequence quantization and embedding", "rating": "5: Marginally below acceptance threshold", "review": "In absence of authors' responses, the rating is maintained.\n\n---\n\nThis paper introduces an approach for learning predictive time series models that can handle heterogenous multivariate sequence. The first step is in three possible ways to perform embedding of the d-dimensional sequences into d-character words, or a sum of d character embeddings, or a concatenation of d character embeddings. The embedding layer is the first layer of a deep architecture such as LSTM. The models are then trained to perform event prediction at a fixed horizon, with temporal weighting, and applied to hard disk or heating system failures or seizures.\n\nThe approach is interesting and the results seem to outperform an LSTM baseline, but need additional clarification.\n\nThe experimental section on seizure prediction is very short and would need to be considerably extended, in an appendix. What are the results obtained using LSTM vs. RNN? What is the state-of-the-art on that dataset? Given that EEG data contain mostly frequential information, how is this properly handled in per-sample embeddings?\n\nPlease also extend your reference and previous work section to include PixelRNN as well as: \n* van den Oord, et al. (2016)\nWaveNet: A Generative Model for Raw Audio\narXiv 1609.03499\n* Huang et al. (2013)\n\"Learning deep structured semantic models for web search using clickthrough data\"\nCIKM\nIn the latter paper, the authors embedded 3-gram hashes of the input sequences (e.g., text), which is somewhat similar to a time-delay embedding of the input sequence.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512682869, "id": "ICLR.cc/2017/conference/-/paper150/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer1", "ICLR.cc/2017/conference/paper150/AnonReviewer2"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512682869}}}, {"tddate": null, "tmdate": 1482187541347, "tcdate": 1482187541347, "number": 3, "id": "BypzQJLNg", "invitation": "ICLR.cc/2017/conference/-/paper150/official/review", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper compare three representation learning algorithms over symbolized sequences. Experiments are executed on several prediction tasks. The approach is potentially very important but the proposed algorithm is rather trivial. Besides detailed analysis on hyper parameters are not described. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512682869, "id": "ICLR.cc/2017/conference/-/paper150/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer1", "ICLR.cc/2017/conference/paper150/AnonReviewer2"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512682869}}}, {"tddate": null, "tmdate": 1481938078404, "tcdate": 1481938078404, "number": 3, "id": "rk8oEfM4e", "invitation": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer1"], "content": {"title": "Is the ad hoc symbolization step necessary?", "question": "Can you comment on the following:\n- did you investigate how sensitive model architecture (# hidden layers, # units in each layer, etc.) was to choice of partition size? If your claim is \"not very sensitive,\" can you offer empirical evidence to back this up?\n- for your non-symbolic LSTM baselines, how did you preprocess the continuous channels? Did you do any kind of normalization or rescaling? If not, could the inferior performance of the non-symbolic LSTM be explained by poor conditioning of the inputs?\n- did you try using an embedding layer for the raw continuous values, rather than feeding them directly into the LSTM?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481938079072, "id": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer2", "ICLR.cc/2017/conference/paper150/AnonReviewer1"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481938079072}}}, {"tddate": null, "tmdate": 1481601433152, "tcdate": 1481600846734, "number": 4, "id": "S1DL1game", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "HyRBiq_mx", "signatures": ["~Shengdong_Zhang1"], "readers": ["everyone"], "writers": ["~Shengdong_Zhang1"], "content": {"title": "Re: Alphabet size", "comment": "Thank you very much for the question. Below is the answer:\n\nThe Symbol size is a hyper-parameter to be tuned for the 3 symbolization methods. But it has different effects.\n\n1. The symbol size for WdE determines the number of OOV words. So a large symbol size for a continuous variable is likely to cause failure of training.\n\n2. The symbol size for SCE determines the number of parameters for a variable. If the symbol size of a variable is D and embedding size of it is N, then the number of parameters for this variable will be D*N. Larger D could result in overfitting.\n\n3. The symbol size for ICE does not seriously influence a model's performance, because it only adds D parameters to the model and the order constraint serves as regularization for the added parameters.\n\nFor all the experimental results in the paper, we only used small symbol sizes of 3 or 4 for a continuous variable when using WdE; symbol size of 20 for SCE; for ICE, we observed that the model's performance was not sensitive to this number, so we selected 50 as the symbol sizes for all continuous variables.\n\nIf there is still something not clear to you, please kindly let us know.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}, {"tddate": null, "tmdate": 1481317189601, "tcdate": 1481317189596, "number": 2, "id": "HyRBiq_mx", "invitation": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer2"], "content": {"title": "Alphabet size", "question": "I guess the alphabet size is a magic number. How does the performance change over different alphabet sizes? Do you pick 4 or 50 which maximizes the performance? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481938079072, "id": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer2", "ICLR.cc/2017/conference/paper150/AnonReviewer1"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481938079072}}}, {"tddate": null, "tmdate": 1481142967378, "tcdate": 1481142967371, "number": 3, "id": "SkkazeU7g", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "BJHN_hyme", "signatures": ["~Soheil_Bahrampour1"], "readers": ["everyone"], "writers": ["~Soheil_Bahrampour1"], "content": {"title": "Re: References and comparison to PixelRNN", "comment": "Thank you for the question. Below are the differences between our work and the PixelRNN paper:\n\n1. Problems these models are trying to tackle: PixelRNN models perform an unsupervised task of probability density estimation for input images whereas our models perform a supervised task of event prediction.\n\n2. Property of Input Data: Input data to PixelRNN models are images with pixel values from 0 to 255. In our work, input data are heterogeneous time-series data consisting of both numerical and categorical data. \n\n3. Symbolization: In PixelRNN paper, image input is directly fed into the network. In our paper, input data are first symbolized using algorithms such as maximum entropy partitioning. The representation of the symbolized data is then jointly learned in an embedding layer along with the rest of the architecture. Symbolization and embedding representation proposed in our work provides a unified framework for handling heterogeneous inputs. The PixelRNN paper does not contain any symbolization step or embedding layer.\n \nSimilarities to PixelRNN paper:\nThe pixelRNN uses a discrete multinomial distribution (softmax) to model the distribution of images, which has the advantage of being arbitrarily multimodal without prior on the shape of the input. This shares similarity with our work where the numerical inputs can be arbitrary multimodal although we do not explicitly model it using a multinomial distribution. Another obvious similarity is of course the fact that both papers process the inputs in a sequential fashion.\n\nIf there\u2019s a specific aspect of PixelRNN work that you had in mind which we missed to address above, kindly let us know and we can clarify that part."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}, {"tddate": null, "tmdate": 1480734764751, "tcdate": 1480734764747, "number": 1, "id": "BJHN_hyme", "invitation": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["ICLR.cc/2017/conference/paper150/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper150/AnonReviewer3"], "content": {"title": "References and comparison to PixelRNN", "question": "This work on symbolisation and word embedding on symbolised sequences shares similar principles with the recently introduced work on PixelRNNs (quantization of the continuous signal, classification instead of regression tasks). Could you elaborate on similarities and differences between your method and PixelRNN?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481938079072, "id": "ICLR.cc/2017/conference/-/paper150/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper150/AnonReviewer3", "ICLR.cc/2017/conference/paper150/AnonReviewer2", "ICLR.cc/2017/conference/paper150/AnonReviewer1"], "reply": {"forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481938079072}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478643736834, "tcdate": 1478249501766, "number": 150, "id": "Hy8X3aKee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hy8X3aKee", "signatures": ["~Soheil_Bahrampour2"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1478643673339, "tcdate": 1478643672672, "number": 2, "id": "rkbkeAy-l", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "SJ17TUCll", "signatures": ["~Soheil_Bahrampour2"], "readers": ["everyone"], "writers": ["~Soheil_Bahrampour2"], "content": {"title": "Resubmitted the paper with correct font", "comment": "Thanks for the comment. The paper has been resubmitted with the correct font."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}, {"tddate": null, "tmdate": 1478554481327, "tcdate": 1478548758784, "number": 1, "id": "SJ17TUCll", "invitation": "ICLR.cc/2017/conference/-/paper150/public/comment", "forum": "Hy8X3aKee", "replyto": "Hy8X3aKee", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Submission Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series Classification", "abstract": "In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.", "pdf": "/pdf/9c219f891cd8702b713d29638f7b6e0696347536.pdf", "paperhash": "zhang|deep_symbolic_representation_learning_for_heterogeneous_timeseries_classification", "keywords": [], "conflicts": ["us.bosch.com", "ee.psu.edu", "sc.sfu.ca", "uic.edu"], "authors": ["Shengdong Zhang", "Soheil Bahrampour", "Naveen Ramakrishnan", "Mohak Shah"], "authorids": ["zhangshengdongofgz@gmail.com", "Soheil.Bahrampour@us.bosch.com", "Naveen.Ramakrishnan@us.bosch.com", "mohak@mohakshah.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287710648, "id": "ICLR.cc/2017/conference/-/paper150/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy8X3aKee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper150/reviewers", "ICLR.cc/2017/conference/paper150/areachairs"], "cdate": 1485287710648}}}], "count": 14}