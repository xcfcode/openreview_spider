{"notes": [{"id": "4c3WeBTErrE", "original": "AA99rYLjWNj", "number": 2867, "cdate": 1601308318201, "ddate": null, "tcdate": 1601308318201, "tmdate": 1614985760647, "tddate": null, "forum": "4c3WeBTErrE", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ILVrfoHgIk", "original": null, "number": 1, "cdate": 1610040373221, "ddate": null, "tcdate": 1610040373221, "tmdate": 1610473964999, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose a \"jumpy RNN\" to adaptively change the step size of an RNN to match the time scales of the system dynamics. Reviewers found merit in the simple and intuitive idea, but were less enthusiastic about the experimental results and the comparison to existing work. (Adaptive step size methods have been a subject of recent work in RNNs, not to mention in numerical methods for ODE solvers.) Overall, I think the additions the authors made in the discussion phase did strengthen the paper, but further work is necessary before publication. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040373208, "tmdate": 1610473964982, "id": "ICLR.cc/2021/Conference/Paper2867/-/Decision"}}}, {"id": "lPeGuurvsv5", "original": null, "number": 4, "cdate": 1603920183264, "ddate": null, "tcdate": 1603920183264, "tmdate": 1606809254390, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review", "content": {"title": "Jumpy RNNs", "review": "Main problem: The paper tries to tackle the problem of deciding how to make jumpy predictions or more precisely event based updates in a RNN as compared to clock based updated. Normal RNNs/LSTMs update their hidden states at every time step. Predicting the hidden states at each time step often lead to compounding errors i.e., small errors accumulate while making predictions for many time steps in the future. I really like this problem.\n\nMethod: The paper proposes a method to update hidden states in an event driven manner. The paper defines the hidden state as a continuous, piece wise linear function. The paper proposes to predict the jump interval as well a hidden velocity which basically signifies the \"change\" in hidden dynamics over the jump interval, thus the update function of the proposed method is a function of both the jump interval as well as hidden velocity.  In order to encourage the jump_interval of more than 1, the paper defines an auxiliary parameter such that the prediction loss b/w the actual input and predicted input is less than \\epsilon, where epsilon is a hyper-parameter. \n\nStrong points:\n\n- I really like the underlying idea, irrespective of the results.\n- I like the preliminary results in figure 3 which basically justifies the core behind the proposed method.\n- I also like the idea, and preliminary results for jumpy planning. it was also studied in [a, b].\n\nQuestions:\n\n1.  A main drawback of the proposed method is the reliance on the \\epsilon parameter. In many cases, it would be the case that their are multiple entities in the environment, like for ex 2 bouncing balls as compared to one, and then the model has to adaptively decide as to for which part of the input, the model should skip the update. So, I'm curious as to how the model with perform when their are multiple bouncing balls (or multiple entities in the env.) against each other.\n\n2. It would also be interesting to study the generalization performance of the proposed method. Does the ability to skip update gives the ability to generalize better to out of distribution examples.\n\n3.  \"Finally, we vary the dimensionality of our model\u2019s hidden state and retrain. In figure 4, we find a positive\nrelationship between model capacity and jumpiness\" . The paper mention that they find a positive correlation b/w the model capacity and jumpiness.  Does this behaviour exists in all the datasets which the paper explore ?\n\n4. Related work: I think, many of the important references are missing. There are various important references. Like there's some work done where the idea is to learn how to skip updates in RNN like in SkipRNN (c), or in Adaptive Skip Intervals (d), or in the context of RIMs (e), where each module decides whether to update it's hidden state or not update their hidden state. There's also some work where the idea is to learn event based representations like in PhaseLSTM (f). It would be nice to see how the proposed method compares to any of these methods.\n\nReferences:\n\n- (a) Time-Agnostic Prediction: Predicting Predictable Video Frames, https://arxiv.org/abs/1808.07784\n- (b). InfoBot, https://arxiv.org/abs/1901.10902\n- (c). Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks, https://arxiv.org/abs/1708.06834\n- (d). Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models, https://arxiv.org/abs/1808.04768\n- (e). Recurrent Independent Mechanisms, https://arxiv.org/abs/1909.10893 \n- (f). PhaseLSTM, https://papers.nips.cc/paper/6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf\n\n5. I'm not sure what's the take away message from the paper as of now. In the current form the paper shows computational benefits i.e., the proposed method can achieve similar results, by updating dynamically based on events, and hence establishes a trade-off between the temporal resolution of the input and the computational expense. The results for planning are also very interesting. So, one suggestion could be to do extensive experiments and formulate the entire paper in the context of planning because as of now, the bouncing ball results for test mse are not better as compared to the baseline. \n\n=======\n\nAfter Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I really like the idea, but its important to evaluate the idea with respect to a downstream task to get a better idea on how to use the learned structure. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087068, "tmdate": 1606915764978, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2867/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review"}}}, {"id": "7fPfbNKL9fZ", "original": null, "number": 3, "cdate": 1603877474707, "ddate": null, "tcdate": 1603877474707, "tmdate": 1606785002689, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review", "content": {"title": "Well-written paper with simple and interesting ideas", "review": "## Paper Summary\n\nThis paper proposes a recurrent network architecture for future prediction where the hidden states (and the outputs) aren't updated step by step as done traditionally. Instead, the network models the hidden state dynamics as being piecewise linear over varying time spans. It learns to produce the linear dynamics together with each time span, and can \"jump\" to the next time span according to its own predictions. Hidden states for any time step within a span can be easily obtained by interpolation using the predicted linear dynamics. Experiments on a series of synthetic benchmarks are used to demonstrate that the model can learn to utilize this structure to reduce the amount of computation.\n\n## Strengths\n\n- The proposed approach is simple and intuitive. The study is likely to be valuable to researchers in sequence modeling, by showing that these simple ideas can work surprisingly well. One might expect that the non-stationarity of the \"jump size\" targets might make training such models too difficult, but the authors show that other than requiring a couple of tricks (setting $\\epsilon$ using a baseline and \"jump bootstrapping\"), training works well.\n\n- The paper is an enjoyable read due to its clarity and presentation. The claims made are modest and clear without over-reaching, the experiments use tasks that are directly motivated by the goals of the study, and the related work is clearly and fairly discussed.\n\n## Weaknesses\n\n- A clear weakness of the paper is that all experiments were conducted on synthetic tasks of low complexity. Although these experiments were illustrative and helpful, it is unclear if the approach works reasonably well on more realistic problems. One \"failure\" mode could be that the Jumpy RNN tends to jump almost every step for noisy and real-world data. At this stage this study does not really say whether this architecture is also likely to work well on such problems, but is more of a proof of concept.\n\n## Review Summary\n\nI think the strengths of this paper outweigh the weaknesses. While the experiments were not conducted on common benchmarks or real-world data, I think this paper is well-written as a proof of concept, does not overclaim and should motivate further work on similar ideas. My recommendation is to accept.\n\n## Questions for Authors\n\n1. Have you tried more complex datasets or RL problems? Are there any negative results you can report?\n2. Have you found the non-stationarity of the targets to be issue for training under any conditions?\n\n## Post-rebuttal\n\nI thank the authors for their hard work, and for incorporating my suggestions into the paper. I believe the paper has improved.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087068, "tmdate": 1606915764978, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2867/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review"}}}, {"id": "99BgauCQ5j", "original": null, "number": 1, "cdate": 1603095465964, "ddate": null, "tcdate": 1603095465964, "tmdate": 1606750659422, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review", "content": {"title": "Interesting problem but lacking comparisons and experimentation", "review": "Summary: This work presents Jumpy RNNs, a recurrent network that learns to take variable length steps based on time-scales of the data. The core idea of the paper is to learn a hidden velocity and time span, along with the standard hidden state. The hidden velocity is then used to linearly interpolate the hidden state within the learned time span.  This leads the proposed model to update the hidden state only at the end of time span thus allowing the model to jump over by certain steps proportional to time span. The model has flexibility to produce fine-grained or continuous-time predictions as well as predicting far into the future. Since, Jumpy RNNs do not update the hidden state at every time step, they are computationally efficient than standard RNNs. \n\nPositives:\n1. The idea of building a model to provide short term and long term prediction using just one hidden state update is interesting. The problem addressed is important in time series predictions in many domains.\n2. The paper is well written and easy to follow.\n3. Experiments show that the model works well on simulated datasets.\n\nConcerns:\n1. The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. The current experiments are mostly based on simulations and are too simple to evaluate the effectiveness of the proposed model. In almost all the cases, the rate of change is constant with few sharp changes. In particular, it would be interesting to see how the model performs on real world datasets, where the hidden state dynamics is unknown, for short-term and long-term time series predictions.\n2. Despite the paper stating that there have been some recent work on Neural ODEs (Chen et al., 2018), the paper does not compare with them. Neural ODEs are one of the possible candidates for solving the given problem and it would be interesting to see how fast/slow they are compared to the proposed model.\n3. The linear-dynamics of hidden state is too simple and can only handle constant hidden state dynamics or the one with constant slope. Although the authors mention that the hidden-state linearity does not translate to linearity in output space as the learned decoder can be an arbitrarily complex non-linear function, but the experiments are only based on constant or linear hidden state/output dynamics.\n4. What's the difference between Test MSE and Sample MSE? Based on the results, standard RNNs achieve lower MSE scores than the proposed model in most cases but use more computations. An interesting comparison would be to compare with a standard RNN with similar computational power as the final learned Jumpy RNN. Possible baseline could be training standard RNN with reduced sampling rate and do linear interpolations in between the predictions.     \n\nOther comments:\n1. What does baseline model refer to here: 'setting $\\epsilon$\u000f to the final training loss of the baseline model'?\n2. In the 3rd line of 2nd para in section 3.1, shouldn't it be 'c' instead of 'x'?\n3. Could the authors comment on the possibility of using the proposed approach to learn from irregularly sampled time series?\n\n==========Post-rebuttal comments================\n\nBased on other reviews and authors response, I have decided to keep my score. I still feel this paper need more work such as experiments on real world datasets and more comparisons as pointed out in the review. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087068, "tmdate": 1606915764978, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2867/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review"}}}, {"id": "-he9wJkXxOM", "original": null, "number": 2, "cdate": 1603760178015, "ddate": null, "tcdate": 1603760178015, "tmdate": 1606413130961, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review", "content": {"title": "Unconvincing experiments and lack of novelty", "review": "This paper proposes Jumpy Recurrent Neural Network, an RNN model with non-uniform time steps. To train this model, the authors propose to use a greedy supervision to determine optimal time intervals. The experiments on linear dynamics prediction and planning show comparable performance of proposed model against standard RNNs. The advantage is that the proposed model can significantly speed up RNNs under relatively linear dynamics prediction tasks. Even though non-uniform time step RNNs have been studied in many literature, this proposed training supervision method seems novel.\n\n+ves: \n\n+ Overall, the paper is well written. In particular, the proposed model and its training methods are clearly explained.\n\n+ The results section is well structured. The hyperparameters, architecture, and experimental settings are well demonstrated. \n\n \nConcerns: \n\n- The proposed method, non-uniform time interval RNN, is not novel. For example, Che et al 2018, Lipton et al 2016, have very similar contributions. This paper needs to clarify the difference.\n\n \n- The proposed greedy training method seems have significant flaw. Clearly it will have Delta=1 at the beginning of the training. Then it can only improve corresponding Delta one by one at each timestep. The loss corresponding to Delta is highly non-continuous and it will definitely not learn to find the real optimal Delta.\n\n\n- The model does not seem to learn meaningful Delta. See Figure 3 and 4. For such clean data, the model should learn to predict only turning points but it seems it just predict random intervals. Also, there are no verification of learning meaningful Delta.\n\n \n- The experiment is clearly not fair for a standard RNN. Since Delta=1 is always true for standard RNN, clearly it will be slower. The speedup ratio will only be the average of predicted Delta in the proposed model. However, what if the RNN just sets a larger Delta, what if the RNN just set a random predicted Delta? \n\n \n- The last experiment is not convincing. First, it's a simple supervised task, not real \"model-based planning\". Second, Figure 6 seems not converged. No experimental detail is given for this task. Third, twice as fast mean the model generally just predict Delta=2.\n\n \n- All the experiments have internal bias towards linear dynamics. This favors the model with corresponding inductive bias. To prove the model is useful, I would recommend to run standard RNN synthetic task like copying task, adding task etc, to prove its basic functionality.\n\n \nMinor comments: \n\n* Figure 1, in basic RNN, it should be an arrow instead of a line from h_i to GRU.\n\n* Equation 3, people usually don't write output as x, otherwise it can only be a sequence prediction task like language modeling. Please write it y. Also, it creates confusion to equation 7 whether it's input or output.\n\n* The \"Jump Bootstrap\" seems very important technique for the proposed method. But there's no experiment or discussion on it.\n\n=====POST-REBUTTAL COMMENTS========\n\nI thank the detailed response from the authors. The authors addressed the novelty of this paper. The experimental results on toy tasks are convincing. However, the way this method increases Delta still seems very problematic to me and not seem robust in complex real world cases.\n\nI increased my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4c3WeBTErrE", "replyto": "4c3WeBTErrE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087068, "tmdate": 1606915764978, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2867/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Review"}}}, {"id": "MpjkQocvi-", "original": null, "number": 6, "cdate": 1606282306495, "ddate": null, "tcdate": 1606282306495, "tmdate": 1606284465588, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "7fPfbNKL9fZ", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Answers to your questions as well as additional experiments. Thanks for your review", "comment": "Thanks for your very clear summary of the paper, positive comments about the technique and its usefulness to the community, and constructive questions. We reply below.\n\n**[1] \u201cclear weakness of the paper is that all experiments were conducted on synthetic tasks of low complexity\u201d**\n\n_We have added an experiment featuring a real-world dataset in response to your request._ It is the Jena weather dataset which is used as a canonical real-world time series forecasting baseline in a number of previous works. We show that our model matches and, in the case of autoregressive sample error, improves over two baseline RNN models, while using fewer steps to make these predictions. Please refer to the new Section 3.3 (in blue) in the updated pdf for details. We hope that this addresses your comment!\n\n**[2] \u201cHave you tried more complex datasets or RL problems? Are there any negative results you can report?\u201d**\n\nWe have converted the billiards 2D environment into an RL environment and begun using it to benchmark our two models. We found that adding action information (representing the forces applied to the cue ball) was difficult to integrate into both the baseline and the Jumpy model, especially when these actions are taken at a sparse rate (eg every 30 timesteps, on average). We plan to address these challenges in the future, but it is beyond the scope of this work.\n\n**[2.3] \u201cHave you found the non-stationarity of the targets to be [an] issue for training under any conditions?\u201d**\n\nWe have not encountered significant trouble from the non-stationarity of the $\\Delta$ targets. In fact, early experiments that kept $\\Delta$ fixed for multiple epochs (i.e.more stationary targets) yielded slower convergence and worse results. Early in training (the first thousand steps of gradient descent) the training loss decreases a bit more slowly as the $\\Delta t$ predictor adjusts to changes in the representation of the internal state. But aside from these discrepancies very early in training, we do not observe adverse effects.\n\n_Thanks again for your positive feedback and constructive criticism. We believe it has improved the paper._\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}, {"id": "l5XDcsVjow", "original": null, "number": 9, "cdate": 1606284037095, "ddate": null, "tcdate": 1606284037095, "tmdate": 1606284450986, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "99BgauCQ5j", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Answers to questions and additional experimental results. Thanks for your review.", "comment": "**[1] \u201cThe key concern about the paper is the lack of rigorous experimentation\u201d**\n\nWe disagree with the characterization of our experiments lacking rigor, but agree they are all performed on synthetic settings. We deliberately began with simple tasks so as to perform carefully-controlled experiments that would isolate specific aspects of our model. In our experience, leaping directly to noisy real-world datasets can actually lead to less-rigorous conclusions because the causal factors behind a model\u2019s success or failure are more difficult to disentangle from the dataset. The intent of this work was not to obtain the best results on a particular large-scale baseline, but rather to propose and demonstrate a qualitatively new approach to jumpy autoregressive sampling.\n\n_In order to address this concern, we have added a seventh experiment in which we train our models on a real-world weather forecasting dataset._ Our jumpy model performs well against two baseline RNN models; please see the blue diffs in the updated pdf for more details.\n\n**[2] \u201cThe linear-dynamics of hidden state is too simple and can only handle constant hidden state dynamics or the one with constant slope.\u201d**\n\nWe are not sure what supports the claim that piecewise linear hidden dynamics are too simple. We are also uncertain how simplicity is measured or what the claim suggests they are \u201ctoo simple\u201d for. Piecewise linear approximations are arbitrarily accurate as the span of each linear segment decreases and our model has control of this span via predicted Deltas. As the reviewer notes, our piecewise linear dynamics are in latent space and are further passed through arbitrarily complex decoders that are learned end-to-end with the linear hidden dynamics.\n\n**[3] \u201cexperiments are only based on constant or linear hidden state/output dynamics.\u201d**\n\nThis is not exactly true. It is true that our settings have simple linear motion; however, collisions are nonlinear events and occur frequently. Moreover, the pixel settings (Task 5 and 6) certainly lack linear input and output dynamics when encoding and generating pixel grids. \n\n**[4] \u201cWhat's the difference between Test MSE and Sample MSE?\u201d**\n\nTest MSE is the test loss of our model; it is being trained using teacher forcing and a MSE loss function over predictions. Sample MSE is the same metric, computed between a test trajectory and an autoregressive sample from our model, after our model was run on the first four observations of the test trajectory. This is called \u201cAutoregressive error\u201d in some works.\n\n**[5] \u201cCompare with a standard RNN with similar computational power\u201d or \u201cwith reduced sampling rate and do linear interpolations in between the predictions.\u201d**\n\nOur jumpy model has a complexity comparable to the standard RNN models. If anything, its representational capacity is diminished because only half of the cell state $h$ is used to represent state whereas the other half is used to represent velocity.\n\nRunning a standard RNN at a range of step sizes and then selecting the best would be unfair to our model -- essentially assuming some oracle could tell us the optimal step size a priori. When in practice, it can be quite difficult to know what step size would be best for a given dataset. Our approach discovers this dynamically during training. Further, running an RNN at a higher step rate produces no output for steps that have been skipped. As the reviewer suggests, we could perhaps resolve this by interpolating in the coordinate-based tasks but it is unclear what to do when the output is pixels. In either case, this would constitute an additional method wrapping around a standard RNN which could itself be a research topic.\n\n**[6] Comparison to Neural ODEs**\n\nWhile both Neural ODEs and Jumpy RNNs define continuous time dynamics, they are composable techniques.\n\nNeural ODEs are dynamics functions whereas Jumpy RNNs are models that predict dynamic functions. You could image, for example, a Jumpy RNN that predicts a sequence of Neural ODEs with each being integrated over the range Delta. Given this composability, we did not compare directly, but do agree comparing these approaches purely as continuous models could be valuable.\n\n**[7] Response to minor comments**\n\nQ: \u201cWhat is the baseline model in \u2018setting to the final training loss of the baseline model?\u2019\u201d\nA: In that context, the \u201cbaseline model\u201d refers to the regular RNN model with a GRU cell that ticks through time at a constant rate. We have clarified this point in the paper.\n\nQ: \u201cShouldn\u2019t it be \u2018c\u2019 instead of \u2018x\u2019?\u201d\nA: Yes, that is correct. We have made the correction in the paper; thanks for pointing this out\n\nQ: \u201c...possibility of \u2026[learning] from irregularly sampled time series?\u201d\nA: This is certainly possible, and we have preliminary experiments to this effect. In order to do this, one would simply set \\Delta t to the interval (in time) between adjacent observations. One could use the same criterion used in Equation (7) to train such a model.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}, {"id": "eIa-t_772Rs", "original": null, "number": 7, "cdate": 1606283080155, "ddate": null, "tcdate": 1606283080155, "tmdate": 1606283293352, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "-he9wJkXxOM", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Part II: Response to concerns 4, 5, and 6", "comment": "**[4] \u201cThe experiment is clearly not fair for a standard RNN\u2026what if the RNN just sets a larger Delta?\u201d**\n\nWe disagree. For both models, a single training run is performed starting from the data\u2019s native sample rate -- a reasonably standard thing to do for sequence modelling. Running a standard RNN at a range of step sizes and then selecting the best would make it unfair to our model -- essentially assuming some oracle could tell us the optimal step size a priori. When in practice, it can be quite difficult to know what step size would be best for a given dataset. Our approach discovers this dynamically during training.\n\nIt\u2019s worth noting that running an RNN at a higher step rate produces no output for steps that have been skipped. We could perhaps resolve this by interpolating in the coordinate-based tasks but it is unclear what to do when the output is pixels. In either case, this would constitute an additional method wrapping around a standard RNN. In contrast, our approach can output predictions at any point in time, regardless of step rate.\n\n**[5] \u201cThe last experiment is not convincing.\u201d It is (a) \u201ca simple supervised task, not real model-based planning\u201d, (b) \u201cFigure 6 seems not converged.\u201d, and (c) \u201cthe model generally just predict Delta=2\u201d**\n\nWe object to all three assertions:\n* (a) This is certainly a form of model-based planning. We learned a dynamics model and then leveraged rollouts from that model to make a plan for a finite horizon, which was then executed in the environment. This is similar to continuous trajectory optimization ubiquitous in robotics. Further, Reviewer 1 points out that this style of planning was also explored in related works and suggests pushing further in this direction. It is unclear to us from the reviewer\u2019s comments what precisely they take exception to here or why it is a \u201csimple supervised task\u201d -- hopefully we can clear this misunderstanding. \n* (b) We ran this experiment for longer and across several seeds; neither model exceeded 60% performance on the task and we believe that the plot is a fair representation of asymptotic performance. Section 3.3 gives the experimental setup for this task -- what experimental details do you see missing? Note that some experimental details were fixed across all experiments -- these can be found at the beginning of Section 3. Let us know if you feel something could be clearer.\n* (c) This is not true because the individual jumps taken by our model vary widely: even though the average jump length is 2, individual jumps are frequently much smaller or much larger. \n\n**[6a] \u201cAll the experiments have internal bias towards linear dynamics.\u201d**\n\nThis is true, but ignores important details. Our model\u2019s bias is towards linear dynamics in the latent space, not the output or observation spaces. This is why we can handle highly non-linear input/outputs like the pixel billiards task. That said, we view this internal bias towards linear dynamics as a desirable property of our model. Locally-linear approximations of dynamics can be highly effective and many physical processes are linear for long spans of time in the right state space. _The same critique could be posed against the linear bias of Kalman filters, yet they remain a useful and widely used tool._\n\n**[6b] run standard RNN synthetic tasks like copying, adding, etc, to prove its basic functionality.**\n\n_We note that our focus is on continuous-time sequences._ Discrete symbol processing tasks like the reviewer suggests are technically possible, but are non-standard as a benchmark for continuous models. In the usual form of the suggested tasks, each symbol is not predictive of the next so our model would revert to a Delta of 1 (achieving high error any time longer steps were attempted). _At that point, the jumpy model would be roughly equivalent to a standard RNN and perform similarly._ Alternative forms could be derived where long stretches of useless symbols are inserted after identifiable markers, but this is not our focus and beyond the scope of this work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}, {"id": "CBXyKiQ99nl", "original": null, "number": 8, "cdate": 1606283208355, "ddate": null, "tcdate": 1606283208355, "tmdate": 1606283208355, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "-he9wJkXxOM", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Part I: Response to concerns 1, 2, and 3", "comment": "We appreciate the reviewer\u2019s positive comments about our writing and the quality of our experimental results. We believe there are some misunderstandings about our work and its context and we respond to the reviewer\u2019s first three concerns below. We respond to the next three in a second comment.\n\n**[1] \u201cThe proposed method, non-uniform time interval RNN, is not novel.\u201d**\n\nWe strongly object to the notion that our approach is not novel. To our knowledge, our work is the first to define a continuous latent state as a series of predicted linear-latent functions and their associated time spans. There is of course a rich history of work on temporal abstraction, including RNN-type sequence models that operate at non-uniform intervals. _We cite and discuss both Che et al (2018) and Lipton et al (2016) along with other relevant works in the related work section._ For convenience, we reiterate that discussion below.\n\nChe et al (2018) define a continuous time RNN state by applying exponential decay to the hidden state between sequential, fixed time-steps in order to deal with irregularly sampled input data. No notion of jumpiness is proposed and the model operates otherwise as a standard RNN. In contrast, our jumpy model parameterizes the hidden state between ticks as a predicted linear function over a predicted, dynamic span of time. Similarly, Lipton et al (2016) study the use of indicator variables for missing data whereas our model \u201clearns to take variable length steps based on time-scales of the data.\u201d. Critically, neither work examines how to perform autoregressive sampling with non-uniform tick rates [see R4], we do. In response to Reviewer 1, we have extended our Related Work section to add further context to similar works.\n\n\n**[2] \u201cThe proposed greedy training method seems to have a significant flaw...Then it can only improve corresponding Delta one by one at each timestep.\u201d**\n\nThis is a misunderstanding of our approach -- Deltas can change by arbitrary amounts. The procedure described in Section 2.2 solves for the maximum jump such that prediction loss over the jump is less than an error threshold Epsilon (Eq. 7). As error is monotonic, this maximization is exact. In other words, if a model\u2019s linear latent state function correctly predicted the entire sequence, Delta would be set to that duration regardless of its previous value. \n\n**[3] \u201cThe model does not seem to learn meaningful Delta[s].\u201d**\n\nWe disagree. From the comment, we take the reviewer to have equated \u201cmeaningfulness\u201d of Deltas to their correspondence to collisions. This is one interpretation and looking at Task 3 (Figure 5), _we see all jump points in this example correspond closely to collisions in one (or both) of the lines_. Perhaps the reviewer did not realize the model is joint over the pair of lines and thus ticks whenever either collides in this random example. Or maybe mistook the four warm-up inputs as short jumps. We will clarify. For more complex task settings, this alignment of ticks to collisions is indeed less coherent. \n\nHowever, Deltas functionally correspond to jump lengths of bounded error in training. As a result, what is easy / difficult to model (and thus where the model ticks) depends on the complexity of the task and the model capacity. For example in Figure 4, increasing the capacity of our model in the circle task results in increased jump lengths, eventually saturating at the average full arc length. \n\nFurther, our results show that predicted Deltas are dynamic, context-dependent, and lead to \u201cmeaningful\u201d improvements over a jumpy RNN operating at a static tick rate. In our plots in Figure 5, 4th column -- we show a gray line corresponding to running the trained jumpy model at fixed Deltas. Across all datasets we find improved performance for the full model that predicts Deltas dynamically."}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}, {"id": "CVlge7RIJH-", "original": null, "number": 5, "cdate": 1606281823111, "ddate": null, "tcdate": 1606281823111, "tmdate": 1606281935494, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "lPeGuurvsv5", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Part I: Discussion of references and corresponding changes to the paper", "comment": "We appreciate the reviewer\u2019s positive comments and their thorough comments. We respond to these below.\n\n**[1]  \u201cRelated work: I think, many of the important references are missing\u201d**\n\nThank you for your recommendations regarding related work. _We have added all six of them to the paper, along with extensive discussion (see blue paragraphs in the new pdf)._ This area of research is quite rich, and we want to do our best to cover it thoroughly. Broadly, we see related works as falling into two categories: Works which are truly jumpy in time, i.e. skipping arbitrary lengths of time without expending computation based on the duration (a,b,d); and works which focus on hidden state update mechanics that enable multi-time-scale reasoning (c,e,f). In addition to the updates to the paper, here is some detailed commentary on the six references:\n\n--- In work (a), _Time-Agnostic Prediction...\u2019_ the authors perform jumpy planning by predicting bottleneck states in a time-agnostic manner (using a minimum-over-time loss). The key differences between their approach and ours is that 1) their model cannot interpolate between bottlenecks in order to reconstruct the intervening dynamics and 2) their model is a conditional VAE or a conditional GAN, and does not maintain a hidden representation of the state of the system. \n\n--- In work (b), _Infobot:\u2026_ the authors leverage model-based planning information only at decision states; elsewhere they use \u201c...model-free knowledge to navigate between bottlenecks\u201d. The key difference between our approach and theirs is that our model is designed for continuous-time, model-based planning whereas theirs is built upon an underlying notion of discrete states and does not perform jumpy computation (though it does run model-based planning adaptively). \n\n--- In work (c), _Skip RNN..._ the authors propose a model similar to Hierarchical Multiscale RNNs (HM-RNNs) except the skip operation is \u201capplied to the whole stack of RNN layers at the same time.\u201d This work differs from ours in the same way that HM-RNNs differ: even though some model updates can be skipped during the forward pass, there are still parts of the model that need to \u201ctick\u201d at every step (in particular, the encoder). In contrast, our model jumps directly over such intervals. The difference is most pronounced in the context of planning, where the Skip RNN model would need to sample N time steps in N ticks, whereas ours can simulate the same number of time steps in many fewer ticks.\n\n--- In work (d), _Adaptive Skip Intervals:..._ the authors propose a model which performs temporally-abstract forward simulation in a manner similar to (a): they use a minimum-over-time loss to predict the next predictable state. In their experiments, this corresponds to, for example, predicting collisions with ramps in a funnel board domain. Like our work, theirs is able to perform jumpy forward sampling of dynamics. Unlike our work, theirs does not match forward prediction to physical time steps and cannot reconstruct the dynamics that occur during a jumpy transition. \n\n--- In work (e), _Recurrent Independent Mechanisms_ the authors propose a recurrent model which consists of a set of recurrent models which make dynamics predictions semi-independently and compete with one another when making a global prediction by way of an attention mechanism. This leads to specialization of individual RIMs and permits some to function at different timescales (eg Figure 2). This work is analogous to Hierarchical Multiscale RNNs in that, during sampling, parts of this model will still need to \u201ctick\u201d at every time step (the encoder will, at the very least). Our model differs in that it permits continuous-time dynamics and does not need to perform any computation/\u201dticks\u201d at a given time step.  \n\n--- In work (f), _Phased LSTM:..._ the authors propose a learnable periodic gating function for an LSTM that enables different neurons to update themselves at different rates. As a result, \u201cit acts like a learnable, gated Fourier transform on its input\u201d and this is useful in a number of cases. This work is closely related to Koutnik et al (2014), which we discuss in Related Work, except in this work the periodic gating function is learnable. Like our model, this work permits computation across adaptive timescales; unlike our model, it does not learn continuous-time dynamics and (parts of) it _must_ \u201ctick\u201d at every time step in a sequence. \n\n--- We have also added a few other related works including: [Legendre Memory Units](https://bit.ly/2KEWGLO),  [Variable computation in RNNs](https://arxiv.org/abs/1611.06188), and two papers about bottleneck discovery in RL (McGovern & Barto, \u201cAutomatic discovery...\u201d and Stolle & Precup \u201cLearning options...\u201d)\n\n_In addition to related work, the author brought up several other questions. We respond to these in the next comment \"Part II: Responses to other questions.\"_"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}, {"id": "5UFF7ccSs5", "original": null, "number": 4, "cdate": 1606281778904, "ddate": null, "tcdate": 1606281778904, "tmdate": 1606281778904, "tddate": null, "forum": "4c3WeBTErrE", "replyto": "lPeGuurvsv5", "invitation": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment", "content": {"title": "Part II: Responses to other questions ", "comment": "**[2] \u201cA main drawback...is the reliance on the \\epsilon parameter.  [...] I'm curious as to how the model will perform when there are multiple bouncing balls (or multiple entities in the env.) against each other.\u201d**\n\nWe agree that adding a hyperparameter can represent a technical challenge, especially during training. However, in the process of constructing any jumpy dynamics model, one must introduce an additional hyperparameter that controls the tradeoff between length of jump and accuracy. In works (a) and (b) the authors use the hyperparameter $\\beta$ to weight the time horizon and information regularization term, respectively. In work (c), the authors use an \u201cupdate budget\u201d hyperparameter $\\lambda$ to control the skip rate. The other works do likewise. Fortunately, we found that our model was stable and resilient to non-optimal values of $\\epsilon$. For example, multiplying $\\epsilon$ by 0.5 or 2 did not substantially impact our model\u2019s performance.\n\n**[3] \u201cDoes this behavior [increasing jump sizes with increasing capacity] exist in all the datasets which the paper explores?.\u201d**\n\nYes, this behavior occurs in all the datasets we explored. It\u2019s a good question. We have noted this in the caption of Figure 4.\n\n**[4] Does the ability to skip updates give the ability to generalize better to out of distribution examples?**\n\nWe did not examine the OOD setting explicitly; however, as the environments became more complex (e.g. 2D billiards) the training set represented a sparser sampling of possible dynamics. Our jumpy model generalized better than the baseline RNN. Ball paths generated by the baseline often veered left or right in the absence of external forces, producing non-physical dynamics. Our jumpy model achieved lower sample error, and when it did make mistakes, they were often related to the fact that a jumpy forward prediction overshot a collision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2867/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jumpy Recurrent Neural Networks", "authorids": ["~Samuel_James_Greydanus1", "~Stefan_Lee1", "~Alan_Fern1"], "authors": ["Samuel James Greydanus", "Stefan Lee", "Alan Fern"], "keywords": ["RNNs", "temporal abstraction", "planning", "intuitive physics"], "abstract": "Recurrent neural networks (RNNs) can learn complex, long-range structure in time series data simply by predicting one point at a time. Because of this ability, they have enjoyed widespread adoption in commercial and academic contexts. Yet RNNs have a fundamental limitation: they represent time as a series of discrete, uniform time steps. As a result, they force a tradeoff between temporal resolution and the computational expense of predicting far into the future. To resolve this tension, we propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time. Instead, it predicts a sequence of linear dynamics functions in latent space and intervals of time over which their predictions can be expected to be accurate. This structure enables our model to jump over long time intervals while retaining the ability to produce fine-grained or continuous-time predictions when necessary. In simple physics simulations, our model can skip over long spans of predictable motion and focus on key events such as collisions between two balls. On a set of physics tasks including coordinate and pixel observations of a small-scale billiards environment, our model matches the performance of a baseline RNN while using a fifth of the compute. On a real-world weather forecasting dataset, it makes more accurate predictions while using fewer sampling steps. When used for model-based planning, our method matches a baseline RNN while using half the compute.", "one-sentence_summary": "We propose a Jumpy RNN model which does not predict state transitions over uniform intervals of time, but rather predicts a sequence of linear dynamics functions and intervals of time over which their predictions can be expected to be accurate. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "greydanus|jumpy_recurrent_neural_networks", "supplementary_material": "/attachment/3b3e243ee3dca0599057bc2975956abdc43d7782.zip", "pdf": "/pdf/1b0fd7e3a9f5565d3ceb2a65d81946fda0bcd11d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=STMiL-QmYx", "_bibtex": "@misc{\ngreydanus2021jumpy,\ntitle={Jumpy Recurrent Neural Networks},\nauthor={Samuel James Greydanus and Stefan Lee and Alan Fern},\nyear={2021},\nurl={https://openreview.net/forum?id=4c3WeBTErrE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4c3WeBTErrE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2867/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2867/Authors|ICLR.cc/2021/Conference/Paper2867/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2867/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843671, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2867/-/Official_Comment"}}}], "count": 12}