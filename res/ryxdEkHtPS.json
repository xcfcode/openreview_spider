{"notes": [{"id": "LPQbyXfA8ru", "original": null, "number": 7, "cdate": 1585856872835, "ddate": null, "tcdate": 1585856872835, "tmdate": 1585856872835, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment", "content": {"title": "Camera Ready Uploaded", "comment": "We have updated the paper to the camera-ready version now. While updating it, we saw a bug in our implementation of KL divergence calculation---we have ensured that the results remain accurate by re-running all of the experiments in our paper (the graphs have been updated). Below is a list of the minor edits we made in the camera-ready version:\n- While we were rerunning everything, we used many more agents to ensure the trends held up. All of the trends have been verified on 24 random agents.\n- We performed a much finer grid search to find the best agent parameters\n- We removed the \"PPO-M\" line from the graphs, as we did not introduce PPO-M in this paper and it was an artifact from an earlier revision.\n- We removed the 2-3 sentences about the high-sample regime, since we were unable to reproduce it reliably with the new code/old parameters (and lacked the compute to do another full grid in the high-sample regime)\n- We updated the value baseline results to use 5 million state-action pairs instead of 500K\n- We give finer detail about the hyperparameters used in Appendix A\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxdEkHtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1660/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1660/Authors|ICLR.cc/2020/Conference/Paper1660/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152744, "tmdate": 1576860533717, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment"}}}, {"id": "ryxdEkHtPS", "original": "BygoPa3dvH", "number": 1660, "cdate": 1569439536109, "ddate": null, "tcdate": 1569439536109, "tmdate": 1585854463504, "tddate": null, "forum": "ryxdEkHtPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "EYZztuKXx", "original": null, "number": 1, "cdate": 1576798729122, "ddate": null, "tcdate": 1576798729122, "tmdate": 1576800907410, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "The paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory. All three reviewers are excited about this work and recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719276, "tmdate": 1576800269897, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Decision"}}}, {"id": "rkgkefNAYr", "original": null, "number": 3, "cdate": 1571860966712, "ddate": null, "tcdate": 1571860966712, "tmdate": 1573938921133, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This is an interesting and important paper, it emphasizes and analyzes how policy gradient methods modify their objective functions and how this leads to training differences (and often errors w.r.t. the true objective). I have some minor comments on terminology used that I would like to see properly defined within the paper, but otherwise believe this should be accepted for its useful insights.  \n\nAssorted Comments:\n+ Maybe I simply have a difference of opinion or have misunderstood, but I am hesitant to agree that the work is comparing the surrogate *reward* function, but rather the surrogate objective. You'll notice that in the TRPO paper, it is called a surrogate objective not a surrogate reward: https://arxiv.org/pdf/1502.05477.pdf .\n+ I think better specification of what exactly is being plotted (pointing to an equation) or defining very concretely what is a surrogate reward or true reward (which I suspect is the objective) will make this paper much clearer.\n+ In fact, it was a bit unclear whether the comparisons were of the sampled/observed reward function R(s,a) (provided by the environment and sampling regime) or the objective function often the advantage A(s,a) (or the surrogate objective, GAE, etc.) I assume it should be the latter, but the wording of the paper makes this a bit unclear. I suggest discussing things in terms of objectives not rewards -- unless in fact the paper does approximate reward functions in which case this should be specified in much more detail.  \n+ Also, in a lot of places it seems like there's a mixup between rewards and returns. I think typically in the literature reward = r_t and return = V_t (sum of reward). Perhaps, in places the paper truly speaks of rewards, but from the context it seems as though it mainly refers to returns. Examples: \" Evidently (since the agent attains a high reward) these estimates are sufficient to consistently improve reward\" \" This is in spite of the fact that our agents continually improve throughout training, and attain nowhere near the maximum reward possible on each task\"\n ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878017821, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Reviewers"], "noninvitees": [], "tcdate": 1570237734142, "tmdate": 1575878017833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review"}}}, {"id": "rygtBhTOor", "original": null, "number": 3, "cdate": 1573604417467, "ddate": null, "tcdate": 1573604417467, "tmdate": 1573604417467, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "r1g2J6P6tr", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your comments.\n\nResponding to your questions: The step direction is the direction of the update computed for the current agent, and the points do indeed correspond to linearly combined mixtures of a random (Gaussian) vector and the step direction. Your conclusion that following a random direction would be more beneficial than following the step in Figure 8 is correct --- this misalignment between true reward and surrogate objective is a core finding of our work.\n\nWe have fixed the citation, thank you!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxdEkHtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1660/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1660/Authors|ICLR.cc/2020/Conference/Paper1660/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152744, "tmdate": 1576860533717, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment"}}}, {"id": "BylmMh6djH", "original": null, "number": 2, "cdate": 1573604362957, "ddate": null, "tcdate": 1573604362957, "tmdate": 1573604396194, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "Sye656FTFr", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your comments and suggestions. We have addressed your concerns below:\n\nIn Figure 4 (b), the returns MRE hovers around 1/2, which is where we obtain the \u201c50% off of the true value function\u201d conclusion. \n\nIn (13), the $\\pi$ refers to the current policy, and $\\pi_\\theta$ refers to the policy we are optimizing to solve the maximization problem (which will become the agent\u2019s new policy).\n\nWe have added references to previous work --- Schulman et al 2015 [0] and Sutton and Barto 2018 [1] --- that support the assertion that learned baselines result in significant improvements in agent performance.\n\nIn Figure 6 and 7, the changing factor of sample size and objective do not change the way that the agent was run. We ablate these factors to indicate the objective mismatch, and the noisiness of the reward landscape around agents.\n\nThank you for the catch, the $V_{\\theta_{t-1}}$ in Eq (4) is indeed a function of state and we have corrected this notation accordingly.\n\n[0] https://arxiv.org/abs/1506.02438\n[1] http://incompleteideas.net/book/the-book.html\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxdEkHtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1660/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1660/Authors|ICLR.cc/2020/Conference/Paper1660/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152744, "tmdate": 1576860533717, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment"}}}, {"id": "B1lOJhaOir", "original": null, "number": 1, "cdate": 1573604320175, "ddate": null, "tcdate": 1573604320175, "tmdate": 1573604320175, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "rkgkefNAYr", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment", "content": {"title": "Response ", "comment": "Thank you for your feedback, and we are happy that you enjoyed the paper.\n\nSurrogate objectives/\u201dSurrogate rewards\u201d terminology: we indeed refer to the surrogate objective when we refer to the surrogate reward --- we have corrected this in the revision by replacing all instances of surrogate reward with surrogate objective. The terminology of \u201csurrogate reward\u201d simply refers to the fact that instead of optimizing over the true rewards, agents optimize over a surrogate function. To address your point of concretely defining the surrogate objective, we have placed a reference in the main text to the surrogate objective\u2019s definition (which can be found in the Appendix).\n\nWith respect to our experiments/comparisons, our experiments use the surrogate objective or the true reward information depending on the section. We measure steps optimizing the surrogate objective in our gradient estimation quality experiments, and plot both the true reward and the surrogate objective in our landscape experiments.\n\nWe agree that it would be an interesting line of work to investigate how the misalignment of the surrogate reward impacts value learning.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxdEkHtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1660/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1660/Authors|ICLR.cc/2020/Conference/Paper1660/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152744, "tmdate": 1576860533717, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Authors", "ICLR.cc/2020/Conference/Paper1660/Reviewers", "ICLR.cc/2020/Conference/Paper1660/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Comment"}}}, {"id": "Sye656FTFr", "original": null, "number": 2, "cdate": 1571818901026, "ddate": null, "tcdate": 1571818901026, "tmdate": 1572972439753, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n[Summary]\nThis paper empirically studies the behavior of deep policy gradient algorithms during the optimization. The conclusion is that, while these methods generally improve the policy, their behavior does not comply with the underlying theoretical framework. First, sample gradients obtained with a reasonable batch size have little correlation with each other and with the true gradient. Second, a larger batch size requires a smaller step-size. Third, the value baseline is far from true values and only marginally reduces variance, yet it considerably helps with optimization. Finally, the optimization landscape highly varies with the choice of objective function and the number of samples used to estimate it.\n\n[Decision]\nI vote for acceptance. To the best of my knowledge, the findings of this paper are new and not predictable by the current theory. These negative results have some merit as they call for theory that explains the behavior of these algorithms, or an algorithm whose behavior is predictable by the current theory. The paper is well-written, with a few small issues in presentation that should to be addressed in the final revision.\n\n[Comments]\nIn Fig. 4 (b) it does not look like that the value error is high. It is said that \"the learned value function is off by about 50% w.r.t. the underlying true value function.\" This sentence should be clarified or visualized.\n\nWhat is \\pi in Eq (13) in A1? If it is the agent's current policy, how is it different than \\pi_\\theta? If \\pi corresponds to the distribution of state-action pairs in the replay buffer, how can one obtain a policy \\pi that has led to this distribution of states in order to construct the importance sampling ratio?\n\nIn 2.2, the claim that a learned value baseline results in significant improvement in performance should be supported by results or reference to previous work.\n\nFigs. 6 and 7 compare the loss surface with different objectives and sample regimes. Do these factors (objective and sample size) affect the part of the parameter space that is visualized (by changing the origin and the update direction), or are they only used to evaluate the values on the z-axis for the same area in the parameter space? Observing a different landscape in a different part of the parameter space is not surprising.\n\n[Minor comments]\n- Is V_\\theta_{t-1} in Eq (4) a function of state? If so, a (s_t) is missing before the plus sign."}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878017821, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Reviewers"], "noninvitees": [], "tcdate": 1570237734142, "tmdate": 1575878017833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review"}}}, {"id": "r1g2J6P6tr", "original": null, "number": 1, "cdate": 1571810531944, "ddate": null, "tcdate": 1571810531944, "tmdate": 1572972439707, "tddate": null, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "invitation": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper explores a critical divergence between theory and practice, emphasizing that while deep policy gradient algorithms seem to work in certain cases, they don't seem to be working foor the reasons underlying their derivations. It particularly looks at how closely the sample-based approximation of the objective's gradient aligns with the true gradient of the objective, how accurately learned values match the true expected returns, and how well the optimization landscapes of surrogate objectives line up with the objective of maximizing the return.\n\nI propose accepting this paper, as it reveals a key gap in our understanding of why policy gradient methods work. Such emphasis can suggest why deep RL results tend to be inconsistent and irreplicable, and spark future work on closing the gap between theory and practice. Further, the paper is overall well written.\n\nI primarily would like clarification on the optimization landscape visualizations:\n\n1) Is the step direction the direction of the update actually performed at that time step?\n\n2) Would moving diagonally in this space correspond to a mixture of following the update direction and a normally-distributed random direction? Concretely, in the true reward plot at Step 0 for few state-action pairs in Figure 8, does this suggest that mixing a random direction with the update direction would be better than moving cmopletely in the step direction?\n\nMinor:\nTypo in citation \"...policy improvement theorem of Kakade and Langford Kakade & Langford (2002)\""}, "signatures": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1660/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Closer Look at Deep Policy Gradients", "authors": ["Andrew Ilyas", "Logan Engstrom", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"], "abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "pdf": "/pdf/ff9e9a4c93a033da68f30df6d8e1bf9e0611b130.pdf", "paperhash": "ilyas|a_closer_look_at_deep_policy_gradients", "_bibtex": "@inproceedings{\nIlyas2020A,\ntitle={A Closer Look at Deep Policy Gradients},\nauthor={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxdEkHtPS}\n}", "original_pdf": "/attachment/5c61fd475af87c0822358f0e7099a8774c56568c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxdEkHtPS", "replyto": "ryxdEkHtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1660/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878017821, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1660/Reviewers"], "noninvitees": [], "tcdate": 1570237734142, "tmdate": 1575878017833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1660/-/Official_Review"}}}], "count": 9}