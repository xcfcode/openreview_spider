{"notes": [{"id": "HyzMyhCcK7", "original": "r1xsVYhKtm", "number": 966, "cdate": 1538087897979, "ddate": null, "tcdate": 1538087897979, "tmdate": 1550531051381, "tddate": null, "forum": "HyzMyhCcK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeGvgxPg4", "original": null, "number": 1, "cdate": 1545171033880, "ddate": null, "tcdate": 1545171033880, "tmdate": 1545354476629, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Meta_Review", "content": {"metareview": "A novel  approach for quantized deep neural nets is proposed,  which is more principled than commonly used  straight-through gradient method. A theoretical analysis of the algorithm's converegence  is presented, and empirical results show advantages of the proposed approach. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A novel and promising approach to quantized deep nets"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper966/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353015953, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353015953}}}, {"id": "HJe54G3sRm", "original": null, "number": 13, "cdate": 1543385649877, "ddate": null, "tcdate": 1543385649877, "tmdate": 1543385649877, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "S1eLOUDJn7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Response to the updated review", "comment": "Thank you for the quick response after the rebuttal. We respond to the added comments in the following.\n\n--- \u201cNovelty is limited\u201d\nFirst, we would like to clarify that the difference between our method and BNN are two-fold: our method is a {non-lazy, soft} prox-gradient method whereas BNN (BinaryConnect) is {lazy, hard} (see the discussion in Section 2 and 3.) The difference between lazy projection and standard projection is important as well.\n\nSecond, our \u201cmotivating storyline\u201d is not only the observation that BinaryConnect is Nesterov\u2019s dual-averaging, but also the observation that BinaryConnect suffers from non-convergence on fairly simple toy problems (Figure 1), and that all lazy prox algorithms suffer from a fundamental information-theoretic limit due to the lack of gradient information as well (Figure 1 & Section 2.3). That poses an issue to address.\n\nMore importantly, the contribution of our paper is not merely \u201cproposing yet another alternative to the (already abundant) literature of quantization algorithms\u201d, but also \u201cunveiling the properties of all these algorithms with theories (Section 5) and diagnostic experiments (Appendix C)\u201d. We believe that it will indeed be \"an interesting addition to the literature\", as Reviewer 1 kindly commented. \n\n--- \u201cConvergence is a decoration\u201d\nWe wanted to point out that our theoretical results contain 3 parts: (1) convergence of ProxQuant, (2) non-convergence of lazy proximal gradient descent under the same problem assumptions, and (3) a characterization that BinaryConnect is very unstable and unlikely to converge in practice (with corroborating experiment). \n\nThe combination of (1) and (2) points out a clear advantage of ProxQuant over the lazy version in theory, which to the best of our knowledge is novel. Further, the counterexample we construct to show (2) is a very natural simple problem on 1d for binary quantization, which suggest a serious potential drawback of all lazy proximal algorithms in quantization applications. We believe that this result will convey an interesting message to the community about the limitation of the lazy projection mechanism.\n\nRe \u201cconvergence can be easily obtained\u201d. We didn\u2019t either claim that the convergence is hard. Indeed, before we present the proof in Appendix D.1, we already remarked that such type of convergence is fairly standard in the literature of proximal algorithms. We will add a reference and attribute the asymptotic critical point convergence guarantee to Atouch et al. (2013). We are not sure if our Ghadimi & Lan (2013) style of convergence **rate** result is new either, but given that it is not hard, we are not claiming any credits for that.\n\nThe point of presenting the convergence analysis is to have a self-contained discussion with elementary proofs that separates the standard and lazy prox-gradient algorithms for the problem of model quantization. This particular insight, to the best of our knowledge, is new to the current paper.\n\n- Attouch, H., Bolte, J., & Svaiter, B. F. (2013). Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013Seidel methods. Mathematical Programming, 137(1-2), 91-129.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "BklDCt7c37", "original": null, "number": 2, "cdate": 1541188047136, "ddate": null, "tcdate": 1541188047136, "tmdate": 1543373909527, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "content": {"title": "Limited theoretical contribution and concerns about experiments", "review": "This paper proposed ProxQuant method to train neural networks with quantized weights. ProxQuant relax the quantization constraint to a continuous regularizer and then solve the optimization problem with proximal gradient method. The authors argues that previous solvers straight through estimator (STE) in BinaryConnect (Courbariaux et al. 2015) may not converge, and the proposed ProxQuant is better.\n\n I have concerns about both theoretical and experimental contributions\n\n1. The proposed regularizer for relaxing quantized constraint looks similar to BinaryRelax (Yin et al. 2018 BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights.), which is not cited. I hope the authors can discuss this work and clarify the novelty of the proposed method. One difference I noticed is that BinaryRelax use lazy prox-graident, while the proposed ProxQuant use non-lazy update. It is unclear which one is better.\n\n2. On page 5, the authors claim \u2018\u2019Our proposed method can be viewed as \u2026 generalization ...\u2019\u2019 in page 5. It seems inaccurate because unlike proposed method, BinaryConnect use lazy prox-gradient.\n\n3. What\u2019s the purpose of equation (4)? I am confused and did not find it explained in the content.\n\n4. The proposed method introduced more hyper-parameters, like the regularizer parameter \\lambda, and the epoch to perform hard quantization. In section 4.2, it is indicated that parameter \\lambda is tuned on validation set. I have doubts about the fairness comparing with baseline BinaryConnect. Though BC does not have this parameter, we can still tune learning rate.\n\n5. ProxQuant is fine-tuned based on the pre-trained real-value weights. Is BinaryConnect also fine-tuned? For a CIFAR-10 experiments, 600 epochs are a lot for fine-tuning. As a comparison, training real-value weights usually use less than 300 epochs. BinaryConnect can be trained from scratch using same number of epochs. What does it mean to hard-quantize BinaryConnect? The weights are already quantized after projection step in BinaryConnect.  \n\n6. The authors claim there are no reported results with ResNets on CIFAR-10 for BinaryConnect, which is not true. (Li et al. 2017 Training Quantized Nets: A Deeper Understanding) report results on ResNet-56, which I encourage authors to compare with. \n\n7. What is the benefit of ProxQuant? Is it faster than BinaryConnect? If yes, please show convergence curves. Does it generate better results? Table 1 and 2 does not look convincing, especially considering the fairness of comparison.\n8. How to interpret Theorem 5.1? For example,  Li et al. 2017 show the real-value weights in BinaryConnect can converge for quadratic function, does it contradict with Theorem 5.1?\n\n9. I would suggest authors to rephrase the last two paragraphs of section 5.2. It first states \u2018\u2019one needs to travel further to find a better net\u2019\u2019, and then state ProxQuant find good result nearby, which is confusing. \n\n10.  The theoretical benefit of ProxQuant is only intuitively explained, it looks to me there lacks a rigorous proof to show ProxQuant will converge to a solution of the original quantization constrained problem.\n\n11. The draft is about 9 pages, which is longer than expected. Though the paper is well written and I generally enjoyed reading, I would appreciate it if the authors could shorten the content. \n\nMy main concerns are novelty of the proposed method, and fairness of experiments. \n\n\n\n\n======================= after rebuttal =======================\n\nI appreciate the authors' efforts and am generally satisfied with the revision. I raised my score. \n\nThe authors show advantage of the proposed ProxQuant over previous BinaryConnect and BinaryRelax in both theory and practice. The analysis bring insights into training quantized neural networks and should be welcomed by the community. \n\nHowever, I still have concerns about novelty and experiments.\n\n- The proposed ProxQuant is similar to BinaryRelax except for non-lazy vs. lazy updates. I personally like the theoretical analysis showing ProxQuant is better, although it is based on smooth assumptions. However, I am quite surprised BinaryRelax is so much worse than ProxQuant and BinaryConnect in practice (table 1). I would encourage the authors to give more unintuitive explanation.\n\n-  The training time is still long, and the experimental setting seems uncommon. I appreciate the authors' efforts on shortening the finetuning time, and provide more parameter tuning.  However, 200 epochs training full precision network and 300 epochs for finetuning is still a long time, consider previous works like BinaryConnect can train from scratch without a full precision warm start. In this long-training setting, the empirical advantage of ProxQuant over baselines is not much (less than 0.3% for cifar-10 in table 1, and comparable with Xu 2018 in table 2).\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "cdate": 1542234336480, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335843274, "tmdate": 1552335843274, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eLOUDJn7", "original": null, "number": 1, "cdate": 1540482670163, "ddate": null, "tcdate": 1540482670163, "tmdate": 1543300100396, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "content": {"title": "Interesting idea but novelty may not be enough", "review": "After the rebuttal:\n\n1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is \"using soft instead of hard constraint\" for BNN. \n\n2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].\n\n---------------------------\nThis paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited.\n\n1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example,\n- A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training\n- Training Ternary Neural Networks with Exact Proximal Operator\n- Loss-aware Binarization of Deep Networks\n\nAll above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). \n\n2. Could authors perform experiments with ImageNet?\n\n3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "cdate": 1542234336480, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335843274, "tmdate": 1552335843274, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlGBiktAX", "original": null, "number": 2, "cdate": 1543203641714, "ddate": null, "tcdate": 1543203641714, "tmdate": 1543203641714, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Public_Comment", "content": {"comment": "Hi authors!\n\nI have a question in regards to the binary quantization performed in the experiments. I am curious about how you choose the binary weights. Are they chosen from {-1,1} or {-\\alpha, \\alpha} with some adaptive real scalar \\alpha>0? \n\nI think the adaptive scalar is important to maintain satisfactory precision, e.g., see Xnor-net. But your convergence analysis seems tied to {-1,1}, not {-\\alpha, \\alpha}. \n\nCan the authors clarify this? Thank you.", "title": "The adaptive scalar?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311710469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyzMyhCcK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311710469}}}, {"id": "HJxhpp6DAm", "original": null, "number": 8, "cdate": 1543130564104, "ddate": null, "tcdate": 1543130564104, "tmdate": 1543131165353, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Paper revised: adding theoretical analyses + new experimental results", "comment": "We have made a revision to our paper, adding a section on theoretical analysis, as well as some new experimental results. For convenience to the reviewers and readers, we have temporarily highlighted the changes in red (for updated experiments) and blue (for stuff related to theoretical results). \n\nDetails of the changes are summarized as follows. Noticably, we now have both theoretical and empirical evidence of our advantage over Yin et al. as described below.\n\n(1) Theoretical analysis\nWe added a new section for theoretical analysis (Section 5). Specifically, we show that our ProxQuant converges to stationary points under mild smoothness assumptions on the problem (Section 5.1). In the same setting, lazy prox-gradient method (e.g. BinaryRelax of Yin et al.) fails to converge in general -- we construct a fairly natural example in 1d to show that. \n\nOur previous convergence analysis of BinaryConnect is now Section 5.3, and the corresponding sign change experiment is now in Appendix C.\n\n(2) New experimental results\nWe have shortened the CIFAR-10 training from 600 epochs to 300 epochs (200 training + 100 BatchNorm layer stabilizing) and re-done the experiments. In this new setting, our ProxQuant maintains the advantage over BinaryConnect. This setting also matches the 300 epoch training setup of Yin et al., and our performance drop (~1% - 1.3%) is significantly lower than the reported results of their BinaryRelax (~2% - 4%). We have also added an experiment on ResNet-56.\n\nDue to space constraints and the added binarization results, we have moved the results of ternarization to Appendix B.\n\nFor the LSTM experiment, we performed an additional learning rate tuning for the binarized LSTMs. Improved PPW is seen on both BinaryConnect (419.1 -> 372.2) and ProxQuant (321.8 -> 288.5), and ProxQuant still maintains a significant advantage over BinaryConnect."}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "BJxebk0vA7", "original": null, "number": 9, "cdate": 1543130872003, "ddate": null, "tcdate": 1543130872003, "tmdate": 1543130872003, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyxI9lvQAX", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Revised, addressing novelty (comparison with Yin et al.) and fairness", "comment": "Thank you again for the suggestions. We have revised our paper again, adding a new section on theoretical analysis and some new experimental results (details also appearing as a new public comment).\n\nAddressing the comments:\n\n(1) Novelty, comparison with Yin et al.: We have shown the advantage of our ProxQuant over their BinaryRelax with both theoretical and empirical evidence. \n\nTheoretically, we have added a non-convergence result for lazy prox-gradient method (e.g. their BinaryRelax) in Section 5.2, which works under the same setting in which our ProxQuant converges (Section 5.1). Specifically, the counter-example we constructed for the non-convergence result is a fairly natural problem in 1d and not very adversarial: quadratic loss, smoothed W-shaped regularizer. Together, we have a comprehensive comparison over lazy and non-lazy prox-gradient methods and shown the advantage of our non-lazy version (ProxQuant).\n\nEmpirically, we have added a comparison of the performance drops of binarization in Table 1, Section 4.1. Our performance drop on CIFAR-10 is typically 1% - 1.3%, much lower than the reported result (2% - 4%) in Yin et al..\n\n(2) Fairness: number of epochs, ResNet-56, comparison with Li et al.\n\nWe have re-done our CIFAR-10 binarization experiments with 300 epochs (200 training, 100 BatchNorm stabilization), half of what we had before. Our ProxQuant maintains the advantage over BinaryConnect (see Table 1). We have also done experiments on ResNet-56, on which the classification error of {FP, BinaryConnect, ProxQuant} is {6.54%, 7.97%, 7.70%}. \n\nSpecifically, about the comparison with Li et al.: their reported result on ResNet-56 was 8.10% for FP net and 8.83% for BinaryConnect. Though they achieved the <1% performance drop with BinaryConnect, we suspect that may come from the much inferior initializing full-precision net they used (8.10% compared with our 6.54%), so that binarization will cause a lower performance drop on that particular net. In fact their initializing net is even inferior than our full-precision ResNet-20 (8.06%).\n\n(3) LSTM experiments\nWe have done a learning rate tuning on binary LSTMs. Both BinaryConnect and our ProxQuant have improved perplexities (BinaryConnect: 372.2, ProxQuant: 288.5) and ProxQuant is still significantly better than BinaryConnect (see Table 2).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "HyxI9lvQAX", "original": null, "number": 7, "cdate": 1542840461666, "ddate": null, "tcdate": 1542840461666, "tmdate": 1542840461666, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "r1gpSKtvam", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Thanks for response, concerns about novelty and fairness", "comment": "I appreciate the authors' detailed response. My main concerns are still novelty and fairness. I am willing to raise my score after my main concerns are resolved. \n\n1. I understand there are other contributions such as the analysis of BinaryConnect and ProxQuant in the paper. However, I am still worried about the difference with Yin et al. Like in the authors' response, the main difference seems to be the lazy vs. non-lazy update. Which one is better? Could theoretical or empirical analysis be done for the difference?\n\n2. I am still concerned about experiments and would love to see the authors' response. It looks to me fine-tuning 400 epochs (much more than 200 epochs for standard training) is an uncommon setting. Regarding resnet-56 in Li et al. , it is more about relative number. Li et al. show that with same number of epochs (about 200 for standard training), BinaryConnect can approximate full precision result within <1% difference. In table 1, ProxQuant is better than BinaryConnect within <0.5% difference, but with 600 epochs for training. \n\nThanks."}, "signatures": ["ICLR.cc/2019/Conference/Paper966/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "r1gpSKtvam", "original": null, "number": 2, "cdate": 1542064452659, "ddate": null, "tcdate": 1542064452659, "tmdate": 1542076374171, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "BklDCt7c37", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Revision and Responses", "comment": "Thank you for the very concrete and thoughtful feedback! We have found the comments very useful and constructive for revising the paper.\n\nWe have made some initial revisions to address the comments -- please find our changes as well as our response to the comments below.\n\nNovelty and Fairness of Experiments\n\nPoint 1 -- As you have pointed out, the main algorithmic difference between ours and Yin et al. (2018) is that we use a non-lazy, standard prox-gradient method whereas their BinaryRelax is a lazy prox-gradient. \n\nThe further novelty of our paper lies in the new observation that BinaryConnect suffers from more optimization instability, which are both theoretically and empirically justified in our Section 5.\n\nWe have addressed Yin et al. (2018) as well as a few other related literature in the Prior Work subsection (within the \u201cPrincipled Methods\u201d paragraph), comparing them with our work and highlighting our novelty.\n\nPoint 5 -- Both BinaryConnect and our ProxQuant are initialized at pre-trained full-precision nets, which are trained with 200 epochs over CIFAR-10.\n\nFor quantization, our schedule is essentially 400 epochs training, and the additional 200 epochs after hard quantization is mostly for fine-tuning the BatchNorm layers. Such fine-tuning was found very useful for *both ProxQuant and BinaryConnect*. Indeed, for BinaryConnect, the signed net keeps changing (in a tiny proportion) even at epoch 400, and the BatchNorm layer hesitates around without being optimized towards any fixed binary net. Hard quantizing forces BinaryConnect to stay at a specific binary net, after which the BatchNorm layer can approach this optimal and boosts performance.\n\nWe have modified Section 4.1 to clarify this.\n\nTheoretical Results\n\nPoint 8 -- Li et al.\u2019s convergence bound involves an additive error O(\\Delta) that does not vanish over iterations, where \\Delta is the grid size for quantization. Hence, their result is only useful when \\Delta is small. In contrast, we consider the original BinaryConnect with \\Delta = 1, in which case the error makes Li et al.\u2019s bound vacuous.\n\nWe have added a remark after Theorem 5.1 to clarify that.\n\nPoint 9 -- We have rephrased the last two paragraphs in Section 5.2 a bit, to first state our finding and then analyze why it shows the power of ProxQuant over BinaryConnect.\n\nPoint 10 -- We have added a convergence guarantee for ProxQuant in Appendix D, showing that ProxQuant converges to a stationary point of the regularized loss.\n\nPresentation\n\nPoint 2 -- We have added that we are also using the non-lazy prox to highlight our difference from BinaryConnect.\n\nPoint 3 -- The Eq (4) was just an expanded formula for the prox-gradient method. As it did not really mean to say anything and the prox operator has been already defined, we have removed it for clarity. \n\nPoint 11 -- We would indeed like to shorten the paper. We will do that once we have a better idea of the potential additional materials that we would present. Please stay tuned.\n\nAdditional Experiments\n\nPoint 4, 6, 7 -- We will work on some additional experiments to address these points. Please stay tuned and we will let you know once it\u2019s done. \n\nFor Point 6 -- The baseline classification error of Adam + BinaryConnect on ResNet56 in Li et. al  is 8.10%, whereas we already achieve a better error 7.79% on ResNet44. We suspect this is due to the difference in the initializing FP net."}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "rygyZPnvpX", "original": null, "number": 5, "cdate": 1542076151081, "ddate": null, "tcdate": 1542076151081, "tmdate": 1542076151081, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "rJe9XiOgq7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Added some references in the revision", "comment": "We have added Yin et al., as well as a couple of other relevant literature, into our related work section."}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "SJxojtYwpm", "original": null, "number": 4, "cdate": 1542064547006, "ddate": null, "tcdate": 1542064547006, "tmdate": 1542064547006, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HJeb-tr93Q", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you very much for the valuable feedback!"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "HJgzFtYvaX", "original": null, "number": 3, "cdate": 1542064506463, "ddate": null, "tcdate": 1542064506463, "tmdate": 1542064506463, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "S1eLOUDJn7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the valuable feedback! We have made a revision to the paper to address all the comments. We will respond to the specific questions in the following.\n\nNovelty --- We agree that there has been a large literature on replacing the straight-through estimator with prox-type algorithms. Our novelty comes in two aspects:\n\n(1) The proposal of combining non-lazy proximal gradient method with a finite (soft) regularization, as well as principled methods for quantizing to binary, ternary, and multi-bit.\n\n(2) A new challenge to the straight-through gradient estimate in its optimization instability through systematic theoretical and empirical investigations. In particular, we show that the convergence criterion of BinaryConnect is very stringent (Theorem 5.1), while our proposed ProxQuant is guaranteed to converge on smooth problems Theorem D.1). Our sign change experiment in Section 5.2 further shows that BinaryConnect is indeed highly unstable in its optimization, as well as giving a lower-performance solution, compared with ProxQuant.\n\nWe have updated the related work section (in particular the \u201cPrincipled methods\u201d part) to include these citations.\n\nImageNet experiments --- Due to time constraints, we didn\u2019t have time to perform ImageNet experiments for this submission. We have experimental results on LSTMs (Section 4.2) to be complementary with the CIFAR-10 results. Performing ImageNet experiments will be of our interest as a future direction.\n\nExperiments with \\lambda_t --- We have thought about that, but we chose to use the linear scheme \\lambda_t = \\lambda * t for simplicity and to demonstrate that a simple choice would work well. We suspect that changing the schemes would not boost the performance by a great deal -- but we would like to test it experimentally. Please stay tuned and we would potentially add that in our next revision. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "HJeb-tr93Q", "original": null, "number": 3, "cdate": 1541196025383, "ddate": null, "tcdate": 1541196025383, "tmdate": 1541533539757, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "content": {"title": "An interesting addition to the (large) literature on methods to learn deep networks with quantized weights.", "review": "This paper proposes a new approach to learning quantized deep neural networks, which overcome some of the drawbacks of previous methods, namely the lack of understanding of why straight-through gradient works and its optimization instability. The core of the proposal is the use of quantization-encouraging regularization, and the derivation of the corresponding proximity operators. Building on that core, the rest of the approach is reasonably standard, based on stochastic proximal gradient descent, with a homotopy scheme.\n\nThe experiments on benchmark datasets provide clear evidence that the proposed method doesn't suffer from the drawbacks of  straight-through gradient, does contributing to the state-of-the-art of this class of methods.\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper966/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Review", "cdate": 1542234336480, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335843274, "tmdate": 1552335843274, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJe9XiOgq7", "original": null, "number": 1, "cdate": 1538456353900, "ddate": null, "tcdate": 1538456353900, "tmdate": 1538456353900, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HJ7xolRtX", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "content": {"title": "Will cite & Differences", "comment": "Thanks for bringing the work by Yin et al. to our attention. We were not aware of this paper and did our work independently. We will carefully address this work in our next revision.\n\nWe would like to take this opportunity to point out several major differences between our work and Yin et al.:\n\n(1) While we both arrived at the observation that BinaryConnect has a simple expression (our Eq (1) and Yin et al.\u2019s Eq (12)), Yin et al. did not point out this is exactly the dual-averaging algorithm or the lazy-projected gradient descent with constraint set {-1, 1}^d, which dates back to at least Nesterov (for the convex case):\n\n- Nesterov, Y. (2009). Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1), 221-259.\n\n(2) Our algorithm is in fact *different* from Yin et al.: they used the lazy proximal gradient descent (Eq (10), Yin et al.), whereas we used the standard non-lazy proximal gradient descent (our Eq (5)), which is one step further different from the straight-through gradient method.\n\n(3) We proposed and experimented with (1) non-smooth L1-like regularizers for binary quantization; (2) multi-bit quantization with adaptive levels, both not covered in Yin et al..\n\n(4) Our theoretical insights on BinaryConnect (Figure 1 and Section 5) are novel, and in stark contrast with Yin et al.. Our Theorem 5.1 shows that the actual convergence criterion of BinaryConnect is very stringent. We provide a simple 1-d example of such non-convergence in Figure 1.\n\nOur further experimental evidence (Section 5.2) shows that BinaryConnect indeed fails to converge on CIFAR-10 in every run, demonstrating that the condition in Yin et al.\u2019s convergence theorem are quite unlikely to hold in practice. "}, "signatures": ["ICLR.cc/2019/Conference/Paper966/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612077, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyzMyhCcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper966/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper966/Authors|ICLR.cc/2019/Conference/Paper966/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612077}}}, {"id": "HJ7xolRtX", "original": null, "number": 1, "cdate": 1538292458515, "ddate": null, "tcdate": 1538292458515, "tmdate": 1538346143311, "tddate": null, "forum": "HyzMyhCcK7", "replyto": "HyzMyhCcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper966/Public_Comment", "content": {"comment": "The authors should have cited the paper by Yin et al., first appeared on arXiv  in Jan 2018: https://arxiv.org/pdf/1801.06313.pdf\n\n1. In section 3.1,  the authors propose to replace the hard constraint that imposes the quantization of weights with, for example, a quadratic penalty/regularizer.  The formula of the proximal operator for the quadratic regularizer is derived, which is a weighted average between the weights to be quantized and and the quantized weights as shown in items (1)&(2) below Eq. (11) on page 6.  These contributions are the same as those in section 2.3 of the earlier paper by Yin et al.. Proposition 2.3 in Yin et al.'s paper provided essentially the same proximal operator formula. \n\n2. The authors observe that BinaryConnect iteration can be nicely expressed by Eq. (1) on page 4. The original BinaryConnect paper did not present it explicitly in this way. Their observation of Eq. (1) is basically the same as Eq. (12) on page 9 in Yin et al.'s paper. \n", "title": "A highly related paper not cited"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper966/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works.\nBuilding upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.", "keywords": ["Model quantization", "Optimization", "Regularization"], "authorids": ["yub@stanford.edu", "yuxiangw@cs.ucsb.edu", "libertye@amazon.com"], "authors": ["Yu Bai", "Yu-Xiang Wang", "Edo Liberty"], "TL;DR": "A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.", "pdf": "/pdf/68b0003a578aa8a76e43de251a471b2513292df8.pdf", "paperhash": "bai|proxquant_quantized_neural_networks_via_proximal_operators", "_bibtex": "@inproceedings{\nbai2018proxquant,\ntitle={ProxQuant: Quantized Neural Networks via Proximal Operators},\nauthor={Yu Bai and Yu-Xiang Wang and Edo Liberty},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyzMyhCcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper966/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311710469, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyzMyhCcK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper966/Authors", "ICLR.cc/2019/Conference/Paper966/Reviewers", "ICLR.cc/2019/Conference/Paper966/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311710469}}}], "count": 16}