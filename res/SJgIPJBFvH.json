{"notes": [{"id": "SJgIPJBFvH", "original": "r1ltVhTdvH", "number": 1767, "cdate": 1569439581847, "ddate": null, "tcdate": 1569439581847, "tmdate": 1583912034396, "tddate": null, "forum": "SJgIPJBFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "8EatWJ_2_U", "original": null, "number": 1, "cdate": 1576798732016, "ddate": null, "tcdate": 1576798732016, "tmdate": 1576800904420, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718972, "tmdate": 1576800269545, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Decision"}}}, {"id": "ryeab8ZKjS", "original": null, "number": 4, "cdate": 1573619205489, "ddate": null, "tcdate": 1573619205489, "tmdate": 1573629982941, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJewlC1jFr", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment", "content": {"title": "Author Response to Review #1", "comment": "Thank you for your encouraging review! We next address your questions:\n\n    1) The stochasticity due to initialization and optimization: We have added this discussion to the revision. We ran all experiments 5 times and calculated the standard deviation (table 8&9). The resulting standard deviation shows that our result is statistically significant. Please see the paper for more discussion.\n\n    2) All results are reported on CIFAR10: Have now repeated all experiments on Street View House Numbers (SVHN) dataset and added the result to the revision  (table 6). This result is consistent with that of CIFAR10 which shows that our conclusions are not limited to CIFAR10. In order to improve the strength of the paper even further, we are currently running similar experiments on STL10 and CIFAR100 which will be added to the final version.\n\n    3) Making make P & Q have the same variance equation 43 in App. C: Similar to Neyshabur et al, we mainly do this since it makes the derivation simpler and the final bound more interpretable. That is however not the optimal choice. \n\n    4) Relationship between sharpness and grad noise: You are right that these quantities seem very related. We indeed believe that this relationship is very interesting and can be a subject of future study! \n\nFinally, thank you for the minor suggestions. We have incorporated all of them in the revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1767/Authors|ICLR.cc/2020/Conference/Paper1767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151183, "tmdate": 1576860547573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment"}}}, {"id": "S1xtJSZtjH", "original": null, "number": 2, "cdate": 1573618913023, "ddate": null, "tcdate": 1573618913023, "tmdate": 1573629749511, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment", "content": {"title": "Revision Summary", "comment": "We thank all reviewers for their valuable feedback. We have updated the paper to include new experiments and revisions suggested.\n    1. We repeated all the experiments in the paper on SVHN and show that the result is consistent with what we already observed on CIFAR-10 (Table 6).\n    2. We repeated all the experiments on CIFAR-10 5 times and show that the observation is robust to stochasticity of initialization and optimization (mean in Table 8 and std in Table 9). \n    3. We compute the same statistics for CIFAR-10 at cross-entropy=0.1 and the performance of the majority of the measures (most notably spectral and sharpness) are consistent with cross-entropy=0.01. This shows that we are indeed capturing the generalization gap rather than test error (Table 7).\n    4. We have added more analysis in the appendix B using conditional mutual information. The results are consistent with the ones reported in the main paper.\n    5. Improved writing including related work, notation etc and incorporated all the suggestions made by reviewer 1&2.\nMerged related work and subsection into subsections of section 1\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1767/Authors|ICLR.cc/2020/Conference/Paper1767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151183, "tmdate": 1576860547573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment"}}}, {"id": "HkeGTU-KoH", "original": null, "number": 5, "cdate": 1573619385636, "ddate": null, "tcdate": 1573619385636, "tmdate": 1573629722194, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "ByxTlM6e5H", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment", "content": {"title": "Author Response to Review #3", "comment": "Thank you for your thoughtful feedback. We have added many more experiments and addressed all of your concerns. We hope that after looking at the revision and reading our response, you would consider increasing your score. \n\nWe will next address your concerns in details below:\n\n    1) We have significantly improved the strength of our empirical results in the revision. More specifically, we repeated all experiments for Street View House Numbers (SVHN) dataset  (table 6). Furthermore, we ran all experiments on CIFAR-10 5 times and calculated the standard deviation over different runs. Both these results are indeed consistent with the observations we made on CIFAR-10. Note that this means training over 10K networks! Please see the revision for details. Even though we believe current experimental results are more than enough to verify the conclusions in the paper, we are currently running additional experiments on CIFAR100 and STL10 datasets to make the experiments even more comprehensive. Given the scale of the experiments, these results would be available for the final version.\n\n    2) When studying generalization, we follow the common practice in the field which mainly focuses on over-parameterized regime where the model capacity is high enough to fit the training data. In this regime, the test error and generalization gap become the same. We believe this regime is more interesting as in the under-parametrized regime the test error can be usually improved by using better optimization algorithms and hence generalization is less mysterious. We agree with you that observations in over-parameterized regime might not generalize to the under-parametrized regime. However, in order to address your concern about test error vs generalization, we included the results of CIFAR-10 at cross-entropy=0.1 (table 7) in the revision where the training errors are much higher and the observations are largely consistent with cross-entropy=0.01, indicating that we are indeed capturing the generalization gap rather than test accuracy.\n\n    3) We believe there might be a confusion here. What we stated is that only changing a single hyperparameter and only looking at correlation when changing one single hyperparameter can be misleading. Instead, we do controlled experiments on seven hyperparameters, meaning that we change them one by one (and each time, we change one, the other six hyperparameters are kept fixed). The expected performance in the controlled experiments are much better indicator compared to only changing one hyperparameters or changing all of them together. You also asked why we didn\u2019t sample all hyperparameters together. We actually do report that as well in the table column called \u201coverall $\\tau$\u201d. Please see section 2.3 for discussions on why the controlled experiments are preferred to overall $\\tau$. Finally, we added even further analysis using conditional mutual information in the appendix B that is consistent with the rank correlation results reported in the main paper. \n\n    4) Not all these measures can be converted to a bound. Furthermore, a tighter bound does not necessarily correlate better with generalization. That is mainly because all current bounds are very loose so their tightness relative to each other is not informative. What we try to study is whether these measures can predict the observed generalization rather than bounding it. We chose convolutional neural networks because they are heavily used in both research and in industry. We agree with you that these results might not generalize to other types of networks such as recurrent neural networks and we have made this clear on the conclusion of the original submission. However, we believe the protocols we establish in this work will be useful for future research on other types of networks.\n\n    5) Canonical order is not a single measure but rather a number specific to the changes made to a single hyperparameter, which means that there are 7 canonical measures (please see Section 4 for definition). Hence, each canonical measure is only predictive when we change its corresponding hyperparameter and is uncorrelated with generalization when we change any other hyperparameter. Therefore, each of these measures perform poorly overall. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1767/Authors|ICLR.cc/2020/Conference/Paper1767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151183, "tmdate": 1576860547573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment"}}}, {"id": "ByxTYS-KsB", "original": null, "number": 3, "cdate": 1573619076897, "ddate": null, "tcdate": 1573619076897, "tmdate": 1573619076897, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "HJxvboKicr", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment", "content": {"title": "Author Response to Review #2", "comment": "Thank you for your encouraging and thoughtful comments! We added more details to section 6 and address your other concerns below:\n\n     1) Margin normalization: We agree that these measures should be normalized. However, we want to point out that there is an implicit normalization effect by the fact that we stop at a certain cross-entropy which is closely related to the margin when training error is zero or very small. Therefore, the margin values of all these networks are similar (but not the same). That being said, we have normalized all quantities that usually require normalization. If you look at the normalized versions of the measures, they do not behave significantly different than the unnormalized ones. The reason we do not Euclidean distance measure is that they usually shows up in bounds together with other norm-based terms and the right way to normalize it is not clear (again since the cross-entropy and margin values are almost the same, we don\u2019t think this makes a big difference).\n\n    2) Including sum-of-frob/margin: Thanks for pointing this out. We agree the quantity normalized by margin is important and have included it in table 2, and added corresponding discussion in the first paragraph of page 8.\n\n    3) Negative correlation of spectral bound: We agree that it is not surprising that spectral bound would be extremely large and it is not surprising if it doesn\u2019t correlate with generalization. However, we believe that the negative correlation is very surprising. Note that the negative correlation is constantly present in the controlled experiments when two model different only differ in learning rate, or batch size, or dropout, or optimization algorithm. Therefore, this negative correlation cannot be relevant to the model size since in all these cases, two models have the exact same architecture.\n\n    4) Sum-of-fro: We completely agree with you that the main term is fro/spec and we are surprised that it performed worse than sum-of-fro. This perhaps can be a subject of future study.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1767/Authors|ICLR.cc/2020/Conference/Paper1767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151183, "tmdate": 1576860547573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment"}}}, {"id": "SJewlC1jFr", "original": null, "number": 1, "cdate": 1571646958684, "ddate": null, "tcdate": 1571646958684, "tmdate": 1572972426184, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work all starts with the observation, made by many, that deeper (>=2 layers) networks generalise differently to the simple models we used to use, and that the good old bias-variance trade-off doesn't really apply.\n\nThis paper is a natural successor to a long line of papers, most recently Zhang at el (best paper at ICLR 2017, though some like Tom Dietterich weren't impressed), Neyshabur et al (NIPS 2017) and Jiang et al (ICLR 2019).  Whereas Jiang etal and Neyshabur etal had a few metrics, a few data sets and a few models, this paper instead has loads and loads of metrics and just one data set and one model class.  You have, however, been clearly strongly influenced by the prior work and attempted to fix what you saw as their empirical shortcomings.\n\nI agree with the choice of stopping according to cross entropy.  One thing not done in Section 5 is a discussion of\nvariation:  choice of initialisation and the impact of stochasticity in the optimisation. \n\nAppendix C lists your copious metrics.  I am not deeply embedded in the theory, though I understand the major concepts. I was impressed with the coverage and variations done.  I expect Appendix C may raise copius matching arguments amongst the theory community, with some valid and some invalid complaints.  For me, I wonder why you make P & Q have the same variance (see before equation 43 in App. C).  You credit Neyshabur, but I'm left wondering.\n\nSo, the short version of my review is that I believe this paper is imcomplete, in that not enough different data sets were investigated.  With this, I fear you could well be uncovering perculiarities pertinant to CIFAR-10 and the ease of getting near perfect training error on it.  However, I am impressed with what you have done, and think you made a great starting point.  So I say, publish and let the real battle begin.  Let the theoreticians argue (about metrics) and the practicioners implement (with more data), and let's see what happens.  You have laid a great ground work for folks, bulding on the earlier work.  Perhaps Google can be encouraged to support this in that Jiang's paper is a precursor.\n\nInteresting observations in Appendix A.1, as are some of the discussions in Sections 7, 8, 9.\n\nGiven the best metric seems to be sharpness, which exceeds PAC-Bayesian, shouldn't you relate this to grad noise (metrics 61 and 62) which measure the flatness.  Are these related?  Note, also, a Bayesian always wants broad peaks for their parameter surfaces.\n\nMINOR ISSUES: (1) some repeated words: \"an an\", \"the the\" (2) some strange grammar \"each canonical ordering that only be predictive for its own category\", \"VC-dimension bounds or and parameter counting\", \"possible to find to calculate\", \"in the Equation equation C.3 (3)\" Generalisation gap is poorly introduced.  Its officially defined in footnote 4 but used way earlier!  Put in the main text:  its an important clarification. (4) Table 1 legend should mention the \"numerical results\" are for equations 2, 4 and 5.  (5)  Somewhere you need to mention that numbers in red in tables refer to equations in appendix C.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574778810981, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Reviewers"], "noninvitees": [], "tcdate": 1570237732584, "tmdate": 1574778810996, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review"}}}, {"id": "ByxTlM6e5H", "original": null, "number": 2, "cdate": 1572028916670, "ddate": null, "tcdate": 1572028916670, "tmdate": 1572972426149, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper aims at providing a better understanding of generalization for Deep Learning models. The idea is really interesting for the ML community as, despite their broad use, the astounding property of deep neural networks to generalize that well is still not well understood.\nThe idea is not to show new theoretical bounds for generalization gaps but stress the results of an empirical study comparing the already existing measures. The authors choose 7 common hyperparameters related to optimization and analyze the correlation between the generalization  gaps effectively observed and the ones predicted by different measures (VC dim, cross-entropy, canonical ordering \u2026).\nThe writing of the paper is clear and easily understandable. Besides, I believe that the study is relevant to ICLR conference.\n\nHowever, I believe the level of the paper is marginally below the threshold of acceptance and therefore would recommend to reject it.\n\nThe paper is solely empirical but I believe that the empirical section is a bit weak, or at least some important points remain unclear. If I appreciate the extent efforts made in trying to evaluate different measures of generalization gaps, I do not believe that the findings are conclusive enough.\n\n1) First, all this empirical result are based on one-dataset (CIFAR-10) only thus limiting the impact of the study. Indeed, a given measure might very correlate with generalization gap on this specific dataset but not on others.\n\n2) Specifically, we see that on this specific dataset, all training accuracies are already quite good (cf : Figure 1, distribution of the training losses). Consequently, authors are more correlating the chosen measures with the test error rather than with the generalization gaps. On other more complicated datasets where the training loss is higher, the VC dimension might consequently have way better results.\nSimilarly, in Section 6, the authors say that the results \u00abconfirm the widely known empirical observation that over-parametrization improves generalization in deep learning. \u00bb In this specific case, no reference was given to support the claim. I would agree with the claim \u00ab over-paramatrization improves test accuracy (reduces test error) \u00bb but the link between over-parametrization and generalization is less clear.  \n\n3) In Section 4, the authors say \u00ab drawing conclusion from changing one or two hyper-parameters \u00bb can be a pitfall as \u00ab the hyper-parameter could be the true cause of both change in the measure and change in the generalization \u00bb. I totally agree with the authors here. Consequently, I do not understand why the correlations were measured by only changing one hyper-parameter at a time instead of sampling randomly in Theta.\n\n4) It is still not clear to me how the authors explain why some measures are more correlated with generalization gaps than others. Are some bounds tighter than others ? This empirical study was only applied to convolutional neural networks and consequently one may wonder that for example the VC dim bounds computed in the specific case of neural networks are too loose. However, this measure could be efficient for type of models.\n\n\nI would like the authors to clear the following points :\n- How do you ensure that the empirical study clearly correlated measures predictions with generalization gaps and not simply with test errors (or accuracies) ? (point 2)\n- Could you please also answer Point 4 ?\n- Finally, how would you explain the fact that the canonical order performs so well compare to many other measures and that it is a really tough-to-beat baseline ?\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574778810981, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Reviewers"], "noninvitees": [], "tcdate": 1570237732584, "tmdate": 1574778810996, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review"}}}, {"id": "HJxvboKicr", "original": null, "number": 3, "cdate": 1572735743191, "ddate": null, "tcdate": 1572735743191, "tmdate": 1572972426108, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper addresses the well-known generalization problem for deep networks: why do these models successfully generalize even though they contain more parameters than the size of the training set? Recent works on this problem have proposed various measures of model complexity (that is, different from the number of parameters) with the claim that they (at least partially) capture the \"true\" capacity of deep networks in order to answer this question. The authors report the results of a large-scale, systematic study of the empirical effect of these complexity measures on the observed generalization behavior of deep networks.\n\nThe empirical study of generalization behavior is inherently tricky, and the authors make a non-trivial contribution just by designing an experiment that can convincingly address this question. The authors give careful attention to various potential obstacles related to experimental design and statistical validity. \n\nThe results of the authors' experiments are highly valuable to the research community. I recommend to accept.\n\n*********************************************************************************\n\nTechnical comments for the authors: \n-Please rewrite or elaborate on the first two paragraphs of section 6 for clarity.\n\n-It seems to make little sense to consider norm-based complexity measures that are not margin-normalized (or at least normalized in some other way). Please include a margin-normalized version of each such measure in Table 5; certain key quantities (e.g. \"frob-distance\") have no corresponding margin-normalized entry.\n\n-The quantity \"sum-of-frob/margin\" is certainly a quantity of interest and it should be included in table 2 and possibly merits some discussion.\n\n-I disagree somewhat with the discussion of the spectral bound in section 7. In particular, I would not agree that the poor performance of \"spec-orig\" is surprising, because it contains an astronomically-sized \"proof artifact\" (as you call it). Namely, in this bound and in similar norm-based bounds, the term which is the product of the matrices in the network is (heuristically) unnecessary; in particular it (heuristically) ought to be able to be replaced with the network's \"average-case Lipschitz constant\" (this is one of the issues which Arora et al. 2018 attempted to address; unfortunately formalizing an adequate notion of \"average-case Lipschitz constant\" can be quite cumbersome). This superfluous product of matrix norms is of course highly correlated with model size, so it is not surprising that the quantity would have a sizable negative correlation with generalization. The quantity I would consider to be the \"main term\" is the quantity \"fro/spec\". Furthermore, the fact that this quantity (which appears in both Bartlett et al. and Arora et al.) performs somewhat worse than simply \"sum-of-fro\" is fairly interesting and possibly merits some discussion.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574778810981, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Reviewers"], "noninvitees": [], "tcdate": 1570237732584, "tmdate": 1574778810996, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Review"}}}, {"id": "H1eyz2M2DH", "original": null, "number": 1, "cdate": 1569627143182, "ddate": null, "tcdate": 1569627143182, "tmdate": 1569627530559, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "rJxvnPrqwS", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment", "content": {"comment": "Thank you so much for reading our paper and posting your feedback (on the second day after submission!) We are glad that you liked our paper! \n\nWe agree with your comment that in many applications we are interested in the test error rather than he generalization gap. As you pointed out yourself, we chose a setting where training error is almost zero so that these two coincide. This choice was indeed intentional to avoid choosing between test error and the generalization gap. A very interesting observation that we had during our experiments was that we realized if we do not control for the training accuracy, then by simply looking at the training loss one could easily predict which model will generalize better: while this is not always true, in our case we observed that at convergence models with higher loss generalize better. That made us choose training performance as a stopping criterion and look at the generalization gap. Another potential choice was to stop at a higher training error say 0.1 in which case the test error and generalization error only differ by 0.1 for all models, instead of 0.01. One issue with that choice is that there is not enough variation in the generalization of different models at that training accuracy as the models are not close to convergence. Another issue is that since the training error improves with a fast rate at each update around 0.1, it is almost impossible to stop the optimization at that exact stopping criterion. Also, one could argue that at training error 0.1, the main bottleneck to improve the test error is to fit training data better and even though this is important, it is not the focus of this study. All these difficulties led us to believe that the current setting is the best choice. We will add more discussions around this issue in next update. \n\nOnce again, thank you very much for reading the paper and catching the typos, and for the encouraging comment!\n", "title": "Thank you for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1767/Authors|ICLR.cc/2020/Conference/Paper1767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151183, "tmdate": 1576860547573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Official_Comment"}}}, {"id": "rJxvnPrqwS", "original": null, "number": 1, "cdate": 1569507247180, "ddate": null, "tcdate": 1569507247180, "tmdate": 1569507247180, "tddate": null, "forum": "SJgIPJBFvH", "replyto": "SJgIPJBFvH", "invitation": "ICLR.cc/2020/Conference/Paper1767/-/Public_Comment", "content": {"title": "Whether generalization gap is what we should be looking at", "comment": "Thank you for the paper. I enjoyed reading it. I wanted to know your opinion on the following point.\n\nYou define the generalization gap as the difference between the train and test accuracy and this gap plays the central role in the whole empirical study (i.e. in conclusions regarding which hyper-parameter values are beneficial and which are not). In short, the paper assumes that the smaller this gap, the better. I would like to argue that what is more important (and what the paper actually looks at) is the test accuracy, not the generalization gap. Notice that by design all the models in the study achieved very small training loss (high training accuracy). According to Figure 2 in appendix, majority of the models achieved training misclassification error of less than 1%. This means, generalization gap takes the form of $acc_train - acc_test$, where $acc_train$ takes values in [0.99, 1]. In that context saying \"small generalization gap\" is equivalent to saying \"large test accuracy\".\n\nI am pointing this out because in many other contexts these two statements (\"small generalization gap\" / \"large test accuracy\") may actually have two different meanings. Indeed, it seems that in most of the interesting applications DNNs manage to achieve the perfect training accuracy and thus the argument above holds (i.e. the two statements are equivalent). But I feel, what we are really looking for are the cases when we achieve a good test accuracy. For DNNs it just happens so that in those cases the generalization gap is very small. But small generalization gap obviously does not imply that the training succeeded: think of cases where the hyper-parameters are completely off and you end up with accuracy at random level both at training and test: the generalization gap is zero, but the training failed and there is no generalization. \n\nThat being said, your experimental design (looking only at models with high training accuracy) resolves this problem and indeed makes you focus on the good test performance. Nevertheless, I feel that making this point explicit (or even switching from \"generalization gap\" to \"test loss\" in your discussions) could be very useful in helping future follow up works (which I am sure will arrive sooner or later) to focus on important questions / numbers. \n\nSmall typos:\nPage 5: \"this this\", \"to choose ensure\" \nPage 8: sentence starting with \"We next look at...\". \nPage 8 \"to find to calculate\""}, "signatures": ["~Ilya_Tolstikhin1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ilya_Tolstikhin1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ydjiang@google.com", "neyshabur@google.com", "dilipkay@google.com", "hmobahi@google.com", "bengio@google.com"], "title": "Fantastic Generalization Measures and Where to Find Them", "authors": ["Yiding Jiang*", "Behnam Neyshabur*", "Hossein Mobahi", "Dilip Krishnan", "Samy Bengio"], "pdf": "/pdf/c915c7896213fa5b3abc239ba4c4a5a800b404ef.pdf", "TL;DR": "We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on measures in our study.", "abstract": "Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "code": "https://drive.google.com/open?id=1_6oUG94d0C3x7x2Vd935a2QqY-OaAWAM", "keywords": ["Generalization", "correlation", "experiments"], "paperhash": "jiang|fantastic_generalization_measures_and_where_to_find_them", "_bibtex": "@inproceedings{\nJiang*2020Fantastic,\ntitle={Fantastic Generalization Measures and Where to Find Them},\nauthor={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgIPJBFvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/241a2093b80b7b153c2866218d3014f93fc23b7d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgIPJBFvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504190077, "tmdate": 1576860580917, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1767/Authors", "ICLR.cc/2020/Conference/Paper1767/Reviewers", "ICLR.cc/2020/Conference/Paper1767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1767/-/Public_Comment"}}}], "count": 11}