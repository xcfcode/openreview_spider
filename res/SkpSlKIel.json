{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488559657979, "tcdate": 1478033476637, "number": 31, "id": "SkpSlKIel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkpSlKIel", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396316444, "tcdate": 1486396316444, "number": 1, "id": "r1VjjzU_l", "invitation": "ICLR.cc/2017/conference/-/paper31/acceptance", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.\n \n The reviewers recommend acceptance of the paper and I am happy to go with their recommendation.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396316930, "id": "ICLR.cc/2017/conference/-/paper31/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396316930}}}, {"tddate": null, "tmdate": 1482363057059, "tcdate": 1482363057059, "number": 5, "id": "S1Yhgqd4l", "invitation": "ICLR.cc/2017/conference/-/paper31/public/comment", "forum": "SkpSlKIel", "replyto": "B1hcwxLVl", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["~Shiyu_Liang1"], "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments.  We would also like to point out that the upper bound in our paper holds for a larger class of functions than strongly convex functions. For example, piecewise linear functions are one class of examples which are not strongly convex, but for which our results hold. The lower bound is indeed for strongly convex functions.\n\nWe have now cited Montufar et al (2014) in our paper. Thank you very much for bringing this paper to our attention. We have also addressed your other comments in the revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287757494, "id": "ICLR.cc/2017/conference/-/paper31/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkpSlKIel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper31/reviewers", "ICLR.cc/2017/conference/paper31/areachairs"], "cdate": 1485287757494}}}, {"tddate": null, "tmdate": 1482192787916, "tcdate": 1482192787916, "number": 3, "id": "B1hcwxLVl", "invitation": "ICLR.cc/2017/conference/-/paper31/official/review", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/conference/paper31/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper31/AnonReviewer3"], "content": {"title": "review of ``WHY DEEP NEURAL NETWORKS FOR FUNCTION APPROXIMATION?'' ", "rating": "7: Good paper, accept", "review": "SUMMARY \nThis paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. \n\nPROS \nThe paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. \n\nCONS\nThe main result appears to address only strongly convex univariate functions. \n\nSPECIFIC COMMENTS \n\n- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. \nI still have not read the revision and maybe you have already addressed some of these points there. \n\n- The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. \n\n- In Lemma 3, there is an i that should be x\n\n- In Theorem 4, ``\\tilde f'' is missing the (x). \n\n- Theorem 11, the lower bound always increases with L ? \n\n- In Theorem 11, \\bf x\\in [0,1]^d? \n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721974, "id": "ICLR.cc/2017/conference/-/paper31/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper31/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper31/AnonReviewer2", "ICLR.cc/2017/conference/paper31/AnonReviewer1", "ICLR.cc/2017/conference/paper31/AnonReviewer3"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721974}}}, {"tddate": null, "tmdate": 1481943485368, "tcdate": 1481943485368, "number": 4, "id": "ByHTFmfEx", "invitation": "ICLR.cc/2017/conference/-/paper31/public/comment", "forum": "SkpSlKIel", "replyto": "rkrLAnRXl", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["~Shiyu_Liang1"], "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287757494, "id": "ICLR.cc/2017/conference/-/paper31/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkpSlKIel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper31/reviewers", "ICLR.cc/2017/conference/paper31/areachairs"], "cdate": 1485287757494}}}, {"tddate": null, "tmdate": 1481943440573, "tcdate": 1481943440573, "number": 3, "id": "SJFqYXfNg", "invitation": "ICLR.cc/2017/conference/-/paper31/public/comment", "forum": "SkpSlKIel", "replyto": "Syll93lVg", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["~Shiyu_Liang1"], "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments and suggestions.\n\n1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?\n\n(1) The result can be extended to use only ReLUs, but the approximation error will then have to be measured in the L_1 sense rather than the L_\\infty sense that we have used in the paper.\n\n2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?\n\n(2) Indeed our lower bound provides such an example: the function f(x)=x^2 is one such example.\n\nAgain, thanks for your comments and suggestions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287757494, "id": "ICLR.cc/2017/conference/-/paper31/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkpSlKIel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper31/reviewers", "ICLR.cc/2017/conference/paper31/areachairs"], "cdate": 1485287757494}}}, {"tddate": null, "tmdate": 1481849320403, "tcdate": 1481849320403, "number": 2, "id": "Syll93lVg", "invitation": "ICLR.cc/2017/conference/-/paper31/official/review", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/conference/paper31/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper31/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:\n\n1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?\n\n2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721974, "id": "ICLR.cc/2017/conference/-/paper31/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper31/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper31/AnonReviewer2", "ICLR.cc/2017/conference/paper31/AnonReviewer1", "ICLR.cc/2017/conference/paper31/AnonReviewer3"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721974}}}, {"tddate": null, "tmdate": 1481719373096, "tcdate": 1481719373089, "number": 1, "id": "rkrLAnRXl", "invitation": "ICLR.cc/2017/conference/-/paper31/official/review", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/conference/paper31/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper31/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper shows:\n\n  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.\n  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.\n  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.\n\nThe paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721974, "id": "ICLR.cc/2017/conference/-/paper31/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper31/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper31/AnonReviewer2", "ICLR.cc/2017/conference/paper31/AnonReviewer1", "ICLR.cc/2017/conference/paper31/AnonReviewer3"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721974}}}, {"tddate": null, "tmdate": 1481074396809, "tcdate": 1481074396804, "number": 2, "id": "BJHJPJHmx", "invitation": "ICLR.cc/2017/conference/-/paper31/public/comment", "forum": "SkpSlKIel", "replyto": "B1ai7oyme", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["~Shiyu_Liang1"], "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments. We have not directly tried to relate our results to the VC dimension or Rademacher complexity of neural networks.  We were aware of the work on the VC dimension of neural networks, but we were not aware of the paper that you have cited. Thank you again for bringing this paper to our attention. \n\nIt is indeed true that the VC dimension and Rademacher complexity increase with the depth of a neural network. Our results do not advocate the use of neural networks with arbitrarily large depth. We show that using O(polylog(1/epsilon)) layers minimizes the number of neurons required to achieve an approximation error of epsilon. Your question on how this relates to complexity measures is very interesting. The generalization error is upper bounded by the sum of the empirical risk and a term that depends on the complexity measure.  We believe our results are more relevant to the first term, but it is ongoing work to make this more precise. The results in our paper (and earlier papers we have cited) indicate that the piecewise linear approximation that a neural network provides depends on the number of neurons per layer and the number of layers. Increasing the number of layers increases the number of linear pieces that we can work with, although we haven't characterized precisely how this affects the empirical risk.  \n\nWe are not sure if this response addresses your comment on the magnitude of the weights, we may not be fully understanding this part of your comment. Thanks.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287757494, "id": "ICLR.cc/2017/conference/-/paper31/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkpSlKIel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper31/reviewers", "ICLR.cc/2017/conference/paper31/areachairs"], "cdate": 1485287757494}}}, {"tddate": null, "tmdate": 1480797258784, "tcdate": 1480797203564, "number": 1, "id": "Sk2M3oemx", "invitation": "ICLR.cc/2017/conference/-/paper31/public/comment", "forum": "SkpSlKIel", "replyto": "H1RXHSyXg", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["~Shiyu_Liang1"], "content": {"title": "Thanks for the comments", "comment": "Thanks for the comments. Regarding Corollary 12, we have added a short proof now in the appendix, which shows why we can use far fewer neurons in deep networks compared to shallow networks.\n\nWe have also added a short discussion at the end of the proof of Theorem 11, which shows that the lower bound is Omega((1/epsilon)^(1/L)) for shallow networks with a fixed depth L, and Omega(log(1/\\epsilon)) for deep networks. Note that L is a parameter we can choose. For shallow networks, by definition, L is fixed independent of epsilon, whereas by optimally choosing L to be a function of epsilon (specifically L is Theta(log (1/epsilon)), we get the Omega(log(1/\\epsilon)) result. In particular, one should not interpret the lower bound as increasing in L since L is not optimized in the statement of the theorem. Without optimizing L, the number of neurons required will indeed increase in L since we need at least one neuron in each layer by the definition of an L-layer network.\n\nAgain, thanks for the comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287757494, "id": "ICLR.cc/2017/conference/-/paper31/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkpSlKIel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper31/reviewers", "ICLR.cc/2017/conference/paper31/areachairs"], "cdate": 1485287757494}}}, {"tddate": null, "tmdate": 1480729509242, "tcdate": 1480729509237, "number": 2, "id": "B1ai7oyme", "invitation": "ICLR.cc/2017/conference/-/paper31/pre-review/question", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/conference/paper31/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper31/AnonReviewer1"], "content": {"title": "Connections to Rademacher Complexity", "question": "This results are interesting. How do you connect this with the results in \"Norm-Based Capacity Control in Neural Networks\" proving that the Rademacher complexity of neural nets with ReLU activations grows exponentially with the depth? Also, this shows that deep networks with a few hidden units can present an extremely large class of functions which is preferable for approximation but not preferable for learning. Why does the fact that complexity is hidden in the magnitude of weights rather than number of parameters show that deep networks are preferred to shallow networks with many hidden units?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959501590, "id": "ICLR.cc/2017/conference/-/paper31/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper31/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper31/AnonReviewer3", "ICLR.cc/2017/conference/paper31/AnonReviewer1"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959501590}}}, {"tddate": null, "tmdate": 1480705318552, "tcdate": 1480705318545, "number": 1, "id": "H1RXHSyXg", "invitation": "ICLR.cc/2017/conference/-/paper31/pre-review/question", "forum": "SkpSlKIel", "replyto": "SkpSlKIel", "signatures": ["ICLR.cc/2017/conference/paper31/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper31/AnonReviewer3"], "content": {"title": "Conclusion of the paper", "question": "Section 5 claims that exponentially more units are needed when using more shallow networks, which seems to be referring to Corollary 12, but precisely this statement is given without a proof and only pointing to Theorem 11. \nIs the conclusion of the paper referring to some statement other than Corollary 12? \nWhat is the proof of Corollary 12 and how is the lower bound from Theorem 11, which increases with L, resolved? \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.\n", "pdf": "/pdf/0f0f440312a1984ed6e87368345d2a9e4ed36a02.pdf", "paperhash": "liang|why_deep_neural_networks_for_function_approximation", "conflicts": ["illinois.edu"], "authorids": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "keywords": [], "authors": ["Shiyu Liang", "R. Srikant"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959501590, "id": "ICLR.cc/2017/conference/-/paper31/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper31/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper31/AnonReviewer3", "ICLR.cc/2017/conference/paper31/AnonReviewer1"], "reply": {"forum": "SkpSlKIel", "replyto": "SkpSlKIel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper31/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959501590}}}], "count": 12}