{"notes": [{"id": "YTyHkF4P03w", "original": "RYRRsbvEzjJ", "number": 3234, "cdate": 1601308359358, "ddate": null, "tcdate": 1601308359358, "tmdate": 1614985647002, "tddate": null, "forum": "YTyHkF4P03w", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sCLrkQPh2LY", "original": null, "number": 1, "cdate": 1610040515371, "ddate": null, "tcdate": 1610040515371, "tmdate": 1610474123556, "tddate": null, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "invitation": "ICLR.cc/2021/Conference/Paper3234/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work appears to be a promising start to a research direction. However, as the reviewers noted, the work does not compare to alternative approaches and the presentation of the work overall is incomplete."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "tags": [], "invitation": {"reply": {"forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040515359, "tmdate": 1610474123539, "id": "ICLR.cc/2021/Conference/Paper3234/-/Decision"}}}, {"id": "3tVtAYTFUYu", "original": null, "number": 4, "cdate": 1604289731219, "ddate": null, "tcdate": 1604289731219, "tmdate": 1606799444931, "tddate": null, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "invitation": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review", "content": {"title": "interesting ideas, but the paper is incomplete and the experiments are insufficient", "review": "# No Rebuttal\n\nSince there is no rebuttal, I have not modified my score.\n\n# Overall\n\nThis paper presents some interesting ideas. In particular, this is the first time I have seen genetic algorithms used in the context of pruning, and I encourage the authors to explore this concept further. However, the paper has some severe weaknesses:\n\n(1) The paper doesn't appear to be complete. The latex is a mess, with broken references, missing formulas, citations that aren't in parentheticals, math notation that is broken, etc. Key aspects of the methodology (as mentioned in the notes below) are missing. This paper was not in submittable condition, and the authors should probably have continued to work on the paper and submitted it to another conference.\n\n(2) The two methods studied in the paper have little to do with each other. It's unclear why they're both included in the same paper, and they should be studied separately. The dissipated gradients method has little justification itself, and it is simply declared.\n\n(3) The paper only studies MNIST and FashionMNIST, which are known to allow for many findings that do not scale to larger networks. I encourage the authors to extend their method to a larger-scale setting (e.g., ResNet-20 on CIFAR-10) to show that the results also work there.\n\n(4) The paper does not include any baselines, including other early pruning methods, making it impossible to evaluate whether these results are trivial or SOTA in any way. The authors cannot plead ignorance on this matter: they cite a paper (Frankle et al., 2020) in their abstract that explicitly surveys methods for pruning at initialization. In fact, one of the primary findings of Frankle et al. 2020 is that it's important to include baselines.\n\nI cannot recommend this paper for acceptance, and I suggest that the authors re-submit the paper at a time when it is in more complete shape.\n\n# Notes\n\n## Abstract\n\nThe citations in the abstract are garbled.\n\nThe abstract mentions two approaches but three are given.\n\nMNIST and FashionMNIST are not sufficient datasets to justify any pruning results. Many pruning results (e.g., lottery tickets) work on MNIST but not on larger-scale settings. The authors will need to include a larger-scale setting (e.g., CIFAR-10 and ResNet-20) to substantiate any results in this paper.\n\n## Intro\n\nThe authors should use the \"\\citep\" macro to ensure that citations are parenthetical rather than appearing in the middle of the text.\n\nPruning was generally claimed as a way to reduce overfitting in the 1980s and early 1990s, but it generally has not been used in this way in almost 30 years. Today, it is used solely to reduce costs (e.g., storage, running time, FLOPs, etc.).\n\nPruning is used on all sorts of networks, not just fully-connected networks.\n\nThe citations and sentence structure are garbled throughout this section.\n\n## Section 2\n\nI have no idea what Equation 2 means, but I assume you're keeping population members that lead to the highest accuracy before any training occurs?\n\nI assume that, once an individual makes it to the next generation, its weights are perturbed in some way to create new members of that generation? This doesn't seem to be explained in the text.\n\nWhat is the justification for dissipated gradients? Why this design and not other designs.\n\nDissipated gradients and the genetic algorithm (GA) approach are completely unrelated. Why are they in the same paper? Perhaps each should receive consideration in a separate paper.\n\nWhy combine these two approaches and not any two other approaches?\n\n## Section 5\n\nThe paper frequently uses the word \"Dropout\" when it means \"Pruning.\" Dropout is something different entirely.\n\n## Section 6:\n\nWhat does the magnitude heuristic mean? Why don't these fitness heuristics appear in Section 2?\n\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3234/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3234/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079572, "tmdate": 1606915804423, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3234/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review"}}}, {"id": "azrEDsYxfQe", "original": null, "number": 1, "cdate": 1602637390220, "ddate": null, "tcdate": 1602637390220, "tmdate": 1605024041040, "tddate": null, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "invitation": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review", "content": {"title": "I think this paper still needs more efforts, in terms of both experiments and paper writing. I give a clear rejection.", "review": "Overview:\n\nThis paper proposed two methods, K-starts and dissipating gradients approach to sparsity the network before training and achieve better performance than a random dropout.\n\n\nStrength bullets:\n1. The idea is interesting, but needs more effort to complete it.\n\n\nWeakness bullets:\n1. poor writing, for example, citation error in abstract line 3 //? exist in 2.1.2 line 2 and the last line of 3.1  // the explanation of sparsity is colloquial in 2.1.3 \n2. As for SparseMatrices in equation 1, does it mean the explanation of K? It's easy to confuse with a minus sign.\n3. because the classification on MNIST and FashionMNIST is too simple, the improvement of k-starts is marginal\n4. As dissipating gradient dropout needs training for a couple of epochs, it's unfair to compare with random dropout.\n5. The fatal limitation is the lack of comparison with previous methods and related works, need to compare with other pruning from scratch methods, like GraSP (https://arxiv.org/abs/2002.07376), SNIP (https://arxiv.org/abs/1810.02340).\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3234/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3234/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079572, "tmdate": 1606915804423, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3234/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review"}}}, {"id": "YrnPZmVbppV", "original": null, "number": 2, "cdate": 1603961703339, "ddate": null, "tcdate": 1603961703339, "tmdate": 1605024040965, "tddate": null, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "invitation": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review", "content": {"title": "Novel pruning techniques but limited in scope", "review": "Summary: This paper looks at two new pruning methods, k-starts and dissipating gradients, as well as a combination of both. They both perform better than random, and the combination performs the best.\n\nPros\n* K-starts and dissipating gradients are novel techniques to my knowledge.\n* It makes sense that weights with zero gradient over time can be considered unimportant (though this could mean that you should freeze the weight at its current value rather than prune it and set it to zero). It is also a promising idea to prune early based on gradient info from the early epochs.\n\nCons\n* The results of this paper are limited to MLPs MNIST and Fashion MNIST. Methods that work on these small datasets do not always generalize well to other datasets. MNIST with MLPs is a particularly special case for pruning because of input pixels that are always zero. I would expect to see at least CIFAR-10 and some convolutional networks.\n* While the techniques are compared to each other and to random pruning, there are no comparisons to other existing pruning methods.\n* Some unclear points in writing: pruning seems to be conflated with \u201cdropout\u201d, even though they are two very different techniques. The authors say they use autoencoders, but I believe they should be MLPs if the task is MNIST.\n* Figure 2 should have error bars. Is this test or train loss/accuracy?\n* Section 7.2: performance at different sparsity levels has been studied by many other papers (e.g. https://arxiv.org/pdf/2003.03033.pdf), and it\u2019s not a novel finding that accuracy can increase for some p, but large p causes performance to degrade.\n\nAdditional clarification questions:\n* 2.1.2: text description does not match equation 1, do you multiply the model weights with K and then subtract a matrix as in the equation, or multiply model weights with each of the K matrices as in the text?\n* What is the rationale behind the fitness function in 2.1.4? If you are optimizing for the sum of the individual\u2019s weights (assuming that\u2019s what ind[j] means), why not just optimize for that directly, without using multiple matrices and the evolutionary algorithm? Likewise, in algorithm 1, why not just prune by weight values directly?\n* Equation 3: it supposed to be a sum over the absolute values of the gradients? If not, weights with strong negative gradients should be treated the same as those with strong positive gradients.\n* Section 4: what are the details of \u201cstandard manner\u201d and what is \u201cbatch update\u201d?\n\nMinor: there are some latex errors, such as \u201ccite\u201d in the abstract, invalid equation references, and incorrect quote formatting. The abstract should probably be shorter. \n\nOverall: this paper poses two novel techniques for pruning, but do not evaluate them thoroughly on enough models and datasets and with comparisons to other pruning methods. Due to the limited scope, I do not recommend acceptance.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3234/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3234/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079572, "tmdate": 1606915804423, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3234/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review"}}}, {"id": "2nByxB4bQkz", "original": null, "number": 3, "cdate": 1604003866140, "ddate": null, "tcdate": 1604003866140, "tmdate": 1605024040895, "tddate": null, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "invitation": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review", "content": {"title": "A sketch of suggestions with no serious intention", "review": "Summary: The authors propose two approaches for pruning: (a) \"Evolution-style\": start with K random masks associated with the weights, update weights on gradient descent corresponding to those active in the \u201cfittest\u201d mask, and overtime throw away all but one masks which are less fit. (b) \"Dissipating-gradients\u201d: Here those weights are removed which are not being updated as much, measured by their sum of gradients over a number of iterations. This is shown for elementary networks on MNIST datasets without any serious experiments or comparisons or even presentation. \n\n\nPros:\n-It is quite possible that these nuggets of \u201cideas\u201d proposed in this paper could be useful, in some shape. But right now their form is *as rough as a draft can possibly get*. \n-Another thing which is mentioned rightfully in their paper is the \u201cLimitations\u201d section, of which are plenty.\n\nCons:\nEmpirical analysis\n-No serious experiments or comparisons to any baseline apart from random pruning. Even so, all networks used comprise of a few layers on MNIST & Fashion MNIST. \n-The performance of their proposed methods is basically on par with random pruning, sometimes even worse (Figure 2)!\n-Why do you sum over gradients? Doesn\u2019t it make more sense to sum the absolute value of gradients? (There could be a weight which is changing a lot, but at the time when you decided to prune, its sum of updates was very small.)\n-What happens in Figure 3a and 3b if you use all the samples from MNIST?\n-Given the extremely rough shape of the paper, I don\u2019t think if the other results should even be taken seriously!\n\nPresentation:\n-The authors keep saying post-training \u201cdropout\u201d, instead of post-training \u201cpruning\u201d. Dropout is just used as a verb in place of pruning. It\u2019s not that authors do not know that Dropout and they cite the corresponding paper. Yet, it is used by the authors as if dropout and pruning are the same things.\n-The equations are so poorly written, why even bother?\n-The writing is absolutely dismal, I don\u2019t even think I could enumerate over all the issues here. \n-There are several vague statements: \"It is also averse to retraining losses.\u201d, \"which can be only 2 if the image is very monochrome\u201d, etc.\n-There is one paragraph on the connectivity factor, which is trivial. It\u2019s basically what fraction of weights are pruned. Funnily, this concept is accompanied with diagrams. \n\n\nSorry if this whole review sounds rude, but to be honest I am amazed that authors thought about submitting a paper in this state to ICLR.\n", "rating": "1: Trivial or wrong", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3234/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3234/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What to Prune and What Not to Prune at Initialization", "authorids": ["~Maham_Haroon1"], "authors": ["Maham Haroon"], "keywords": ["Network Sparsity", "Machine Learning", "Initialization Pruning"], "abstract": "Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout. It is also averse to retraining losses.\n\nIn approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the \"fittest\" of these p-sparse matrices in an attempt to find the \"lottery ticket\" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.\n\nThe benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.\n\nWe evaluate the efficacy of the said methods on Autoencoders and Fully Connected Multilayered Perceptrons. The datasets used are MNIST and Fashion MNIST.", "one-sentence_summary": "Provides methods to prune weights at initialization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "haroon|what_to_prune_and_what_not_to_prune_at_initialization", "pdf": "/pdf/032a6594153b919b8bd489958cc98ea55d34510a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oBuciqKb8M", "_bibtex": "@misc{\nharoon2021what,\ntitle={What to Prune and What Not to Prune at Initialization},\nauthor={Maham Haroon},\nyear={2021},\nurl={https://openreview.net/forum?id=YTyHkF4P03w}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTyHkF4P03w", "replyto": "YTyHkF4P03w", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079572, "tmdate": 1606915804423, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3234/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3234/-/Official_Review"}}}], "count": 6}