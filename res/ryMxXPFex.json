{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488508589457, "tcdate": 1478222570329, "number": 103, "id": "ryMxXPFex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryMxXPFex", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "content": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396358246, "tcdate": 1486396358246, "number": 1, "id": "BkA6ozUul", "invitation": "ICLR.cc/2017/conference/-/paper103/acceptance", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not\n contribute to the KL term. \n \n One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.\n \n The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396358720, "id": "ICLR.cc/2017/conference/-/paper103/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396358720}}}, {"tddate": null, "tmdate": 1484357726948, "tcdate": 1484357726948, "number": 17, "id": "Syvwl-w8l", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Revised version uploaded", "comment": "In response to the helpful comments from the peer reviewers and other readers, I have made some small additions to the paper.  The new version contains:\n\n* A discussion of the relationship to deep belief networks to Section 1.2\n\n* A note in Section 2.1 that a model without continuous smoothing variables zeta can easily be produced by taking the limit beta -> \\infty, in which case zeta_i = z_i almost surely\n\n* A discussion of variance of derivatives of the inverse CDF in Appendix D.3\n\n* Some results on simplified probabilistic models in Appendix J\n\n* A few small clarifications.  In particular, I have clarified the use of the term \"restricted Boltzmann machine.\"  The Boltzmann machine used in the prior is bipartite, but all Boltzmann machine variables are directly connected to the smoothing variables zeta, and through them to the rest of the model.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1484357197930, "tcdate": 1484357197930, "number": 16, "id": "HJLLAevLe", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "r16qB307e", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Review response", "comment": "Thank you very much for your review.  We have added the results of some experiments on simpler probabilistic models in Appendix J.  We are currently working on refactoring our code into a clear, clean implementation, and plan to publicly release it upon completion.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1484357071852, "tcdate": 1484357071852, "number": 15, "id": "S1dRaxPUg", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "rk8jJ0VNx", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Review response", "comment": "Thank you very much for your review.  \n\nWe always use a bipartite Boltzmann machine (i.e., an RBM) for our experiments.  Our derivations are consistent with fully connected Boltzmann machines, but sampling from fully connected Boltzmann machines is much more computationally demanding.  \n\nWe use an approximating posterior (q distribution) that is autoregressive with respect to groups.  Group j depends upon all preceding groups i < j.  In the limit of each group comprising a single variable, the variables are processed one at a time.  In our experiments, we generally use four groups, each containing 32 variables, for a total of 128 binary variables.  We investigate the effect of the number of groups, holding the number of binary variables constant, in Figure 6c.\n\nWithin each group, the approximating posterior is a product of independent distributions, conditioned on the preceding groups.  As a result, all the approximating posteriors in a group can be sampled in parallel, by applying the inverse CDF (e.g., Equation 9) to each variable in parallel.  The number of sequential operations required is only equal to the number of groups, rather than the number of variables.  \n\nI have added Appendix D.3 to discuss the issue you identified regarding the variance of the gradient estimates when the smoothing transformation has insufficient overlap between the modes.  Thank you for pointing this out; I'm sure it will help anyone who tries to build upon this work.  I have also added the results of experiments on a few simplified versions of the architecture in Appendix J, which should help the reader to evaluate the contributions of the various components."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1484356828450, "tcdate": 1484356828450, "number": 14, "id": "S1E1plPIg", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "Bkf8UgS4l", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Review response", "comment": "Thank you very much for your review.  \n\nThe generative model underlying the discrete variational autoencoder resembles a deep belief network DBN.  A DBN comprises a sigmoid belief network, the top layer of which is conditioned on the visible units of an RBM.  In contrast to a DBN, we use a bipartite Boltzmann machine, with both sides of the bipartite split connected to the rest of the model.  Moreover, all hidden layers below the bipartite Boltzmann machine are composed of continuous latent variables with a fully autoregressive layer-wise connection architecture.  Each layer j receives connections from all previous layers i < j, with connections from the bipartite Boltzmann machine mediated by a set of smoothing variables.  However, these architectural differences are secondary to those in the gradient estimation technique.  Whereas DBNs are traditionally trained by unrolling a succession of RBMs, discrete variational autoencoders use the reparameterization trick to backpropagate through the evidence lower bound.  I have added this discussion to Section 1.2 of the paper.\n\nThe factored conditional restricted Boltzmann machine for modeling motion style (Taylor & Hinton, 2009) is an interesting connection.  However, their one-hot style variables are always observed.  As a result, they train a family of models with discrete latent variables (traditional Gaussian-Bernoulli RBMs), conditioned on a pseudo-continuous component (as well as a buffer of previous time steps).  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1482540459387, "tcdate": 1481928766947, "number": 13, "id": "SyvregzVg", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "BJMletbNl", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Appropriate performance comparisons", "comment": "Thank you for the continued discussion.  It is true that an fDARN is less powerful than an arbitrary autoregressive distribution, which would require a number of parameters exponential in the number of units. Nevertheless, an fDARN does have almost twice as many parameters as an RBM defined over the same number of latent units.  Moreover, an fDARN can indeed use some of its units exclusively for modeling higher-order correlations between the other units. These higher-order units need only precede the units they control in the hierarchy of latent units, and learn zero connections to the observed units.  \n\nI can see why smoothing variables might increase the capacity of a generative model in general, but I suspect this is not the case in a discrete variational autoencoder.  When using the spike-and-exponential smoothing transformation of Section 2.1, the approximating posterior over the smoothing variables is fixed.  That is, it is unparameterized (if beta is fixed), and does not depend upon the input, so it cannot be used to carry additional information through the autoencoding loop.  Discrete variational autoencoders differ from the Concrete distribution / Gumbel-softmax in this regard.  My intuition is that the smoothing variables serve primarily as a source of noise, and reduce rather than increase the model capacity.  This is analogous to the way in which (Gaussian) dropout decreases model capacity.  Indeed, the smoothing variables are equivalent to multiplying all discrete variables (in both the prior and the approximating posterior) by an independent continuous random variable, with distribution r(zeta | z=1).\n\nAll of that aside, I agree that the discrete variational autoencoder without Gaussian latent layers is significantly different from an fDARN, and the two models are not amenable to a detailed comparison. I apologise for failing to notice that the fDARN results, and results from Gu et al. (2016) in general, are ELBOs rather than log-likelihoods. To better address your question, I've started some runs with the lateral connections in the RBM disabled, reducing it to a set of independent binary random variables (and without Gaussian latent variables below the smoothing variables). The resulting network is a noisy sigmoid belief network. That is, samples are produced by drawing samples from the independent binary random variables, multiplying by an independent noise source, and then sampling from the observed variables as in a standard SBN.  \n\nWith this SBN-like architecture, the discrete variational autoencoder achieves a negative log-likelihood of 97.0.  With a factorial approximating posterior in addition to this SBN-like architecture, the discrete variational autoencoder achieves a NLL of 102.9.  Results reported in the literature for the corresponding 200-784 sigmoid belief network are: \nMaddison, Mnih, & Teh (2016) - Concrete distribution: 104.3, VIMCO: 98.8, NVIL: 104.4 \nBornschein & Bengio (2015) - RWS SBN: 103.1, RWS NADE, 95.0"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1482126922001, "tcdate": 1482126922001, "number": 3, "id": "Bkf8UgS4l", "invitation": "ICLR.cc/2017/conference/-/paper103/official/review", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/conference/paper103/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper103/AnonReviewer1"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). \n\nOverall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:\n\nGraham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025\u20131032, 2009.\n\nDiscussion of this should certainly be added. \n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696126, "id": "ICLR.cc/2017/conference/-/paper103/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper103/AnonReviewer2", "ICLR.cc/2017/conference/paper103/AnonReviewer3", "ICLR.cc/2017/conference/paper103/AnonReviewer1"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696126}}}, {"tddate": null, "tmdate": 1482117021876, "tcdate": 1482117021876, "number": 2, "id": "rk8jJ0VNx", "invitation": "ICLR.cc/2017/conference/-/paper103/official/review", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/conference/paper103/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper103/AnonReviewer3"], "content": {"title": "clever and useful contribution; clear and thorough exposition", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. \n\nThe paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!\n\nThe log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)\n\nI\u2019m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment \u201cvariance of the derivatives of F^{-1}\u201d below. I think the response is convincing, but the problem (as well as \u201cengineering principles\u201d for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients \u2014 something not commonly done in the age of autodiff frameworks.)\n\nAnother concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we\u2019re stuck handling all the variables sequentially, which might get expensive. \n\nMinor: the second paragraph of Section 3 needs a reference to Appendix A.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696126, "id": "ICLR.cc/2017/conference/-/paper103/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper103/AnonReviewer2", "ICLR.cc/2017/conference/paper103/AnonReviewer3", "ICLR.cc/2017/conference/paper103/AnonReviewer1"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696126}}}, {"tddate": null, "tmdate": 1481939117624, "tcdate": 1481858210902, "number": 11, "id": "BJjs3RxNg", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "rysCdgeNl", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Appropriate performance comparisons", "comment": "Thank you for your comment.  As you have observed, the discrete variational autoencoder is a complex architecture, comprising both a novel generative model, and a novel adaptation of the variational autoencoder framework to this generative model.  The generative model and the training algorithm are intertwined; the novel generative model is essential to facilitate the variational autoencoder approach.  Nevertheless, the gradients of the discrete variational autoencoder are estimated using the variational autoencoder framework.  As a result, the variational autoencoder-based algorithms cited in Table 1 constitute a sensible baseline for the architecture as a whole, comparable to those reported in other variational autoencoder papers.\n\nCompletely decomposing the contributions of the various components of the generative model and training algorithm is beyond the scope of the current paper.  For the generative model alone, an exhaustive decomposition would need to examine the effect of varying the number of continuous latent layers or removing them entirely; removing lateral connections in the RBM; using multiple layers of discrete latent variables (as in a sigmoid belief network), rather than multiple layers of continuous latent variables; and using unsmoothed discrete latent variables at test time, rather than smoothing them.  Note that this last variant can be realized by setting beta -> infinity in Equation 9, and is analogous to the approach used by the Concrete distribution (Maddison, Mnih, & Teh, 2016).  Moreover, as you suggest, each of these generative models could be trained using algorithms such as NVIL, reweighted wake-sleep, MuProp, and VIMCO, in addition to the variational autoencoder described here.\n\nNevertheless, it might be useful to present an informal comparison between a discrete variational autoencoder with just the RBM and smoothing variables, and other training methods from the literature applied to an analogous sigmoid belief network (SBN) or fDARN.  To this end, I am currently training some discrete variational autoencoders without additional layers of Gaussian continuous latent variables below the RBM and the smoothing variables.  With a bipartite Boltzmann machine (RBM) over 200 discrete units in the prior, I obtain a negative log-likelihood of 88.8 on the statically binarized MNIST test set with a linear transformation from latent to observed units, and a negative log-likelihood of 85.2 with a nonlinear transformation from latent to observed units.  I have not tuned the architecture of these models, and better performance might be achievable with a little time and effort.\n\nThis compares favorably to 200-784 SBNs, with a linear transformation between the latent variables and the observed variables:\nGu, Levine, Sutskever, & Mnih (2016) -  MuProp: 113.1, NVIL: 113.5 (ELBOs)\nMaddison, Mnih, & Teh (2016) - Concrete distribution: 104.3, VIMCO: 98.8, NVIL: 104.4\nBornschein & Bengio (2015) - RWS SBN: 103.1, RWS NADE, 95.0\n\nIt also compares favorably to 200-784 SBNs, with a nonlinear transformation between the latent variables and the observed variables:\nMaddison, Mnih, & Teh (2016) - Concrete distribution: 88.5, VIMCO: 89.3, NVIL: 93.8\n\nFinally, and most tellingly, this simplified discrete variational autoencoder is superior to a 200-784 fDARN trained with conventional approaches (edit: these numbers cannot be compared directly, since these are ELBOs rather than NLLs).  The fDARN architecture consists of a directed autoregressive graphical model on the hidden units, with directed (linear) connections to the visible units, but no autoregressive connections amongst the visible units.  It is the directed analogue of this simplified discrete variational autoencoder (with a linear transformation from latent to observed units).  \nMnih & Gregor (2014) - NVIL: 92.5, Wake-sleep: 95.9  (ELBOs)\nGu, Levine, Sutskever, & Mnih (2016) -  MuProp: 92.7, NVIL: 92.1  (ELBOs)\nIn principle, the fDARN probabilistic model has more representational power than the corresponding discrete variational autoencoder, since a fully autoregressive direction graphical model can represent any distribution, whereas an undirected graphical model with only pairwise connections is constrained.  The superior performance of the discrete variational autoencoder relative to NVIL and MuProp on the corresponding fDARN suggests that the discrete variational autoencoder realizes a lower-variance estimate of the gradient.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1481938945717, "tcdate": 1480549982908, "number": 9, "id": "r1vwL16fl", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "H1a1793ze", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Results without additional latent layers", "comment": "In brief:\nThank you for your question.  I have trained models with the smoothed RBM prior, but without the Gaussian latent layers.  These models train properly, but do not achieve state-of-the-art performance, since they do not match the structure of the dataset, as I discuss below; I have not pursued this direction in detail.  Rather, I prefer to evaluate the utility of the discrete latent variables by examining what they contribute on top of a VAE with continuous latent variables.  This allows the discrete units to capture the intrinsically discrete aspects of the dataset, while the continuous units represent the intrinsically continuous aspects of the dataset.  The results of this evaluation are reported in the paper; adding in discrete units significantly improves the log-likelihood over a VAE with only continuous units (with the possible exception of dynamically binarized MNIST).  The effect on the log-likelihood is likely to be larger on datasets with a more prominent discrete aspect; e.g., more classes, or multiple objects or object parts, each with an interrelated discrete type.\n\nIn more detail:\nNatural images, including even simple examples such as the handwritten digits in MNIST, are composed of discrete objects.  These objects are of a discrete set of types, subject to continuous transformations.  There are dogs and cars in the world, but no objects that are halfway between a dog and a car.  Obvious examples of continuous transformations to which these discrete objects are subject include position, pose, scale, and lighting.  \n\nMachine learning algorithms in general, and generative models in particular, generalize to unobserved data because they match the structure of the dataset to which they are applied.  An ideal generative model would capture both the discrete nature of the types of objects, and the continuous transformations to which they are subject.  \n\nA generative model with purely continuous latent variables, such as a traditional VAE, can approximate the discrete structure of the types using very sharp nonlinearities, so that distinct regions of the latent space are associated with different discrete types.  This is indeed observed in traditional variational autoencoders; e.g., see Figure 4 of Kingma & Welling (2013; https://arxiv.org/abs/1312.6114), specifically the transition from 6's to 2's in the top left corner.  However, once such sharp nonlinearities have formed, they are difficult to train further.  The situation is analogous to a sigmoid nonlinearity that is always within the saturation regime, so gradients go to zero.  \n\nA generative model with purely discrete latent variables, such as the model of this paper with the hierarchy of Gaussian latent variables removed, will have difficulty capturing the continuous transformations to which the discrete types are subject.  In MNIST, this includes size, line thickness, slant, and digit style. \n\nThe critical question for assessing the efficacy of the discrete random variables is whether they capture the discrete aspects of the dataset.  In MNIST, the primary discrete aspect is the digit type.  And indeed, in Figure 5, it is apparent that the digit type is represented by the discrete latent variables.  Each horizontal group of five shares a common configuration of the discrete latent variables, and shares a common digit type (with the exception of the 3/5 and 4/9 distinction; these groups are merged, reflecting the fact that 3's can be continuously deformed into 5's and 4's can be continuously deformed into 9's while remaining on the manifold of handwritten digits).  The other aspects of image are primarily represented by the continuous latent variables.  The images across each horizontal group of five, which have independent continuous variables but shared discrete variables, exhibit significant variations in size, line thickness, slant, and style, even as they share a common type.  \n\nUpdate: \nI have recently trained some models with the smoothed RBM prior but without the Gaussian latent layers between it and the data.  As described in the comments above (appropriate performance comparisons), with a bipartite Boltzmann machine (RBM) over 200 discrete units in the prior, I obtain a negative log-likelihood of 88.8 on the statically binarized MNIST test set with a linear transformation from latent to observed units, and a negative log-likelihood of 85.2 with a nonlinear transformation from latent to observed units.  With 128 discrete units in the prior (matching the architecture used in the paper), I obtain a NLL of 92.7 with a linear transformation from latent to observed units, and a NLL of 86.9 with a nonlinear transformation from latent to observed units.  I have not carefully tuned the architecture of these models, and better performance might be achievable.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1481900010423, "tcdate": 1481900010423, "number": 12, "id": "BJMletbNl", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "BJjs3RxNg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Appropriate performance comparisons", "comment": "Thank you for your response.\n\nAppealing to the complexity of a system is not a good argument against performing ablation studies in order to understand which of its aspects are essential and which are not. In fact, having to perform many experiments to understand a system is simply the price one has to pay for its complexity. These experiments also serve as sanity checks, as the likelihood of various mistakes in a system grows with its complexity.\n\nThank you for reporting the numbers for the DVAE without the intermediate Gaussian layers. These are interesting and should be included in the paper, ideally along with some samples generated from the models. Unfortunately these numbers are not directly comparable to the single hidden layer SBN numbers you quoted, as the SBN models had factorial priors and posteriors and thus were much weaker than your smoothed RBM DVAEs. And while the fDARN models are a closer match for the DVAEs, they do not have the continuous (smoothing) variables and thus are likely to have considerably lower capacity. Unless you train exactly the same model using different algorithms, there will always be this kind of confounding, which is why I asked about comparing to the alternative training methods. Moreover, the fDARN numbers you quoted are variational lower bounds (i.e. underestimates) on the log-likelihood and therefore are not comparable to the direct log-likelihood estimates you report.\n\nFinally, while an autoregressive model with infinite capacity can represent any distribution, this is not the case for fDARN, as its autoregressive prior is linear. Given that the RBM prior of a DVAE can choose to use some units only for modeling the higher-order correlations between the other units, it's at least as powerful as the linear autoregressive prior of fDARN, if not more so. Therefore I don't think we can conclude anything about the gradient estimate variance from the reported results due to many confounding factors."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1481799891036, "tcdate": 1481799891030, "number": 10, "id": "rysCdgeNl", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Missing baselines", "comment": "This paper proposes a complex new architecture with some discrete latent variables along with a specialized method for training them. The trouble is that it's unclear how effective the method actually is at training the discrete part of the system. Is it better or worse at this than the existing methods?There're many such methods to choose from:\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995). The \"wake-sleep\" algorithm for unsupervised neural networks. \nBengio, Y., Leonard, N., & Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., & Wierstra, D. (2014). Deep autoregressive networks.\nMnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief networks. \nBornschein, J., & Bengio, Y. (2014). Reweighted wake-sleep.\nGu, S., Levine, S., Sutskever, I., & Mnih, A. (2015). MuProp: Unbiased Backpropagation for Stochastic Neural Networks.\nMnih, A., & Rezende, D. J. (2016). Variational inference for Monte Carlo objectives.\nAll of these algorithms have been used to train models with hundreds of variables, so they should be able to handle models with up to 128 discrete variables used in the paper easily, as long as the reparameterization trick is used for the Gaussian latent variables. Moreover, the above methods are applicable to any architecture with discrete latent variables, unlike the algorithm proposed in the paper, which can only be used if the discrete variables are smoothed out with the continuous ones. Why not compare to at least one of these more general algorithms? It would be good to know whether anything is gained by taking advantage of the model structure in this case."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1481717141139, "tcdate": 1481717141134, "number": 1, "id": "r16qB307e", "invitation": "ICLR.cc/2017/conference/-/paper103/official/review", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/conference/paper103/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper103/AnonReviewer2"], "content": {"title": "Rich set of ideas on how to make VAEs work better. ", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). \nA second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. \nThe framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.\nOverall, the paper is very rich with ideas so I think it would be a great contribution to the conference. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696126, "id": "ICLR.cc/2017/conference/-/paper103/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper103/AnonReviewer2", "ICLR.cc/2017/conference/paper103/AnonReviewer3", "ICLR.cc/2017/conference/paper103/AnonReviewer1"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696126}}}, {"tddate": null, "tmdate": 1480802294994, "tcdate": 1480802294988, "number": 2, "id": "SJyWe6lml", "invitation": "ICLR.cc/2017/conference/-/paper103/pre-review/question", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/conference/paper103/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper103/AnonReviewer1"], "content": {"title": "Pre-review", "question": "This is an interesting and mathematically rather dense paper. I am slowly making my way through it. No specific questions at this time. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959460714, "id": "ICLR.cc/2017/conference/-/paper103/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper103/AnonReviewer3", "ICLR.cc/2017/conference/paper103/AnonReviewer1"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959460714}}}, {"tddate": null, "tmdate": 1480547868175, "tcdate": 1480547868170, "number": 8, "id": "ryNmCAnGe", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "rJZlC3hfx", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Gradient derivations", "comment": "Thanks for continuing to work through these questions with me.\n\nI think there might still be a miscommunication between us regarding the \"hidden\" or \"visible\" nature of units in the Boltzmann machine.  All units in the Boltzmann machine are latent variables; but in the experiments reported in this paper, all Boltzmann machine units are connected, through their smoothing zetas, to the rest of the generative distribution (including the observed variables).  This is analogous to a DBN in which all units of the top-level (bipartite) Boltzmann machine are connected to the next hidden layer (or perhaps to all lower layers).  With regards to the (bipartite) Boltzmann machine in the discrete variational autoencoder, I am inclined to say that all units are \"hidden,\" since they are all latent units, as opposed to observed units.  However, it sounds like you think it would be clearer to call them all \"visible,\" since they are all directly connected to the rest of the model, even though they are not observed.  Given that neither \"hidden\" nor \"visible\" really captures the nature of the Boltzmann machine units, I think it would be best to simply refer to them all as \"latent.\"\n\nDBNs are a conceptual relative, as are sigmoid belief networks.  In addition to the lack of a distinction between \"hidden\" and \"visible\" units in the (bipartite) Boltzmann machine and the use of continuous latent variables, the generative model of a discrete variational autoencoder differs from a DBN in that the continuous latent layers and the observed layer are fully autoregressive.  Layer j is conditioned on all layers i<j, including the smoothed version of the discrete units.  This sort of autoregressivity is used within layers, rather than between layers, in DARNs (http://www.jmlr.org/proceedings/papers/v32/gregor14.pdf) and NADEs (https://arxiv.org/abs/1605.02226).  I will add a note to the paper highlighting these connections.\n\nYou are correct that the derivation of Equation 30 (and thus Equation 12) depends upon choosing r(zeta_i | z_i=0) to be a delta function at zero.  This is consistent with the spike-and-slab transformation in Section D.2, and the spike-and-Gaussian transformation in Section E.1, but not with the mixture of ramps in Section D.1.  The mixture of ramps can be trained using the REINFORCE approach to compute the gradient of the cross-entropy term, as in Section F.3; the reparameterization trick can still be applied to the autoencoding term.\n\nThe discrete z_i's must depend on each other only through the continuous zeta_i's so that the reparameterization trick can be applied to the approximating posterior (i.e., the encoder).  In the derivation of Equation 30, this is used implicitly in all terms of the form \\partial q_i(z_i=1) / \\partial phi, when the phi is a parameter of some q_j where j<i.  \n\nYou are correct that, when backpropagating through the encoder with fixed rho, occurrences of z_i = 0 (equivalently, zeta_i = 0) break the gradient flow, since \\partial F^{-1}_q(rho) / \\partial q = 0.  This is almost identical to what happens in a ReLU when the input is less than 0.  In both cases, the nonlinear transfer function has an all-zero region, which blocks gradients, and a strictly monotonically increasing region, which passes gradients.  It is also analogous to dropout when a unit is set equal to 0.  In both cases, the nonlinear transfer function is stochastically chosen from a fixed family, depending upon an independent random variable.  Some members of the family block gradients (e.g., the all-zero function when a unit is dropped out); other members of the family pass gradients.  As with ReLUs and dropout, some units must pass gradients, unless all units are equal to zero; and some units must be non-zero to pass information from the input through the network.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480539625212, "tcdate": 1480539625204, "number": 7, "id": "rJZlC3hfx", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "SkNmPcjGx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Gradient derivations", "comment": "Thank you for the clarifications.\n\nThe unusual RBM-related notation is worth describing in more detail. Using a different letter for the \"hidden\" RBM units is likely to make things clearer though it would require updating a few equations. What is the relative ordering of the \"hidden\" and \"visible\" parts of z in the factorization of q? Regarding the issue of \"observed\"/\"visible\" and \"hidden\" units in an RBM prior, it's useful to preserve this terminology even when we are talking about an RBM which is used as a prior, as was done in the Deep Belief Networks paper (Hinton et al., 2006). The point is simply to distinguish between the latent variables local to the prior which the rest of the model does not use and the \"external\" variables the prior is actually over, which are used by the rest of the model.\n\nSpeaking of DBNs, they seem to be the closest conceptual relatives to the models you propose in the paper. Both model families use an RBM prior to capture the correlations between the binary variables in the deepest latent layer and have directed layers going from that layer to the observation, though DBNs use binary latent variables in those intermediate layers, while your models use continuous latents. This might be worth discussing in the paper.\n\nThere're several aspects of the approach in Appendix F.4 that are still unclear to me. The derivation of Eq. 30 seems to assume that the conditional distribution r(zeta_i|z_i=0) is a delta function at zero. Does this mean that the resulting gradient will not work with the alternative smoothing transformations from Appendix D? Which part of the proof depends on the fact that z_i's depend on each other only through zeta_i's? Also, when backpropagating through the encoder, while keeping rho fixed, don't occurrences of z_i=0 break the gradient flow, making it zero?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480528612724, "tcdate": 1480528612720, "number": 6, "id": "H1a1793ze", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Results without additional latent layers", "comment": "Have you tried training models with the smoothed RBM prior but without the Gaussian latent layers between it and the data? This seems like a crucial experiment to perform if you want to highlight the effectiveness of the aspects of your model that make it different from a VAE."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480464155573, "tcdate": 1480464155567, "number": 5, "id": "SkNmPcjGx", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "rJyC1dsGg", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Gradient derivations", "comment": "Thank you for your response.  I think your description of the application of the reparameterization trick to discrete variables is correct.  The discrete latent variables are augmented with continuous latent variables, which smooth the marginal CDF and render its inverse differentiable.  The phrase \"discrete reparameterization trick\" does not appear within the paper.  Nevertheless, the model is a variational autoencoder with discrete latent variables; the prior is defined directly over these discrete latent variables; and the application of the reparameterization trick accommodates the presence of the discrete variables.  Moreover, it is trivial to recover the discrete z_i from the continuous zeta_i when using the spike-and-exponential smoothing transformation of Section 2.1.  Almost surely (i.e., with probability 1), zeta_i = 0 <=> z_i=0, and zeta_i > 0 <=> z_i = 1.  \n\nI use the term \"RBM\" to denote a bipartite Boltzmann machine, rather than a particular assignment of visible and hidden units.  The motivation for the bipartite restriction is the same as in a conventional RBM: it enables the use of simple block Gibbs sampling.  In fact, the Boltzmann machine used in the discrete variational autoencoder is fully hidden, rather than fully visible.  All of its variables are latent, rather than observed.  \n\nThe use of the familiar term \"RBM\" is intended to save the reader from the more cumbersome \"fully hidden bipartite Boltzmann machine.\"  I will edit the text to clarify this point.  Nevertheless, in the probabilistic model of the prior, it is easy to render one of the biparatite halves \"hidden\" by setting all parameters (in the prior) connecting to the associated zetas to \\mathfrak{z} and x to zero.  All equations remain unchanged; it is not necessary to marginalize out the \"hidden\" units analytically.  \n\nWhile the gradient w.r.t. the parameters in Equation 24 resembles that of a fully observed BM, it is in fact the gradient for KL[q||p] with respect to the parameters of the RBM (i.e., the fully hidden bipartate Boltzmann machine) in the prior.  Note that the first expectation is with respect to the approximating posterior q, which is a distribution over latent (i.e., hidden) units.  \n\nIn the derivation on the top of page 25, q_i(z_i) affects terms W_{kj}*z_k*z_j, with i<j and i<k, through d q_j(z_j) / d phi and d q_k(z_k) / d phi, where I write 'd' in place of '\\partial.'  These partial derivatives with respect to the parameters of q_i are not zero, even for i<j and i<k; they capture the effect of changing q_i(z_i).  In contrast, due to the use of the chain rule at the bottom of page 24, changing q_i(z_i) itself has no direct effect on terms W_{kj}*z_k*z_j, with i<j and i<k.  This is a matter of reordering where these contributions are dealt with, so as to ensure that gradients can be calculated.  The contributions of the hierarchically earlier q_i on z_j where i<j are shifted into d q_j(z_j) / d phi.  Using the chain rule, d q_j(z_j) / d phi can be decomposed into components such as [d q_j(z_j) / d q_i(z_i)] * [d q_i(z_i) / d phi].  However, this is left implicit, because it is done automatically by backpropagation through the encoder when rho is held fixed.  \n\nIf you have any other questions regarding the derivation in appendix F.4, I would be happy to address them.  Thank you for catching the missing minus sign in Section F.1; the term of interest is the negative entropy, rather than the entropy itself.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480454086682, "tcdate": 1480454086677, "number": 4, "id": "rJyC1dsGg", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "SkCCsHqfl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Gradient derivations", "comment": "Thank you for the detailed response Jason. It was very helpful, but I still have a few concerns about the method.\n\nIt seems that the key trick in the paper is efficiently marginalizing z out of q(zeta, z|x), which is possible because q(z_i|...) depends on z_{j<i} only through zeta_{j<i}. In particular, in the paper each q(zeta_i|zeta_{j<i}, x) is a mixture with two components, so the marinalization of z_i involves adding only two terms. This form of q allows the gradients of E_q[log p(x|zeta)] to be computed using the reparameterization trick on the continuous variables zeta, without explicitly dealing with the discrete variables z. This is analogous to fitting a mixture model using gradient of its density w.r.t. to the mixture parameters. Given that the approach requires introducing continuous latent variables to \"smooth out\" the discrete ones, I am not sure it is reasonable to claim that this amounts to a \"discrete reparameterization trick\". After all, the model is being changed (augmented with continuous variables) as opposed to being merely reparameterized.\n\nThe use of an RBM instead of a BM affects more than just the diagram. For example, the gradient w.r.t. the parameters of the undirected model given by Eq. 24 is for a fully observed BM, not an RBM. Note that a bipartite BM is not necessarily an RBM, as it might be fully observed. Unlike a fully observed BM, an RBM has a vector of hidden units h (in addition to the observation z), and the energy function is defined on joint configurations as E(z, h) instead of just the observed ones used in the paper (i.e. E(z)). To get something resembling E(z) used in the paper for an RBM, we need to marginalize out h, obtaining the free energy of z: F(z) = -log(sum_h exp(-E(z, h))). Note that the form of F(z) is much more complicated than E(z) = -z^T*W*z, which is linear in W and bilinear in z. This means that the gradient of -KL w.r.t. the parameters phi of q derived in appendix F is also incorrect if the prior on z is indeed an RBM, since it's derived for E(z) = -z^T*W*z. Did I miss something here?\n\nMoreover, even if the model really is a fully observed BM with an energy function bilinear in z, the gradient of the cross-entropy derived in appendix F.4, does not look correct to me. Its derivation at the top of page 25 is not rigorous, with the transition from line 2 to line 3 being especially problematic. The derived gradient seems to neglect the effect of changing q_i(z_i) on terms W_{kj}*z_k*z_j, with i < j and i < k, which indirectly depend on z_i through the intermediate variables in the hierarchical posterior. As is noted in section F.3, the REINFORCE gradient reflects this dependency by including all terms W_{kj}*z_k*z_j except for those with k < i and j < i. How does the gradient given by Eq. 30 capture this dependence? \n\nA minor correction: the expressions for entropy and its gradient in appendix F.1 should have a minus sign in from of them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480379350292, "tcdate": 1480379350286, "number": 3, "id": "SkCCsHqfl", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "S1sr2zczx", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "The gradient of the autoencoding term is computed using the reparameterization trick", "comment": "Thank you for your comments.  It seems like many of the questions you raise are based on the use of the reparameterization trick.  To frame my response, it might help to summarize the basic idea of the reparameterization trick, using the notation of the present paper, but ignoring the discrete nature of the random variables z.\n\nThe reparameterization trick converts sampling with respect to a parameterized distribution to sampling with respect to a fixed distribution, followed by a deterministic, parameterized transformation.  The gradient of the expectation can then be approximated, in an unbiased manner, by sampling from the fixed distribution, and backpropagating through the (deterministic, parameterized) function conditioned on this sample.\n\nMore specifically, the reparameterization trick defines a function F^{-1}_{q(z | x, phi)}(rho), so that sampling from q(z|x) is equivalent to sampling from rho ~ U[0,1]^n (the uniform distribution between 0 and 1), and then computing F^{-1}_{q(z | x, phi)}(rho).  In this equation, q(z | x, phi) is a set of deterministic values between 0 and 1.  The autoencoding term, E_q[log p(x|z)], is thus the expectation with respect to rho of p(x | F^{-1}_{q(z | x, phi)}(rho)).  The gradient of the autoencoding term can be stochastically approximated (in an unbiased manner) by sampling from rho, and backpropagating through p(x | F^{-1}_{q(z | x, phi)}(rho)).\n\nTo address your particular points, the gradient of -KL(q||p) with respect to phi (the gradient of the KL term in Equation 2)  is treated separately from the gradient of E_q[log p(x | zeta)] with respect to phi (the gradient of the autoencoding term in equation 2).  The former is stated explicitly in equation 12, and derived in Appendix F.  The latter is computed via backpropagation with the reparameterization trick, and described by equation 8 and 9, with the hierarchical structure of Figure 3.  \n\nBoth the gradient of the KL term, and the gradient of the autoencoding term, are used to train q(z|x).  As a result, q(z|x) learns to be strongly dependent on x.  In all the experiments described in the paper, r(zeta | z) is factorial and independent of x; it is specified in the first set of equations in Section 2.1.  \n\nAs you suggest, the gradient backpropagated to zeta_i does depend upon zeta_j for j>i.  It does so, once again, via backpropagation through the reparameterization trick, now applied hierarchically as in Figure 3.  Since the sampling procedure for the approximating posterior is hierarchical, backpropagation is also hierarchical.  REINFORCE is not necessary, since we use the reparameterization trick.  To the best of my knowledge, no probabilistic dependencies are ignored, and the stochastic estimate of the gradient is unbiased.\n\nThe primary novel contribution is the extension of the reparameterization trick to discrete variables z, by introducing smoothing variables zeta, and marginalizing over z when applying the reparameterization trick.  Gradients are indeed analytically (although implicitly) backpropagated through the discrete random variables, via backpropagation through Equations 8 and 9.  The discrete variables z do not appear explicitly in Equation 8, since they have been analytically marginalized out in equation 9.  Nevertheless, the discrete random variables remain an explicit component of the generative model, and the approximating posterior; and they appear explicitly in Equations 11 and 12.\n\nThe RBM used in the experiments is bipartite, and thus restricted.  I depicted a fully connected network in Figure 1 since three units fit more easily on the page than four, and I wanted to emphasize the undirected connections amongst the discrete units.  However, I believe the equations for the gradients are still correct if the Boltzmann machine is unrestricted.  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480377550028, "tcdate": 1480377550023, "number": 2, "id": "ryLCVr5zx", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "Sy5I9pufe", "signatures": ["~Jason_Tyler_Rolfe1"], "readers": ["everyone"], "writers": ["~Jason_Tyler_Rolfe1"], "content": {"title": "Well-engineered smoothing distributions control the variance of the derivatives of F^{-1}", "comment": "Thank you for your question.  I believe you are correct that, given the particular smoothing distributions r(zeta | z) that you propose, much of the expected value of the gradients will occur in a region of low probability, and the stochastic approximation to the gradient will have high variance.  While this is a problem for some smoothing distributions r(zeta | z), it can be addressed by engineering a better r(zeta | z).  Viewed another way, I believe the difficulty you identify results because the smoothing distribution in question is poorly matched to the discrete variational autoencoder.  This is analogous to the way in which the transfer function logistic(c*x) is poorly matched to feedforward neural networks as c goes to infinity, even though it is fine when c=1.  \n\nThe smoothing provided by the continuous random variables zeta is only effective if there is a region of meaningful overlap between r(zeta | z=0) and r(zeta | z=1).  In particular, there must be some zeta' such that r(zeta' | z=0) is not approximately zero, and r(zeta' | z=1) is not approximately zero.  As you observe, this does not hold in your example.  However, it is possible to significantly increase the amount of overlap, while maintaining a distinctly multimodal marginal distribution over zeta, by simply shifting your normal distributions closer together.  For instance, consider:\nr(zeta | z=0) ~ N(0,1)\nr(zeta | z=1) ~ N(4,1)\nIn this case, both the probabilities and the derivatives remain moderate in the region of overlap.  This overlap property can be ensured by either choosing a fixed r(zeta | z), or bounding the allowed parameters of r(zeta | z).  In the spike-and-exponential distribution described in Section 2.1, this overlap can be ensured by fixing or bounding beta.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480367171190, "tcdate": 1480367171183, "number": 1, "id": "S1sr2zczx", "invitation": "ICLR.cc/2017/conference/-/paper103/public/comment", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Gradient of the likelihood term", "comment": "This looked like an exciting though mathematically dense paper, so I spent a couple of days working through it. The presentation in the paper is not very clear and is in places incomplete, so I hope the author will correct me if I misunderstood some of the details of the proposed method.\n\nAs far as I can tell, the gradients of the variational objective w.r.t. the parameters phi of the variational posterior q(z, zeta) are incorrect, due to some of the probabilistic dependences being ignored. Most significantly, the gradient w.r.t parameters of q(z|x) seems to contain only the term obtained by differentiating -KL(q||p), but no contribution from the term E_q[log P(x|zeta)] in the objective. This likelihood term is considerably harder to deal with than the KL term as it does not factorize, so it is important to explain how it is taken into account when updating parameters of q(z|x). Ignoring this term amounts to training q(z|x) to match the prior p(z), teaching it to ignore the current observation x (assuming a factorial q(zeta|z) and r(zeta|z) that does not depend on x). This makes q(z|x) a completely ineffective recognition model.\n\nNote that making r dependent on x or using a hierarchical posterior does allow q(z|x) to learn some dependence on x as z_i will depend on zeta_{j<i}, which in turn will contain some information about x. This might explain why the state of the RBM clearly does influence the samples from the model as shown in Figure 5. However, if the term E_q[log P(x|zeta)] indeed makes no contribution to the gradient of q(z|x) as suggested above, the proposed approach does not \"backpropagate\" through the discrete latent variables in any sense.\n\nUnfortunately, since all the results in the paper use hierarchical posteriors (which are not factorial), it's impossible to tell whether q(z|x) does indeed learn to ignore the input x. However, the surprisingly small differences in performance obtained by removing the RBM from the model reported at the top of page 9 are not inconsistent with the above theory. It would be really good to see some results for the most basic proposed setup: a factorial posterior, r(zeta|z) that does not depend on x, and no intermediate Gaussian layers between zeta and x. The results included in the paper involve such complicated models that it is hard to make any strong conclusions about the impact of the discrete latent variables and the effectiveness of the algorithm for training them.\n\nThere are more dependences that seem to be ignored (or at least not mentioned) in the paper. For example, when using a hierarchical posterior, zeta_i will influence zeta_j and z_j for j > i, which means that the gradient of zeta_i should contain terms dependent on log probabilities of the values of those variables. This results in a REINFORCE-like contribution to the gradient of the parameters of q(theta|...) (see e.g. Schulman et al., 2015), but this issue is not mentioned in the paper.\n\nMore fundamentally, I don't see how smoothing discrete variables in the manner proposed in the paper would allow to completely avoid a REINFORCE-like term for the gradient of E_q[log P(x|zeta)] w.r.t. phi, since this term does not factorize over the individual z_i's. Titsias and L\u00e1zaro-Gredilla (2015) made some interesting progress in this direction by differentiating the local expectations and their paper highlights the difficulties that methods of this type seem bound to run into. If this paper does indeed overcome these substantial difficulties it's important to spell out how exactly this is achieved.\n\nAs far as I can tell, the \"RBM\" in the paper is actually a fully connected Boltzmann Machine/MRF (and thus is not \"restricted\"). Is this correct? Given the difficulty of sampling from such models, what was the motivation for using an undirected prior over the discrete latent variables?\n\nReferences\nSchulman, J., Heess, N., Weber, T. and Abbeel, P., 2015. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems\nTitsias, M. and L\u00e1zaro-Gredilla, M., 2015. Local expectation gradients for black box variational inference. In Advances in Neural Information Processing Systems"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727133, "id": "ICLR.cc/2017/conference/-/paper103/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryMxXPFex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper103/reviewers", "ICLR.cc/2017/conference/paper103/areachairs"], "cdate": 1485287727133}}}, {"tddate": null, "tmdate": 1480280658276, "tcdate": 1480280658271, "number": 1, "id": "Sy5I9pufe", "invitation": "ICLR.cc/2017/conference/-/paper103/pre-review/question", "forum": "ryMxXPFex", "replyto": "ryMxXPFex", "signatures": ["ICLR.cc/2017/conference/paper103/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper103/AnonReviewer3"], "content": {"title": "variance of the derivatives of F^{-1}", "question": "It seems like the derivatives of F^{-1} could have high variance in a multimodal setting. For instance, consider this simplified example. We let z be a Bernoulli random variable with mean parameter \\theta, and\n\n\\zeta | z=0 ~ N(0, 1)\n\\zeta | z=0 ~ N(10, 1)\n\nSuppose \\theta = 0.5 and \\rho = 0.5, so that \\zeta = 5 is in between the two modes. By my calculation, dF^{-1}(\\rho) / d\\theta is approximately 1/r(\\zeta), which is 6.72e5. Since the derivative in this region is about 1/r(\\zeta), the derivatives get large as fast as the probabilities get small, so maybe these low probability regions could still contribute significantly to the variance.\n\nBecause derivatives of dF^{-1} appear in the backprop computations, this could lead to high variance stochastic gradients in general. Is there some reason (either theoretical or empirical) to believe this isn't a problem?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Variational Autoencoders", "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.", "pdf": "/pdf/b75cbe30ce7fb780afb82305892b689df134639b.pdf", "TL;DR": "We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables.", "paperhash": "rolfe|discrete_variational_autoencoders", "conflicts": ["dwavesys.com"], "authors": ["Jason Tyler Rolfe"], "authorids": ["jrolfe@dwavesys.com"], "keywords": ["Deep learning", "Unsupervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959460714, "id": "ICLR.cc/2017/conference/-/paper103/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper103/AnonReviewer3", "ICLR.cc/2017/conference/paper103/AnonReviewer1"], "reply": {"forum": "ryMxXPFex", "replyto": "ryMxXPFex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959460714}}}], "count": 24}