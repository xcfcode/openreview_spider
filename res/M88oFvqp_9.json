{"notes": [{"id": "M88oFvqp_9", "original": "sdQrX2hyvP1", "number": 12, "cdate": 1601308010536, "ddate": null, "tcdate": 1601308010536, "tmdate": 1615968923632, "tddate": null, "forum": "M88oFvqp_9", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lFjv-lypP7", "original": null, "number": 1, "cdate": 1610040370003, "ddate": null, "tcdate": 1610040370003, "tmdate": 1610473961313, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper swaps characteristics of an object in one image onto those of another object in another image--for example, adding fur to a car.  The authors give some examples where the task could be useful.  Further, they successfully argue  that this task is an illustration that the disentanglement task has been done well.\n\nTwo reviewers argued for acceptance, two for just-below-the-bar rejection.  The 2nd of those in favor of rejection engaged thoughtfully with the authors and raised the score by 1 after that engagement.\n\nWe have decided to accept the submission as a poster."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040369986, "tmdate": 1610473961294, "id": "ICLR.cc/2021/Conference/Paper12/-/Decision"}}}, {"id": "g-muCRg51kG", "original": null, "number": 4, "cdate": 1604032412374, "ddate": null, "tcdate": 1604032412374, "tmdate": 1606764550535, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Review", "content": {"title": "Nice extension of shape/appearance disentanglement to images from different domains", "review": "### Summary\nThis paper proposes a generative model as an extension of FineGAN that aims to learn a disentangled representation for image shape and appearance across different domains rather than \"intra-domain\" disentanglement. To this end, the authors adopt the prior that features that correspond to an object's appearance should preserve frequency histograms. In order to incorporate this prior into the differential learning procedure, they learn a library of convolutional filters using a contrastive learning framework. They provide many convincing baselines and comparisons to related work and are able to attain reasonable results for style/content transfer between unrelated domains.\n\n### Explanation of rating\nI think this paper is a good steps towards truly being able to learn generic disentangled representations. Although the kind of data used for experiments is relatively simple, the results that are achieved go beyond existing state-of-the art generative models.\n\n### Pros\n- This provides some new insight into the kinds of disentanglements that previous generative models are (and are not) able to discover.\n- The frequency histogram assumption is a nice prior that is general enough to apply to different domains\n- The evaluation and comparisons are quite extensive and convincing.\n\n### Cons/questions\n- It might help to clarify and emphasize the novelty of the proposed method vs. the parts of FineGAN that it builds upon. For instance, the authors claim that that their method supports intra-domain disentanglement. While this is true, it seems like this is a feature of the base model and not really a contribution.\n- All of the results shown involve images with a single subject that takes up most of the canvas? How does this behave on less obvious images, e.g. with less prominent or multiple subjects?\n- How are $N_x$, $N_y$. $N_b$ chosen?\n- How is sim in eq. 1 defined?\n- What is temperature $\\tau$ in eq. 1? How is it chosen / is the method stable to choice of temperature?\n- Page 1: \"it's appearance\" -> \"its appearance\"\n- Page 2: \"acros\" -> \"across\"\n\n---\n\nThanks to the authors for the clarifications. I have read the other reviews and responses and still believe that the paper is a good contribution. Therefore, I am keeping my score.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151631, "tmdate": 1606915764055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper12/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Review"}}}, {"id": "Fdm0c6C7kjY", "original": null, "number": 1, "cdate": 1603858405842, "ddate": null, "tcdate": 1603858405842, "tmdate": 1606618009343, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Review", "content": {"title": "Interesting direction, limited contribution", "review": "The submission describes a method to disentangle shape and appearance of images across two domains such that new images can be generated that have appearance and shape from either of these domains while still being visually convincing. Starting from an established method (FineGAN) to disentangle shape, appearance, background identity as well as a set of \"nuisance\" factors such as pose in one domain, the paper proposes to add a loss term that aims at retaining appearance when moving from one domain to another. This additional loss term essentials tries to keep the low-level image statistics between two images when both of them are generated with the same appearance, but possibly different shapes. It is trained such that it is invariant to the nuisance parameters (same object under differing views has same statistics), but discriminative towards the object appearance (different objects from the same view have differing statistics). The low level features are expressed as histograms of responses of convolution filters over the masked foreground pattern. The paper provides empirical evidences in the form of example images where appearance and shape are combined from two different domains (out of cars, birds, dogs, animals) as well as proxy measurements for the quality of the transfer: (a) how much do the low-level statistics differ in terms of $\\chi^2$ distance, (b) how well is shape disentangled under changing appearance by measuring the foreground overlap between samples, (c) a user preference study (which method transfers shape and appearance better?). The results are compared to some relevant baselines (FineGAN, CycleGan, AdaIn, MUNIT), and show moderate improvements over those.\n\n### Strengths\n**[S1]** The paper is written well and it seems that one could reproduce the method reasonably.\n\n**[S2]** The authors lay out their claims clearly.\n\n**[S3]** Based on the given motivation, retaining the low level statistics of the image, the authors derive how to model a solution, optimize and evaluate it with respect to this motivation in a structured manner.\n\n**[S4]** The empirical evaluation seems to demonstrate the effectiveness of the proposed solution towards the posed objective.\n\n**[S5]** The authors allude to the method being able to translate appearance under shape change when there is either part-part correspondence between the domains (dogs$\\leftrightarrow$cats) or none (dogs$\\leftrightarrow$cars). However, see [W2,W3].\n\n### Weaknesses\n**[W1]** While the overall motivation is fairly clear: How can I retain more appearance between domains where I do not have access to actual labeled samples for training?, the particular heuristical choice of approach, retaining the frequency statistics of low-level filter bank responses, is not well motivated. Low level filter bank response statistics are known to encode texture-like properties in the sense of repeatable patterns such as a cheetah's fur texture (Figure 5, rightmost panel). Are they sufficient to capture other properties of appearance that are less readily encoded in low-level frequency statistics? What other choices of embodying appearance, including texture, are there, and why is the choice of the presented low-level statistic favorable? I feel that the paper is lacking in setting the heuristic in context so that the reader can be confident in the choice of heuristic.\n\n**[W2]** The definition of intra- vs. inter-domain seems somewhat vague. It would be helpful to characterize this distinction either more theoretically or empirically: When are two domains close enough so that I do not need the additional term, when are they too far apart for even this method to work? Cats and dogs compared to cats and cars seem qualitatively different concept relations. The authors do imply hierarchies of closeness of domains (e.g. Section 2: having part level or no part level correspondences), but this is not used in the paper to clearly design or evaluate the method with respect to differing inter/intra-domain distributions or levels of domain proximity.\n\n**[W3]** Weakly supported claim: Section 2 claims \"Moreover, when part-level correspondences do exist (e.g., dogs \u2194 tiger), it combines appearance and shape in a way which preserves them.\" This indeed could be a strong point, see W2. As I read the paper however, this claim seems only anecdotally supported, e.g. by individual samples in figures 5/6. I feel that more thorough investigation and evidencing of this claim would strengthen the paper.\n\n**[W4]** Reference missing: I feel that [B1] below does indeed discuss and investigate related concepts, albeit from the perspective of style rather than object appearance. Nevertheless, it discusses disentanglement under disjoint domains, and it would be interesting to include this in the discussion of related work.\n\n**[W6]** I read the submission as more of an empirical paper than theoretical paper. From this perspective, the presented empirical evidence seems limited. In order to gauge the effectiveness and benefit of the method better, I wished for a wider range of data, where I can see the relation of shape/appearance transfer and the \"distance\" of the domains better. The authors do mention, but do not show for instance furniture. Can I transfer from cups to people or vice versa?\n\n### Further comments\n**[C1]** There is a flipped $\\pm$ in table 2 (\"Ours vs. FineGAN / dogs $\\leftrightarrow$ birds\")\n\n### Summary\nI feel that there is benefit to the direction that the submission is taking. Although at this point the weaknesses outweigh the strengths, a revision could make a strong contribution if for instance the choice of heuristic is motivated and evidenced more strongly in the context of potential alternatives, and the intra-/inter-domain distinction is worked out more clearly theoretically and/or empirically. As it is I feel that the paper would need significant revision for acceptance at ICLR.\n\n**[B1]** @InProceedings{Lee_2018_ECCV,\nauthor = {Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},\ntitle = {Diverse Image-to-Image Translation via Disentangled Representations},\nbooktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\nmonth = {September},\nyear = {2018}\n}", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151631, "tmdate": 1606915764055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper12/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Review"}}}, {"id": "wS-4cfs6hR_", "original": null, "number": 8, "cdate": 1606255226984, "ddate": null, "tcdate": 1606255226984, "tmdate": 1606255226984, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Updates to the submission", "comment": "We thank the reviewers for their thoughtful comments. We have provided direct and detailed responses to each reviewer below.  We have also updated our submission (changes denoted in red), which we summarize here:\n\n- More details about the base model, FineGAN, that we build upon, including its architecture, loss functions, and the overall image generation process (Section 3.2 and Figure 3). \n- Comparison to StarGANv2 on various datasets (Appendix A.7 and Figures 13, 14).\n- A discussion on DRIT (Section 2).\n- Empirical analysis on how our model\u2019s appearance transfer respects part-level correspondences between two related domains (Appendix A.4.3).\n- More details and ablation studies on some of the hyperparameters we use in our experiments: how we choose $N_x$ (number of shapes) and the temperature parameter (Appendix A.1, A.2 and Figure 9).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "MWN_rEL2Jra", "original": null, "number": 7, "cdate": 1606097529729, "ddate": null, "tcdate": 1606097529729, "tmdate": 1606157092459, "tddate": null, "forum": "M88oFvqp_9", "replyto": "Dj8zw3YARNI", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Authors' response (continued)", "comment": "**Empirical evidence seems limited:**\n\nFirst, we respectfully disagree that the empirical evidence is somewhat limited, since we present experiments on various challenging multi-domain settings, and demonstrate the effectiveness through several measures, including user studies, as shown in Tables 1, 2, 3 and Figures 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, and 14.  Second, the reviewer implies that the datasets used in the paper don\u2019t have enough variety, and consequently one cannot properly study the distance between two domains. We would like to clarify that our goal with this work is not to study how close two domains are: it is to learn a disentangled representation of shape and appearance \u201cregardless\u201d of how similar (or dissimilar) two domains are. As for showing the results on more arbitrary datasets (e.g. humans/cups), we believe that combining factors from a bird and a car (or a car and a dog) is similarly difficult as combining factors from a human and a cup, and hence believe that the dataset settings used for this work do offer significant challenges for a method to succeed. \n\n**Improvements are moderate over the baselines:**\n\nWe respectfully disagree. First, baselines like CycleGAN/MUNIT/AdaIn illustrate clear issues while solving the problem at hand (Figure 5). We have also included a new comparison to StarGANv2 (Figures 13, 14) which again show clear limitations. Second, compared to FineGAN, we achieve superior results qualitatively (Figure 6) and quantitatively (Tables 1 and 2). It would be helpful if the reviewer could point out specific instances where our improvements over FineGAN (or any other baseline) appear moderate.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "Dj8zw3YARNI", "original": null, "number": 6, "cdate": 1606097407177, "ddate": null, "tcdate": 1606097407177, "tmdate": 1606156974250, "tddate": null, "forum": "M88oFvqp_9", "replyto": "Fdm0c6C7kjY", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Thank you for your time and feedback. We address the mentioned concerns below. All references to figures and sections are based on our revised paper pdf. \n\n**Choice of heuristic:**\n\nThe key property that we desire out of our heuristic is that it should not capture any shape information, and only encode appearance properties. Capturing the appearance through a low-level, frequency-based representation helps satisfy this aspect. It ensures that, for example, when transferring the appearance of a car to a dog, we only transfer color/texture blobs, and don\u2019t transfer higher level concepts (e.g., wheels). Furthermore, our method can capture the low level features across a range of spatial resolutions. Figure 4 depicts a special case of our method, where only one layer of convolutional filters (i.e. filter bank) is used to approximate the frequency based appearance representation; however, we can easily stack additional convolutional layers, and use their responses as well while computing the appearance representation (Appendix A.2 - \u201cReceptive field size of the filters\u201d). This increases the effective receptive field of filters in deeper layers, which results in capturing bigger color/texture blobs. The following examples illustrate this point: (i) in Figure 1 (bottom left), the black beak and neck region of the bird; (ii) in Figure 2, the blue/red color blocks in the bird; (iii) in Figure 6 (\u2018ours\u2019; dogs to cars; 2nd row), the yellow/black blocks in the car; (iv) in Figure 6 (\u2018ours\u2019; animals to animals; last row), the brown/black patches of the dog. \n\n**Intra vs inter domain clarification:**\n\nWe define domains to correspond to \u201cbasic-level categories\u201d [1]; e.g. birds vs dogs vs cars. With this definition, images belonging to the same domain typically have more visual similarity than those belonging to different domains. As such, intra-domain disentanglement is an easier task compared to inter-domain disentanglement, since it concerns a single domain, where images are more similar to each other. We agree that certain pairs of domains could be closer than others, e.g. dogs are more similar to cats than cars, which can even have part-level correspondences. We empirically study these settings, as suggested by the reviewer (please see the next section).  We also admit that the boundaries between basic-level categories can be fuzzy (e.g., is a rickshaw a car or a motorbike or neither?) However, the main advantage of our approach is that it is domain agnostic: one does not need to worry whether the two domains are similar or not, as our method can work for both the intra-domain and inter-domain settings.  \n\nRegarding the scenarios where our method wouldn\u2019t work, as explained in Section 3.2, our base model FineGAN makes certain assumptions about the data (existence of a hierarchy between shape and appearance factors) to learn intra-domain disentanglement of factors. Since our method builds upon FineGAN, it is suited for datasets which possess those properties (e.g. birds/dogs), and won\u2019t be as effective otherwise.\n\n**Part-level correspondences preserving appearance:**\n\nWe agree that this claim would benefit from more evidence. To this end, we have included a section in the appendix (A.4.3), where we quantitatively study the effect of transferring appearance when two domains have part-level correspondences, i.e. the animals dataset. In summary, we detect 6 keypoints (nose, two eyes, forehead center, two ears) in the source image, and compute the color histogram for a patch around each of them. When transferring the appearance to another animal, we detect the new keypoints, and see if it has a similar color/texture histogram around the corresponding keypoints. For example, in the second last result of Figure 6 (Ours; animals to animals), the dog from which appearance is borrowed has brown patches over the eyes. Hence, when transferring it to a husky\u2019s shape, the resulting animal should also have brown patches over the eyes, i.e. a similar color/texture distribution around the eye keypoint. We observe that our method better preserves this property compared to the baseline, FineGAN.\n\n**DRIT reference missing:**\n\nThank you for bringing this work to our attention. We have updated the related work section to include its discussion and high level comparison with our problem setting. In summary, the attribute conditioned image-to-image translation application explored in DRIT was limited to cases where the attribute image shares some structure with the content image (e.g., natural and sketch images of face domain as content/attribute images respectively). In this work, we aim to combine factors from entirely different domains, having no structural similarity. \n\n**References**\n\n[1] Principles of categorization. Cognition and Categorization. Eleanor Rosch, pages 27\u201348, 1978"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "vXtaOsfVZ_q", "original": null, "number": 5, "cdate": 1606065142153, "ddate": null, "tcdate": 1606065142153, "tmdate": 1606156864704, "tddate": null, "forum": "M88oFvqp_9", "replyto": "Cr3lUEvLAdD", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Thank you for your time and feedback. We address the mentioned concerns below. All references to figures and sections are based on our revised paper pdf. \n\n**Comparisons to recent image-to-image translation methods (e.g., StarGANv2):**\n\nWe agree that such comparisons would strengthen the results. We\u2019ve hence included StarGANv2 (as suggested by the reviewer) in the related work section, and an empirical comparison in the appendix (A.7) for different datasets. In sum, StarGANv2 struggles to disentangle shape and appearance when the domains are very different (see Figure 13). Even for domains that are similar (animals $\\leftrightarrow$ animals; see Figure 14 left), StarGANv2 is only able to disentangle pose from the other factors (shape and appearance); i.e., when combining the properties from a source and reference image, the object shape and appearance factors are taken from the reference image, and only the object pose factor is taken from the source image (so that a tiger\u2019s pose changes to match the pose of a dog). In contrast, our approach can disentangle shape from appearance so that e.g., a dog is texturized with a leopard\u2019s texture (see Figure 14 right).\n\n**Motivation not clear:**\n\nWe discuss some possible applications of our work in the last paragraph of Section 1. We provide more examples here. Let\u2019s consider the virtual try-on application, where people virtually try different kinds of clothes over the same body figure. One of the desired features is, for example, to keep the same t-shirt type (e.g. round-necked), but borrow it\u2019s appearance from a different t-shirt (with collar). So, the options available to choose the appearance from is limited to the domain of t-shirts. But what if one sees a leopard and wishes to try on a t-shirt with its appearance?  What if one wishes to imagine how a handbag would look with a golden retriever\u2019s fur texture? The reviewer points out that there isn\u2019t much application of applying a car\u2019s appearance to a dog. We agree, but we believe there could be an application for the inverse task: imagining a car (or other man-made objects, e.g. shirts, handbag, furniture) in the appearance of other arbitrary objects (e.g. dog, leopard). The fashion/design industry has been evolving, where people are increasingly interested in unorthodox design choices for a variety of man-made objects. With this work, we aim to provide users with the \"option\" to borrow appearance from a much wider variety of objects (Section 1, last paragraph).\n\nFurthermore, we believe that learning disentangled representations for shape and appearance across multiple domains serves as a more challenging and accurate testbed for evaluating disentanglement. As mentioned in Section 1, last paragraph, if shape and appearance can be combined from different objects within a domain, but not when they belong to different domains, then those factors are not completely independent of each other, and hence not accurately disentangled.\n\n**Using CNN to extract features:**\n\nThe filter banks used to extract appearance features, as illustrated in Figure 4, are in fact implemented using convolutional layers.  Section A.2 in Appendix, \u201cReceptive field size of filters\u201d, provides further details and also experiments with different variants, by using 1 or 2 or 3 layer convolutional networks to extract the appearance based features for constructing the histograms. Furthermore, by computing a histogram-based representation, we remove the spatial structure of the CNN features for an input image, and as a result, the comparison between images is done based only on low-level appearance feature statistics (and not on shape-level information). If we were to instead directly compare the CNN features, then $L_{hybrid}$ would constrain not only the images' appearance feature statistics to be similar, but also their shape features to be similar."}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "oPGLYIMifUl", "original": null, "number": 3, "cdate": 1606064711230, "ddate": null, "tcdate": 1606064711230, "tmdate": 1606156788544, "tddate": null, "forum": "M88oFvqp_9", "replyto": "g-muCRg51kG", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Thank you for your time and feedback. We address the mentioned concerns below. All references to figures and sections are based on our revised paper pdf.\n\n**Novelty of the proposed method vs. the parts of FineGAN it builds upon:** \n\nOur key novelty over FineGAN is that we learn disentangled representations across multiple domains, so that we can combine one factor from one domain (e.g. shape of a car) and another factor from another domain (e.g., appearance of a bird), as mentioned in Section 1 and Figure 2. FineGAN, on the other hand, only addressed learning disentangled representations within a single domain (Figure 3). \n\nIn terms of the approach, we build upon the architecture of FineGAN (Figure 3), and introduce additional layers of filters that are designed to capture a low-level frequency based representation of an object (Figure 4a). To enable this, we introduce additional objective functions ($L_{filter}$ and $L_{hybrid}$), which use contrastive learning to learn the desired filters, and consequently enforce the model to better disentangle shape from appearance. \n\n**Behavior with less prominent or multiple subjects:**\n\nThe base model, FineGAN, is designed to disentangle factors for images that contain a single foreground object. Since we build upon FineGAN, we are also limited in the same way.  Thus, although the multiple subjects scenario is interesting, it is beyond the scope of this work.  In order to extend the approach to work for multi-subject images, we would need to have at a minimum separate latent codes for each object (e.g., a separate background/shape/appearance/pose code for each object in the image).\n\nAs for less prominent objects (i.e., object area small compared to overall image area), our paper does present results where appearance is being borrowed from a relatively small-sized object. For example, (i) Figure 6 (Ours, birds to cars, third row): the yellow bird only occupies a small area of the overall image; (ii) Figure 12 (first row, first column): the greenish/bluish bird forms a small part of the image, but our method still transfers its appearance accurately to the dog.\n\n**How are $N_x$, $N_y$, $N_b$ chosen:**\n\nSection A.1 (paragraph 1) discusses the values used for $N_x$, $N_y$, $N_b$ for different domains. We\u2019ve further added more details pertaining to the choice of these hyperparameters. In sum, we set $N_x$ and $N_b$ to be the total number of ground-truth fine-grained (subordinate) object categories across the two domains, and $N_y$ to be 1/10 of that number.  This follows what was done in the FineGAN paper, which also found that the disentanglement results are largely agnostic to these specific choices.\n\n**How is \u2018sim\u2019 defined in eq. 1?**\n\nsim refers to cosine similarity. We have updated the text to include this detail.\n\n**Information about the temperature parameter:**\n\nWe updated the paper to provide the temperature value, below Equation 1. In our initial experiments, we used the same value of 0.5 for the temperature hyperparameter for all experiments. In Section A.2 (Effect of temperature), we conducted additional experiments where we vary the temperature value. In sum, our method is largely independent of the specific choice of temperature value.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "2kvWofd3dK6", "original": null, "number": 4, "cdate": 1606064842379, "ddate": null, "tcdate": 1606064842379, "tmdate": 1606067184731, "tddate": null, "forum": "M88oFvqp_9", "replyto": "8UBel5tLps2", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment", "content": {"title": "Authors' response", "comment": "Thank you for your time and feedback. All references to figures and sections are based on our revised paper pdf.\n\n**More space describing FineGAN:**\n\nWe agree that the paper could benefit from more details about FineGAN, our base model. We have hence updated Section 3.2 to include more details about FineGAN\u2019s loss functions that constitute $L_{base}$ (their equations and precise inputs), its overall image generation process, and a new figure (Figure 3 left) illustrating its model architecture. We would be happy to further revise the content if needed.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M88oFvqp_9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper12/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper12/Authors|ICLR.cc/2021/Conference/Paper12/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923875198, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Comment"}}}, {"id": "8UBel5tLps2", "original": null, "number": 3, "cdate": 1603929966302, "ddate": null, "tcdate": 1603929966302, "tmdate": 1605024777092, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Review", "content": {"title": "Good results, but paper could benefit from clearer explanation", "review": "The authors build upon prior work (FineGAN) in intra-domain disentanglement to extend to inter-domain transfer of separate attributes. Since no ground truth data exists for inter-domain transfer, they use contrastive losses to enforce similar statistics of low-level filter activations (averaged over the image) as a proxy for appearance similarity. \n\nAppearance transfer experiments on performed on a good selection of datasets and the results of the proposed method represent a qualitative step forward in unsupervised conditional generation across domains. Quantitative metrics support this point.\n\nThat said, the paper could be significantly improved by spending less space motivating and defining the problem, and more space describing the actual method used. The authors mention FineGAN in passing as their base model, but it is essential to the proposed method and could use further elaboration. As is, the relevant details of the losses and architecture choices are not contained within the paper itself. For instance, the loss terms in L_{base} are not defined and no diagrams are given to help understand the workings of the base model.  Similarly, the training procedure is a bit unclear from the text. If the content of Figure 3 were expanded, or a training algorithm table provided, even in the supplemental it would significantly improve the paper by not relying on a reference to provide the description of the core technique.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151631, "tmdate": 1606915764055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper12/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Review"}}}, {"id": "Cr3lUEvLAdD", "original": null, "number": 2, "cdate": 1603874345893, "ddate": null, "tcdate": 1603874345893, "tmdate": 1605024777030, "tddate": null, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "invitation": "ICLR.cc/2021/Conference/Paper12/-/Official_Review", "content": {"title": "Slightly novel approach, but need to improve experiment results", "review": "--Summary:\nThe paper proposed a method to learn disentangled representation of shape and appearance for cross-domain (different object categories) data. Build upon FineGAN, the method uses contrastive learning combined with normalized temperature-scaled cross-entropy loss to further disentangle the shape and appearance information.\n\n--Strongness:\n1. The model is slightly novel. They combine contrastive learning with normalized temperature-scaled cross-entropy loss to learn the filter bank to construct the appearance feature histogram.\n2. They perform many experiments including comparisons with baselines and ablation studies on the proposed loss terms. They demonstrate the effectiveness of their approach to generating hybrid images.\n3. The paper is well-organized.\n\n--Weakness:\n1. The motivation is still unclear. I still don't get the point for the usefulness of appearance transfer across two different types of objects (e.g. car and animal) which they claim as their contribution. For example, I don't see the application for applying car appearance to animals.\n2. The comparison baselines are too old. For the appearance transfer comparisons as shown in Figure 4 are the papers before 2018. For example, why don't you compare your model with StarGANv2[1] which also demonstrates appearance transfer to different shapes (e.g. Figure 10)?\n\n--Questions:\n1. I'm curious about the results if you replace the histogram method by just using CNN to extract features on the masked output from FineGAN?\n\n--Recommendation:\nAlthough the authors demonstrate the effectiveness of the proposed method, there are some concerns to be addressed: \n1) Motivation is not intuitive. \n2) There are many more recent papers for transferring appearance to another shape, e.g. StarGANv2[1], which is not included in the experiment.\n\nI currently vote negatively but the authors are strongly encouraged to address these concerns.\n\n[1] StarGAN v2: Diverse Image Synthesis for Multiple Domains, CVPR'20", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper12/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper12/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains", "authorids": ["~Utkarsh_Ojha1", "~Krishna_Kumar_Singh4", "~Yong_Jae_Lee2"], "authors": ["Utkarsh Ojha", "Krishna Kumar Singh", "Yong Jae Lee"], "keywords": ["multi-domain disentanglement", "generative adversarial networks", "appearance transfer"], "abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.", "one-sentence_summary": "We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ojha|generating_furry_cars_disentangling_object_shape_and_appearance_across_multiple_domains", "pdf": "/pdf/b8b31818deaaa37925dbeb6143231cd0ef62b630.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nojha2021generating,\ntitle={Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains},\nauthor={Utkarsh Ojha and Krishna Kumar Singh and Yong Jae Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=M88oFvqp_9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M88oFvqp_9", "replyto": "M88oFvqp_9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151631, "tmdate": 1606915764055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper12/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper12/-/Official_Review"}}}], "count": 12}