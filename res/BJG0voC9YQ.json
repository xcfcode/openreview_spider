{"notes": [{"id": "BJG0voC9YQ", "original": "BkxG7daDtm", "number": 311, "cdate": 1538087781962, "ddate": null, "tcdate": 1538087781962, "tmdate": 1550062274567, "tddate": null, "forum": "BJG0voC9YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1lsV_JbgN", "original": null, "number": 1, "cdate": 1544775731447, "ddate": null, "tcdate": 1544775731447, "tmdate": 1545354522705, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Meta_Review", "content": {"metareview": "see my comment to the authors below", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting idea, scope is much narrower than presentation would suggest"}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper311/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353261233, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353261233}}}, {"id": "Ske2pWsvg4", "original": null, "number": 7, "cdate": 1545216451760, "ddate": null, "tcdate": 1545216451760, "tmdate": 1545216451760, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "Skl3eOkblN", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Re", "comment": "We thank the area chair for pointing out the references, we will add them to our\nmanuscript. As stated in the response to the reviewers, we agree that our\nexperiments test our algorithm only in the idealized setting of known transition\nand reward kernels and unknown initial state. We will change the wording in the\nintroduction to better reflect the scope of our experiments. We maintain\nhowever, that the underlying idea of inferring scenarios (that can influence all\ntransitions) in hindsight from off-policy data, and re-using these for\ncounterfactual policy evaluation in principle applies to a wider setting. Given\nthe close connection of our proposed algorithm to the GPS algorithm as presented\nby [Levine, Abbeel. 2014], we prefer to keep it's name (CF-GPS) as well as the\ntitle of the paper as is."}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper311/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "Skl3eOkblN", "original": null, "number": 6, "cdate": 1544775667883, "ddate": null, "tcdate": 1544775667883, "tmdate": 1544775667883, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Interesting contribution, title and introduction to reflect more narrow scope ", "comment": "\n\n\nThis is a clear topic of increasing importance to the community-- combining causality / counterfactual reasoning with sequential decision making. The authors draw their perspective from the view of structural causal modeling and it would also be beneficial to reference the body of literature from more of the potential outcomes framework-- see below for a few of these references in the RL counterfactual/ off policy policy evaluation community. In particular, the proposed approach here is general but only instantiated (in terms of inference algorithms and experiments) for when the initial starting state is unknown in a deterministic POMDP environment, where the dynamics and reward model is known. The authors show that they can use inference over the full trajectory (or some multi-time-step subpart) to get a (often delta function) posterior over the initial starting state, which then allows them to build a more accurate initial state distribution for use in their model simulations than approaches that do not use more than 1 step to do so. This is interesting, but it\u2019s not quite clear where this sort of situation would arise in practice, and the proposed experimental results are limited to one simulated toy domain. This is fine, but the title and introduction seem to suggest a much more general contribution, as opposed to this much more restricted (though interesting) setting of inferring the initial starting state distribution when the dynamics and reward model are known. Therefore I encourage the authors to update their title and introduction to narrow the scope of the proposed contribution. \n\nMandel, Liu, Levine, Brunskill, Popovic AAMAS 2014.\nThomas and Brunskill ICML 2016\nSchulam, Saria. NeurIPS 2017\nGuo, Thomas, Brunskill NeurIPS 2017\nParbhoo, Gottesman, Ross, Komorowski, Faisal, Bon, Roth, Doshi-Velez, PloS one 2018\nLiu, Gottesman, Raghu, Komorowski, Faisal, Doshi-Velez, Brunskill NeurIPS 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "SklAR5DLpm", "original": null, "number": 3, "cdate": 1541991125900, "ddate": null, "tcdate": 1541991125900, "tmdate": 1543208674503, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "content": {"title": "Interesting problem and approach; more experimental domains and careful analysis on experiment results would be appreciated.", "review": "Summary:\n\nThis paper proposes a policy evaluation and search method assisted by a counterfactual model, in contrast previous work using vanilla (non-causal) models. With \u201cno model mismatch\u201d assumption the policy evaluation estimator is unbiased. Empirically, the paper compares Guided Policy Search with counterfactual model (CF-GPS) with vanilla GPS, model based RL algorithm and show benefit in terms of (empirical) sample complexity.\n\nMain comments:\n\nThis paper studies several interesting problems: 1) policy learning with off-policy data; 2) model based RL and how to use model to help policy learning. By capturing a nice connection between causal models and MDP/POMDP model with off-policy data, this paper can leverage SCMs to help the model guided policy search in POMDP. The combination of those ideas is novel and enjoyable.\n\nOn the negative side, I find I met several confused points as a reader with more RL background and less causal inference background. It would be better if the authors could clarify what is the prior distribution P(u) and posterior distribution P(u|h) exactly means in terms of CF-PE algorithm and MB-PE algorithm. I would also appreciate if a more detailed proof of corollary 1 and 2 are included in the appendix, and a higher level intuition/justification about those two results in main body. Maybe I am missing these points due to my limited background in causal inference, but I think those clarification can definitely be helpful for RL audience without that much knowledge in causal inference.\n\nThe main theoretical result seems to be based on the assumption of no model mismatch, and I guess here how the model is estimated from sample are ignored, unless I missed anything. Thus I assume the main contribution of this paper should be algorithmic and empirical. I expect to see the empirical study in more domains with more informative results about how this CF model get the benefit of sampling from p(u|h) rather than p(u) (as an evidence to support motivation paragraph on page 5). ", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper311/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "cdate": 1542234490671, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335695834, "tmdate": 1552335695834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxsxWflC7", "original": null, "number": 5, "cdate": 1542623474972, "ddate": null, "tcdate": 1542623474972, "tmdate": 1542623474972, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "B1lQbh_c37", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Re review", "comment": "We added a paragraph on the \u201cauto-regressive uniformization\u201d in the appendix, showing how any joint distribution over random variables can be converted into independent noise variables and deterministic functions. Please also see our reply to reviewer 1.\n\nConcerning the choice of prior $p(u)$ in the experiments: $U$ was defined as the initial state or \u201clevel\u201d of the environment. The prior, as well as the posterior, were chosen to be DRAW latent variable models. The only difference between these models was that the one encoding the posterior was conditioned on observed data $h$. This parametrization is also discussed in the appendix D."}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "ryxd2ezlRQ", "original": null, "number": 4, "cdate": 1542623407962, "ddate": null, "tcdate": 1542623407962, "tmdate": 1542623407962, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "Bye_P5EZT7", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Re review", "comment": "We agree that our algorithm makes strong assumptions about the model, and that we have not yet studied theoretically or experimentally the important question raised by the reviewer of how violations of these assumptions influence performance. \n\nWe want clarify however, that the assumption of the model class consisting of deterministic functions and independent noise variables is not restrictive in itself, any joint probability over random variables can be written in this way by iteratively applying the \u201cinverse-CDF\u201d method. For a joint Gaussian for example, this corresponds to sampling one variable at a time (conditioned on the previous ones) by sampling an RV uniformly in [0,1], passing it through the inverse standard-Gaussian CDF and scaling it with the conditional standard deviation and adding the conditional mean. We added a paragraph in the appendix to clarify this point."}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "H1l_SlGxCX", "original": null, "number": 3, "cdate": 1542623295771, "ddate": null, "tcdate": 1542623295771, "tmdate": 1542623295771, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "SklAR5DLpm", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Re review", "comment": "For improved readability, we added a proof for corollary 1 in the appendix. Corollary 2 is a direct application of lemma 1 to the SCM prepresentation of a POMDP.\n\nConcerning the difference between $p(u)$ vs $(u\\vert h)$:\nStandard model based RL (MBRL) algorithms usually try to learn a model over unobserved variables $U$ of the environment. If there is uncertainty over these given the observations, then a natural approach for MBRL would to learn a distribution, ie prior $p(u)$. At model test time, one usually samples from this prior to generate rollouts for policy evaluation (or learning). This corresponds to the MB-PE procedure. We propose, instead of sampling from the prior, given concrete observed data $h$, sample from the posterior $p(u\\vert h)$, yielding the CF-PE algorithm. As argued in the paper, $p(u\\vert h)$ should be easier to learn than $p(u)$. We hope the \u201cmotivation\u201d paragraphs in the introduction and Ch2 can give an intuitive understanding of the difference."}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "H1xnN1GxAm", "original": null, "number": 2, "cdate": 1542623027775, "ddate": null, "tcdate": 1542623027775, "tmdate": 1542623027775, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "content": {"title": "Reply to reviewers", "comment": "We thanks the reviewers for the their thoughtful comments, some of which we address individually below.\n\nGenerally, we want to emphasize that the main contribution of the paper is to show quantitatively that counterfactual reasoning can be beneficial for learning policies in reinforcement learning, admittedly in a highly idealized but not trivial task. In our opinion, this is an important, novel result, given that humans almost constantly engage in counterfactual reasoning, for which a vague functional role was hypothesised but no learning mechanism has been proposed (see [Roese 97]). \nUltimately, we think our proposed method can contribute to novel methods to the important problem of off-policy learning.\n\nWe are currently working on applying the proposed methods to partially observed problems in continuous control, to study if the observed benefits carry over to less idealized settings."}, "signatures": ["ICLR.cc/2019/Conference/Paper311/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618000, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJG0voC9YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper311/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper311/Authors|ICLR.cc/2019/Conference/Paper311/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers", "ICLR.cc/2019/Conference/Paper311/Authors", "ICLR.cc/2019/Conference/Paper311/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618000}}}, {"id": "Bye_P5EZT7", "original": null, "number": 2, "cdate": 1541651039526, "ddate": null, "tcdate": 1541651039526, "tmdate": 1541651039526, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "content": {"title": "Interesting ideas; unclear if assumptions are too strong", "review": "Summary: by assuming a correct, strongly factored environment model, improved estimators useful for policy search can be derived by \"counterfactual reasoning\", where data sampled from experience is used to refine initial conditions in the model; this translates into improved estimators of policy values, which improves policy search.\n\nMajor comments:\n\nI enjoyed this paper.  I think that model-based RL deserves more work, and I think that this is a simple, reasonably workable approach with some nice theoretical benefits.  I like the idea of SCMs; I like the idea of counterfactual reasoning; I like the idea of leveraging models in this unique way.\n\nOn the negative side, I felt that the paper makes some rather strong assumptions - specifically, that the agent has access to a perfect model with no mismatch, and that the model decomposes neatly into noise variables plus deterministic functions.  Given such a model, one wonders if there are other techniques, say, from classical planning, that could also be used for some sort of policy search.\n\nI have a few questions about approximations.  First, I see that probabilistic inference is a core element of each algorithm (where p(u|h) must be computed).  For large, complex models, I assume this must be approximate inference.  This leads naturally to questions about accuracy (does approximate inference result in biased estimators? [probably yes]), efficacy (do the inaccuracies inherent in approximate inference outweigh the benefits of using p(u|h) vs. p(u)?) and scalability (how large of a model can we reasonably cope with before degradation is unacceptable, or no better than non-CF algorithms?).  As far as I can tell, none of this was addressed in the paper, although I do not expect every paper to answer every question; this is a first step.\n\nI wish the experiments were a little more varied.  The experimental results really only show marginal improvement in one small task.  While I understand that this is not an empirical paper, neither does it fit strongly into the category of \"theory paper\".  For example, there are no theory results indicating what sort of benefit we might expect from using the methods outlined here, and in the absence of such theory, we might reasonably look to various experiments to demonstrate its effectiveness.\n\nPros:\n+ Integration with SCMs is interesting\n+ Counterfactual variants of algorithms are clearly motivated and interesting\n+ Paper is generally well-written\n\nCons:\n- Assumption that the agent is given a model with no mismatch is very strong\n- Model class (noise variables + deterministic functions) seems potentially restrictive\n- Questions about impact of approximate inference\n- Experiments could have been more varied\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper311/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "cdate": 1542234490671, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335695834, "tmdate": 1552335695834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lQbh_c37", "original": null, "number": 1, "cdate": 1541209083395, "ddate": null, "tcdate": 1541209083395, "tmdate": 1541534102715, "tddate": null, "forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "content": {"title": "Interesting approach to relevant problem; nice integration of causal reasoning with RL; experiment setup avoids dealing with some practical challenges", "review": "Summary:\nProposes Counterfactual Guided Policy Search (CF-GPS), which uses counterfactual inference from sampled trajectories to improve an approximate simulator that is used for policy evaluation. Counterfactual inference is formalized with structural causal models of the POMDP. The method is evaluated in partially-observed Sokoban problems. The dynamics model is assumed known, and a learned model maps observation histories to a conditional distribution on the starting state. CF-GPS outperforms model-based policy search and a \"GPS-like\" algorithm in these domains. GPS in MDPs is shown to be a particular case of CF-GPS, and a connection is also suggested between stochastic value gradient and CF-GPS.\n\nReview:\nThe work is an interesting approach to a relevant problem. Related literature is covered well, and the paper is well-written in an approachable, conversational style. \n\nThe approach is technically sound and generally presented clearly, with a few missing details. It is mainly a combination of existing tools, but the combination seems to be novel. \n\nThe experiments show that the method is effective for these Sokoban problems. A weakness is that the setting is very \"clean\" in several ways. The dynamics and rewards are assumed known and the problem itself is deterministic, so the only thing being inferred in hindsight is the initial state. This could be done without all of the machinery of CF-GPS. I realize that the CF-GPS approach is domain-agnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties. The issue of inaccurate dynamics models seems especially relevant, and is not addressed by the Sokoban experiment. It's also notable that the agent cannot affect any of the random outcomes in this problem, which I would think would make counterfactual reasoning more difficult.\n\nComments / Questions:\n* Please expand on what \"auto-regressive uniformization\" is and how it ensures that every POMDP can be expressed as an SCM\n* What is the prior p(U) for the experiments? \n* \"lotion-scale\" -> \"location-scale\"\n\nPros:\n* An interesting and well-motivated approach to an important problem\n* Interesting connections to GPS in MDPs\n\nCons:\n* Experimental domain does not \"exercise\" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known\n* Work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper-)parameter settings ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper311/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.", "keywords": ["reinforcement learning", "generative models", "model-based reinforcement learning", "causal inference"], "authorids": ["lbuesing@google.com", "theophane@google.com", "yori@google.com", "heess@google.com", "sracaniere@google.com", "aguez@google.com", "jblespiau@google.com"], "authors": ["Lars Buesing", "Theophane Weber", "Yori Zwols", "Nicolas Heess", "Sebastien Racaniere", "Arthur Guez", "Jean-Baptiste Lespiau"], "pdf": "/pdf/89825a97bda724af6db07fb3af839ea67973e72d.pdf", "paperhash": "buesing|woulda_coulda_shoulda_counterfactuallyguided_policy_search", "_bibtex": "@inproceedings{\nbuesing2018woulda,\ntitle={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},\nauthor={Lars Buesing and Theophane Weber and Yori Zwols and Nicolas Heess and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJG0voC9YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper311/Official_Review", "cdate": 1542234490671, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJG0voC9YQ", "replyto": "BJG0voC9YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper311/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335695834, "tmdate": 1552335695834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper311/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}