{"notes": [{"id": "rJgDT04twH", "original": "HyeC1McdDB", "number": 1396, "cdate": 1569439422623, "ddate": null, "tcdate": 1569439422623, "tmdate": 1577168285147, "tddate": null, "forum": "rJgDT04twH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Bb8Co6P16F", "original": null, "number": 1, "cdate": 1576798722236, "ddate": null, "tcdate": 1576798722236, "tmdate": 1576800914350, "tddate": null, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "invitation": "ICLR.cc/2020/Conference/Paper1396/-/Decision", "content": {"decision": "Reject", "comment": "The paper explores the idea of using implicit human feedback, gathered via EEG, to assist deep reinforcement learning. This is an interesting and at least somewhat novel idea. However, it is not clear that there is a good argument why it should work, or at least work well. The experiments carried are more exploratory than anything else, and it is not clear that much can be learned from the results. It's a proof of concept more than anything else, of the type that would work well for a workshop paper. More systematic empirical work would be needed for a good conference paper.\n\nThe authors did not provide a rebuttal to reviewers, but rather agreed with their comments and that the paper needs more work. In light of this, the paper should be rejected and we wish the authors best of luck with a new version of the paper.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726978, "tmdate": 1576800279173, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1396/-/Decision"}}}, {"id": "B1gR7wt3sH", "original": null, "number": 1, "cdate": 1573848870234, "ddate": null, "tcdate": 1573848870234, "tmdate": 1573848870234, "tddate": null, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "invitation": "ICLR.cc/2020/Conference/Paper1396/-/Official_Comment", "content": {"title": "Thank you for the feedback and suggestions.", "comment": "We thank the reviewers for their valuable feedback. The concerns pointed out by the reviewers are helpful to strengthen our contributions. Since the major issues with the current version demand conducting more experiments, we have decided not to provide a rebuttal response and would be working to incorporate the feedback by reviewers into our work for the future iterations of this work. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1396/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1396/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgDT04twH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1396/Authors", "ICLR.cc/2020/Conference/Paper1396/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1396/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1396/Reviewers", "ICLR.cc/2020/Conference/Paper1396/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1396/Authors|ICLR.cc/2020/Conference/Paper1396/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156641, "tmdate": 1576860533392, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1396/Authors", "ICLR.cc/2020/Conference/Paper1396/Reviewers", "ICLR.cc/2020/Conference/Paper1396/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1396/-/Official_Comment"}}}, {"id": "Bkl3cLLIFS", "original": null, "number": 1, "cdate": 1571346067528, "ddate": null, "tcdate": 1571346067528, "tmdate": 1572972474257, "tddate": null, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "invitation": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces several methods for training reinforcement learning agents from implicit human feedback gained through the use of EEG sensors affixed to human subjects. The EEG data is interpreted into error-related event potential which are then incorporated as a form of noisy feedback for training three different reinforcement learning agents. The first agent (full-access) requires feedback for each state-action selected. The second agent (active-learning) requires feedback on trajectories generated every N_E episodes. The third (learning from imperfect demonstrations) requires the human to provide EEG feedback over an initial set of demonstration trajectories, and subsequently requires no further human access. These methods are evaluated across several handmade gridworld-like environments including a 10x10 maze, a game of catch, and 1-D cursor game called Wobble. Using the EEG training procedures is shown to improve the speed of reaching a good RL policy for all of the three different training algorithms. Additional experiments are conducted to test the generalizability of the error-related event potentials across games, with results indicating a reasonable degree of generalization.\n\nI like the idea of using EEG a way to reduce the burden of collecting human feedback for training reinforcement learning agents. This paper does a good job of investigating several different methodologies for combining ErrP feedback into the main loop of DQN-based RL agents. Additionally, the fact that ErrP feedback seems to generalize between domains is a promising indicator that a single person may be able to provide feedback across many different domains without re-training the ErrP decoder. While I like the paper as an interesting idea and proof of concept, there are some flaws that make me doubt it would be realizable for more complex tasks.\n\nThe drawback of this paper are the many open questions relating to the experiments:\n\n1) In Figures 4b, 6b, and 6d, what is the meaning of 'Complete Episode'?\n\n2) In order to assess how efficient each of these methods was in terms of the number of human labels required, how many human responses were needed for the \"full-access\" and \"First Framework\" experiments?\n\n3) In Figure 6 - what happened to the \"No Errp\" baseline?\n\n4) In Figure 5c - what are Game 1 and Game 2?\n\n5) Why are all the results shown on the Maze domain? Why are no results shown for Catch or Wobble?\n\n6) At an action speed of 1.5 seconds per action, I imagine that EEG is not much faster than having a human subject press a button to indicate their label. What prevents the use of faster speeds?\n\nMore broadly, I think it would be interesting to compare how effective is EEG at collecting human preferences versus pressing buttons (such as in Knox et al) or selecting preferences between trajectories (as in Christiano et al)? \n\nIt's my feeling that the experiments are more of a proof of concept and many open questions exist about whether this method would scale beyond these simple domains that DQN masters in ~300 episodes. In particular, scaling up to actual Atari games as a would go a long way towards showing scalability to a well-studied RL domain.\n\nI thought the overall clarity of the writing was somewhat lacking with many grammatical mistakes throughout, and the necessity to refer repeatedly to the Appendices in order to understand the basic functioning of the RL algorithms and reward learning (7.4). It took several passes to understand the full approach."}, "signatures": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575760448844, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1396/Reviewers"], "noninvitees": [], "tcdate": 1570237737993, "tmdate": 1575760448862, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review"}}}, {"id": "Syg9sWlRFH", "original": null, "number": 2, "cdate": 1571844513996, "ddate": null, "tcdate": 1571844513996, "tmdate": 1572972474221, "tddate": null, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "invitation": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose using implicit human feedback, by means of error potential (ErrP) measured using an EEG cap, as an additional input in deep reinforcement learning. The authors first argue that even though feedback is implicit, it is still costly. To overcome this, they propose: (i) using active learning, i.e., an acquisition function, during training in order to reduce the need of ErrP's and (ii) building a model of ErrP that they fit during training and then use to simulate ErrP during test. The authors evaluate their ideas in a real experiment which consists of game playing with three different games and argue that ErrP generalize across games.\n\nThe use of ErrP in RL appears novel, however, there are several concerns that prevent me from recommending acceptance:\n\n1. The motivation for the use of ErrP is very weak. The authors claim that it is better to use ErrP than explicit feedback by users. However, to use ErrP, the humans have to still pay attention and follow the game. In the BCI literature, ErrP have been used mainly in the context of people with disabilities who are unable to provide explicit feedback. \n\n2. The authors use known methods to detect ErrP. As a result, basically, their system just receives a binary signal which indicates the presence of ErrP and the methodological contribution in this paper is minimal.\n\n3. Given the minimal methodological contribution, the experiments should be much more thorough. \n\n4. The authors only provide a hand wavy visual validation of the consistency of the potentials.\n\n5. In section 5.2.1, the authors talk about AUC in the first paragraph, however, they do not clearly specify what was the prediction/classification task. It is also unclear overall what is the \"success rate\". Mor generally, the authors do not provide sufficient details of their experimental setup and evaluation metrics.\n\nMinor comment: Last paragraph of section 2, adpoted -> adopted. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575760448844, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1396/Reviewers"], "noninvitees": [], "tcdate": 1570237737993, "tmdate": 1575760448862, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review"}}}, {"id": "Hyx8T-E-qS", "original": null, "number": 3, "cdate": 1572057533746, "ddate": null, "tcdate": 1572057533746, "tmdate": 1572972474167, "tddate": null, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "invitation": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles the problem of obtaining feedback from humans for training RL agents, in a way that does not require too much time or mental burden. The proposed approach does this by using implicit human feedback, obtained by measuring EEG signals from the brains of human observers. When humans spot an error (e.g., in the behaviour of the RL agent), an error potential (ErrP) is generated, which differs from human to human. The proposed approach first uses supervised learning to learn what the ErrP looks like for a particular person, and then uses this to provide an additional reward signal to the agent -- the agent receives a negative reward for state-action pairs that result in an ErrP.\n\nThe goal of this paper is to show that using implicit human feedback via ErrPs is a feasible way to speed up the training of RL agents. Towards this, the paper makes two main contributions: (1) experimental evidence that the ErrP for a human can be learned for one game, and transferred zero-shot to other games, and (2) two approaches of collecting this implicit feedback from humans for a smaller number of trajectories, instead of collecting it for all trajectories encountered by the agent during learning. The experimental evaluations, on three simple Atari-like domains, show that agents learn more quickly with implicit feedback, even when gathered for a small number of trajectories, compared to not having any feedback.\n\nThis paper is well-motivated, and tackles an important problem. However, I have several concerns related to feasibility and reproducibility, as described below. The writing also needs quite a bit of editing; there are typos, grammatical errors, and incorrect / missing figure and section references.\n\nRegarding feasibility, my first concern is that the domains considered here are relatively simple. Is there evidence that learned ErrPs would also generalize between tasks with more complex visuals? Also, in order to obtain these error signals, agent trajectories need to be slowed down significantly (from multiple actions a second to one action every 1.5 seconds). I imagine this would quickly become very tedious for people. This seems to be an inherent limitation of this approach, because relevant EEG signals happen up to 1+ seconds after an event occurs. I have doubts that this would generalize to more complex, long-horizon tasks, in which trajectories are at least hundreds of timesteps long. Finally, the study was conducted on a small number of participants (five), with a narrow range of ages and limited gender diversity. I sympathize with the difficulty in obtaining subjects for such studies, but the small sample size makes it questionable whether these results on transferability of learned ErrPs would apply to the wider population.\n\nReproducibility questions:\n- How are the labels (of error vs. not error) obtained for learning per-person error potentials? For more complex domains, it seems that it would be harder to obtain these ground-truth labels, because if there are substantially different strategies for playing the game, humans could disagree on which state-action pairs they consider to be errors.\n- In Section 4.2, how exactly does the RL agent use the saved state-action error labels (from queries to the human) to determine if the current state-action pair is an error? Is it using some form of nearest-neighbor? The notion of buckets isn't clearly described.\n\nAdditional questions and comments:\n- Why does Wobble have the worst AUC for learning ErrPs (Figure 5a)? It seems to be the game that has the most obvious errors (i.e., agent moves away from the target), so I would expect it to have the best AUC.\n- In Section 5.2.1 (Generalizability), what are hypotheses for *why* the ErrP trained on Wobble generalizes better to certain situations in Catch and worse for others? It would be useful to be able to characterise / predict how good generalization will be from one type of game / situation to another.\n- I would like to see reports of wall-clock time for collection of implicit human feedback, for full access and the two other approaches. In other words, how long do humans have to spend connected to the electrodes?\n- I'm curious how imperfect the demonstrations need to be, in order for the ErrPs for those demonstrations to provide a useful reward signal to the agent. In the paper, incorrect actions appear with a probability of 0.2, but there's no explanation for how this number was selected, and whether others were tested.\n\nMinor comments:\n- The diagram in Figure 2a is confusing because DQN is there twice \u2014- explicitly as a component, and implicitly as part of the RL agent.\n- The x-axis labels in Figure 5b don't agree with the subject labels in Figures 4b and 7b.\n- In Figure 6, data for only two of the five participants is reported; it would be useful to include the results for the other participants in the Appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1396/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Reinforcement Learning with Implicit Human Feedback", "authors": ["Duo Xu", "Mohit Agarwal", "Raghupathy Sivakumar", "Faramarz Fekri"], "authorids": ["dxu3016@gatech.edu", "me.agmohit@gatech.edu", "siva@ece.gatech.edu", "faramarz.fekri@ece.gatech.edu"], "keywords": ["Error-Potentials", "Implicit Human Feedback", "Deep Reinforcement Learning", "Human-assistance"], "TL;DR": "We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.", "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human's intrinsic reactions to the agent's behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent's learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials  of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work:\n(i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials.  \n(ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent.\n(iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.\n", "code": "https://drive.google.com/open?id=1hxyFLA86nOaLZo0XDrtGPjeeTWJgwU03", "pdf": "/pdf/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "paperhash": "xu|deep_reinforcement_learning_with_implicit_human_feedback", "original_pdf": "/attachment/44cda98831ea9ff4ea352a0b5970ebf2ff94b609.pdf", "_bibtex": "@misc{\nxu2020deep,\ntitle={Deep Reinforcement Learning with Implicit Human Feedback},\nauthor={Duo Xu and Mohit Agarwal and Raghupathy Sivakumar and Faramarz Fekri},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgDT04twH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgDT04twH", "replyto": "rJgDT04twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1396/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575760448844, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1396/Reviewers"], "noninvitees": [], "tcdate": 1570237737993, "tmdate": 1575760448862, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1396/-/Official_Review"}}}], "count": 6}