{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1492520106858, "tcdate": 1478283795802, "number": 291, "id": "Sy2fzU9gl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sy2fzU9gl", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "content": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396484404, "tcdate": 1486396484404, "number": 1, "id": "B13rhf8de", "invitation": "ICLR.cc/2017/conference/-/paper291/acceptance", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396484970, "id": "ICLR.cc/2017/conference/-/paper291/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396484970}}}, {"tddate": null, "tmdate": 1485017266333, "tcdate": 1485017266333, "number": 1, "id": "Hk92efZwl", "invitation": "ICLR.cc/2017/conference/-/paper291/official/comment", "forum": "Sy2fzU9gl", "replyto": "B1vdd9-8g", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "content": {"title": "Comments to response", "comment": "Thank you for the authors' detailed reply. The descriptions of the evaluation metric are much better. \n\nThe discussion in A.8 is very helpful. In the meanwhile, learning disentangled features with beta-VAE on relatively discontinuous data also seems to be difficult. \n\nFor Table 2, the results are fine, but also a bit mixed (compared to PCA). Moreover, in order to show the degree of disentanglement, it is also important to show the classification rate using features mismatched with the tasks (e.g., id recognition using rotation features). I understand it may be cumbersome to report all possible combinations, but focusing on one or two recognition tasks (e.g., the id recognition) is definitely doable. \n\nIn general, I recommend acceptance for this paper, because 1) disentangling factors of variations in an unsupervised way is important; 2) the visualization results are interesting. In the meanwhile, the quantitive results and practical significance are not so strong as expected (I am not saying they are too weak). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637118, "id": "ICLR.cc/2017/conference/-/paper291/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637118}}}, {"tddate": null, "tmdate": 1484059206402, "tcdate": 1484059206402, "number": 8, "id": "rkCBMdz8x", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "By7JdqZLx", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Small correction to response to AnonReviewer1", "comment": "Sorry, we initially entered wrong cross-correlation values for VAE and beta-VAE in the original response, but have now corrected them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1484058919307, "tcdate": 1484003291200, "number": 5, "id": "By7JdqZLx", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "HyRZoSLVe", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Response to AnonReviewer1", "comment": "We\u2019d like to thank the reviewer for their thorough review and helpful suggestions.\n\n1. We acknowledge the importance of clearly explaining our disentangled metric and we agree that our initial attempt can be improved. In order to address that, we added three new paragraphs at the beginning of Section 3 to provide an intuitive explanation for our disentanglement metric. We also rewrote the end of that section to simplify the formal presentation of the metric. We hope this provides enough background and support for why we devised it that way and what exactly it is assessing.\n\n2. With respect to the bottom 50% question: we have added a sentence at the end of the second paragraph of Appendix A.4 to address it. The bottom 50% of the resulting scores were discarded to control for the outlier results from the few experiments that diverged during training.\n\n3. Thank you for the suggestion. The cross-correlation fails to fully capture the properties we would look for in a disentanglement metric, as we now discuss in the new first 3 paragraphs of Section 3.\nWe did calculate it though (average absolute values of upper triangular terms in correlation matrix, estimated over the entire dataset), but unfortunately found that the cross-correlation does not seem to provide an appropriate measure (we\u2019re also concerned with the large standard deviations). \nHere are the values we obtained:\nPCA: 4.7e-9 (std 5e-9) // ICA: 1.15e-6 (std 9e-7) // DC-IGN: 0.15 (std 0.16) // \nInfoGAN: 0.09 (std 0.08) // VAE 0.039 (std 0.04) // beta-VAE: 0.057 (std: 0.053)\nAs expected, PCA and ICA produce the least correlated latents, since these approaches directly optimise for reducing cross-correlation between latents. The other approaches, including the semi-supervised DC-IGN, produce more correlated latents. However, these are nevertheless disentangled according to a human interpretation of what disentangled factors should look like.\nThis suggests that humanly interpretable disentangled factors of variation are still somewhat correlated in the real world, which suggests that PCA and ICA may be optimising the wrong objective for interpretable disentangling.\nHence we think that our metric provides a better test for interpretable disentangled representations, which a simple cross-correlation fails to capture.\n\n4. We have added a sentence in the relevant paragraph in Section 4.2. If $\\beta$ is too high and the resulting capacity of the latent channel is lower than the number of data generative factors, then the learnt representation necessarily has to be entangled (as we are compressing more than what a factorial representation would require).\n\nWe hope this addresses the reviewer\u2019s concerns and welcome any further feedback.\n\n[EDIT: sorry, we initially entered wrong cross-correlation values for VAE and beta-VAE, but have now corrected them]"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1484003517492, "tcdate": 1484003517492, "number": 7, "id": "HJrTOqWUx", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "H16z7IT4l", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for his work, but given their review we are afraid they may have misunderstood the point of our paper and did not provide a fair assessment of our contribution.\nWe hope our responses below and the comments of the other reviewers may help clarify the scope of our work and its significance.\n\n1. We do not present any quantitative result on log-likelihood estimation, because it is not the main point of our paper (as also pointed out by AnonReviewer3). This also relates to the reviewer\u2019s comment about the quality of beta-VAE samples and how they compare to those by InfoGAN. The quality of reconstructions of beta-VAE and InfoGAN are in line with the base methods the two approaches are built upon - the VAE and GAN. Hence our reconstructions are blurrier than those of InfoGAN due to the limitations of VAEs. Saying this, InfoGAN suffers from the shortcomings of GANS, such as training instability and mode jumping. We stress again: we do not strive in this paper for good quality reconstructions; the main point of our paper is the quality of the learned representations i.e. the disentangling capability of our approach, and in this regard we clearly demonstrate that our method outperforms InfoGAN, which is the current state of the art fully unsupervised method for disentangled factor learning. In addition, we address the other main shortcoming of the InfoGAN paper (and indeed, most other unsupervised representation-learning papers): a lack of justification for the conclusion apart from weak empirical evidence, which we address by our quantitative disentanglement metric.\n\n2. The benefits of the representations learnt by beta-VAE over those learnt by a standard VAE come from their disentangled property. The benefits of disentangled representations have been argued for by many, including Bengio (2013) and most recently in a review article on representation learning that has just come out on arxiv (https://arxiv.org/abs/1612.05299). These benefits include novel example generation through interpretable factor recombination (problems that arise from recombining entangled factors can be seen in the reduced quality of samples generated by an entangled VAE vs a disentangled beta-VAE as shown in Figure 9, which we have now added to the appendix), compression (according to the minimum description length principle), and performance in zero-shot inference, transfer and domain adaptation scenarios. We have amended our introduction to present these benefits more clearly.\nOur current manuscript describes a new state of the art framework for disentangled factor learning. We leave the demonstration of benefits that arise from learning such disentangled representations to future work. We have, however, added a new section in Appendix A.5, as well as Table 2 to describe the performance of a simple classifier trying to predict the data generative factor values using different latent representations. We find that such a classifier performs significantly better using the disentangled representation of beta-VAE compared to the entangled representations of VAE.\n\n3. We did not cross-validate the parameter \\beta, as this doesn\u2019t really make sense in an unsupervised learning setting. Indeed, as is usually the case in the literature (see InfoGAN paper for example), we train on the full dataset and assess characteristics of the representation obtained. \nAs we mention in the text, in this current work we chose \\beta using visual assessments of the latent traversals. Alternatively, we also mention that our disentanglement metric score could be used for this purpose. This is validated by the fact that \\beta values found to be optimal through the visual assessment heuristic on the dataset of 2D shapes were also found to be within the optimal range of \\beta\u2019s as assessed with the disentanglement metric. We have now also included Appendix A.5 as further validation; in this Appendix, we use the \\beta-VAE representation to directly predict the values of the ground truth factors on the 2D shapes dataset with a linear classifier. Although this does not measure disentangling directly (and hence is not used as our preferred disentangling metric), it further backs up the efficacy of \\beta-VAE relative to other models in the literature.\nAdditionally, we always train multiple networks with random initialisations of weights for every value of \\beta we tried (as can be seen in Figure 6). We did not see any strong variations in their behaviour, and hence we believe that \\beta is not a particularly sensitive parameter.\n\n4. We have trained VAEs with different latent sizes (z_size \\in [2, 10]) and found that reducing z_size was linearly *reducing* the disentangling metric score. \nIn particular, a VAE with the \u201coptimal\u201d z_size (e.g. z_size = 5 for our 2D shape dataset, which has 5 ground truth factors of variation) was not able to learn latents corresponding to the ground truth generative factors, in contrast with the beta-VAE (as shown in Fig 7 and by the disentangling metric score).\nHence beta-VAE is indeed different from a VAE with a reduced/optimal latent channel capacity.\n\n\nWe hope this addresses the reviewer\u2019s concerns and welcome any further feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1484003438872, "tcdate": 1484003438872, "number": 6, "id": "B1vdd9-8g", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "B1-0khZEl", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Response to AnonReviewer3", "comment": "We are very grateful to the reviewer for their very thorough assessment of our work and the numerous suggestions. We have made a number of changes to fully address them.\n\n1. About our presentation of our disentanglement metric, this was a concern shared with Reviewer 3. We have now addressed this by introducing three new paragraphs at the start of Section 3 to provide an intuitive explanation of our disentanglement metric. We also rewrote the end of Section 3 to simplify the presentation.\n\n2. Z_diff is now properly defined, sorry for this oversight.\n\n3. We changed \u2018decode\u2019 to \u2018simulate\u2019 and \u2018re-encode\u2019 to \u2018infer\u2019 in order to avoid confusion between our disentanglement metric and the trained decoder, thank you for mentioning it.\n\n4. We have added a sentence in Conclusions with a reference to Section A.8 in the Appendix. This section discusses the effects of data generative factor sample density on the learnt representations of beta-VAE, with empirical results.\n\n5. We have added extra traversal plots in the appendix, with more seeding images. These include all latents that learnt to be informative when presented with 3D chairs or CelebA. \n\n6. About the generalizability of our model: currently all seeding images are from the same full training set. As done in this literature [e.g. InfoGAN], we did not hold out any test set. This would involve handling a domain adaptation scenario, which we are interested in tackling but decided to leave for future work.\n\n7. On the topic of running the disentanglement metric on real datasets: we explored the possibility on the CelebA dataset, but did not report a score for two main reasons. \nFirst, as the reviewer suggested, many of the labels in the dataset are not those that could be reasonably expected to be disentangled, and indeed may not correspond to ground truth factors of variation. For example, \u2018attractiveness\u2019 is such a label. \nSecondly, even though there were a (small) subset of labels that could potentially disentangle well, these labels do not necessarily correspond to factors that control for large visual changes in the subject\u2019s faces. Visually, we feel that the primary factors of variation are \u201cbackground colour\u201d, \u201cazimuth/rotation\u201d, \u201cfacial identity\u201d, and \u201chair colour\u201d. Of these, only hair colour was present as an available label (albeit with quite poor labelling, e.g. many subjects did not have a particular hair colour specified). \nIn future work we would hope to demonstrate the efficacy of the metric on better labeled datasets, such as Caltech-UCSD Birds (http://www.vision.caltech.edu/visipedia/CUB-200.html) or Oxford Flowers (http://www.robots.ox.ac.uk/~vgg/data/flowers/).\n\n8. The reviewer mentions using another simpler protocol to assess our disentanglement metric, this is indeed a great suggestion.\nWe have added a new section in Appendix A.5, with a new Table 2, to describe the performance of a simple classifier trying to predict the data generative factor values of the 2D shapes dataset using different latent representations.\nWe find that the results of this classification test match those of our proposed disentanglement metric, thus providing further validation for it as the reviewer was suggesting.\n\n9. We are not entirely sure why PCA representations are more disentangled than ICA representations according to our metric. Both approaches score similarly well in terms of cross correlations between the latents, as would be expected (as mentioned to AnonReviewer1). According to our new Table 2 in Appendix A.5, the independent components learnt by ICA tend to be very well suited for object identity identification, while those learnt by PCA are more representative of the object location.\n\n10. Thank you for finding the mention of the \u201cfactor change accuracy\u201d term, this indeed was a typo. We updated Fig.6 to use \u201cDisentanglement Metric Score\u201d as expected.\n\n11. We also now added a reference to Appendix A.4 in the main text, sorry for the oversight.\n\n12. Finally about your points following the mention of Figure 6 (right):\nWe have included random samples from both disentangled and entangled models in Appendix A.9, Figure 9.\nAs for latent traversal visualisations for both conditions, this is actually what can already be seen in Fig. 7, panels A and B, if we understood the question correctly?\n\nWe hope this addresses the reviewer\u2019s concerns and welcome any further feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1483757795647, "tcdate": 1483756914485, "number": 1, "id": "Hk9OrCTHl", "invitation": "ICLR.cc/2017/conference/-/paper291/public/review", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["~Galin_Georgiev1"], "readers": ["everyone"], "writers": ["~Galin_Georgiev1"], "content": {"title": "Asking very important questions of non-linear PCA (learning latent manifolds of datasets), often way beyond even human capabilities. Offering a toy solution, partly superseded by existing approaches. Non-scalable to real-life, non-toy data-sets.", "rating": "7: Good paper, accept", "review": "The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question.\n\nThe paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will  testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. \n\nThis of course does not handle complex factors like \"type of legs\" but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!)\n\nBy complete luck, the authors may have hit upon something else of fundamental importance: the letter \"beta\" for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative \"energy\" and \"entropy\" and is  partially addressed in section 2.9 of reference [1]. The correct definition of \"generative temperature\" is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the \"correct\" physical model instead.\n\nA question to the authors: creating a new metric (\"disentanglement\") as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along  different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])?\n\n[1] https://arxiv.org/pdf/1508.06585v5.pdf\n[2] https://arxiv.org/pdf/1511.02841v3.pdf", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1483756915058, "id": "ICLR.cc/2017/conference/-/paper291/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com", "ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs", "~Galin_Georgiev1"], "cdate": 1483756915058}}}, {"tddate": null, "tmdate": 1482674964613, "tcdate": 1482674964613, "number": 3, "id": "H16z7IT4l", "invitation": "ICLR.cc/2017/conference/-/paper291/official/review", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes beta-VAE which strengthen the KL divergence between the recognition model and the prior to limit the capacity of latent variables while sacrificing the reconstruction error. This allows the VAE model to learn more disentangled representation. \n\nThe main concern is that the paper didn't present any quantitative result on log likelihood estimation. On the quality of generated samples, although the beta-VAE learns disentangled representation, the generated samples are not as realistic as those based on generative adversarial network, e.g., InfoGAN. Beta-VAE learns some interpretable factors of variation, but it still remains unclear why it is a better (or more efficient) representation than that of standard VAE.\n\nIn experiment, what is the criteria for cross-validation on hyperparameter \\beta?\n\nThere also exists other ways to limit the capacity of the model. The simplest way is to reduce the latent variable dimension. I am wondering how the proposed beta-VAE is a better model than the VAE with reduced, or optimal latent variable dimension.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482674965507, "id": "ICLR.cc/2017/conference/-/paper291/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper291/AnonReviewer3", "ICLR.cc/2017/conference/paper291/AnonReviewer1", "ICLR.cc/2017/conference/paper291/AnonReviewer2"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482674965507}}}, {"tddate": null, "tmdate": 1482214150451, "tcdate": 1482214150451, "number": 2, "id": "HyRZoSLVe", "invitation": "ICLR.cc/2017/conference/-/paper291/official/review", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer1"], "content": {"title": "Simple and effective", "rating": "7: Good paper, accept", "review": "Summary\n===\n\nThis paper presents Beta-VAE, an augmented Variational Auto-Encoder which\nlearns disentangled representations. The VAE objective is derived\nas an approximate relaxation of a constrained optimization problem where\nthe constraint matches the latent code of the encoder to a prior.\nWhen KKT multiplier beta on this constraint is set to 1 the result is the\noriginal VAE objective, but when beta > 1 we obtain Beta-VAE, which simply\nincreases the penalty on the KL divergence term. This encourages the model to\nlearn a more efficient representation because the capacity of the latent\nrepresentation is more limited by beta. The distribution of the latent\nrepresentation is rewarded more when factors are independent because\nthe prior (an isotropic Gaussian) encourages independent factors, so the\nrepresentation should also be disentangled.\n\nA new metric is proposed to evaluate the degree of disentanglement. Given\na setting in which some disentangled latent factors are known, many examples\nare generated which differ in all of these factors except one. These examples\nare encoded into the learned latent representation and a simple classifier\nis used to predict which latent factor was kept constant. If the learned\nrepresentation does not disentangle the constant factor then the classifier\nwill more easily confuse factors and its accuracy will be lower. This\naccuracy is the final number reported.\n\nA synthetic dataset of 2D shapes with known latent factors is created to\ntest the proposed metric and Beta-VAE outperforms a number of baselines\n(notably InfoGAN and the semi-supervised DC-IGN).\n\nQualitative results show that Beta-VAE learns disentangled factors\non the 3D chairs dataset, a dataset of 3D faces, and the celebA dataset\nof face images. The effect of varying Beta is also evaluated using the proposed\nmetric and the latent factors learned on the 2D shapes dataset are explored\nin detail.\n\n\nStrengths\n===\n* Beta-VAE is simple and effective.\n\n* The proposed metric is a novel way of testing whether ground truth factors\nof variation have been identified.\n\n* There is extensive comparison to relevant baselines.\n\n\nWeaknesses\n===\n\n* Section 3 describes the proposed disentanglement metric, however I feel\nI need to read the caption of the associated figure (I thank for adding\nthat) and Appendix 4 to understand the metric intuitively or in detail.\nIt would be easier to read this section if a clear intuition preceeded\na detailed description and I think more space should be devoted to this\nin the paper.\n\n* Appendix 4: Why was the bottom 50% of the resulting scores discarded?\n\n* As indicated in pre-review comments, the disentanglement metric is similar\nto a measure of correlation between latent features. Could the proposed metric\nbe compared to a direct measure of cross-correlation between latent factors\nestimated over the 2D shapes dataset?\n\n\n* The end of section 4.2 observes that high beta values result in low\ndisentanglement, which suggests the most efficient representation is not\ndisentangled. This seems to disagree with the intuition from the approach\nsection that more efficient representations should be disentangled. It would\nbe nice to see discussion of potential reasons for this disagreement.\n\n* The writing is somewhat dense.\n\n\nOverall Evaluation\n===\nThe core idea is novel, simple and extensive tests show that it is effective.\nThe proposed evaluation metric is novel might come into broader use.\nThe main downside to the current version of this paper is the presentation,\nwhich provides sufficient detail but could be more clear.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482674965507, "id": "ICLR.cc/2017/conference/-/paper291/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper291/AnonReviewer3", "ICLR.cc/2017/conference/paper291/AnonReviewer1", "ICLR.cc/2017/conference/paper291/AnonReviewer2"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482674965507}}}, {"tddate": null, "tmdate": 1481912264622, "tcdate": 1481912264622, "number": 1, "id": "B1-0khZEl", "invitation": "ICLR.cc/2017/conference/-/paper291/official/review", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "content": {"title": "Very interesting results, but more details and more quantitative results are needed", "rating": "6: Marginally above acceptance threshold", "review": "\nThis paper proposes the beta-VAE, which is a reasonable but also straightforward generalization of the standard VAE. In particular, a weighting factor beta is added for the KL-divergence term to balance the likelihood and KL-divergence. Experimental results show that tuning this weighting factor is important for learning disentangled representations. A linear-classifier based protocol is proposed for measuring the quality of disentanglement. Impressive illustrations on manipulating latent variables are shown in the paper. \n\nLearning disentangled representations without supervision is an important topic. Showing the effectiveness of VAE for this task is interesting. Generalizing VAE with a weighting factor is straightforward (though reformulating VAE is also interesting), the main contribution of this paper is on the empirical side. \n\nThe proposed protocol for measuring disentangling quality is reasonable. Establishing protocol is one important methodology contribution of this paper, but the presentation of Section 3 is still not good. Little motivation is provided at the beginning of Section 3. Figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation). Moreover, 1) z_diff appeared without any definition in the main text. 2) Use \u201cdecoding\u201d for x~Sim(v,w) may make people confuse the ground truth sampling procedure w ith the trained decoder. \n\nThe illustrative figures on traversing the disentangled factor are impressive, though image generation quality is not as good as InfoGAN (not the main point of this paper). However, 1) it will be helpful to discuss if the good disentangling quality only attribute to the beta factor and VAE framework. For example, the training data in this paper seems to be densely sampled for the visualized factors. Does the sampling density play a critical role? 2) Not too many qualitative results are provided for each experiment? Adding more figures (e.g., in appendix) to cover more factors and seeding images can strength the conclusions drawn in this paper. 3) Another detailed question related to the generalizability of the model: are the seeding image for visualizing faces from unseen subjects or subjects in the training set? (maybe I missed something here.)\n\nQuantitative results are only presented for the synthesized 2D shape. What hinders this paper from reporting quantitative numbers on real data (e.g., the 2D and 3D face data)? One possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some well-defined factor to measure the quantitative performance. \n\nQuantitative performance is only measured by the proposed protocol. Since the effectiveness of the protocol is something the paper need to justify, reporting quantitative results using simpler protocol is helpful both for demonstrating the disentangling quality and for justifying the proposed protocol (consistency with other measurement). A simple experiment is facial identity recognition and pose estimation using disentangled features on a standard test set (like in Reed et al, ICML 2014). \n\nIn Figure 6 (left), why ICA is worse than PCA for disentanglement? Is it due to the limitation of the ICA algorithm or some other reasons? \n\nIn Figure 6 (right), what is \u201cfactor change accuracy\u201d? According to Appendix A.4 (which is not referred to in the main text), it is the \u201cDisentanglement metric score\u201d. Is that right?\nIf so Figure 6 (right) shows the reconstruction results for the best disentanglement metric score. Then, 1) how about random generation or traversing along a disentangled factor? 2) more importantly, how is the reconstruction/generation results when the disentanglement metric score is suboptimal. \n\nOverall, the results presented in this paper are very interesting, but there are many details to be clarified. Moreover, more quantitative results are also needed. I hope at least some of the above concerns can be addressed. \n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482674965507, "id": "ICLR.cc/2017/conference/-/paper291/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper291/AnonReviewer3", "ICLR.cc/2017/conference/paper291/AnonReviewer1", "ICLR.cc/2017/conference/paper291/AnonReviewer2"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482674965507}}}, {"tddate": null, "tmdate": 1481211428806, "tcdate": 1481211428800, "number": 3, "id": "HJ6mRgwmx", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "HkMbG6kQx", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Clarifying section 3", "comment": "What do you mean by \u2018group-wise disentangling\u2019? Do you mean conditionally dependent factors (i.e. given identity X, factors such as face shape, eye colour, hair style etc can be explained away)? If this is the case, we believe that identity would be a \u2018complex\u2019 factor consisting of a particular combination of more \u2018simple\u2019 factors of face shape, eye colour etc. Learning such \u2018complex\u2019 factors would require learning hierarchical latent representations, which is something we do not address in the current work but leave for future efforts. The present paper only attempts to learn a set of \u2018simple\u2019 data generative factors that lie at the bottom of such potential hierarchy.\n\n\nOur model can only learn to disentangle factors that are statistically independent in the data. If in the data certain features are strongly correlated (e.g. full lips often co-occur with wider noses due to the two features being encoded in the same genes), then our model will learn a factor that encodes both of these features, resulting in \u2018group-wise disentangling\u2019 of a sort. An example of such coding can be found in CelebA, where it appears that the majority of young people are female and the majority of older people are male. Hence our model learns to disentangle out a \u2018group-wise\u2019 factor of age/gender (as seen in Figure 4). \n\n\nLinear classifier helps our disentanglement metric have a single interpretable score that we can use to better compare different models in a way that reflects disentanglement. Calculating the average variance per factor per X batches would be a worse measure, because we are interested in factor k having lower variance on average compared to other factors i != k. In absolute terms such average variance is not informative with regards to the level of disentanglement achieved. We believe that using a linear classifier is one of the easier ways to quantify whether factor k has lower average variance.\n\n\nWe have added Figure 2 in Section 3 to add an intuitive clarification of our proposed metric.\n\n\nWe did not explicitly try to maximise the disentanglement metric to determine beta, because this metric works in a semi-supervised case and we wanted to demonstrate that beta-VAE works in a completely unsupervised setting. We did show, however, that the best values of betas chosen through visual inspection heuristics match well the optimal betas found through calculating the disentanglement metric (see Fig. 5 right)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1481211359855, "tcdate": 1481211359849, "number": 2, "id": "r1uJ0gwmg", "invitation": "ICLR.cc/2017/conference/-/paper291/public/comment", "forum": "Sy2fzU9gl", "replyto": "BJgaN-zQe", "signatures": ["~Irina_Higgins1"], "readers": ["everyone"], "writers": ["~Irina_Higgins1"], "content": {"title": "Disambiguating the disentanglement metric", "comment": "The intuition is almost correct. In the terms suggested by the reviewer, the correct intuition would be:\n\n\n1. Choose some factor k.\n2. For a batch of N samples:\na. Sample two sets of latent representations, A and B, where v_k^A = v_k^B (and all other v_i may change).\n\tb. Decode then re-encode each latent representation.\n\tc. Calculate the difference z_diff between re-encoded latent representations \n3. Use the average z_diff over N samples to predict k and measure the accuracy of this predictor.\n\n\nWe have modified Section 3 to add an intuitive clarification.\n\n\nYour intuition is correct - the metric does say that a representation is disentangled when its components are not correlated, whereby p(z) = \u220f_i \u222b_x p(z_i | x) . The components may be conditionally dependent given a particular image though: p(z_x) != \u220f_i p(z_i | x). The metric can deal with such conditional dependencies due to its batch averaging of z_diff.\n\n\nAlso the model can only learn to disentangle factors that are statistically independent in the data. For example, if in CelebA the majority of young people are female and the majority of older people are male, the best one can hope for is for the model to disentangle out a joint factor of age/gender (as seen in Figure 4). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287637271, "id": "ICLR.cc/2017/conference/-/paper291/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy2fzU9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper291/reviewers", "ICLR.cc/2017/conference/paper291/areachairs"], "cdate": 1485287637271}}}, {"tddate": null, "tmdate": 1480885432407, "tcdate": 1480885432403, "number": 2, "id": "BJgaN-zQe", "invitation": "ICLR.cc/2017/conference/-/paper291/pre-review/question", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer1"], "content": {"title": "Questions about the disentanglement metric", "question": "* I'm having trouble understanding the proposed disentanglement metric. Here's my attempt at summarizing the metric\n\n1. Choose some factor k.\n2. Sample two sets of latent representations, A and B, one with perturbed v_k.\n3. Decode the re-encode each latent representation.\n4. Use the difference between re-encoded latent representations to predict k and measure the accuracy of this predictor.\n\nIf the latent representation v_k is disentangled then step 4 will be easy, otherwise it will be hard. Thus, the performance of the predictor measures something like disentanglement.\n\nIs that right? Could section 3 be clarified in the paper?\n\n* Supposing my understanding of the metric is accurate, if v_k is correlated with other latent factors then it's impossible to distinguish between v_k and the correlated factors using a simple classifier. This makes the prediction task in step 4 hard. Otherwise, when v_k is not correlated with other factors, it's possible that z_diff can distinguish between v_ks, so step 4 could be easy. If this is the right idea, then is the proposed metric simply saying that a representation is disentangled when its components are not correlated with each other?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959358383, "id": "ICLR.cc/2017/conference/-/paper291/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper291/AnonReviewer3", "ICLR.cc/2017/conference/paper291/AnonReviewer1"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959358383}}}, {"tddate": null, "tmdate": 1480737273674, "tcdate": 1480737273668, "number": 1, "id": "HkMbG6kQx", "invitation": "ICLR.cc/2017/conference/-/paper291/pre-review/question", "forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "signatures": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper291/AnonReviewer3"], "content": {"title": "About the assumption and measurement", "question": "(v,w) categorizes the latent representations into disentangled dimensions and non-disentangled dimensions. Is it possible to consider group-wise disentangling? This is more practical for complex factors, such as human identity.\n\nThe measurement proposed in Section 3 is intuitively to measure if a certain dimension (or certain dimensions) has lower variance when a certain factor is fixed. Compared to more basic measurements, such as variance, what are the benefits of using a linear classifier? \n\nThe presentation of Section 3 is confusing. Is it possible to make the intuition clearer in the main text?\n\nIn Section 4, was the measurement in Section 3 maximized to determine beta? (I have not read all details in Section 4, so I might just miss this)\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "pdf": "/pdf/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf", "TL;DR": "We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.", "paperhash": "higgins|betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework", "conflicts": ["google.com"], "keywords": [], "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "authorids": ["irinah@google.com", "lmatthey@google.com", "arkap@google.com", "cpburgess@google.com", "glorotx@google.com", "botvinick@google.com", "shakir@google.com", "lerchner@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959358383, "id": "ICLR.cc/2017/conference/-/paper291/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper291/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper291/AnonReviewer3", "ICLR.cc/2017/conference/paper291/AnonReviewer1"], "reply": {"forum": "Sy2fzU9gl", "replyto": "Sy2fzU9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper291/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959358383}}}], "count": 15}