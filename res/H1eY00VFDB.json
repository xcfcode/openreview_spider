{"notes": [{"id": "H1eY00VFDB", "original": "S1xXoK5OvB", "number": 1439, "cdate": 1569439441227, "ddate": null, "tcdate": 1569439441227, "tmdate": 1577168212506, "tddate": null, "forum": "H1eY00VFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dSb6fWeC0N", "original": null, "number": 1, "cdate": 1576798723300, "ddate": null, "tcdate": 1576798723300, "tmdate": 1576800913249, "tddate": null, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states.  The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions). Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. Therefore, I am recommending rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722758, "tmdate": 1576800274122, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Decision"}}}, {"id": "rJePCn43YB", "original": null, "number": 1, "cdate": 1571732687305, "ddate": null, "tcdate": 1571732687305, "tmdate": 1574441455959, "tddate": null, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "The paper proposes a new loss function which adds to the training objective another term that pulls the current parameters of a neural network further away from the parameters at a previous time step.\nIntuitively, this aims to push the current parameters further to the local optimum.\nOn a variety of benchmarks, optimizing the proposed loss function achieves better results than just optimizing the training loss.\n\nThe paper is well written and easy to follow.  However, I am not entirely convinced about the intuition of the proposed method and I think further investigation are necessary.\nWhile the method is simple and general, it also seems to be rather heuristic and requires carefully chosen hyperparameters.\nHaving said that, the empirical evidence shows that the proposed loss function consistently improves performance.\nThe following details should be addressed further:\n\n- I am a bit confused by the definition of the loss function. In Equation 1 it seems that the term on the left represents the training objective. If that is correct than Equation 2 second case contains the training objective twice?\n\n- F in Section 3 after Equation 2 is not properly defined\n\n- Could it happen that the proposed loss function leads to divergence, for example if the parameter from a previous time step theta^Tp is close to the optimum theta_star?\n\n- What is the motivation to use the L1 norm? How does this choice affect convergence compared to let's L2 norm?\n\n- Section 4.1 typo in first paragraph: K instead of \\kappa\n\n- Section 4.1 the results would be more convincing if all networks were trained multiple times with a different random initialization and Table 1 would include the mean and std.\n\n- Why is no warm-up period used for the GAN experiments?\n\n- Section 4.3: why is \\kappa increase by 1% for the speech recognition experiments where as by 2% for all other experiments?\n\n- I suggest to increase the line width of all figures since they are somewhat hard to identify on a print version.\n\n- Why is the momentum set to 0.5 for SGD in the ablation study? Most frameworks use a default value of 0.9.\n\n- I would like to see the affect of the warm-up period to the performance in the ablation study.\n\n- How does the choice of learning rate schedule, such as for example cosine annealing, affect the loss function?\n\n\n\npost rebuttal\n------------------\n\nI thank the authors for clarifying my questions and providing additional experiments. I think that especially the additional ablation studies and reporting the mean and std of multiple trials make the contribution of the paper more convincing. Hence, I increased my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576381084194, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Reviewers"], "noninvitees": [], "tcdate": 1570237737379, "tmdate": 1576381084209, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Review"}}}, {"id": "SyejA3FNjB", "original": null, "number": 12, "cdate": 1573326035200, "ddate": null, "tcdate": 1573326035200, "tmdate": 1573614531317, "tddate": null, "forum": "H1eY00VFDB", "replyto": "SyeRYntEiB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #1 (Part 2/4) ", "comment": "**CONTINUED FROM PART 1/4****\n\nQ: \u201c...Could it happen that the proposed loss function leads to divergence...?\u201d\nR: As the training proceeds, \\theta^{T_p} is more likely to become closer to \\theta^{*} in later stages. In this case, the proposed formulation of the retrospection loss, where \\kappa increases with training steps, increases the penalty on the divergence of the current state from the optimal parameter state (left-term of the loss) which helps facilitate convergence (please see the line in Sec 3 before Fig 1).\n\nIn general, the recursive formulation of the retrospective loss helps in improving convergence. For instance, in case \\theta^{T_p}, a previous parameter state diverges, in subsequent training steps, the retrospective loss will push \\theta^T (the current state at step T) away from \\theta^{T_p} and towards \\theta^{*} (the optimal state), helping restore the balance.\n\nWe agree that there are cases where the proposed method can diverge. However, from our experiments and studies, we found that the proposed loss function almost always helps in better convergence in practice. If the update frequency is very large, this could potentially lead to lack of convergence. Considering this can however be controlled using a hyperparameter (which is only one of two hyperparams in our entire framework), this issue is easily overcome. \n=============================================\n\nQ: \u201c...motivation to use the L1 norm? ...choice affect convergence compared to L2 norm..?\u201d\nR: Our framework is independent of the norm, and we in fact experimented with other norms in our experiments. We have included the results with L1 and L2 norms below. L1 norm provided the best results overall and hence was presented in the paper. Considering the choice of L1 norm is an implementation detail, we have moved the sentence regarding L1-norm in methodology to the experiments section. \n\nIMAGE CLASSIFICATION TEST ERROR ON F-MNIST (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nNetwork     || Original || L1-norm || L2-norm\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nLeNet         ||    10.8   ||     9.4     ||     9.7 \nResNet-20  ||    7.6     ||     6.8     ||     7.3\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nIMAGE CLASSIFICATION TEST ERROR ON SVHN (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nNetwork    || Original || L1-norm || L2-norm\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nVGG-11     ||    5.54   ||     4.70   ||     5.15 \nResNet-18 ||    4.42   ||     4.06   ||     4.27       \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWe, in fact, even tried a KL-divergence based formulation of the retrospection loss. Consider an input (x_i, y_i) and network G_{\\theta} parameterized by \\theta. Here G_{\\theta}(x_i) are the activations of the softmax layer and y_i is the ground-truth class embedding. For the loss, we define: output_curr = G_{\\theta^T}(x_i) ; output_prev = G_{\\theta^T_p}(x_i) ; target = y_i. For KL_div, we used the following formulation of the retrospective loss at a training step T: Loss(KL) = -1*KLDiv(output_curr, output_prev) + CrossEntropy(output_curr, target). In the above experiment on SVHN, we obtained 5.45 and 4.31 as error rates for VGG-11 and ResNet-18 respectively. \nWe have added these results to appendix E.\n\nWhile all our variants, L1-norm, L2-norm and KL_div, improved upon baselines, L1-norm resulted in better performance across tasks, except in unconditional GANs, where L2-norm is used to apply the retrospective loss on the adversarial predictions of the generator (Sec 4.2). One hypothesis for the outperformance of L1-norm is that when the L1-norm is used, the gradient is simply a dimension-wise sign (+ve vs -ve), which provides a clearer direction to gradient updates, especially when training to a fixed target embedding in predictive tasks.\n=============================================\n\nQ: \u201c...results \u2026 include the mean and std...\u201d\nR: We did run multiple trials during our studies, and our results in the paper were consistent across these trials. We, however, ran the experiments again, and are reporting our results below for Image Classification, Speech Recognition and Text Classification tasks averaged over 10 runs. (Note that for few-shot learning, we already included this information in the original submission). We also note that all the results in the submitted paper are in the same range as the mean +- std in the results below, although these were separately performed - showing the consistency.\n\nIMAGE CLASSIFICATION TEST ERROR (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nDataset     ||  Network       ||     original      ||  Retrospective\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nSVHN       ||   VGG-11      || 5.51 +- 0.08  ||  4.75 +- 0.09 \nSVHN       ||   ResNet-18  || 4.38 +- 0.09  ||  4.01 +- 0.07\nF-MNIST  ||   LeNet          ||10.74 +- 0.19 ||  9.37 +- 0.14\nF-MNIST  ||   ResNet-20  || 7.63 +- 0.04  ||  6.85 +- 0.06 \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n***RESPONSE CONTINUED IN PART 3/4***\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "rkldcPYViB", "original": null, "number": 6, "cdate": 1573324688199, "ddate": null, "tcdate": 1573324688199, "tmdate": 1573360093810, "tddate": null, "forum": "H1eY00VFDB", "replyto": "H1xvLwKEoH", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #2 (Part 2/2) ", "comment": "***CONTINUED FROM PART 1/2***\n\nGRAPH NODE CLASSIFICATION TEST ACCURACY (Higher the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nDataset       ||               GCN                          ||               ARMA\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \n                   ||      Original        Retrospective  ||      Original       Retrospective\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nCORA         ||  80.85 +- 0.53    81.23 +- 0.27  ||  78.53 +- 1.5    79.45 +- 1.15       \nCITESEER  ||  70.65 +- 0.93    71.25 +- 0.75  || 63.63 +- 1.3    64.22 +- 1.2\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nQ: I'm curious whether the use of the L1 norm is critical or not in the retrospective loss.\nR: Our framework is independent of the norm, and we in fact experimented with other norms in our experiments. We have included the results with L1 and L2 norms below. L1 norm provided the best results overall, and hence was presented in the paper. Considering the choice of L1 norm is an implementation detail, we have moved the sentence regarding L1-norm in methodology to the experiments section. \n\nIMAGE CLASSIFICATION TEST ERROR ON F-MNIST (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nNetwork     || Original || L1-norm || L2-norm\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nLeNet         ||    10.8   ||     9.4      ||     9.7\nResNet-20  ||    7.6     ||     6.8      ||     7.3\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nIMAGE CLASSIFICATION TEST ERROR ON SVHN (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nNetwork    || Original || L1-norm || L2-norm\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nVGG-11     ||    5.54   ||     4.70    ||    5.15 \nResNet-18 ||    4.42   ||     4.06   ||    4.27       \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWe, in fact, even tried a KL-divergence based formulation of the retrospection loss. Consider an input (x_i, y_i) and network $G_{\\theta}$ parameterized by $\\theta$. Here $G_{\\theta}(x_i)$ are the activations of the softmax layer and y_i is the ground-truth class embedding. For the loss, we define: output_curr = $G_{\\theta^T}(x_i)$ ; output_prev = $G_{\\theta^T_p}(x_i)$ ; target = y_i. For KL_div, we used the following formulation of the retrospective loss at a training step T: Loss(KL) = -1*KLDiv(output_curr, output_prev) + CrossEntropy(output_curr, target). In the above experiment on SVHN, we obtained 5.45 and 4.31 as error rates for VGG-11 and ResNet-18 respectively. \nWe have added these results to the Appendix E.\n\nWhile all our variants, L1-norm, L2-norm and KL_div, improved upon baselines, L1-norm resulted in better performance across tasks, except in unconditional GANs, where L2-norm is used to apply the retrospective loss on the adversarial predictions of the generator (Sec 4.2). One hypothesis is that when the L1-norm is used, the gradient is simply a dimension-wise sign (+ve vs -ve), which provides a clearer direction to gradient updates, especially when training to a fixed target embedding in predictive tasks.\n\nThank you again for your comments, and we will be happy to discuss further for any clarifications/questions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "SJgKOpYNsr", "original": null, "number": 14, "cdate": 1573326193001, "ddate": null, "tcdate": 1573326193001, "tmdate": 1573360080231, "tddate": null, "forum": "H1eY00VFDB", "replyto": "HkeQS6tEor", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #1 (Part 4/4) ", "comment": "\nQ:\u201d \u2026 \\kappa increase by 1% for the speech recognition ... by 2% for all other...?\u201d\nR: In the paper, we reported the best performance for all experiments by doing a randomized search for hyperparameter values. While increasing kappa by 2\\% at each retrospective update improved performance, we reported values for 1\\% since it provided marginally better performance. In the table below, we compare error rates when \\kappa is increased by 1\\% vs 2\\%. For consistent comparison, parameters in all experiments were restored from the same initialization. To give some context of improvement, we have also reported error rates when training without retrospection (Original).\n\nSPEECH RECOGNITION ERROR RATE WITH DIFFERENT \\kappa CONFIGS (Lower the Better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nNetwork  ||           Validation Set        ||              Testing Set                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n               ||    Original     1\\%     2\\%   ||     Original     1\\%         2\\%\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nLeNet     ||     9.8          9.6       9.6     ||     10.3           9.9         10.0\nVGG-11  ||     5.2          4.4       4.5     ||     5.0             4.2          4.4 \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n============================\n\nQ: \u201c...momentum set to 0.5 for SGD in the ablation study?... most...default value of 0.9\u2026\u201d\nR: Thank you for pointing out this issue. Since the ablation studies were intending to show robustness to other hyperparameters (batch_size, F, k), we did not explicitly tune for the best momentum values and went with 0.5. \nNow, we are presenting the same ablation study with different momentum values. We observe our approach is independent of the choice of momentum value since that retrospective training is better than the original training for all the different momentum values. The results of our experiments on FMNIST (as in Fig 6 of paper) are shown below.\n\nABLATION STUDY: DIFFERENT MOMENTUM FOR SGD (Lower the Better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-----\n       Momentum=0.5          ||        Momentum = 0.9         ||     Momentum = 0.7                   \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n Original    Retrospective  ||    Original     Retrospective  ||  Original     Retrospective\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014-\n     10.8             9.4           ||    10.05              9.06            ||     9.51             8.94\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\n\n=================================\n\nQ:\u201d...choice of learning rate schedule, ...example cosine annealing, affect the loss...?\u201d\nR:  Thanks for suggesting this experiment. We studied the impact of using cosine annealing LR schedule on the task of Image Classification and Speech Recognition when training with retrospection loss. In both cases, using retrospection loss still improves upon baselines that are trained without retrospection showing that the proposed loss is independent of the type of LR scheduler.\n\nIMAGE CLASSIFICATION ERROR RATE (FMNIST) WITH COSINE ANNEALING (Lower the Better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\nNetwork     ||  Original  ||  Retrospection\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\nLeNet         ||     9.71     ||        8.20\nResNet-20  ||    7.09      ||        6.44\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n\nSPEECH RECOGNITION ERROR RATE WITH COSINE ANNEALING (Lower the Better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nNetwork  ||            Validation Set           ||                   Testing Set                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n                     Original     Retrospective   ||    Original    Retrospective \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nVGG-11  ||      5.24            4.88               ||    5.73            5.01     \nLeNet     ||      9.94            9.63               ||    10.48          10.04\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nFurther, the various techniques we compare against on several diverse tasks ranging from image classification, speech recognition, text classification, few-shot classification, etc. follow distinct LR schedules, as mentioned in our experimental settings, and the proposed method results in performance improvement irrespective of the disparate configurations\n\n=========================\n\nQ:\u201d...increasing the line width of all figures \u2026 somewhat hard to identify on a print version\u2026\u201d\nR: Thanks a lot for suggesting this, we will incorporate this change in the camera-ready.\n======\n\nQ: \u201c..no warm-up period used for the GAN experiments?..\u201d\nR: We believe that since GANs are inherently unstable and do not train to a fixed target, the warm-up period is unlikely to have an impact. Hence, we reported experiments in our original submission with a warm-up period of 0 epochs which resulted in performance improvement (max IS value) by faster convergence (Fig 3, Fig 4). Now, we have included an ablation study on the impact warm-up period on GAN training in appendix D, which corroborates the hypothesis.\n\n=====\nThank you again for your comments, and we will be happy to discuss further any clarifications/questions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "HkeQS6tEor", "original": null, "number": 13, "cdate": 1573326138673, "ddate": null, "tcdate": 1573326138673, "tmdate": 1573360070257, "tddate": null, "forum": "H1eY00VFDB", "replyto": "SyejA3FNjB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #1 (Part 3/4) ", "comment": "**CONTINUED FROM PART 2/4****\n\nTEXT CLASSIFICATION TEST ERROR (H = Higher the better, L = lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\n    Method       ||               IECOMAP                ||                        AVEC \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n                       || F1-Score (H)   || Accuracy  (H)  ||        MSE     (L)    ||   r (Pear Score) (H)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nRetrospective || 64.40 +- 0.4    ||  64.97 +- 0.5    ||  0.1772 +- 0.0006  ||  0.332 +- 0.008\nOriginal           || 62.60 +- 0.9    ||  62.70 +- 0.7    ||  0.1798 +- 0.0005  ||   0.317 +- 0.007\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n\nSPEECH CLASSIFICATION ERROR RATES (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nNetwork  ||                   Validation Set             ||                   Testing Set                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n                        Original        Retrospective     ||      Original                Retrospective \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nLeNet     ||    9.77 +- 0.05     9.60 +- 0.03       ||     10.26 +- 0.05        9.86 +- 0.04  \nVGG-11  ||    5.15 +- 0.08     4.37 +- 0.04       ||     5.03 +- 0.06          4.16 +- 0.05  \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nAs an extension, we also did experiments on the task of node classification using Graph Neural Networks, where the performance is ideally reported by averaging over several runs. Here, we report performances by averaging over 30 runs each, where each run was trained for 100 epochs. We carried out experiments on CORA and CITESEER datasets using two widely used/state-of-the-art networks: ARMA (Bianchi et al., CoRR 2019), and GCN (Kipf & Welling, ICLR 2017). Using retrospection loss improves both accuracy and std. deviation in almost all cases. \nThese experiments with details have been added to Appendix A of the revised paper.\n\nGRAPH NODE CLASSIFICATION TEST ACCURACY (Higher the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nDataset       ||               GCN                          ||               ARMA\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \n                   ||      Original        Retrospective  ||      Original       Retrospective\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nCORA         ||  80.85 +- 0.53    81.23 +- 0.27  ||  78.53 +- 1.5    79.45 +- 1.15       \nCITESEER  ||  70.65 +- 0.93    71.25 +- 0.75  || 63.63 +- 1.3    64.22 +- 1.2\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n=============================================\nQ: \u201c... the effect of warm-up period ...in ablation study\u2026\u201d\nR:. As in the ablation studies, we trained LeNet on the F-MNIST dataset (60k images) for 70k iterations with batch_size = 32 (we use momentum=0.9). Hence, 1 epoch lasts for around 2k iterations. The error rates with different warm-ups are mentioned in the table below. We observed that on simpler datasets (like FMNIST), since networks start training at a reasonable accuracy, retrospection is effective even when we introduce it with a very low warm-up period (Iw = 0 steps). \n\nABLATION STUDY: DIFFERENT WARM-UP PERIOD (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n        Original    ||                     Retrospective                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n  Iw = Infinity   ||    Iw= 0  ||  Iw=10k || Iw = 15k || Iw = 20k  \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n      10.05           ||    9.06   ||      9.3      ||    9.33     ||   9.06     \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nFurther, we observed that for tasks on more complex datasets, it is best to introduce the retrospection loss after training the network for some epochs when the network has started to converge to some extent, empirically around 50-75% of the training epochs. While introducing retrospection early also improves over the baseline, later introduction of the retrospection loss further improves performance. For instance, we trained ResNet-56 on the task of image classification using CIFAR-10 dataset for 200 epochs. Here, when the network is trained without retrospection (the original config as in the ResNet paper), we got an error rate of 6.86 (6.97 is reported in ResNet paper). However, on using retrospection, performance improved to 6.78 when the warm-up period (Iw) of 50 epochs was used and it further improved to 6.52 with a warm-up period of 150 epochs.\n\nIMAGE CLASSIFICATION ERROR RATE FOR RESNET-56 (CIFAR-10) (Lower the Better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n        Original         ||                     Retrospective                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n     Iw = Infinity     ||    Iw = 0    Iw= 50   Iw = 100    Iw= 150  \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n     6.86 (6.97)       ||      6.81         6.78        6.61         6.52     \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n=============================================\n\n***RESPONSE CONTINUED IN PART 4/4***"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "SyeRYntEiB", "original": null, "number": 11, "cdate": 1573325958075, "ddate": null, "tcdate": 1573325958075, "tmdate": 1573359631976, "tddate": null, "forum": "H1eY00VFDB", "replyto": "rJePCn43YB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #1 (Part 1/4) ", "comment": "We thank the reviewer for the detailed review and helpful comments. The comments were insightful in recommending further experiments, which we have described below and also added to the appendix of the revised draft. We have also updated the paper by incorporating the recommended corrections.\n\nBefore we address the specific questions, we summarize our key contributions below for clarity:\n1. We propose a new \u201cretrospective loss\u201d that is based on looking back at the trajectory of gradient descent and providing an earlier parameter state as guidance for further learning.\n2. The key benefits of the proposed loss are that - it is simple and easy to implement (with any existing loss). Its simplicity allows us to easily generalize its use across tasks and application domains.\n3. Our exhaustive experiments on a wide range of tasks including image classification (+ few-shot learning), GANs, speech recognition, text classification, and graph classification (included in the reply here) beat state-of-the-art methods on benchmark datasets with the addition of this loss term. \n4. To the best of our knowledge, this is the first such effort; our empirical studies showed a consistent improvement in performance across the tasks in our multiple trials, demonstrating the potential of this method to have a strong impact on practical use in real-world applications across domains.\n\nWe have also added these at the end of Section 1 in our updated paper.\n\nBelow is our response to the individual comments (Q = question; R = our response):\n\nQ: \u201c...concerned about the intuition of the proposed method\u2026\u201d\nR: The summary of the contributions above answers this question partially. The intuition for the proposed method comes from the observation that the past parameter states (with the timestep chosen reasonably) are generally more erroneous than the current state, and contain valuable information on where the weight update should not go.\nThis information/direction is fully available with us during training but left unused. Our method leverages this information to provide a way to improve training performance. This can be more difficult in SGD (due to the noisy trajectory of updates) than in Batch GD, but we found choosing the timestep to not be difficult even when implementing this for SGD. Note that our approach is different from momentum, which we discuss in Sec 3 and Fig 6. From a different perspective, one can also view the proposed method as replicating the success of triplet loss in the data space, now in the parameter space (as discussed in Sec 3 of the paper).\n\n=============================================\nQ: \u201c...carefully tuning hyperparameters\u2026\u201d\nR: We note that we did not need much careful tuning of hyperparameters for the results presented in this paper. The presented ablation studies and subsequent discussions (answered later) highlight that the proposed loss function outperforms baselines across diverse hyperparameter configurations (momentum, LR schedule, batch_size, etc..) without modifying the retrospective hyperparameters. Hyperparameter tuning to further boost performance was, in fact, for future work (as mentioned in Sec 3)\nWe arrived at this formulation after careful deliberation and empirical studies, and the method is well-motivated as shown by the intuitions above. Importantly, the method\u2019s simplicity and ease of implementation allow us to consistently get better performance than state-of-the-art methods across multiple tasks, as mentioned in the summary above - with potential for practical applicability in real-world applications.\n\n=============================================\nQ: \u201c...confused by loss function...Eqn 1 - term on left represents training objective...in Eqn 2, the second case contains the training objective twice?\u2026\u201d\nR: We introduce the retrospection loss as an additional objective to be used alongside the original task-specific loss to improve training performance. If the task loss is also an Lp-norm, then this might be the case. This was however not the case in any of our experiments or tasks. We note that we also have the \\kappa coefficient in Eqn 1, which allows us to control the influence of this term if this becomes required in such a case, consistent with the geometric intuition in Fig 1.\n\n=============================================\nQ: \u201c...F in Section 3 after Equation 2 is not properly defined\u2026\u201d\nR: Thanks for pointing this out. F is the retrospective update frequency, we have added this to the revised submission.\n\n=============================================\n***RESPONSE CONTINUED IN PART 2/4***"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "H1xvLwKEoH", "original": null, "number": 5, "cdate": 1573324622627, "ddate": null, "tcdate": 1573324622627, "tmdate": 1573359616844, "tddate": null, "forum": "H1eY00VFDB", "replyto": "rygRVjNAFr", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment", "content": {"title": "Response for Reviewer #2 (Part 1/2) ", "comment": "We thank the reviewer for the review and helpful comments. We value every feedback and have updated the draft to address these concerns too. We hope that the editorial suggestions (which have now been addressed) will not be held against the technical merit of the paper.\n\nSome specific changes to the paper are:\na. Updates\n     1. The introduction is updated to highlight our contributions more explicitly.\n     2. Algorithm 1 and Figure 6 replaced with high-res variants.\nb.  Additions (in appendix)\n     1. Additional experiments on graph-structured data (with mean and std)\n     2. Mean and std deviation of current experiments\n     3. Ablation Study: Momentum for SGD\n     4. Ablation Study: Warm-up period\n     5. Ablation Study: Choice of Norm\n\nBefore we address the specific questions, we summarize our key contributions below for clarity:\n1. We propose a new \u201cretrospective loss\u201d that is based on looking back at the trajectory of gradient descent and providing an earlier parameter state as guidance for further learning.\n2. The key benefits of the proposed loss are that - it is simple and easy to implement (with any existing loss). Its simplicity allows us to easily generalize its use across tasks and application domains.\n3. Our exhaustive experiments on a wide range of tasks including image classification (+ few-shot learning), GANs, speech recognition, text classification, and graph classification (included here) beat state-of-the-art methods on benchmark datasets with the addition of this loss term. \n4. To the best of our knowledge, this is the first such effort; our empirical studies showed a consistent improvement in performance across the tasks in our multiple trials, demonstrating the potential of this method to have a strong impact on practical use in real-world applications across domains.\n\nWe have also added these at the end of Section 1 in our updated paper.\n\nBelow is our response to the individual comments (Q = question; R = our response):\n\nQ: ...standard deviations for results\u2026?\nR: We did run multiple trials during our studies, and our results in the paper were consistent across these trials. We, however, ran the experiments again, and are reporting our results below for Image Classification, Speech Recognition and Text Classification tasks averaged over 10 runs. (Note that for few-shot learning, we already included this information in the original submission). We also note that all the results in the submitted paper are in the same range as the mean +- std in the results below, although these were separately performed - showing the consistency.\n\nIMAGE CLASSIFICATION TEST ERROR (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nDataset     ||  Network       ||     original      ||  Retrospective\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nSVHN       ||   VGG-11       || 5.51 +- 0.08  ||  4.75 +- 0.09 \nSVHN       ||   ResNet-18   || 4.38 +- 0.09  ||  4.01 +- 0.07\nF-MNIST  ||   LeNet          ||10.74 +- 0.19 ||  9.37 +- 0.14\nF-MNIST  ||   ResNet-20   || 7.63 +- 0.04  ||  6.85 +- 0.06 \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\nTEXT CLASSIFICATION TEST ERROR (H = Higher the better, L = lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\n    Method       ||             IECOMAP                  ||                   AVEC \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n                       || F1-Score (H)   || Accuracy  (H)  ||        MSE     (L)    ||   r (Pear Score) (H)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nRetrospective || 64.40 +- 0.4    ||  64.97 +- 0.5    ||  0.1772 +- 0.0006  ||  0.332 +- 0.008\nOriginal           || 62.60 +- 0.9    ||  62.70 +- 0.7    ||  0.1798 +- 0.0005  ||   0.317 +- 0.007\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n\nSPEECH CLASSIFICATION ERROR RATES (Lower the better)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nNetwork  ||                   Validation Set             ||                   Testing Set                           \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n                        Original        Retrospective     ||      Original                Retrospective \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nLeNet     ||    9.77 +- 0.05     9.60 +- 0.03       ||     10.26 +- 0.05        9.86 +- 0.04  \nVGG-11  ||    5.15 +- 0.08     4.37 +- 0.04       ||     5.03 +- 0.06          4.16 +- 0.05  \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\nAs an extension, we also did experiments on the task of node classification using Graph Neural Networks, where the performance is ideally reported by averaging over several runs. Here, we report performances by averaging over 30 runs each, where each run was trained for 100 epochs. We carried out experiments on CORA and CITESEER datasets using two widely used/state-of-the-art networks: ARMA (Bianchi et al., CoRR 2019), and GCN (Kipf & Welling, ICLR 2017). Using retrospection loss improves both accuracy and std. deviation in almost all cases. \nThese experiments with details have been added to Appendix A of the revised paper.\n\nDue to space constraints, the response is continued in PART 2/2"}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eY00VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1439/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1439/Authors|ICLR.cc/2020/Conference/Paper1439/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155995, "tmdate": 1576860562099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Authors", "ICLR.cc/2020/Conference/Paper1439/Reviewers", "ICLR.cc/2020/Conference/Paper1439/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Comment"}}}, {"id": "rygRVjNAFr", "original": null, "number": 2, "cdate": 1571863350443, "ddate": null, "tcdate": 1571863350443, "tmdate": 1572972468936, "tddate": null, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "invitation": "ICLR.cc/2020/Conference/Paper1439/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents the retrospective loss to optimize neural network training. The idea behind the retrospective loss is to add a penalization term between the current model to the model from a few iterations before. Extensive experimental results on a wide range of datasets are provided to show the effectiveness of the retrospective loss.\n\nThe retrospective loss is additionally controlled by two hyperparameters, the strength parameter K and the update frequency T_p. This loss, measured in L-1 norm, is added to the training objective. The geometric intuition of the added loss term is that this pushes the model away from the model at iteration T_p. The paper argues that this shrinks the parameter space of the loss function.\n\nOne of the concern regards the writing of the paper.\n- Algorithm 1 and Figure 6 look very blurry, which I think are both below the publication standard.\n- The introduction could be written to be more helpful, such as providing more context on why the obtained experimental results are important (e.g. getting state-of-the-art results on the datasets studied in the experiments)\n- The Related Work contrasts with previous work which is not clear because the precise contribution has not been stated at the point.\n\nMore detailed questions:\n- What are the standard deviations for the experimental results (as you reported in Table 4 but not in other experiments)?\n- I'm curious whether the use of L-1 norm is critical or not in the retrospective loss."}, "signatures": ["ICLR.cc/2020/Conference/Paper1439/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1439/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ayuchopr@adobe.com", "cs17btech11038@iith.ac.in", "msarkar@adobe.com", "kbalaji@adobe.com", "vineethnb@iith.ac.in"], "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks", "authors": ["Ayush Chopra", "Surgan Jandial", "Mausoom Sarkar", "Balaji Krishnamurthy", "Vineeth Balasubramanian"], "pdf": "/pdf/bb63055097e0d7aa897f4be8e09c37477058ae82.pdf", "TL;DR": "A retrospection loss that enables networks  to leverage past parameter states as guidance during training to improve performance", "abstract": "Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "code": "https://github.com/iclr-retrospection/retrospection", "keywords": ["Deep Neural Networks", "Supervised Learning", "Classification", "Training Strategy", "Generative Adversarial Networks", "Convolutional Neural Networks"], "paperhash": "chopra|retrospection_leveraging_the_past_for_efficient_training_of_deep_neural_networks", "original_pdf": "/attachment/d4e5cff3450da6bb5ae8897843b72397197f223c.pdf", "_bibtex": "@misc{\nchopra2020retrospection,\ntitle={Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks},\nauthor={Ayush Chopra and Surgan Jandial and Mausoom Sarkar and Balaji Krishnamurthy and Vineeth Balasubramanian},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eY00VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eY00VFDB", "replyto": "H1eY00VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1439/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576381084194, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1439/Reviewers"], "noninvitees": [], "tcdate": 1570237737379, "tmdate": 1576381084209, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1439/-/Official_Review"}}}], "count": 10}