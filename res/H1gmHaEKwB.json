{"notes": [{"id": "fVu0VW9WElF", "original": null, "number": 1, "cdate": 1612446975845, "ddate": null, "tcdate": 1612446975845, "tmdate": 1612446975845, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "S1girxv_iH", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Public_Comment", "content": {"title": "Upper bound on the norm of query space (\\beta) ", "comment": "Dear Authors, \n\nMy main question is related to the upper bound on the norm of query space (\\beta, using the notations from the paper). When translating this to the neural network pruning, this would mean that while pruning the layer L, \\beta will bound the norm of activations (which will be the queries) of the layer L-1. Since the pruning is performed from the first hidden layer, in sequential order, this would mean that \\beta will be the bound on new activations from layer L-1, after pruning. \n\nMy doubts are the following:\nWhile implementing the algorithm, how do you estimate this (new) upper bound? Is it evaluated mathematically (by assuming a bound on the input space, and then appropriately percolating this deep in the layers), or is it evaluated empirically?\nIn a case where it is evaluated empirically, is complete training data used to evaluate \\beta, or a subset of training data is chosen?  \nWhile reading the discussion here, I made a note that you mentioned using batch norm and input normalization. Therefore, to evaluate \\beta, are you using the batch-norm's scaling (and additive constant)? If yes, then I anticipate that one might need to consider the probability of an event where the norm of the previous layer's pruned activations exceeds \\beta (evaluated using batch-norm). \n\nThanks! "}, "signatures": ["~Gantavya_Bhatt1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference", "~Gantavya_Bhatt1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gmHaEKwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207997, "tmdate": 1576860577066, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper517/-/Public_Comment"}}}, {"id": "H1gmHaEKwB", "original": "Hkxyu6BDPr", "number": 517, "cdate": 1569439034986, "ddate": null, "tcdate": 1569439034986, "tmdate": 1586584778539, "tddate": null, "forum": "H1gmHaEKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "qi-YR3wl6", "original": null, "number": 1, "cdate": 1576798698679, "ddate": null, "tcdate": 1576798698679, "tmdate": 1576800937128, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The rebuttal period influenced R1 to raise their rating of the paper.\nThe most negative reviewer did not respond to the author response.\nThis work proposes an interesting approach that will be of interest to the community.\nThe AC recommends acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720062, "tmdate": 1576800270822, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper517/-/Decision"}}}, {"id": "SkgB97XXqB", "original": null, "number": 2, "cdate": 1572184973068, "ddate": null, "tcdate": 1572184973068, "tmdate": 1574603254192, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "------ after reading authors' response ------\nThanks for the complete response and revision. The new Fig 4 is very nice, and this helps address my concerns. I'm more favorable of the paper now, changing from \"Weak Accept\" to \"Accept\".\n\nNote a small typo in the displayed equation just above Fig 4: the sum is over $P_{test}$ but it's written $P_{t}est$ (and also using \\text makes it look nicer, $P_{\\text{test}}$)\n\n------- original review -----\nThe paper proposes a neuron pruning technique that can compress an existing pre-trained neural net (though the experiments actually do additional unspecified \"fine-tuning\" training). It is motivated by the need to compress neural nets so they work on embedded devices (smart phones, etc.), and it is in contrast to most other techniques that prune at training, or prune after-training but prune weights not nodes. They argue convincingly that pruning weights is awkward, as one has to work with sparse matrices which are only actually effective for extreme sparsity levels. They also claim another big benefit is that theirs is the first with with (1) provable guarantees, and (2) is data-independent.\n\nI have a some criticism of the paper, but before I get lost in the details, let me say that I like the overall paper. I think it's a clever idea, it's a useful topic, the authors show very good understanding of the coreset literature, and it has some nice theory.  The paper is also well-written and easy to understand, and the appendix is short enough that I actually read it.\n\nHowever, I have at least two major comments:\n\n\n(1) The theorems are nice, but with the exception of Thm 6 (which I like), they are simple applications of existing results. My main issue is that you have not provided an end-to-end bound. There are two things lacking:\n\n(1a) Lack of dealing with several layers, e.g., composing your approximation error. With an additive error instead of a relative error, does composition cause a major problem? Seems like this could be an easy theorem.\n\n(1b) Lack of a clear final statement bounding the overall error. This is somewhat trivial (if you have a single pruned layer), but it makes the assumptions more clear. In particular, you assume the input x has norm bounded by beta. In this sense, you have not provided a \"data-independent\" guarantee.  Since you do not have a relative error bound, the norm of x is important.  Yet this also exposes something that really confuses me: for the ReLU activation, with non-negative inputs, this is positive homogeneous, i.e., phi( beta ||p|| ) = beta phi( ||p|| ). If you look at step 2 in Algo 2, you see that the choice of beta does not affect the probabilities (if phi is ReLU or anything else with this property). Thus we can choose beta arbitrarily... and thus you have a fixed additive bound, for an arbitrarily large input, which seems impossible!\n\n(2) Experiments were very promising, but I'm not convinced about the baselines.\n\n(2a) The fine-tuning after pruning wasn't described so I don't know how much effect it had. It makes sense to do this, but it means that it is less clear if your results were due to your theorem.  Please show results with and without the fine-tuning, and describe the fine-tuning (how many epochs of training?)\n\n(2b) You do some abstract experiments with random weights, to test your theorem, which is a nice somewhat direct test of your results (I assume here you are not fine-tuning, as it doesn't make sense, right?). Also in the abstract setup, you could test this as a function of depth, since I'm worried that your error guarantees get worse as a function of depth. The experimental setup was vague: what are the inputs x (from a ball, or sphere? uniform?), was this averaged over many runs? What was this network (you change size when you go to the LeNet-300-100), especially, what was the 100% number of samples (1000?)?.\n\n(2c) For table 3, taking the LeNet for example, you have 90% compression and improved error. This is nice, but to really convince me, in addition to adding the results without the fine-tuning, I'd like to see what you get with uniform pruning (say, with 85% compression) with and without fine-tuning. I don't have a good \"baseline\" expectation here, so while your improved error with 90% compression seems like a fantastic result, I suspect that one might get similar results (with say 85% compression) with trivial subsampling.\n\n\nSome minor comments: \n\n-- abstract, \"guarantees the accuracy of the function\" and \"... on MNIST while improving the accuracy.\"  These are 2 very different meaning of \"accuracy\", so please be more precise, e.g., a per-layer approximation error vs classification error on testing data.\n\n-- First paragraph of intro, saying networks are limited to HPC environments is hyperbole. These networks might need to be trained in an HPC environment, but most can be deployed on laptops (not an HPC environment). A bigger issue is deploying them on a smartphone.  Adding some quantitative numbers would strengthen your case (e.g., size of typical neural nets, and size of RAM in a smartphone).  Note that training requires much more memory due to memory explosion in backpropagation, but this does not effect runtime/deployment.\n\n-- middle of page 2, \"is very fast, ...\" the comma should be a semicolon to make it grammatically correct.  Page 3, near bottom, \"corests\" is a typo.\n\n-- personally I dislike things like Table 1, as they feel too much like boasting. You've chosen the columns carefully so it doesn't feel that meaningful, and you've already stated these things in the text. But it's not my paper.\n\n-- def 4 is very hard to parse. Are these subsets or strict subsets?\n\n-- Notation B_alpha could be explained; I'm used to seeing B_alpha(0), and you can always just write \\forall x with ||x||<= alpha to make it super clear.\n\n-- Your theorems require phi to be non-decreasing, but intuitively you can clearly handle non-increasing, since the set of inputs x \\in B_beta is invariant to sign changes. More generally, you could assume the existence of some 1D function psi(t) such that phi( t ) <= psi( |t| ). I don't know if there are many more common activation functions, but this could give a wider class.  The change to the proofs is trivial, since you just replace psi( |t| ) for phi( alpha beta).\n\n-- If Corollary 9 \"follows directly from Theorem 5\", why didn't Thm 7 and Corollary 8 also follow directly? You mean, it follows, but using the same simple bounding tricks from the appendix as used for Thm 7 and Corollary 8, right?\n\n-- Fig 3 shows very nice results\n\n-- A.1 Proof of Thm 6, you could use \\subsetneq (with amssymb package) to be more clear that it is a strict subset, since I find \\subset vague since different authors have different conventions.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682411095, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper517/Reviewers"], "noninvitees": [], "tcdate": 1570237750995, "tmdate": 1575682411112, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Review"}}}, {"id": "S1girxv_iH", "original": null, "number": 4, "cdate": 1573576771464, "ddate": null, "tcdate": 1573576771464, "tmdate": 1573576771464, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "SkgB97XXqB", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment", "content": {"title": "Reply to Review#3", "comment": "We want to thank the reviewer for the detailed review and very\nhelpful suggestions. We hope that with these improvements, the\nreviewer would consider increasing the score.\n\n1. Indeed, Theorem 6 and 7 are our main theoretical results, while\nthe other are corollaries as stated. Based on these theoretical\nresults, we propose the first applications of coresets for neural\npruning with very good experimental results.\n\n1a. Approximating the entire network even for a simpler case,\naddressed in Baykal et al., 2018, results in a large error, thus we\ntook a layer by layer approach: For every layer, after applying\nneural pruning the remaining weights are fine-tuned until\nconvergence. We do not approximate the whole network and thus there\nis no need to bound this approximation. Deriving a  coreset for approximating \na composition of layers is part of the future work. \nWe have clarified the layer-wise approach in the new version of the paper.\n\n1b. We agree that the probability of sampling a point does not\ndepend on beta. However, the size of the coreset depends on the\ntotal sensitivity, which depends on beta. It is true that the\napproximation error does not depend directly on the norm of the\nquery, but it depends on the size of the coreset (which depends on\nbeta). For an arbitrary large beta, the size of the coreset will\nreach the size of the full data, but it will not change the\napproximation error. Best practices recommend normalizing the input\nof the network and use batch normalization, which normalizes the\ninput to other layers. Thus, the value of beta in real data sets is\nnot arbitrary large, which explains small coresets in our\nexperiments.\n\n2a. We have added the details about fine-tuning to the paper: we\nfine-tune until convergence, but coreset converges in half of the\nconvergence time of the model chosen by the uniform sampling. The\nresults of compression before fine-tuning are provided in new\nSection 4.4.\n\n2b. The results in Figure 2 are provided without fine-tuning. We\nadded the experimental setup to the paper.\n\nThe experimental setting for showing the approximation error as a\nfunction of depth is not clear: First, each layer has several\nneurons and each has its own approximation error, thus it is not\nclear how to combine these errors as a representative error of the\nlayer. Second, each layer might have different scaling of neurons'\noutput. Thus, showing the approximation error in each layer without\nproper normalization (which is not trivial to define) will not\nprovide the information about the error stability vs. depth. We\nfeel that a proper experiment showing the effect of depth requires\nfurther consideration and cannot be finished within the discussion\nperiod. However, we will think about the experimental setting that\nwould show the approximation error vs. depth for the final version\nof the paper.\n\n2c.The fine-tuning is part of the proposed method for compression.\nHowever, to show the contribution of our coreset framework, we\nadded a new section \"Ablation Analysis\", in which we provide a\ncomparison with uniform sampling with and without fine-tuning. It\nshows that before fine-tuning the coreset has huge advantage over\nuniform sampling. After fine-tuning, we still observe that the\ncoreset selection is advantageous over uniform selection. Note that\n4% in MNIST is a large difference in favor of our method.\n\n\"Minor Comments\": Below, we provide our answers to some of the\nquestion. We amended the paper according to reviewer's suggestions.\n\nQ: def 4 is very hard to parse.\nA: Due to space limitation, we\nsummarized a number of concepts in one definition, but we added\nreferences to classic book in PAC-learning for helpful intuition\nand discussion.\n\nQ: Are these subsets or strict subsets?\nA: Subsets, as stated.\nOtherwise the required 2^|C| subsets (all subsets of C) in\nDefinition 4 would never hold.\n\nQ: Your theorems require phi to be non-decreasing, but intuitively\nyou can clearly handle non-increasing. I don't know if there are\nmany more common activation functions, but this could give a wider\nclass.\nA: This is a nice observation! We have change Corollary 8 to\nextended the results from Theorem 7 to negative weights and\nmonotonic function with negative values.\n\nQ: why didn't Thm 7 and Corollary 8 also follow directly from\nTheorem 5?\nA: Theorem 7 follows directly from Theorem 5, after\nbounding s and t. Indeed, this is the essence of its proof.\nCorollary 8 generalizes Theorem 5 to support non-positive weights\n(and as suggested by the reviewers, non-positive activation\nfunctions), which requires a proof.\n\nQ: Corollary 9 follows using the same simple bounding tricks from\nthe appendix as used for Thm 7 and Corollary 8, right? A: Exactly.\nWe added this clarification in the text."}, "signatures": ["ICLR.cc/2020/Conference/Paper517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gmHaEKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper517/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper517/Authors|ICLR.cc/2020/Conference/Paper517/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170259, "tmdate": 1576860543731, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment"}}}, {"id": "r1ePEqLuiB", "original": null, "number": 3, "cdate": 1573575214811, "ddate": null, "tcdate": 1573575214811, "tmdate": 1573575214811, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "SygftbOCKr", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment", "content": {"title": "Reply to Review#1", "comment": "We thank the reviewer for the useful comments. We hope that the\nclarifications below and the improvements we made in the new\nversion of the paper would justify raising the score and giving us\nan opportunity to present our result in ICLR20.\n\n\nOur theoretical results show the approximation error vs compression\ntrade-off which holds for any input in R^d. Since the proof holds\nfor any vector in R^d, it holds for any dataset in particular. We\ndo not put any constraints on the data or on the weights, except\nfor a bound on the norm. The size of the coreset and thus the level\nof compression depends on the norm bound. The higher the norm, the\nlarger the coreset (less compression). Best practices recommend\nnormalizing the input of the network and use batch normalization,\nwhich normalizes inputs to hidden layers. This is done for most of\nthe benchmark networks. Thus, we do not expect any dramatic change\nof results in the experiments on other data sets. We showed the\nresults on two data sets with different properties: hand-written\ndigits and natural images with objects. We do not have time to run\nan additional experiment during the discussion period, but we will\ninclude compression results on an additional domain, such as human\nfaces, and more categories in the final version of the paper.\n\nThe fine-tuning is part of the proposed method for compression.\nHowever, to show the contribution of our coreset framework, we\nadded a new section \"Ablation Analysis\", in which we provide a\ncomparison with uniform sampling with and without fine-tuning. It\nshows that before fine-tuning the coreset is huge advantage over\nuniform sampling. After fine-tuning, we still observe that the\ncoreset selection is advantageous over uniform selection. Note that\n4% in MNIST is a large difference in favour of our method.\n\nIn Figure 2, the labels are correct. We believe that the details of\nuniform sampling algorithm would be helpful to clarify the results.\nSpecifically, after sampling the neurons uniformly at random, their\nweights are updated the same way as it done in Alg 1, line 8, but\nwith probability 1/n (n is the number of neurons in the layer).\nThis is done to maintain the approximation error due to reduction\nin the number of neurons. The percentile method selects a\npredefined number of neurons with the largest norm of incoming\nweights, but it does not care about the weight of the neuron. We\nthank the reviewer for pointing out this misunderstanding. We added\nsimilar clarification in the new version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gmHaEKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper517/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper517/Authors|ICLR.cc/2020/Conference/Paper517/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170259, "tmdate": 1576860543731, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment"}}}, {"id": "Byeh_dLdjH", "original": null, "number": 2, "cdate": 1573574772320, "ddate": null, "tcdate": 1573574772320, "tmdate": 1573574772320, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "rJlj2ZzVcH", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment", "content": {"title": "Reply to Review#2", "comment": "We thank the reviewer for very helpful comments, that inspired us\nto generalize the results to a broader family of activation\nfunctions, which include negative values. Specifically, we have\nupdated Corollary 8 to include both negative weights and arbitrary\nbounded functions. We hope that these improvement justifies a\nhigher score."}, "signatures": ["ICLR.cc/2020/Conference/Paper517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gmHaEKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper517/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper517/Authors|ICLR.cc/2020/Conference/Paper517/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170259, "tmdate": 1576860543731, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper517/Authors", "ICLR.cc/2020/Conference/Paper517/Reviewers", "ICLR.cc/2020/Conference/Paper517/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Comment"}}}, {"id": "SygftbOCKr", "original": null, "number": 1, "cdate": 1571877241787, "ddate": null, "tcdate": 1571877241787, "tmdate": 1572972585512, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed a model compressing method based on coreset framework. The goal of the paper is to reduce the number of neurons. The basic idea is sampling the neurons on each layer with probability equal to the neuron's max share among all the outputs to the next layer, and updating the weights associated with the remained neurons. Another main contribution is the authors provided theoretical analysis to guarantee the accuracy vs compression trade-off for all possible inputs.\nPros:\n\nThe proposed method is easy to understand and seems to make sense.\nThe theoretical analysis seems strong.\nThe experiment results on two datasets show the proposed method achieved high compression rate and improvement of accuracy.\n\n\nCons:\n\nDespite the theoretical guarantee, it is not as clear on the value of the proposed method in real world. I would be better to test on more datasets and networks to verify the effectiveness of the proposed compressing method, as it claimed to be data-independent.\n\n\nAlthough the method achieved very good experiment results, its contribution to the high accuracy is unclear, since the networks were fine-tuned after the compression. So how do we exactly evaluate the accuracy vs compression trade-off when there is no such trade-off shown in the experiments?\n\n\nQuestions and suggestions:\n\n\nIn the Fig 2, it seems that the performances of the proposed method and the percentile-based method should be close to each other, and the uniform sampling method should be worse than them. However the results are opposite. If it was not incorrect labeling in the figure, it would be good to add some analyses about this result.\n\nTo solve the second point in \"Cons\", is it possible to show the accuracy of the compressed model without fine-tune? Or still fine-tune the model but simply set u(q) = w(q) in the line 8 algorithm 1?"}, "signatures": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682411095, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper517/Reviewers"], "noninvitees": [], "tcdate": 1570237750995, "tmdate": 1575682411112, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Review"}}}, {"id": "rJlj2ZzVcH", "original": null, "number": 3, "cdate": 1572245939234, "ddate": null, "tcdate": 1572245939234, "tmdate": 1572972585421, "tddate": null, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper517/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper provides a data-independent way for pruning neutrons in deep neural networks with a provable trade-off between its compression rate and the approximation error. The output of a layer of neurons is approximated by a corset of neurons in its preceding layer. \n\nThe pruning of neurons based on the coresets is shown to be effective when compared with other methods. The authors have validated it on two convolutional network architectures.  \n\nThe paper starts from defining the coresets and introducing the VC dimension, and extends the theorem to more generalized cases. \n\nThe coreset seems to require the activation function to be non-negative, which will possibly limit the scope of application of the proposed theory. "}, "signatures": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper517/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-Independent Neural Pruning via Coresets", "authors": ["Ben Mussay", "Margarita Osadchy", "Vladimir Braverman", "Samson Zhou", "Dan Feldman"], "authorids": ["bengordoncshaifa@gmail.com", "rita@cs.haifa.ac.il", "vova@cs.jhu.edu", "samsonzhou@gmail.com", "dannyf.post@gmail.co"], "keywords": ["coresets", "neural pruning", "network compression"], "TL;DR": "We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.", "abstract": "Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample.\n\nWe propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\\in \\mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.", "pdf": "/pdf/5120c7006cf551983e41c4ccd53877c232cc2c21.pdf", "paperhash": "mussay|dataindependent_neural_pruning_via_coresets", "code": "https://github.com/BenMussay/Data-Independent-Neural-Pruning-via-Coresets", "_bibtex": "@inproceedings{\nMussay2020Data-Independent,\ntitle={Data-Independent Neural Pruning via Coresets},\nauthor={Ben Mussay and Margarita Osadchy and Vladimir Braverman and Samson Zhou and Dan Feldman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gmHaEKwB}\n}", "original_pdf": "/attachment/44b75218f2f612e6648e7de9101adb3e5d66ad7c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gmHaEKwB", "replyto": "H1gmHaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682411095, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper517/Reviewers"], "noninvitees": [], "tcdate": 1570237750995, "tmdate": 1575682411112, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper517/-/Official_Review"}}}], "count": 9}