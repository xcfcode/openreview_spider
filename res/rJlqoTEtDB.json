{"notes": [{"id": "rJlqoTEtDB", "original": "HklQdV1OwH", "number": 753, "cdate": 1569439137553, "ddate": null, "tcdate": 1569439137553, "tmdate": 1577168268088, "tddate": null, "forum": "rJlqoTEtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "41Hn_5nZ2H", "original": null, "number": 1, "cdate": 1576798705073, "ddate": null, "tcdate": 1576798705073, "tmdate": 1576800931023, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Decision", "content": {"decision": "Reject", "comment": "After reading the author's rebuttal, the reviewers still think that this is an incremental work, and the theory and experiments .are inconsistent. The authors are encouraged to consider the the reivewer's comments to improve the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712549, "tmdate": 1576800261955, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper753/-/Decision"}}}, {"id": "BkxI2ZwqYS", "original": null, "number": 1, "cdate": 1571611054484, "ddate": null, "tcdate": 1571611054484, "tmdate": 1574200515022, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper investigates an SGD variant (PowerSGD) where the stochastic gradient is raised to a power of $\\gamma \\in [0,1]$.  The authors introduce PowerSGD and PowerSGD with momentum (PowerSGDM). The theoretical proof of  the convergence is given and experimental results show that the proposed algorithm converges faster than some of the existing popular adaptive SGD techniques.  Intuitively, the proposed PowerSGD can boost the gradient (since $\\gamma \\in [0,1]$) so it may be helpful for the gradient of the lower layers of a deep network which may be hit by the vanishing gradient issue. This may give rise to a faster convergence.   So overall the idea makes sense but I have the following concerns. \n\n1. The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.  At the first glance, it is $O(\\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\\frac{1}{\\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.  In other words, when the number of iterations is large, the batch size will be large too.  I consider this assumption unrealistic.  Given that $T$ is typically very large (it is iterations, not epochs),  it will require a huge batch size, probably close to the whole training set. In this case, it is basically a GD, not SGD any more. That's why the rate is $O(\\frac{1}{T})$, which is the convergence rate of GD.   I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.  Actually in the experiments the authors never use an increasing batch size. Instead, a constant batch size 128 is used. Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2. \n\n2. There are numerous inaccuracies in the proof given the supplementary material.  For instance, in Eq.7,  $\\nabla f(x) \\sigma(\\nabla f(x))$  should be $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$   The random variable $\\xi_{t}$ should be a scalar on training samples, not a vector, etc..  The authors should clean it up. \n\n3. It would be helpful to show the $\\gamma$ value on each experiment with different tasks. It would be good to know how $\\gamma$ varies across tasks. \n\n4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm. \n\n5. The term \"PowerSGD\" seems to have been used by other papers. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017446524, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper753/Reviewers"], "noninvitees": [], "tcdate": 1570237747591, "tmdate": 1575017446537, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Review"}}}, {"id": "B1gPMY5soS", "original": null, "number": 6, "cdate": 1573787918634, "ddate": null, "tcdate": 1573787918634, "tmdate": 1573787918634, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "BkeVF5UOjr", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"title": "update", "comment": "We have completed the comparisons with plain SGD for the first 4 sets of experiments. Figure 8 in Appendix F is now updated. \n\nWe also removed the changes highlighted in blue. You should be able to see the changes from the original submissions using pdf diff. \n\nWe hope that our response has addressed your concerns and look forward to any further comments you may have. "}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "BkeVF5UOjr", "original": null, "number": 5, "cdate": 1573575292314, "ddate": null, "tcdate": 1573575292314, "tmdate": 1573575292314, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"title": "To all reviewers  ", "comment": "We have now included the experiment results for comparisons with the plain SGD as requested by the reviewers. Since Figure.1 is already very crowded, we have included these comparisons in Appendix F. So far the experiments are not complete (due to the time constraints) in the sense that we are missing the experiments on ImageNet and we have only completed 1 run of SGD (whereas all others are averages of 5 runs). We will \n\n- by Nov 15, complete the comparisons with plain SGD for the first 4 sets of experiments;\n- by the time of final submission (should the paper be accepted),  comparisons with plain SGD for all 5 experiments.\n\nWe thank you again for taking the time to review our paper, and hope that you can comment on the updated version and let us know if you have any remaining concerns. We would like to reiterate the main contributions of the paper as follows: \n\n- In the theoretical part, we provided more concise convergence rate analysis for stochastic momentum methods in the non-convex setting. The analysis for $\\gamma\\in(0,1)$ is completely new and our proof derived tighter estimates of the accumulated momentum terms, which may be of interest for other uses. The bounds obtained are concise, change continuously w.r.t to the parameters $\\gamma\\in[0,1]$ and $\\beta\\in[0,1)$, and match the best known convergence rates in special cases. \n\n- In the experimental part, we empirically showed through a comprehensive set of experiments that PoweredSGD can speed up initial training in many cases, can partially mitigate gradient vanishing, and can be easily combined with other techniques to further accelerate optimization. \n\nThe main gap between the theoretical part and experimental part lies in that we are not able to explain the initial acceleration from a theoretical point of view. How to close this gap is of course an interesting and important question. We note however that there are rarely methods that can universally outperform other methods (both theoretically and empirically). We hope this paper can motivate further research on this topic, both in terms of theoretical analysis and practical use of the proposed methods. "}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "rke7afxSjr", "original": null, "number": 4, "cdate": 1573352122568, "ddate": null, "tcdate": 1573352122568, "tmdate": 1573352122568, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "BkxI2ZwqYS", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank you for your comments and hope that the following response will address your concerns. \n\n1. We did stated both in the Theorem statements and Remark 3.4 that the a large batch size $B_t=T$ is used for the convergence proof. This means the effective rate of convergence is $O(1/\\sqrt{T})$ as pointed out by the reviewer. This rate matches the currently best known rate of convergence for SGD (see, e.g. Ge et al., COLT'15). We have now made this very clear in both Remarks 3.3 and 3.5. Please see changes highlighted in blue and also our response to Reviewer 2 on novelty of the convergence analysis. \n\nIf our response addresses your main concern, we sincerely hope you that you can reconsider your score. \n\nFor your other points, we have made the following changes in the paper. \n\n2. We have checked and fixed a few typos in the paper. Please note that we wrote  $\\nabla f(x)\\cdot \\sigma (\\nabla f(x))$ in eq. (7) as a dot product. So it is the same as $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$. This notation was explained in the notation section. If you have any remaining concerns, please let us know. \n\n3. We have added the values for chosen $\\gamma$ in the updated version (see caption of Figure 1). \n\n4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments. We promise to do so in the final version. \n\n5. We were not aware of this at the time of submission. We have changed this to PoweredSGD. If you have any alternative suggestions, please let us know. \n\nWe summarize the main contributions of the paper as follows: \n\n- In the theoretical part, we provided more concise convergence rate analysis for stochastic momentum methods in the non-convex setting. This was made possible by a sharp estimate of the accumulated momentum terms (Lemma B1). We believe this is an important but under-explored topic (Yan et al., 2018). \n\n- In the experimental part, we empirically showed that the proposed optimisation algorithms have potential to solve realistic problems. We are not claiming these variants will outperform all other methods in all training cases, but we sincerely believe that the results are promising. In particular, we have demonstrated their potential benefits of mitigating gradient vanishing and combining other techniques for accelerating optimization.  \n\nWe do admit the gap between our theoretical analysis and experiments in the sense that the analysis does not account for the initial acceleration observed in many experiments. We think this is a very interesting question for future research and hope that this paper can motivate further research in this area. We agree with your intuition that this may have something to do with $\\gamma\\in (0,1)$ boosting the gradients."}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "S1eCuflBiS", "original": null, "number": 3, "cdate": 1573352053872, "ddate": null, "tcdate": 1573352053872, "tmdate": 1573352053872, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "SkgOb3BRKH", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the comments. We justify the novelty and significance of the contributions made by this paper as follows. \n\n1) Novelty of the convergence analysis: The paper by Yuan et al. did not present proof of convergence in the discrete-time setting. The authors only provided convergence of the ODE models. On the other hand, convergence analysis of momentum methods in non-convex setting is an important but under-explored area  (Yan et al., 2018). In the current paper, the convergence results are proved for non-convex objective functions satisfying mild assumptions. Appropriate use of some sharp estimates allowed us to obtain concise bounds on convergence of the entire class of PoweredSGD methods for $\\gamma\\in[0,1]$ and the bounds continuously depend on parameters $\\gamma$ and $\\beta$. In the special cases ($\\gamma=0,1$, $\\beta=0$), these bounds matches the best known bounds for GD/SGD/SGDM in the non-convex setting. More specifically, we would like to draw the reviewer's attention to the following two papers: \n\n*   [Yan18] Yan, Y., T. Yang, Z. Li, Q. Lin, and Y. Yang. \"A unified analysis of stochastic momentum methods for deep learning.\" In IJCAI International Joint Conference on Artificial Intelligence. 2018. \n\n  *  [Bernstein18] Bernstein, Jeremy, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. \"SIGNSGD: Compressed Optimisation for Non-Convex Problems.\" In International Conference on Machine Learning, pp. 559-568. 2018. (Theorem 3)\n\nWe emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers. Please take a look at Theorems 1 and 2 in [Yan18] and Theorem 3 in [Bernstein18]. We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3). \n\n2) Novelty of experiments: The current paper presents substantially more comprehensive experiments for benchmarking the proposed class of optimizers against other popular optimization methods for deep learning tasks. In particular, we highlight the experiments on vanishing gradients and learning rate schedules. This, in addition to the potential to accelerate initial convergence, makes the proposed PoweredSGD methods useful in many potential applications. "}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "BJgxTZgSir", "original": null, "number": 2, "cdate": 1573351863852, "ddate": null, "tcdate": 1573351863852, "tmdate": 1573351863852, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "S1gTSur19S", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the positive assessment of our work. We would like start by stating that we did not mean to claim that the rate of convergence proved in this paper is better that than of Yan et al. We have modified the Remarks to clarify the statements. In the stochastic gradient setting, the number of gradient evaluation is indeed $T^2$. This is consistent with the result in Bernstein et al. (2018). The main point we would like to make is that the bounds are very concise and exactly reduce to that of gradient descent/stochastic gradient descent in the special cases. We thank you for pointing out that the bounded variance assumption may also be restrictive and only satisfied on bounded domains. It is nonetheless a standard assumption made in the literature. We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version. \n\nResponse to other minor points: \n\nOur convergence analysis is done for non-convex objective functions (similar to that of Yan et al. and Bernstein et al.). In the non-convex setting, to the best of our knowledge, there are no theoretical results that show benefits of momentum methods over SGD. For experiments, we speculate that the reason is that the batch size used is too small for (Powered)SGDM to gain an advantage over (Powered)SGD. We plan to add SGD as a reference algorithm (as suggested by another reviewer). Once the experiments are complete, we should be able to see how SGDM compares with SGD in the experiments. This may take a while for the ImageNet experiments, but we promise to do so in the final version. \n\nWe have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know. "}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "SkgOb3BRKH", "original": null, "number": 2, "cdate": 1571867648213, "ddate": null, "tcdate": 1571867648213, "tmdate": 1572972556534, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes PowerSGD for improving SGD to train deep neural networks. The main idea is to raise the stochastic gradient to a certain power. Convergence analysis and experimental results on CIFAR-10/CIFAR-100/Imagenet and classical CNN architectures are given. \n\nOverall, this is a clearly-written paper with comprehensive experiments. My major concern is whether the results are significant enough to deserve acceptance. The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum). I am not sure how novel the convergence analysis for PowerSGD is, and it would be nice if the authors could discuss technical challenges they overcome in the introduction. "}, "signatures": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017446524, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper753/Reviewers"], "noninvitees": [], "tcdate": 1570237747591, "tmdate": 1575017446537, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Review"}}}, {"id": "S1gTSur19S", "original": null, "number": 3, "cdate": 1571932228520, "ddate": null, "tcdate": 1571932228520, "tmdate": 1572972556492, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes, analyzes, and empirically evaluates PowerSGD (and a version with momentum), a simple adjustment to standard SGD algorithms that alleviates issues caused by poorly scaled gradients in SGD. The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms.\n\nOverall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side. Specifically:\n\u2022\u00a0The convergence analysis assumes a batch size equal to T, the number of steps of PowerSGD. This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading. If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).\n\u2022\u00a0In Remark 3.4.3, the authors claim that another point of difference between their results and Yan et al.'s (2018) is that Yan et al. assume bounded gradients, an assumption that is not satisfied for e.g., mean squared error (MSE). But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem\nmin_\u03b2 (1/N)\u03a3_n (y_n \u2013 x_n \u2022\u00a0\u03b2)^2\nwith the minibatch gradient estimator computed over randomly chosen minibatches B:\n\\hat g = (1/|B|) \u03a3_{n \\in B} x_n (y_n \u2013 x_n \u2022\u00a0\u03b2).\nAs the norm of \u03b2 goes to infinity, so does the expected norm of the error of \\hat g. I'm not saying this is a particularly big \ndeal, just that it's not an improvement over Yan et al.'s result.\n\nThat aside, this seems like good work that could have a significant impact on practice.\n\nA couple of other minor points:\n\u2022\u00a0It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD. It would be nice to see some discussion (or at least speculation) on why that is.\n\u2022\u00a0Not all of the arrows in Figure 1 are pointing to the right lines.\n\u2022\u00a0In the abstract, it might be good to clarify that the exponentiation is elementwise."}, "signatures": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017446524, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper753/Reviewers"], "noninvitees": [], "tcdate": 1570237747591, "tmdate": 1575017446537, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Review"}}}, {"id": "HylGODc2_r", "original": null, "number": 1, "cdate": 1570707306044, "ddate": null, "tcdate": 1570707306044, "tmdate": 1570707306044, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "H1gpt95wdH", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment", "content": {"comment": "Thank you for the comment. \n\nWhile there are numerous variants of adaptive methods available, we focused our comparisons with the most popular ones, namely, Adam, AdaGrad and RMSprop with their standard setup for batchsize, step decay scheme. For example, the batch size is set to be 256, the same as [1,2,3]. The stepsize decay scheme is the same as [2,3].   \n\nWe also thank you, as the lead author of NovoGrad for bringing your work to our attention. We note that both Lamb [4] and NovoGrad [5] are also under review for ICLR20 (code not available with the submissions, therefore we cannot reproduce and make a fair comparison). It is worthy mentioning that the batch size and decay scheme set in [4] and [5] are different from [1,2,3]. \n\nIt would be interesting to experiment in future work on how to compare and combine the technique we propose (a nonlinear gradient transformation that is complementary and orthogonal) with other adaptive techniques such as the ones you pointed out.\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep residual learning for image recognition.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.\n\n[2] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. \"Densely connected convolutional networks.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708. 2017.\n\n[3] https://github.com/pytorch/examples/tree/master/imagenet\n\n[4] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh. \"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.\" arXiv preprint arXiv:1904.00962, 2019.\n\n[5] Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, Jonathan M. Cohen. \"Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks.\" arXiv preprint arXiv:1905.11286, 2019.", "title": "Response"}, "signatures": ["ICLR.cc/2020/Conference/Paper753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper753/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper753/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper753/Authors|ICLR.cc/2020/Conference/Paper753/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166746, "tmdate": 1576860540074, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Official_Comment"}}}, {"id": "H1gpt95wdH", "original": null, "number": 1, "cdate": 1570380421200, "ddate": null, "tcdate": 1570380421200, "tmdate": 1570380421200, "tddate": null, "forum": "rJlqoTEtDB", "replyto": "rJlqoTEtDB", "invitation": "ICLR.cc/2020/Conference/Paper753/-/Public_Comment", "content": {"comment": "I would argue that Section 4.2 used wrong adaptive methods for comparison, and this lead to the wrong conclusion: \"For test set, we can notice that although SGDM achieved the best test accuracy of 76.27%, PowerSGD and PowerSGDM gave the results of 73.71% and 73.96%, which were better than those of adaptive methods\"\nThere are many adaptive methods which give top1 accuracy similar toSGDM or better. \nFor example:\n1.  AdamW: top-1=76.3% in 100 epochs ( hype-parameters and training details  https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/image2label/resnet-50v2-adamw.py)\n2. LAMB:  top-1 = 76.6 (https://arxiv.org/pdf/1904.00962.pdf)\n3. NovoGrad: top-1 = 77% (https://arxiv.org/abs/1905.11286 ) \n", "title": "Comparison with adaptive methods for Resnet-50 on  ImageNet"}, "signatures": ["~Boris_Ginsburg1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Ginsburg1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "authors": ["Jun Liu", "Beitong Zhou", "Weigao Sun", "Ruijuan Chen", "Claire J. Tomlin", "Ye Yuan"], "authorids": ["j.liu@uwaterloo.ca", "zhoubt@hust.edu.cn", "sunweigao@outlook.com", "ruijuanchen@hust.edu.cn", "tomlin@eecs.berkeley.edu", "yye@hust.edu.cn"], "keywords": ["stochastic gradient descent", "non-convex optimization", "powerball function", "acceleration"], "TL;DR": "We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. ", "abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ", "pdf": "/pdf/382c34338a2d67d1f42dd117328cecc1ef57d115.pdf", "code": "https://www.dropbox.com/s/kqfyq4xgelqdge3/PowerSGD_ICLR20_code.zip", "paperhash": "liu|powersgd_powered_stochastic_gradient_descent_methods_for_accelerated_nonconvex_optimization", "original_pdf": "/attachment/d774ad133d5821e4b979e66a1c5fb688f446abf6.pdf", "_bibtex": "@misc{\nliu2020powersgd,\ntitle={Power{\\{}SGD{\\}}: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization},\nauthor={Jun Liu and Beitong Zhou and Weigao Sun and Ruijuan Chen and Claire J. Tomlin and Ye Yuan},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlqoTEtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlqoTEtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204668, "tmdate": 1576860573647, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper753/Authors", "ICLR.cc/2020/Conference/Paper753/Reviewers", "ICLR.cc/2020/Conference/Paper753/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper753/-/Public_Comment"}}}], "count": 12}