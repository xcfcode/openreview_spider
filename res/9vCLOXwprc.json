{"notes": [{"id": "9vCLOXwprc", "original": "glXY2eFLTlu", "number": 1109, "cdate": 1601308124694, "ddate": null, "tcdate": 1601308124694, "tmdate": 1614985736405, "tddate": null, "forum": "9vCLOXwprc", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DID9nYOOcA1", "original": null, "number": 1, "cdate": 1610040401960, "ddate": null, "tcdate": 1610040401960, "tmdate": 1610473998025, "tddate": null, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers found the new framework interesting. However, reviewers are unsatisfied with empirical evaluations. More experiments and discussion are needed."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040401946, "tmdate": 1610473998008, "id": "ICLR.cc/2021/Conference/Paper1109/-/Decision"}}}, {"id": "H0Uqn3euSt", "original": null, "number": 3, "cdate": 1604010886822, "ddate": null, "tcdate": 1604010886822, "tmdate": 1606800226650, "tddate": null, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review", "content": {"title": "A new graph neural networks architecture with modified rules for message passing", "review": "Summary:\n\nThis work proposes a new graph neural network architecture with modified rules for message passing, Iterated Graph Neural Network System (IGNNS). The paper then provides a theoretical analysis of the proposed architecture by connecting it with Iterated Function System (IFS), an important research field in fractal geometry. This paper further demonstrates empirically that the proposed architecture outperforms related models on citation network datasets.\n\nPros:\n\n1.The proposed architecture achieves empirical improvement on citation network datasets.\n\n2.This work also provides some theoretical analysis for the proposed architecture: it analyzes the geometric properties of IGNNS from the perspective of dynamical system.\n\n\nCons:\n1.It seems unclear to me how the theoretical analysis via IFS could be used to explain the empirical performance gain on citation networks.\n\n2.According to the formulas in equation (3) and above on page 4, it seems that the mathematical expectation $E_n$ is still linear (affine) w.r.t. to the input $X^{\\text{int}}$. Then if we use a learnable matrix to learn such combinations of matrices $A_i$ and probability vector $p_i$, would this be equivalent to applying an MLP to for each the message passing iteration (the adjacency information in $A_i$  is accessible to update the MLP via backpropagation)?\n\n3.In Figure 1, how would the message passing in d) be different compared to the message passing in c) after two iterations?\n\n4.It would be nice for the authors to provide a theoretical analysis on the computational complexity for the proposed architecture IGNNS.\n\n5.It would be nice for the authors to provide discussions with relevant works [1-3] on graph neural networks. [1] proposes a generalized aggregation function that allows successful and reliable training of very deep GCNs and how the proposed theorems in work could unify the mixed results (as Thm 4.1 and 4.2 state that the characterization ability of IGNNS would decrease with the increase of IFS iterations). [2] proposes a method for directional message passing as well. [3] proposes a theoretical framework for analyzing which type of GNN would achieve better generalization performance on the given task, which is also related to this paper for explaining the performance gain by IGNNS.\n\n6.The quality of the writing could be much more improved. It would be nice for the authors to provide:\na) better intuitions on its analysis using IFS (e.g. what is the physical meaning of the fractal set in IFS and why is it important). \nb) more connections between its proposed architecture and the empirical experiment section (e.g. how the proposed theorem could explain the performance gain connections)\nThere are also grammar mistakes in the paper which may hinder the understanding of the readers (e.g. last sentence in the abstract).\n\n[1] Li, Guohao, et al. \"Deepergcn: All you need to train deeper gcns.\" arXiv preprint arXiv:2006.07739 (2020).\n\n[2] Klicpera, Johannes, Janek Gro\u00df, and Stephan G\u00fcnnemann. \"Directional message passing for molecular graphs.\" arXiv preprint arXiv:2003.03123 (2020).\n\n[3] Xu, Keyulu, et al. \"What Can Neural Networks Reason About?.\" arXiv preprint arXiv:1905.13211 (2019).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126722, "tmdate": 1606915773154, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review"}}}, {"id": "xQhaZs2yMeu", "original": null, "number": 4, "cdate": 1604117158293, "ddate": null, "tcdate": 1604117158293, "tmdate": 1606792232749, "tddate": null, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review", "content": {"title": "Review for Iterated graph neural network system ", "review": "This paper proposes a new framework of GNN which can deal with undirected and directed graphs in a unified way. The authors argue that the size of the symbol space for a message passing path with length n is 2^n, while previous architectures only have constant size. Motivated by this observation, the authors borrow ideas from Iterated Function System to augment the symbol space. \n\nRoughly speaking, the main idea is to use 2 linear mappings f_0 and f_1 which correspond to the two directions. For the given input vector x, in each of the n iterations, we apply one of the two linear mappings randomly (the probability of applying each mapping is a learnable parameter), and we use the expectation of the resulting vector as the representation. The authors argue that the resulting symbol space could have sufficient size if n is sufficiently large. \n\nThe main idea looks reasonable and interesting. My major concern is about the experiment part (and thus my recommendation would just be weak acceptance) . The authors only perform experiments on three datasets, and it is unclear if the same approach will be effective on other datasets/settings. Moreover, although we see performance improvement on two of the three datasets, necessary discussion about the experimental results is missing. It would be great if the authors could explain why the proposed method improves the performance on Cora and Citeseer, and achieves worse performance compared to some of the baselines on Pubmed, to give the readers a better idea when this new framework would be effective. \n\n***Post Rebuttal***\nI have read authors' response and other reviewers' reviews. After reading it is still unclear to me why sparsity of the networks could affect the performance of the proposed framework. Therefore I would like to keep my original score. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126722, "tmdate": 1606915773154, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review"}}}, {"id": "z570MXJR_ti", "original": null, "number": 10, "cdate": 1605600738764, "ddate": null, "tcdate": 1605600738764, "tmdate": 1605836887630, "tddate": null, "forum": "9vCLOXwprc", "replyto": "3JSklwPNdKr", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 3 (Part 3) ", "comment": "__Comment6:__ As regards the experiments, the results seem promising, but information on trining time should be added. \n\n__Reply6:__ We have added training time information in section 5.3, including the total training time and the average training time  per epoch. In section 3.6, we also analyze the time complexity of IGNNS.\n\n__Comment7:__ Unfortunately, however, the description of the datasets used is too simplified. It should explain how the nodes are extracted from those available in the datasets and why only 20 nodes per class are extracted. It seems that only very few nodes are considered despite the availability of a large number of them. In addition, details about the edges connecting the nodes and the features can also be useful, as not all the edges/features will be considered since only 20 nodes per class are used. It is necessary to explain if these nodes belong to only one class or to several classes because there could be unbalance problems. \n\n__Reply7:__ We have added a more detailed description of the dataset in Section 5.2\uff0cthat is \"In these citation\nnetworks, papers are represented as nodes, and citations of one paper by another are denoted as edges. Node features are the bag-of-words vector of papers, and node label is the only one academic topic of a paper.\"  So  these nodes belong to only one class.  For fair comparison,  we adopt the same data split mode as these baseline models, i.e. with 20 nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. Because it is a semi-supervised node classification experiment, only a few nodes are needed for training. Indeed, it is interesting to make full use of the edge information of nodes in semi supervised experiments. In later research, we will try.\n\n__Comment8:__ Also, it is not specified which function is used in the representation layer.\n\n__Reply8:__  In Section 5.2, we have added the relevant description, that is \u201cIn representation layer, we adopt weighted time average to get the global representation of notes.\u201d \n\n__Comment9:__  It would be interesting to understand what happened for the Pubmed dataset. Probably the result depends on the small percentage of nodes considered.\n\n__Reply9:__  Thank you very much for bringing this to our attention. We have analyzed this phenomenon in section 5.3. We already know  the improved performance of model IGNNS in dataset Cora and Citeseer is much higher than that in dataset Pubmed. To understand why this happens, we analyze the characteristics of these citation networks. We consider two statistical properties of networks, one is the Network Density $d(\\mathcal{G})$, which is defined as $d(\\mathcal{G})=\\frac{2L}{N(N-1)}$, where $N$ is the number of nodes and $L$ is the number of edges, and the other is the Average Clustering Coefficient $C$, which is defined as $C=\\frac{1}{N}\\sum_{i\\in V}C_i$, where $V$ is the set of nodes, $C_i=\\frac{2e_i}{k_i(k_i-1)}$, $k_i$ is the number of the neighbors of node $v_i$ and $e_i$ is the number of undirected edges between $k_i$ neighbors. The less Network Density means the more global sparsity of the network, and the less Average Clustering Coefficient means the more sparsity of the neighbors of nodes. The calculation results of the statistical characteristics of the network are shown in Table 1 . We can see  that Pubmed is more sparse than Cora and Citeseer. The performance of IGNNS benefits from the bidirectional mixed propagation of information between nodes, but this sparsity weakens the gain of IGNNS.\n\n__Table1  Statistical characteristics of the networks. Bold for minimum.__\n\n Statistical characteristics &emsp;&emsp;|&emsp; Cora &emsp;&emsp;| &emsp;&emsp;Citeseer &emsp; |&emsp;  Pubmed &emsp;\n:---- | :----: | :----: | :----: \nNetwork Density | 0.00144000 | 0.00084514 | __0.00022805__\nAverage Clustering Coefficient| 0.24067330 | 0.14147102  |__0.06017521__\n\n__Comment10:__ Overall the proposal seems interesting, but its description lacks important definitions.\n\n__Reply10:__ We have corrected these errors.\n\n__Comment11:__ Code and datasets are not available. Unfortunately, this does not help to evaluate the proposal.\n\n__Reply11:__  We re submitted the code and made sure it works locally before committing.\n\n__Comment12:__ Typos The second line of section 3.4: labels instead of lebels In theorem 4.2 furthermore instead of further more In the references, arXiv URLs are not well-formed.\n\n__Reply12:__ Thank you very much for pointing out these mistakes and we have finished correcting them.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "AB6vqgwdTmo", "original": null, "number": 8, "cdate": 1605544285739, "ddate": null, "tcdate": 1605544285739, "tmdate": 1605601311280, "tddate": null, "forum": "9vCLOXwprc", "replyto": "3JSklwPNdKr", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 3 (Part 1)", "comment": "Thank you very much for your comments.\n\n__Comment1:__ The paper proposes an interesting approach, but personally, I found the document a bit foggy in some parts. On page 2, iterated function systems are introduced, but the definition does not explain what the f_i functions are, how many functions there may be and how the probabilities are related to the functions. This detail can be assumed in section 3.2 when the expected values are computed using the probabilities from the p set. However, in this section there are undefined symbols, such as p_{00}, p_{01}, etc., which make it difficult to follow the explanation of the step.\n\n__Reply__ Thank you very much for pointing out these mistakes. We have redescribed IFS in Chapter 2 and given a detailed definition of $f_i$. We have revised the error in formula (1). The correct case is that an $f_i$ corresponds to a probability $p_i$.  We forgot to thicken the font of P, resulting in undefined symbols. The correct definition is  __p__$_{01}$=$p_0p_1$.\n\n\n__Comment2:__  In section 3.2, a short discussion of the value of n (the number of iteration) should be added to say whether this value depends on the input graph, on the values of the H matrices in each iteration or something else.\n\n__Reply2:__  We have added \"Let $n$ be the number of iterations of IFS. For IGNNS, $n$ is a preset parameter. \" to section 3.2.  In section 3.6,  we also discuss the effect of $n$ on the time complexity of IGNNS.\n\n__Comment3:__  As regards the experiments, the results seem promising, but information on trining time should be added. \n\n__Reply3:__  In section 5.3, we have added training time information, including the total training time and the average training time per epoch. In section 3.6, We also discuss theoretical the time complexity of IGNNS.\n\n__Comment4:__  In section 3.3, to compute the global representation R one can decide between two different definitions. These two definitions are very different and combine the results of each iteration. However, it is not clear to me how I should decide which one to use. \n\n__Reply4:__  Indeed, this is a very useful question. So far, I can't prove theoretically which way is better. In my opinion, the most important consideration is the size of the parameter matrix $\\mathbf{W}$ of the output layer. Suppose that the dimension of the hidden space is $H$, the dimension of the output layer is $P$, and the depth of the network is $n$. If the average operator is used, the parameter quantity of $\\mathbf{W}$ is $HP$; if the concatenation operator is used, the parameter quantity of $\\mathbf{W}$ is $nHP$. If $n$ and $H$ are both large, then the $nHP$ is also very large, which makes it difficult to train the network, especially for semi supervised experiments. If $H$ is less and $n$ is not large, we can consider concatenation operator. In short, I don't think there is a unified standard.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "ZXlLGtFfOoP", "original": null, "number": 7, "cdate": 1605536886944, "ddate": null, "tcdate": 1605536886944, "tmdate": 1605597253211, "tddate": null, "forum": "9vCLOXwprc", "replyto": "4nbRwGYPslj", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 1 (Part 2)", "comment": "__Comment3:__ Additionally, there is no further analysis or ablation study provided. While the performance improvement seems to be hugel, without those analysis, it is really hard to understand where the improvement comes from.\n\n__Reply3:__ \n\n__1) We've added a new experiment, which is the performance of completely linear IGNNS (see section 5.4).__\n\nIn Nonlinear IGNNS, we use the nonlinear activation function $\\text{ReLU}(x)$, learn adjoint probability vector $\\mathbf{p}=(p_0,p_1)$ by\n$$p_0\\leftarrow\\frac{\\text{ReLU}(p_0)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}, p_1\\leftarrow\\frac{\\text{ReLU}(p_1)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}$$ and learn the representation layer coefficient $\\mathbf{r}=(r_1,r_2,...,r_n)$ by $r_i\\leftarrow\\text{ReLU}(r_i)$ with initial value $r_i=\\left(\\frac{1}{r}\\right)^{i-1}$ where $r=\\sqrt{\\ln(N)+0.577215664}$. \nIn this experiment, to get a completely linear IGNNS, we let all the activation functions be the identity function, i.e. $\\sigma(x)=x$, and let the adjoint probability vector $\\mathbf{p}=(p_0,p_1)$ and the representation layer coefficient $\\mathbf{r}=(r_1,r_2,...,r_n)$ be hyperparameters without learning. \n\n__Table1   Performance of completely linear IGNNS__\n\n Method &emsp;|&emsp; Cora &emsp; | &emsp; Citeseer &emsp;|&emsp;  Pubmed \n:---- | :----: | :----: | :----: \nGCN |  81.5| 70.3| 79.0\nIGNNS(Linear) |__83.9__| __72.4__ | __79.9__\n\nWe can see from Table 1 that the performance of completely linear IGNNS is better than that of baseline model GCN. Compared with other models, completely linear IGNNS is still competitive. This is due to the fact that the IFS can extract more features than spectral filters. In the new version of the paper, more comments on this conclusion have been added.\n\n__2)  We analyze the original experimental results in detail (see section 5.3).__\n\nWe already know  the improved performance of model IGNNS in dataset Cora and Citeseer is much higher than that in dataset Pubmed. To understand why this happens, we analyze the characteristics of these citation networks. We consider two statistical properties of networks, one is the Network Density $d(\\mathcal{G})$, which is defined as $d(\\mathcal{G})=\\frac{2L}{N(N-1)}$, where $N$ is the number of nodes and $L$ is the number of edges, and the other is the Average Clustering Coefficient $C$, which is defined as $C=\\frac{1}{N}\\sum_{i\\in V}C_i$, where $V$ is the set of nodes, $C_i=\\frac{2e_i}{k_i(k_i-1)}$, $k_i$ is the number of the neighbors of node $v_i$ and $e_i$ is the number of undirected edges between $k_i$ neighbors. The less Network Density means the more global sparsity of the network, and the less Average Clustering Coefficient means the more sparsity of the neighbors of nodes. The calculation results of the statistical characteristics of the network are shown in Table 2 . We can see  that Pubmed is more sparse than Cora and Citeseer. The performance of IGNNS benefits from the bidirectional mixed propagation of information between nodes, but this sparsity weakens the gain of IGNNS.\n\n__Table2  Statistical characteristics of the networks. Bold for minimum.__\n\n Statistical characteristics &emsp;&emsp;|&emsp; Cora &emsp;&emsp;| &emsp;&emsp;Citeseer &emsp; |&emsp;  Pubmed &emsp;\n:---- | :----: | :----: | :----: \nNetwork Density | 0.00144000 | 0.00084514 | __0.00022805__\nAverage Clustering Coefficient| 0.24067330 | 0.14147102  |__0.06017521__\n\n__Comment4:__ \"Therefore, the above architectures can not deal with directed graph directly\". I believe this is not the case: existing GNNs can naively work with directed graphs by doing message passing following the edge direction.\n\n__Reply4:__  We think that the GNN based on spectral analysis cannot deal with directed graph directly because it requires the  adjacency matrix to be symmetric matrix. But there is no denying that it is possible to use some additional techniques to make it suitable for directed graphs. In order to avoid semantic fuzziness, we have modified the relevant statements.\n\n__Comment5:__ The paper mentions after Eq (3) that H^(n) has H x 2^n elements. Will it be a scalability concern?\n\n__Reply5:__ We want to emphasize that this is the maximum possible number. Which is the maximum amount of message that can be obtained by the IFS. When $n$ tends to infinity, it will produce fractal set, which is an uncountable set. So we call H^(n) the fractal representation of IGNNS. We also discuss this in section 5.4. In Appendix A, we present a visualization example, that is, Bi-GCN can only obtain the boundary of fractal set, while IGNNS can obtain all the information. At the same time, we also notice that the ergodic representation of IGNNS makes use of all the information obtained. But now the question is, is information all necessary? Is there redundancy? How to select the valid message from the fractal representation becomes the focus of our research in the next stage."}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "F_jAaArxVX", "original": null, "number": 6, "cdate": 1605533549549, "ddate": null, "tcdate": 1605533549549, "tmdate": 1605596124595, "tddate": null, "forum": "9vCLOXwprc", "replyto": "4nbRwGYPslj", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 1 (Part 1)", "comment": "Thank you very much for your comments.\n\n__Comment1:__ However, there lack enough discussions with existing multi-layer GNNs. The paper mentions that \"the message passing in the two directions is independent and lacks of interaction\". While this is true for a single layer GNN, when the GNN is multi-layer, the messages sent in deeper layers contains fused information from multiple directions. Furthermore, if skip connections are used, the messages sent in deeper layers can have even richer information. These discussions are lacking in the current paper.\n\n__Reply1:__ We define the direction as follows: first, we give the graph node a index number so that the $i$ row of the adjacency matrix represents the adjacency relation between node $i$ and other nodes. Thus, in this way, we can define the forward direction as $i\\rightarrow j$ if $i<j$, where $i,j$ denote the index number of nodes. Similarly, we can define the backward direction as $i \\leftarrow j$ if $i<j$.  For self adjacency node $i$, we can also define two directions (clockwise and anticlockwise).  In this sense, forward direction  matrix $\\mathbf{A}_0$  is  the upper triangular matrix of $\\mathbf{A}$ , backward direction matrix $\\mathbf{A}_1$  is  the lower triangular matrix of $\\mathbf{A}$.   In this sense, Bi-GCN (similar to Bi-LSTM) is equivalent to two independent GCNs induced by $\\mathbf{A}_0$ and $\\mathbf{A}_1$ respectively (simple matrix operation is needed to change $\\mathbf{A}_i$ into symmetric matrix), only in output layer, two independent representations (generated by two independent GCNs) are concatenated together. Of course, you can define various directions. In order to avoid ambiguity, we have revised the relevant statements. In section 4 and section 5.4, We have added relevant discussion.\n\n__Comment2:__ Moreover, the evaluation is very insufficient. Firstly, the paper mentions a General Framework in Section 5, including a new model R-IGNNS. However, no evaluation is made at all. I would regard the experiments as incomplete. \n\n__Reply2:__  We initially included R-IGNNS in the paper in order to emphasize theoretically that IGNNS is a unified architecture. After re evaluation, we believe that R-IGNNS is an independent and complex work, which is worthy of further study and improvement. In addition, we believe that R-IGNNS does not affect the integrity of current work, so we delete the discussion on R-IGNNS in the newly revised papers."}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "N5r7Gg77-lX", "original": null, "number": 2, "cdate": 1605512244640, "ddate": null, "tcdate": 1605512244640, "tmdate": 1605595877849, "tddate": null, "forum": "9vCLOXwprc", "replyto": "xQhaZs2yMeu", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 2.  We added new experiments and analyzed all experimental results.", "comment": "Thank you very much for your comments.\n\n__Comments:__  My major concern is about the experiment part (and thus my recommendation would just be weak acceptance) . The authors only perform experiments on three datasets, and it is unclear if the same approach will be effective on other datasets/settings. Moreover, although we see performance improvement on two of the three datasets, necessary discussion about the experimental results is missing. It would be great if the authors could explain why the proposed method improves the performance on Cora and Citeseer, and achieves worse performance compared to some of the baselines on Pubmed, to give the readers a better idea when this new framework would be effective.\n\n__Reply:__ \n\n\n#1) We've added a new experiment, which is the performance of completely linear IGNNS (see section 5.4). \n\nIn Nonlinear IGNNS, we use the nonlinear activation function $\\text{ReLU}(x)$, learn adjoint probability vector $\\mathbf{p}=(p_0,p_1)$ by\n$$p_0\\leftarrow\\frac{\\text{ReLU}(p_0)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}, p_1\\leftarrow\\frac{\\text{ReLU}(p_1)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}$$ and learn the representation layer coefficient $\\mathbf{r}=(r_1,r_2,...,r_n)$ by $r_i\\leftarrow\\text{ReLU}(r_i)$ with initial value $r_i=\\left(\\frac{1}{r}\\right)^{i-1}$ where $r=\\sqrt{\\ln(N)+0.577215664}$. \nIn this experiment, to get a completely linear IGNNS, we let all the activation functions be the identity function, i.e. $\\sigma(x)=x$, and let the adjoint probability vector $\\mathbf{p}=(p_0,p_1)$ and the representation layer coefficient $\\mathbf{r}=(r_1,r_2,...,r_n)$ be hyperparameters without learning. \n\n__Table1   Performance of completely linear IGNNS__\n\n Method &emsp;|&emsp; Cora &emsp; | &emsp; Citeseer &emsp;|&emsp;  Pubmed \n:---- | :----: | :----: | :----: \nGCN |  81.5| 70.3| 79.0\nIGNNS(Linear) |__83.9__| __72.4__ | __79.9__\n\nWe can see from Table 1 that the performance of completely linear IGNNS is better than that of baseline model GCN. Compared with other models, completely linear IGNNS is still competitive. This is due to the fact that the IFS can extract more features than spectral filters. In the new version of the paper, more comments on this conclusion have been added.\n\n#2)  We analyze the original experimental results in detail (see section 5.3).\n\nWe already know  the improved performance of model IGNNS in dataset Cora and Citeseer is much higher than that in dataset Pubmed. To understand why this happens, we analyze the characteristics of these citation networks. We consider two statistical properties of networks, one is the Network Density $d(\\mathcal{G})$, which is defined as $d(\\mathcal{G})=\\frac{2L}{N(N-1)}$, where $N$ is the number of nodes and $L$ is the number of edges, and the other is the Average Clustering Coefficient $C$, which is defined as $C=\\frac{1}{N}\\sum_{i\\in V}C_i$, where $V$ is the set of nodes, $C_i=\\frac{2e_i}{k_i(k_i-1)}$, $k_i$ is the number of the neighbors of node $v_i$ and $e_i$ is the number of undirected edges between $k_i$ neighbors. The less Network Density means the more global sparsity of the network, and the less Average Clustering Coefficient means the more sparsity of the neighbors of nodes. The calculation results of the statistical characteristics of the network are shown in Table 2 . We can see  that Pubmed is more sparse than Cora and Citeseer. The performance of IGNNS benefits from the bidirectional mixed propagation of information between nodes, but this sparsity weakens the gain of IGNNS.\n\n__Table2  Statistical characteristics of the networks. Bold for minimum.__\n\n Statistical characteristics &emsp;&emsp;|&emsp; Cora &emsp;&emsp;| &emsp;&emsp;Citeseer &emsp; |&emsp;  Pubmed &emsp;\n:---- | :----: | :----: | :----: \nNetwork Density | 0.00144000 | 0.00084514 | __0.00022805__\nAverage Clustering Coefficient| 0.24067330 | 0.14147102  |__0.06017521__\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "1kfRafeP_RJ", "original": null, "number": 5, "cdate": 1605528997082, "ddate": null, "tcdate": 1605528997082, "tmdate": 1605585947845, "tddate": null, "forum": "9vCLOXwprc", "replyto": "H0Uqn3euSt", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 4 (Part 3)", "comment": "__Comment5:__  It would be nice for the authors to provide discussions with relevant works [1-3] on graph neural networks. [1] proposes a generalized aggregation function that allows successful and reliable training of very deep GCNs and how the proposed theorems in work could unify the mixed results (as Thm 4.1 and 4.2 state that the characterization ability of IGNNS would decrease with the increase of IFS iterations). [2] proposes a method for directional message passing as well. [3] proposes a theoretical framework for analyzing which type of GNN would achieve better generalization performance on the given task, which is also related to this paper for explaining the performance gain by IGNNS.\n\n__Reply5:__ Thank you very much for letting us know about these studies which are closely related to our work. We have discussed these works in the newly revised paper( section 1 and section 4 ).\n\n__Comment6:__ The quality of the writing could be much more improved. It would be nice for the authors to provide: a) better intuitions on its analysis using IFS (e.g. what is the physical meaning of the fractal set in IFS and why is it important). b) more connections between its proposed architecture and the empirical experiment section (e.g. how the proposed theorem could explain the performance gain connections) There are also grammar mistakes in the paper which may hinder the understanding of the readers (e.g. last sentence in the abstract).\n\n__Reply6:__  \n1\uff09I added a new explanation to the original experiment results (section 5.3). We conclude that IGNNS is more suitable for dense networks, which is in line with our experience. The performance of IGNNS benefits from the bidirectional mixed propagation of information between nodes, but the sparsity of network will weakens the gain of IGNNS.\n2)  We've added a new experiment, which is the performance of completely linear IGNNS (see section 5.4). The experimental results show that completely linear IGNNS is better than that of baseline model GCN. Compared with other models, completely linear IGNNS is still competitive. Combining with IFS, fractal set and over-smoothing problem of depth GNN, we fully discuss the experimental results.\n3) We present a visualization example (see Appendix A).  This example vividly shows that Bi-GCN and IGGNS are connected by fractal set generated by iterated function system, i.e. Bi-GCN gets boundary messages and IGNNS gets all messages.\n4) We have read through the paper many times and tried to correct all the mistakes.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "qyOF8ZXeUD_", "original": null, "number": 4, "cdate": 1605527339629, "ddate": null, "tcdate": 1605527339629, "tmdate": 1605585844139, "tddate": null, "forum": "9vCLOXwprc", "replyto": "H0Uqn3euSt", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 4 (Part 2)", "comment": "__Comment3:__ In Figure 1, how would the message passing in d) be different compared to the message passing in c) after two iterations?\n\n__Reply3:__ Let's use an idealized example to illustrate the difference, which is  the lossy message passing model of gaph $\\mathcal{G}$ with only one self adjacent node $v$. For the sake of discussion, we assume that the dimension of the hidden space is equal to 1. This means that the input of GNN (Bi-GCN or IGNNS) is a point in $\\mathbb{R}$. For convenience, we ignore the activation function(regard as $\\sigma(x)=x$) and parameter matrix (regard as $\\mathbf{I}_{n,m}$). We assume that messages are sent from node $v$, propagate in two directions (clockwise and anticlockwise), and are finally received by node $v$. The received messages in the clockwise direction become one-third of the original, and the received messages in the anticlockwise direction become one-third of the original plus a constant of $\\frac{2}{3}$. It is expressed by mathematical formula as follows\n$$f_0(x)=\\frac{x}{3},\\quad f_1(x)=\\frac{x}{3}+\\frac{2}{3},\\quad x\\in\\mathbb{R}.$$\nFor Bi-GCN, messages are delivered independently in both directions. In other words, there are two independent channels, and the message passing (transmitting or receiving) can only be carried out by their own channels. Let $x_0\\in\\mathbb{R}$ be the initial message. In the clockwise direction, after $n$ passes, the received message is\n$$\\frac{x_0}{3}, \\frac{x_0}{3^2},..., \\frac{x_0}{3^n}\\rightarrow0.$$\nIn the anticlockwise direction, the received message is\n$$\\frac{x_0}{3}+\\frac{2}{3}, \\frac{x_0}{3^2}+\\frac{2}{3^2}+\\frac{2}{3},..., \\frac{x_0}{3^n}+\\sum_{i=1}^n\\frac{2}{3^i}\\rightarrow1.$$\nFor IGNNS, the two channels have a connection point at node $v$. first, node $v$ sends the message $x_0$ in both directions, and the connection point of node $v$ will receive two message $\\{f_0(x_0), f_1(x_0)\\}$. In the second launch, any message $(f_0(x_0)$ or $f_1(x_0))$ can be sent in both directions, so the received message is $$\\{f_0(f_0(x_0)),f_0(f_1(x_0)),f_1(f_0(x_0)),f_1(f_1(x_0))\\}.$$ In summary, After $n$ passes, the received message is $$\\mathbb{H}^{(n)}=\\{f_{\\textbf{i}}(x_0)\\}_{|\\textbf{i}|=n}\\rightarrow C,$$ where $C$ is the famous Cantor Set.\n\n__Comment4:__ It would be nice for the authors to provide a theoretical analysis on the computational complexity for the proposed architecture IGNNS.\n\n__Reply4:__ In the section 3.6 of the newly  revised papers\uff0c we give theoretical time complexity of IGNNS. That is \n$$\\text{T}(\\text{IGNNS})= O(2^{n}N^2H+N^2P)\\quad \\text{or} \\quad O(2^{n}N^2H+N^2P+nNHP).$$\nWhere $n$ is the number of iterations of IFS, $N$ is the number of nodes, $H$ is the dimension of the latent space and $P$ is the dimension of the output layer. In practice, for large graphs, $2^{n}N^2H\\gg N^2P \\gg nNHP$, thus $2^{n}N^2H$ is the main factor affecting the time complexity of IGNNS. Furthermore, for large graphs of the same size, $n$ is the main important factor affecting time complexity."}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "MyKcHY4x-M", "original": null, "number": 3, "cdate": 1605519620581, "ddate": null, "tcdate": 1605519620581, "tmdate": 1605585268128, "tddate": null, "forum": "9vCLOXwprc", "replyto": "H0Uqn3euSt", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 4 (Part 1)", "comment": "Thank you very much for your comments.\n\n__Comment1:__  It seems unclear to me how the theoretical analysis via IFS could be used to explain the empirical performance gain on citation networks.\n\n__Reply1:__  In response to your question, we present a visualization example (see Appendix A). For this idealized example,  Bi-GCN gets boundary messages of fractal set and IGNNS gets all messages of fractal set (Figure 3). In section 5.4, We also talked about the empirical performance gain on citation networks. \n\nLet's give a simple example to emphasize this point. Let $\\mathcal{G}$ be a completely connected graph with two nodes. Then the normalization adjacency matrix of $\\mathcal{G}$ is $A=[[0.5,0.5],[0.5,0.5]]$.  Then for any $n$, $A^n=[[0.5,0.5],[0.5,0.5]]$. Let $X=[[x_1,y_1],[x_2,y_2]]$ be feature matrix of nodes. Then $A^nX=[[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}],[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}]]$. This is an ordinary feature.  \n\nBut for IGNNS, We will get an extraordinary feature (fractal representation).\nIt is easy to see that $A_0=[[0.5,0.5],[0,1]]$ and  $A_1=[[1,0],[0.5,0.5]]$. Thus\n\n$\\mathbb{H}^{(1)}$ ={$A_0X,A_1X$}={$[[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}],[x_2,y_2]]$, $[[x_1,y_1],[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}]]$}\n$\\mathbb{H}^{(2)}$={$A_0A_0X,A_0A_1X$,$A_1A_0X,A_1A_1X$}\n\n__Comments2:__ According to the formulas in equation (3) and above on page 4, it seems that the mathematical expectation $\\mathbf{E}_n$ is still linear (affine) w.r.t. to the input $\\mathbf{X}^{\\text{int}}$. Then if we use a learnable matrix to learn such combinations of matrices$\\mathbf{A}_i$  and probability vector $\\mathbf{p}$, would this be equivalent to applying an MLP to for each the message passing iteration (the adjacency information in $\\mathbf{A}_i$ is accessible to update the MLP via backpropagation)?\n\n__Reply2:__  Indeed, if $p_0, p_1$ are super parameters that does not need to be learned, then $\\mathbf{E}_i$ is completely linear.   If $p_0, p_1$ are learnable\uff0c we learn them by\n\n$$p_0\\leftarrow\\frac{\\text{ReLU}(p_0)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}, p_1\\leftarrow\\frac{\\text{ReLU}(p_1)+0.1}{\\text{ReLU}(p_0)+\\text{ReLU}(p_1)+0.2}.$$\n\nWe think this is an interesting question. Of course, MLP can approach $\\mathbf{E}_i$  arbitrarily, however, the generalization ability still needs further study.  We do not use this method In IGNNS, because of the following considerations\uff1a\n(1) The introduction of learnable parameter matrix in IFS layer will not be the iterative result of IFS and can not be consistent in mathematical form;\n(2) Even if the shared parameter matrix is used in each layer, the parameter quantity is equal to $N^2$ ($N$ is the number of nodes), which is difficult to train for large graphs. \n(3) A positive number multiplied by the adjacency matrix $\\mathbf{A}$ of graph does not change the inherent adjacency relation of nodes, which is what we need to get. Therefore, we only use  $\\mathbf{p}_i\\mathbf{A}_i$  to keep the multi-hoop or interactive relationship of graph nodes, where $i=i_1i_2...i_m$ .  However, $W\\mathbf{A}_i$ may destroy the graph structure when you use it as input for the next iteration, where $W\\in \\mathbb{R}^{N\\times N}$  a learnable parameter matrix. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "cFaWnXNmskc", "original": null, "number": 9, "cdate": 1605584978698, "ddate": null, "tcdate": 1605584978698, "tmdate": 1605584978698, "tddate": null, "forum": "9vCLOXwprc", "replyto": "3JSklwPNdKr", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment", "content": {"title": "To Reviewer 3 (Part 2) ", "comment": "__Comment5:__  I would like to see in the paper an explanation of what they represent and why it is more useful to calculate an average or a concatenation of the results of each iteration instead of considering only the results of the last iteration (which should depend on all the previous one).\n\nReply5:  We think that your problem is related to the core problem of deep GNN, that is, the overs-moothing problem.  We discussed this issue in Section 4. In short, the features of the last layer of deep GNN will become indistinguishable, so we often combine the previous features to overcome this smoothness.  In order to overcome the over-smoothing problem of deep GNN,  many new methods have been proposed, such as Skip connection[1], Drop edge[2], Residual connection [3], Identity mapping[4], Generalized message aggregation functions[5] and so on.  \n\nLet's give an example. Let $\\mathcal{G}$ be a completely connected graph with two nodes. Then the normalization adjacency matrix of $\\mathcal{G}$ is $A=[[0.5,0.5],[0.5,0.5]]$.  Then for any $n$, $A^n=[[0.5,0.5],[0.5,0.5]]$. Let $X=[[x_1,y_1],[x_2,y_2]]$ be feature matrix of nodes. Then $A^nX=[[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}],[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}]]$. This is an ordinary feature.  \n\nBut for IGNNS, We will get an extraordinary feature (fractal representation).\nIt is easy to see that $A_0=[[0.5,0.5],[0,1]]$ and  $A_1=[[1,0],[0.5,0.5]]$. Thus\n\n$\\mathbb{H}^{(1)}$ ={$A_0X,A_1X$}={$[[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}],[x_2,y_2]]$, $[[x_1,y_1],[\\frac{x_1+x_2}{2},\\frac{y_1+y_2}{2}]]$}\n$\\mathbb{H}^{(2)}$={$A_0A_0X,A_0A_1X$,$A_1A_0X,A_1A_1X$}\n\nAt the same time, we also notice that the ergodic representation of IGNNS makes use of all the information obtained. But now the question is, is information all necessary? Is there redundancy? How to select the valid message from the fractal representation becomes the focus of our research in the next stage.\n\n[1]Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie\nJegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018.\n[2]Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph\nconvolutional networks on node classification. In International Conference on Learning Repre-\nsentations, 2020.\n[3]J. Klicpera, A. Bojchevski, and S. Gunnemann. Predict then propagate: Graph neural networks meet\npersonalized pagerank. In ICLR, 2019a.\n[4]Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph\nconvolutional networks. In International Conference on Machine Learning, 2020.\n[5]Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train\ndeeper gcns. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1109/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9vCLOXwprc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1109/Authors|ICLR.cc/2021/Conference/Paper1109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Comment"}}}, {"id": "3JSklwPNdKr", "original": null, "number": 1, "cdate": 1603287855550, "ddate": null, "tcdate": 1603287855550, "tmdate": 1605024528549, "tddate": null, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review", "content": {"title": "The description of the proposal could be improved", "review": "The paper proposes a new definition of GNNs designed to cope with bi-directional message-passing processes.  To do so, a new symbols space, different from the one adopted by Bidirectional GCN,  is considered, together with an iterated function system. These lead to an architecture composed of 4 steps: an input layer that acts as a classic FC layer; an IFS layer that applies the iterated function system considering the adjacency matrix; a layer to concatenate or sum the expected values of each iteration; and an output layer that combines the results using the functions of the IFS and a new learnable weight matrix.\nExperiments on citations datasets show significantly better results than those obtained by many different related works.\n\nThe paper proposes an interesting approach, but personally, I found the document a bit foggy in some parts. On page 2, iterated function systems are introduced, but the definition does not explain what the f_i functions are, how many functions there may be and how the probabilities are related to the functions. This detail can be assumed in section 3.2 when the expected values are computed using the probabilities from the p set. However, in this section there are undefined symbols, such as p_{00}, p_{01}, etc., which make it difficult to follow the explanation of the step.\n\nIn section 3.2, a short discussion of the value of n (the number of iteration) should be added to say whether this value depends on the input graph, on the values of the H matrices in each iteration or something else.\n\nIn section 3.3, to compute the global representation R one can decide between two different definitions. These two definitions are very different and combine the results of each iteration. However, it is not clear to me how I should decide which one to use. I would like to see in the paper an explanation of what they represent and why it is more useful to calculate an average or a concatenation of the results of each iteration instead of considering only the results of the last iteration (which should depend on all the previous one).\n\nAs regards the experiments, the results seem promising, but information on trining time should be added.\nUnfortunately, however,  the description of the datasets used is too simplified.  It should explain how the nodes are extracted from those available in the datasets and why only 20 nodes per class are extracted. It seems that only very few nodes are considered despite the availability of a large number of them. In addition, details about the edges connecting the nodes and the features can also be useful, as not all the edges/features will be considered since only 20 nodes per class are used. It is necessary to explain if these nodes belong to only one class or to several classes because there could be unbalance problems. Also, it is not specified which function is used in the representation layer.\nIt would be interesting to understand what happened for the Pubmed dataset. Probably the result depends on the small percentage of nodes considered.\n\nPros\n- The results seem interesting\n\nCons\n- Overall the proposal seems interesting, but its description lacks important definitions.\n- Code and datasets are not available. Unfortunately, this does not help to evaluate the proposal.\n\nTypos\nThe second line of section 3.4: labels instead of lebels\nIn theorem 4.2 furthermore instead of further more\nIn the references, arXiv URLs are not well-formed.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126722, "tmdate": 1606915773154, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review"}}}, {"id": "4nbRwGYPslj", "original": null, "number": 2, "cdate": 1603751427443, "ddate": null, "tcdate": 1603751427443, "tmdate": 1605024528486, "tddate": null, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "invitation": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review", "content": {"title": "An interesting method with insufficient discussion and evaluation", "review": "Overall, this paper proposes Iterated Graph Neural Network System, which provides a novel way for computing GNN messages.\n\nHowever, there lack enough discussions with existing multi-layer GNNs. \nThe paper mentions that \"the message passing in the two directions is independent and lacks of interaction\". While this is true for a single layer GNN, when the GNN is multi-layer, the messages sent in deeper layers contains fused information from multiple directions. \nFurthermore, if skip connections are used, the messages sent in deeper layers can have even richer information.\nThese discussions are lacking in the current paper.\n\nMoreover, the evaluation is very insufficient.\nFirstly, the paper mentions a General Framework in Section 5, including a new model R-IGNNS. However, no evaluation is made at all. I would regard the experiments as incomplete.\nAdditionally, there is no further analysis or ablation study provided. While the performance improvement seems to be hugel, without those analysis, it is really hard to understand where the improvement comes from.\n\nMore comments:\n1 \"Therefore, the above architectures can not deal with directed graph directly\". I believe this is not the case: existing GNNs can naively work with directed graphs by doing message passing following the edge direction.\n\n2 The paper mentions after Eq (3) that H^(n) has H x 2^n elements. Will it be a scalability concern?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1109/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1109/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated graph neural network system", "authorids": ["~Hanju_Li1"], "authors": ["Hanju Li"], "keywords": [], "abstract": "We present Iterated Graph Neural Network System (IGNNS), a new framework of Graph Neural Networks (GNNs), which can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layer for iterating, and then obtain the high-level representation of graph nodes. We also analyze the geometric properties of IGNNS from the perspective of dynamical system. We prove that if the IFS induced by IGNNS is contractive, then the fractal representation of graph nodes converges to the fractal set of IFS in Hausdorff distance and the ergodic representation of that converges to a constant matrix in Frobenius norm. We have carried out a series of semi supervised node classification experiments on citation network datasets such as citeser, Cora and PubMed. The experimental results show that the performance of our method is obviously better than the related methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|iterated_graph_neural_network_system", "supplementary_material": "/attachment/b7bdc400b82470a3a975401de0d7845a2d4b6543.zip", "pdf": "/pdf/db91605be08c98a99f63edddc8963e64cb2749d1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sUF1AtRB5D", "_bibtex": "@misc{\nli2021iterated,\ntitle={Iterated graph neural network system},\nauthor={Hanju Li},\nyear={2021},\nurl={https://openreview.net/forum?id=9vCLOXwprc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9vCLOXwprc", "replyto": "9vCLOXwprc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126722, "tmdate": 1606915773154, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1109/-/Official_Review"}}}], "count": 15}