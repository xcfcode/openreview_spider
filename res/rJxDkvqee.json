{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488517370496, "tcdate": 1478287192231, "number": 347, "id": "rJxDkvqee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJxDkvqee", "signatures": ["~Wanjia_He1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396530408, "tcdate": 1486396530408, "number": 1, "id": "Bk9u3fUue", "invitation": "ICLR.cc/2017/conference/-/paper347/acceptance", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396530957, "id": "ICLR.cc/2017/conference/-/paper347/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396530957}}}, {"tddate": null, "tmdate": 1484345936489, "tcdate": 1484345936489, "number": 3, "id": "BydLG08Il", "invitation": "ICLR.cc/2017/conference/-/paper347/public/comment", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["~Weiran_Wang1"], "readers": ["everyone"], "writers": ["~Weiran_Wang1"], "content": {"title": "Responses", "comment": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287614504, "id": "ICLR.cc/2017/conference/-/paper347/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxDkvqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper347/reviewers", "ICLR.cc/2017/conference/paper347/areachairs"], "cdate": 1485287614504}}}, {"tddate": null, "tmdate": 1482115179294, "tcdate": 1482115179294, "number": 3, "id": "rJmOdpVEl", "invitation": "ICLR.cc/2017/conference/-/paper347/official/review", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/conference/paper347/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper347/AnonReviewer4"], "content": {"title": "The paper investigates jointly trained acoustic and character level word embeddings, but only on a very small task.", "rating": "5: Marginally below acceptance threshold", "review": "Pros:\n  Interesting training criterion.\nCons:\n  Missing proper ASR technique based baselines.\n\nComments:\n  The dataset is quite small.\n  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.\n  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection\n  performance of out-of-vocabulary words.\n  It would be interesting to show scatter plots for embedding vs. orthographic distances.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512616757, "id": "ICLR.cc/2017/conference/-/paper347/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper347/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper347/AnonReviewer3", "ICLR.cc/2017/conference/paper347/AnonReviewer1", "ICLR.cc/2017/conference/paper347/AnonReviewer4"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512616757}}}, {"tddate": null, "tmdate": 1482014283866, "tcdate": 1482014283866, "number": 2, "id": "S1Q8CV7Nl", "invitation": "ICLR.cc/2017/conference/-/paper347/official/review", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/conference/paper347/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper347/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.\n\nThere's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.\n\n the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.\n\n it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512616757, "id": "ICLR.cc/2017/conference/-/paper347/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper347/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper347/AnonReviewer3", "ICLR.cc/2017/conference/paper347/AnonReviewer1", "ICLR.cc/2017/conference/paper347/AnonReviewer4"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512616757}}}, {"tddate": null, "tmdate": 1482011776258, "tcdate": 1482011776258, "number": 1, "id": "HydYVVQVx", "invitation": "ICLR.cc/2017/conference/-/paper347/official/review", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/conference/paper347/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper347/AnonReviewer3"], "content": {"title": "well-done domain adaptation", "rating": "6: Marginally above acceptance threshold", "review": "this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512616757, "id": "ICLR.cc/2017/conference/-/paper347/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper347/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper347/AnonReviewer3", "ICLR.cc/2017/conference/paper347/AnonReviewer1", "ICLR.cc/2017/conference/paper347/AnonReviewer4"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512616757}}}, {"tddate": null, "tmdate": 1481584074884, "tcdate": 1481584074878, "number": 2, "id": "BJXA6j27l", "invitation": "ICLR.cc/2017/conference/-/paper347/public/comment", "forum": "rJxDkvqee", "replyto": "HysYkhymg", "signatures": ["~Wanjia_He1"], "readers": ["everyone"], "writers": ["~Wanjia_He1"], "content": {"title": "Reply", "comment": "How big are the training/dev/test sets in seconds?\n\n------Each of them is roughly 10K seconds (10K words, each 0.5-2 seconds long).\n\nHaving 60M test word pair samples, how many samples have the answer yes, how many no? (Prior probabilities of the 2 classes)\n\n------# of yes: 96929, # of no: 60661847.  As is typical in detection/retrieval problems, the vast majority of answers are \u201cno\u201d and the hard part is in finding the \u201cyes\u201ds. \n\nSaying always \"no\" (different words) seems to be a good strategy.\n\n------Saying always \u201cno\u201d corresponds to a single point on the precision-recall curve, where recall equals 0 and precision is undefined (when calculating AP, we ignore this point).  As is typical in detection/retrieval problems, we do not consider a single classifier but rather the entire precision-recall curve, and report the average precision as a measure of performance.\n\nWhy have the authors not considered proper ASR techniques as baselines:\n a, Cross-view word discrimination: Measuring score difference between a background HMM model and a word model, a la keyword spotting.\n\n b, Acoustic word discrimination: Training phone/grapheme based HMM neural network acoustic model, phone posteriors could be extracted.\n    Using DTW in posterior feature space with KL-divergence would definitelly result in a better DTW baseline, aka. posterior based features in template matching.\n\n------For technique (a), one would need to know in advance the set of query words and have a reasonable amount of training data for each.  This is different from the setting we are considering, which is an open-vocabulary task with no prior knowledge of the query words.  We have >3000 unique query words, much larger than in any typical keyword spotting task (but we are open to suggestions -- if there is a reference you can recommend that does address this setting with a more typical keyword spotting-type model, please let us know).\n\n------For technique (b), this has been done by others and the best reported AP on this test set (from Carlin et al. 2011) is 0.497.  This is much worse than our results and also uses a large amount of external training data.  We will add this point to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287614504, "id": "ICLR.cc/2017/conference/-/paper347/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxDkvqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper347/reviewers", "ICLR.cc/2017/conference/paper347/areachairs"], "cdate": 1485287614504}}}, {"tddate": null, "tmdate": 1481584021300, "tcdate": 1481584021295, "number": 1, "id": "Hkp5Tshme", "invitation": "ICLR.cc/2017/conference/-/paper347/public/comment", "forum": "rJxDkvqee", "replyto": "rkgOxB0Mx", "signatures": ["~Wanjia_He1"], "readers": ["everyone"], "writers": ["~Wanjia_He1"], "content": {"title": "Reply", "comment": "have you tried using all combination of losses and doing ablation for each one of them? that would be informative.\n\n------We did not try all possible combinations and ablations, only the most promising ones based on the single-loss results.  Yes, we can consider additional combinations, although we expect that it will not add a very significant value to the paper beyond the combinations we have already included.\n\nhave you tried pretraining characterLSTM on a large text corpus to initialize your model? I think your dataset is too small to learn a good character model.\n\n------That is a good idea.  For the time being we wanted to stick to using the closed data set that others have used, for a fair comparison, but for future work we can expand to larger data sets, for pretraining or otherwise."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287614504, "id": "ICLR.cc/2017/conference/-/paper347/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxDkvqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper347/reviewers", "ICLR.cc/2017/conference/paper347/areachairs"], "cdate": 1485287614504}}}, {"tddate": null, "tmdate": 1480732547208, "tcdate": 1480732547205, "number": 2, "id": "HysYkhymg", "invitation": "ICLR.cc/2017/conference/-/paper347/pre-review/question", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/conference/paper347/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper347/AnonReviewer4"], "content": {"title": "questions", "question": "How big are the training/dev/test sets in seconds?\nHaving 60M test word pair samples, how many samples have the answer yes, how many no? (Prior probabilities of the 2 classes)\nSaying always \"no\" (different words) seems to be a good strategy.\n\nWhy have the authors not considered proper ASR techniques as baselines:\n  a, Cross-view word discrimination: Measuring score difference between a background HMM model and a word model, a la keyword spotting.\n  b, Acoustic word discrimination: Training phone/grapheme based HMM neural network acoustic model, phone posteriors could be extracted.\n     Using DTW in posterior feature space with KL-divergence would definitelly result in a better DTW baseline, aka. posterior based features in template matching.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959330587, "id": "ICLR.cc/2017/conference/-/paper347/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper347/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper347/AnonReviewer3", "ICLR.cc/2017/conference/paper347/AnonReviewer4"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959330587}}}, {"tddate": null, "tmdate": 1480638567798, "tcdate": 1480638567794, "number": 1, "id": "rkgOxB0Mx", "invitation": "ICLR.cc/2017/conference/-/paper347/pre-review/question", "forum": "rJxDkvqee", "replyto": "rJxDkvqee", "signatures": ["ICLR.cc/2017/conference/paper347/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper347/AnonReviewer3"], "content": {"title": "combination of losses & pretraining", "question": "- have you tried using all combination of losses and doing ablation for each one of them? that would be informative.\n- have you tried pretraining characterLSTM on a large text corpus to initialize your model? I think your dataset is too small to learn a good character model.\n\nMINOR issues:\n- in figure-1 half of $f_2(x)$ is missing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "pdf": "/pdf/834028d78a4c420011b4949cd56a203bcfca27b8.pdf", "paperhash": "he|multiview_recurrent_neural_acoustic_word_embeddings", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959330587, "id": "ICLR.cc/2017/conference/-/paper347/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper347/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper347/AnonReviewer3", "ICLR.cc/2017/conference/paper347/AnonReviewer4"], "reply": {"forum": "rJxDkvqee", "replyto": "rJxDkvqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959330587}}}], "count": 10}