{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124477858, "tcdate": 1518445689135, "number": 125, "cdate": 1518445689135, "id": "B1-tVX1Pz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1-tVX1Pz", "signatures": ["~Tom_Zahavy1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning How Not to Act in Text-based Games", "abstract": "Large actions spaces impede an agent's ability to learn, especially when many of the actions are redundant or irrelevant. This is especially prevalent in text-based domains. We present the action-elimination architecture which combines the generalization power of Deep Reinforcement Learning and the natural language capabilities of NLP architectures to eliminate unnecessary actions and solves quests in the text-based game of Zork, significantly outperforming the baseline agents.", "paperhash": "haroush|learning_how_not_to_act_in_textbased_games", "keywords": ["Deep Reinforcement Learning", "Natural Language Processing"], "_bibtex": "@misc{\n  haroush2018learning,\n  title={Learning How Not to Act in Text-based Games},\n  author={Matan Haroush and Tom Zahavy and Daniel J. Mankowitz and Shie Mannor},\n  year={2018},\n  url={https://openreview.net/forum?id=B1-tVX1Pz}\n}", "authorids": ["matan.h@campus.technion.ac.il", "tomzahavy@campus.technion.ac.il", "danielm@campus.technion.ac.il", "shie@ee.technion.ac.il"], "authors": ["Matan Haroush", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "TL;DR": "A DRL agent that learns to eliminate actions in order to solve text-based games with large action spaces ", "pdf": "/pdf/6c6d2a550b8842c8ec561f28ce78d717a5f11886.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582839573, "tcdate": 1520594786304, "number": 1, "cdate": 1520594786304, "id": "SJqv1eetG", "invitation": "ICLR.cc/2018/Workshop/-/Paper125/Official_Review", "forum": "B1-tVX1Pz", "replyto": "B1-tVX1Pz", "signatures": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer3"], "content": {"title": "Noisy results, incremental and questionable method", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes an 'Action Elimination Network' (AEN) in order to impose an a-priory (state dependent) restriction on the actions considered by the RL agent. The AEN is training on a replay buffer of state, action and feedback tuples, where the feedback is coming from the emulator (indicating whether or not an action is valid). \n\nOne concern is that simply adding a shaping reward which puts a large penalty on illegal actions could in principle lead to the same learning dynamics. \nPredicting whether a given action is valid by the AEN is then equivalent to predicting a large negative reward by the DQN. Obviously this would have to combined with boltzman exploration (or similar) in order to ensure that the illegal actions are selected with lower frequency.\n\nAnother concern is that the results are extremely noisy. They seem to be based on a single random seed and lack any statistical validation. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning How Not to Act in Text-based Games", "abstract": "Large actions spaces impede an agent's ability to learn, especially when many of the actions are redundant or irrelevant. This is especially prevalent in text-based domains. We present the action-elimination architecture which combines the generalization power of Deep Reinforcement Learning and the natural language capabilities of NLP architectures to eliminate unnecessary actions and solves quests in the text-based game of Zork, significantly outperforming the baseline agents.", "paperhash": "haroush|learning_how_not_to_act_in_textbased_games", "keywords": ["Deep Reinforcement Learning", "Natural Language Processing"], "_bibtex": "@misc{\n  haroush2018learning,\n  title={Learning How Not to Act in Text-based Games},\n  author={Matan Haroush and Tom Zahavy and Daniel J. Mankowitz and Shie Mannor},\n  year={2018},\n  url={https://openreview.net/forum?id=B1-tVX1Pz}\n}", "authorids": ["matan.h@campus.technion.ac.il", "tomzahavy@campus.technion.ac.il", "danielm@campus.technion.ac.il", "shie@ee.technion.ac.il"], "authors": ["Matan Haroush", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "TL;DR": "A DRL agent that learns to eliminate actions in order to solve text-based games with large action spaces ", "pdf": "/pdf/6c6d2a550b8842c8ec561f28ce78d717a5f11886.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582839383, "id": "ICLR.cc/2018/Workshop/-/Paper125/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper125/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper125/AnonReviewer2"], "reply": {"forum": "B1-tVX1Pz", "replyto": "B1-tVX1Pz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper125/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582839383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582793302, "tcdate": 1520627997725, "number": 2, "cdate": 1520627997725, "id": "Hy87Z_xYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper125/Official_Review", "forum": "B1-tVX1Pz", "replyto": "B1-tVX1Pz", "signatures": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer2"], "content": {"title": "Interesting take on large action spaces", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper describes a method for dealing with large action spaces where a partial model of the environment is learned which predicts the likelihood that a certain action will be 'valid' for a given state.  This model is learned in parallel to the agent's learning, and is used to prune the action space to make it more tractable.   If I understand correctly, the policy network continues to provide value predictions for the full set of N actions, but only a subset will be considered during the argmax calculation.  Therefore, this approach still scales linearly with N which is a bit disappointing, however it learns a sort of 'certainty' estimator that avoids spurious action predictions from the policy network from harming the final action choice (I am gleaming this from the last sentence citing Hester et al.).\n\nI would however appreciate it if you make the action selection process slightly more clear in an eventual final submission, as well as detail the O(.) complexity of your algorithm, as I am not 100% sure I understood the exact way in which everything is put together.\n\nOverall I think the idea of pruning the action set makes a lot of sense, both in large action spaces, but also in situations where experience coverage is not uniform over the action space.  I encourage more work in this direction.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning How Not to Act in Text-based Games", "abstract": "Large actions spaces impede an agent's ability to learn, especially when many of the actions are redundant or irrelevant. This is especially prevalent in text-based domains. We present the action-elimination architecture which combines the generalization power of Deep Reinforcement Learning and the natural language capabilities of NLP architectures to eliminate unnecessary actions and solves quests in the text-based game of Zork, significantly outperforming the baseline agents.", "paperhash": "haroush|learning_how_not_to_act_in_textbased_games", "keywords": ["Deep Reinforcement Learning", "Natural Language Processing"], "_bibtex": "@misc{\n  haroush2018learning,\n  title={Learning How Not to Act in Text-based Games},\n  author={Matan Haroush and Tom Zahavy and Daniel J. Mankowitz and Shie Mannor},\n  year={2018},\n  url={https://openreview.net/forum?id=B1-tVX1Pz}\n}", "authorids": ["matan.h@campus.technion.ac.il", "tomzahavy@campus.technion.ac.il", "danielm@campus.technion.ac.il", "shie@ee.technion.ac.il"], "authors": ["Matan Haroush", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "TL;DR": "A DRL agent that learns to eliminate actions in order to solve text-based games with large action spaces ", "pdf": "/pdf/6c6d2a550b8842c8ec561f28ce78d717a5f11886.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582839383, "id": "ICLR.cc/2018/Workshop/-/Paper125/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper125/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper125/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper125/AnonReviewer2"], "reply": {"forum": "B1-tVX1Pz", "replyto": "B1-tVX1Pz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper125/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582839383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573568949, "tcdate": 1521573568949, "number": 114, "cdate": 1521573568611, "id": "ByYTRRRYz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1-tVX1Pz", "replyto": "B1-tVX1Pz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning How Not to Act in Text-based Games", "abstract": "Large actions spaces impede an agent's ability to learn, especially when many of the actions are redundant or irrelevant. This is especially prevalent in text-based domains. We present the action-elimination architecture which combines the generalization power of Deep Reinforcement Learning and the natural language capabilities of NLP architectures to eliminate unnecessary actions and solves quests in the text-based game of Zork, significantly outperforming the baseline agents.", "paperhash": "haroush|learning_how_not_to_act_in_textbased_games", "keywords": ["Deep Reinforcement Learning", "Natural Language Processing"], "_bibtex": "@misc{\n  haroush2018learning,\n  title={Learning How Not to Act in Text-based Games},\n  author={Matan Haroush and Tom Zahavy and Daniel J. Mankowitz and Shie Mannor},\n  year={2018},\n  url={https://openreview.net/forum?id=B1-tVX1Pz}\n}", "authorids": ["matan.h@campus.technion.ac.il", "tomzahavy@campus.technion.ac.il", "danielm@campus.technion.ac.il", "shie@ee.technion.ac.il"], "authors": ["Matan Haroush", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "TL;DR": "A DRL agent that learns to eliminate actions in order to solve text-based games with large action spaces ", "pdf": "/pdf/6c6d2a550b8842c8ec561f28ce78d717a5f11886.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}