{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730180904, "tcdate": 1509102279264, "number": 349, "cdate": 1518730180896, "id": "BJy0fcgRZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJy0fcgRZ", "original": "HykAGcxRZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": ["H1xPRyzwz"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260092417, "tcdate": 1517249591951, "number": 338, "cdate": 1517249591931, "id": "r1eHVyaBf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper introduces a GAN-based framework for inferring human category representations. The reviewers agree that the idea is interesting and well-motivated, and the results are promising. The technical contribution is not significant, but nevertheless the paper combines existing ideas in an interesting way. The reviewers would also like to see some more work towards the direction of investigation of the results and extraction of insights, without which the paper feels somehow incomplete.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642437159, "tcdate": 1511524787141, "number": 1, "cdate": 1511524787141, "id": "r1i3YtHgG", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "deep extension of MCMCP", "rating": "6: Marginally above acceptance threshold", "review": "Quality\n\nThis paper demonstrates that human category representations can be inferred by sampling deep feature spaces. The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN. The approach is thoroughly validated using two online behavioural experiments.\n\nClarity\n\nThe rationale is clear and the results are straightforward to interpret. In Section 4.2.1 statements on resemblance and closeness to mean faces could be tested. Last sentences on page 7 are hard to parse. The final sentence probably relates back to the CI approach. A few typos.\n\nOriginality\n\nThe approach is a straightforward extension of the MCMCP approach using generative models.\n\nSignificance \n\nThe approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models. Extensive experiments demonstrate the usefulness of the approach.\n\nPros\n\nUseful extension of an important technique backed up by behavioural experiments.\n\nCons\n\nDoes not provide new theory but combines existing ideas in a new manner.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437054, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer2", "ICLR.cc/2018/Conference/Paper349/AnonReviewer1", "ICLR.cc/2018/Conference/Paper349/AnonReviewer3"], "reply": {"forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437054}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642437116, "tcdate": 1511813186902, "number": 2, "cdate": 1511813186902, "id": "H1jrxgclM", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Using GANs for visualizing human representations of visual categories.", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a method based on GANs for visualizing how humans represent visual categories. Authors perform experiments on two datasets: Asian Faces Dataset and ImageNet Large Scale Recognition Challenge dataset.\n\nPositive aspects:\n+ The idea of using GANs for this goal is smart and interesting\n+ The results seem interesting too\n\nWeaknesses:\n- Some aspects of the paper are not clear and presentation needs improvement.\n- I miss a clearer results comparison with previous methods, like Vondrick et al. 2015.\n\nSpecific comments and questions:\n\n-  Figure 1 is not clear. Authors should clarify how they use the inference network and what the two arrows from this inference network represent.\n- Figure 2 is also not clear. Just the FLD projections of the MCMCP chains are difficult to interpret. The legend of the figure is too tiny. The right part of the figure should be better described in the text or in the caption, I don't understand well what this illustrates.\n- Regarding to the human experiments with AMT: how do the authors deal with noise on the workers performance? Is any qualification task used? What are the instructions given to the workers?\n- In section 4.2. the authors state \"We also simultaneously learn a corresponding inference network, .... granular human biases captured\". This seems interesting but I didn't find any result on that in the paper. Can you give more details or refer to where in the paper it is discussed/tested?\n- Figure 4 shows \"most interpretable mixture components\".  How this \"most interpretable\" were selected?\n- In second paragraph Section 4.3, it should be Table 1 instead of Figure 1. \n- It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2. To see the confusion matrixes could be useful.\n\nI like this paper. The addressed problem is challenging and the proposed idea seems interesting.  However, the aspects mentioned make me think the paper needs some improvements to be published.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437054, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer2", "ICLR.cc/2018/Conference/Paper349/AnonReviewer1", "ICLR.cc/2018/Conference/Paper349/AnonReviewer3"], "reply": {"forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437054}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642437070, "tcdate": 1511818726498, "number": 3, "cdate": 1511818726498, "id": "Bk01UWclf", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "This paper proposes a method for using MCMCP to characterize a distribution of GAN-constrained image statistics corresponding to a human concept category. The paper describes a well-motivated method for investigating an interesting problem, but feels incomplete in exploring natural questions presented by the problem.", "rating": "5: Marginally below acceptance threshold", "review": "The idea of using MCMCP with GANs is well-motivated and well-presented\nin the paper, and the approach is new as far as I know.  Figures 3 and 5 are\nconvincing evidence that MCMCP compares favorably to direct sampling of\nthe GAN feature space using the classification images approach.\n\nHowever, as discussed in the introduction, the reason an efficient\nsampling method might be interesting would be to provide insight\non the components of perception.  On these insights, the paper felt\nincomplete.\n\nFor example, it was not investigated whether the method identifies\nclassification features that generalize.  The faces experiment is\nsimilar to previous work done by Martin (2011) and Kontsevich\n(2004) but unlike that previous work does not investgiate whether\nclassification features have been identified that can be added to an\narbitrary image to change the attribute \"happy vs sad\" or \"male vs female\".\n\nSimilarly, the second experiment in Table 1 compares classification\naccuracy between different sampling methods, but it does not provide\nany comparison as done in Vondrick (2015) to a classifier trained\nin a conventional way (such as an SVM), so it is difficult to discern\nwhether the learned distributions are informative.\n\nFinally, the effect of choosing GAN features vs a more \"naive\" feature\nspace is not explored in detail.  For example, the GAN is trained\non an image data set with many birds and cars but not many\nfire hydrants.  Is the method giving us a picture of this data set?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437054, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper349/AnonReviewer2", "ICLR.cc/2018/Conference/Paper349/AnonReviewer1", "ICLR.cc/2018/Conference/Paper349/AnonReviewer3"], "reply": {"forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437054}}}, {"tddate": null, "ddate": null, "tmdate": 1515138856617, "tcdate": 1515138856617, "number": 5, "cdate": 1515138856617, "id": "H1cX123mM", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "forum": "BJy0fcgRZ", "replyto": "BJy0fcgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper349/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper349/Authors"], "content": {"title": "Post-review changes", "comment": "Changes in response to initial reviews: clarifications, fixed typos, extended figure captions, and a small revision to Figure 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735169, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJy0fcgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper349/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper349/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper349/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers", "ICLR.cc/2018/Conference/Paper349/Authors", "ICLR.cc/2018/Conference/Paper349/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735169}}}, {"tddate": null, "ddate": null, "tmdate": 1515138433632, "tcdate": 1515138433632, "number": 4, "cdate": 1515138433632, "id": "BJ5FaihmM", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "forum": "BJy0fcgRZ", "replyto": "r1i3YtHgG", "signatures": ["ICLR.cc/2018/Conference/Paper349/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper349/Authors"], "content": {"title": "Small fixes and a note on theory", "comment": "Thank you for your comments. We have addressed some typos and unclear sentences and agree that additional experiments in the future to understand the nature of the gendered smile bias face results would be interesting.\n\nNote that our work can be viewed as engaging with the theoretical problem of estimating unobservable mental content. MCMCP in pixel space provides the perfect solution to this problem, yet is surely intractable. Here we propose that a tractable first step is to assume a reasonable approximation (using an invertible feature space), from which further iterative improvements can be made."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735169, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJy0fcgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper349/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper349/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper349/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers", "ICLR.cc/2018/Conference/Paper349/Authors", "ICLR.cc/2018/Conference/Paper349/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735169}}}, {"tddate": null, "ddate": null, "tmdate": 1515138364130, "tcdate": 1515138364130, "number": 3, "cdate": 1515138364130, "id": "ByxrasnQf", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "forum": "BJy0fcgRZ", "replyto": "H1jrxgclM", "signatures": ["ICLR.cc/2018/Conference/Paper349/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper349/Authors"], "content": {"title": "All concerns have been addressed", "comment": "Thank you for your comments and suggestions. We include many comparisons with the classification image method used by Vondrick et al. (2015) that focus on the mental content of the captured distributions, which is the goal of our paper (see Figures 2, 3, and 5, as well as Table 1). Like Vondrick et al. (2015), we show that classifiers derived from mental distributions do better than chance in predicting labels of real images. However, unlike Vondrick et al. (2015), note that we are not interested in augmenting computer vision methods to improve benchmark scores, but rather in developing innovative methods for modeling human mental representations.\n\nAnswers to specific comments and questions:\n\n- Our newest draft makes Figure 1 more informative. Note that the inference network does not need to be used, and was not used in Section 4.1, because we know which z vectors generated which images for each set of trials, and do not need to convert the generated images back to inferred z representations in practice. However, the inference network is necessary for any application of our method that requires the use of any image not rendered by the network (i.e., in order to classify new images such as in Section 4.3).\n\n- Figure 2 has been enlarged and the newest uploaded draft extends the caption. The FLD projections simply show that the chains for different categories are well-separated, meaning that they successfully characterize different featural content.\n\n- We told AMT workers that it was important that they answer as best they can and used stringent selection criteria. If a single image did not load, the data was thrown out and a new subject was recruited to continue the chain at its original entry point. \n\n- In order to include inference in our GAN network, we use BiGAN (Donahue et al., 2016).\n\n- In Figure 4, we show the means of the mixture components with the largest mixture weights. We excluded only a small set of components that were presumably useful in explaining holdout samples and classifying, but which had no discernable visual content (washed out brown color). This appears to happen whenever large numbers of samples are summarized by a single mean (the CI method often showed only this behavior).\n\n- Comparing MCMCP Density and MCMCP Mean for groups 1 and 2 tells us little because the individual categories do not always give the same results. More importantly, we see the variation in results as a lesson that an inflexible method may not be able to cope with particular categories and how they interact with the particular latent space learned by the network. Using MCMCP avoids having to make any such limiting assumptions, and we can simply choose the density with the best fit to human samples."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735169, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJy0fcgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper349/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper349/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper349/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers", "ICLR.cc/2018/Conference/Paper349/Authors", "ICLR.cc/2018/Conference/Paper349/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735169}}}, {"tddate": null, "ddate": null, "tmdate": 1515138197170, "tcdate": 1515138197170, "number": 2, "cdate": 1515138197170, "id": "HJp9njnQM", "invitation": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "forum": "BJy0fcgRZ", "replyto": "Bk01UWclf", "signatures": ["ICLR.cc/2018/Conference/Paper349/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper349/Authors"], "content": {"title": "Reasons for the noted omissions", "comment": "Thank you for your comments and suggestions. We agree that more can be done to inspect the nature of the solution obtained by combining MCMCP with modern generative networks, but we see this as future application of the overall toolset we\u2019ve designed and demonstrated in the current work. The suggested method of using classification features to change image attributes assumes a linear/additive feature space. Since we can learn any distribution with MCMCP, these simple methods do not apply to the general case.\n\nOne of the methods we used to compare Classification Images (CI) and MCMCP was to assess how learned mental distributions could predict class labels for images held out from the training set. However, it is important to note that any held out set of images suffers from the same dataset bias that we sought to avoid (see introduction). For this reason, while a better estimate of a mental concept may perform better than other methods in predicting held out sets, there is no guarantee that it will converge to a model that performs equally or better than classifiers trained on those biased datasets. In keeping with the specific goals of our paper, we included no such analysis in our paper. However, the reviewer may find it useful to know that classifiers trained on a similar training set to the held out images were more successful in predicting class labels for those held out images (the test set). Inspecting the samples from the captured mental distributions gives us good reason to believe mental and synthetic concepts are different because many images favored by humans appear a great deal more abstract than what would be expected from current generative models (e.g., see water bottle examples).\n\nIt is unclear how stratification of classes in the datasets used to train our networks detract significantly from the results presented in our paper (i.e., it is unlikely to interact with our finding regarding the improvement over CI). Also note that we strategically avoided classes that are most disproportionately represented in the ILSVRC12 dataset, such as \u201cdog\u201d, which makes up more than 10% of the dataset."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capturing Human Category Representations by Sampling in Deep Feature Spaces", "abstract": "Understanding how people represent categories is a core problem in cognitive science, with the flexibility of human learning remaining a gold standard to which modern artificial intelligence and machine learning aspire. Decades of psychological research have yielded a variety of formal theories of categories, yet validating these theories with naturalistic stimuli remains a challenge. The problem is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires having a workable representation of these stimuli. Deep neural networks have recently been successful in a range of computer vision tasks and provide a way to represent the features of images. In this paper, we introduce a method for estimating the structure of human categories that draws on ideas from both cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep representation learners. We provide qualitative and quantitative results as a proof of concept for the feasibility of the method. Samples drawn from human distributions rival the quality of current state-of-the-art generative models and outperform alternative methods for estimating the structure of human categories.", "pdf": "/pdf/25517f85b1d4ecfe69c658faadbbf877b2e69a0f.pdf", "TL;DR": "using deep neural networks and clever algorithms to capture human mental visual concepts", "paperhash": "peterson|capturing_human_category_representations_by_sampling_in_deep_feature_spaces", "_bibtex": "@misc{\npeterson2018capturing,\ntitle={Capturing Human Category Representations by Sampling in Deep Feature Spaces},\nauthor={Joshua Peterson and Krishan Aghi and Jordan Suchow and Alexander Ku and Tom Griffiths},\nyear={2018},\nurl={https://openreview.net/forum?id=BJy0fcgRZ},\n}", "keywords": ["category representations", "psychology", "cognitive science", "deep neural networks"], "authors": ["Joshua Peterson", "Krishan Aghi", "Jordan Suchow", "Alexander Ku", "Tom Griffiths"], "authorids": ["peterson.c.joshua@gmail.com", "kaghi@berkeley.edu", "suchow@berkeley.edu", "alexku@berkeley.edu", "tom_griffiths@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735169, "id": "ICLR.cc/2018/Conference/-/Paper349/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJy0fcgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper349/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper349/Authors|ICLR.cc/2018/Conference/Paper349/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper349/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper349/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper349/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper349/Reviewers", "ICLR.cc/2018/Conference/Paper349/Authors", "ICLR.cc/2018/Conference/Paper349/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735169}}}], "count": 9}