{"notes": [{"id": "SJMBM2RqKQ", "original": "HJgLez65FQ", "number": 1264, "cdate": 1538087949426, "ddate": null, "tcdate": 1538087949426, "tmdate": 1545355377752, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJlBJw6bxN", "original": null, "number": 1, "cdate": 1544832733357, "ddate": null, "tcdate": 1544832733357, "tmdate": 1545354531187, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to reject. However, the reviewers did not engage at all with the authors, and did not acknowledge whether their concerns have been answered. I therefore lean to reject, and would recommend the authors to resubmit. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n\n", "confidence": "2: The area chair is not sure", "recommendation": "Reject", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1264/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352901679, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352901679}}}, {"id": "HkgRrpYcAX", "original": null, "number": 4, "cdate": 1543310662335, "ddate": null, "tcdate": 1543310662335, "tmdate": 1543310662335, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "B1l2REPYh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "content": {"title": "Responses to individual comments from R#3", "comment": "We would like to thank you again for your time and your feedback. Please see first our meta response to major comments shared between reviewers. Here we refer to separate comments/questions we received from you.\n\nWe also found it behaving inconsistent through the new experimental setting we adapted per reviewers' request. Therefore, as an alternative, we used a simple regularization trick on the learning rates instead of directly minimizing the changes on network parameters. Please see our updated draft regarding the altered regularization method. \n\nWe have tried to address this feedback provided by all reviewers by replicating the experimental setting used in the literature to be able to make fair comparisons. Specifically asked by R#1, we have a full comparison with 7 other baselines and 8 datasets. Please see the updated version of the paper regarding this matter. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604841, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJMBM2RqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1264/Authors|ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604841}}}, {"id": "SylXWhF5CX", "original": null, "number": 3, "cdate": 1543310330730, "ddate": null, "tcdate": 1543310330730, "tmdate": 1543310330730, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "BJx-HPCY3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "content": {"title": "Responses to individual comments from R#2", "comment": "We would like to thank you again for your time and your feedback. Please see first our meta response to major comments shared between reviewers. Here we refer to separate comments/questions we received from you.\n\nWe acknowledge all your concerns regarding our proposed regularization technique. We also found it behaving inconsistent through the new experimental setting we adapted per reviewers' request. Therefore, as an alternative, we used a simple regularization trick on the learning rates instead of directly minimizing the changes on network parameters. Please see our updated draft regarding the altered regularization method. \n\nPruning percentage: this is a valid concern which we have now addressed in Figure 1 and a subsection under section 6.1. We briefly explain here that in the continual learning regime we do not know how many tasks we yet have to learn. Therefore, we can only decide based on the current performance of our model on a held-out validation set. By computing validation accuracy as a function of pruning percentage we can set a threshold beyond which, we do not wish to downgrade in our performance. Figure 1 shows such a plot for MNIST Split dataset when incrementally learned in two tasks.\n\nYour concern regarding the datasets size and number of baselines are now addressed in the updated draft with having 7 baselines with 8 short and long sequences of tasks\n\nThe listed typos and minor issues are now fixed."}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604841, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJMBM2RqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1264/Authors|ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604841}}}, {"id": "SklZQcuqR7", "original": null, "number": 2, "cdate": 1543305753077, "ddate": null, "tcdate": 1543305753077, "tmdate": 1543305753077, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "Syl6wT6qnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "content": {"title": "Responses to individual comments from R#1", "comment": "Please see the addressed comments shared between reviewers first. Here we refer to separate comments/questions we received from you:\n\n(1) We have corrected this in the manuscript (page 2 paragraph 2)\n\n(2) Fixed.\n\n(3)  We would like to thank you again for suggesting the mentioned paper by Serr\u00e0, Joan, et al. (2018). Per your request, we have changed our experimental setting in accordance to it and included full comparison with the datasets provided in [*]. Paper is fully updated with the obtained results.\n\n\nQuestions you raised:\n\n(1) We have modified our regularization approach explained in our shared meta response. Instead of minimizing the changes between current parameters and updated values, we scale up or down their learning rate conditioned on how important they are, i.e. how big their STD is. Hence equation 6 no longer exists in the paper. \n\n(2) This is a valid point and we agree that the memory size overhead was not clearly explained so let us clarify this with explaining how much encoding a mask and writing it to memory will cost us. The overhead memory per parameter in encoding the mask as well as saving it on the disk is as follows. Assuming we have $n$ tasks to learn using a single network, the total number of required bits to encode an accumulated mask for a parameter is at max $\\log_2{n}$ bits assuming a parameter was found to be important from task $1$ and kept being encoded in the mask. Saving the binary mask for a typical model with $n$ tasks results in a mask size of $1/n^2$ with respect to the initial model size. \n\n\n*** Per your request on improving the text we have re-written large parts of the text."}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604841, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJMBM2RqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1264/Authors|ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604841}}}, {"id": "B1e-kOu90X", "original": null, "number": 1, "cdate": 1543305176977, "ddate": null, "tcdate": 1543305176977, "tmdate": 1543305176977, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "content": {"title": "Addressing comments shared between reviewers ", "comment": "We thank all the reviewers for their constructive feedback and time. We would like to address some common concerns across all the reviewers first before going to individual responses.\n\n1- All reviewers had fairly asked for more experiments and baselines, usage of larger datasets and deeper analysis. \n \nWe believe this was a valid point which we have tied to address as much as we can. Upon Reviewer #1\u2019s request we have used similar experimental setting introduced in (Serr\u00e0, Joan, et al. 2018) and compared against 7 baselines including HAT, EWC, PathNet, PNN, LWF, LFL, IMM on short and long sequences of 8 datasets in total including Split MNIST, Permuted MNIST, Alternating incremental CIFAR10/100  FaceScrub, Not NotMNIST, SVHN, TrafficSigns, and FashionMNIST. \nWe have also included reference baselines such as fine-tuning and feature extraction, as well as joint training using both Bayesian and non-Bayesian networks.\n\nUpon Reviewer#3\u2019s request  we have also compared against VCL, as well as GEM, and IS on Permuted MNIST. Due to the extensive evaluations, we fully switched to the tasks provided in Serr\u00e0, Joan, et al. 2018 and abandoned fine grained classification datasets we had in the initial version. \n\n2. All reviewers had comments and questions regarding the regularization method. While experimenting with the new datasets with our Bayesian approach, we came to realize that the regularization method which was initially introduced in our paper exhibits inconsistent behavior to overcome forgetting on different datasets, leading us to believe it is not a promising approach. Instead, we were able to find an alternative simpler regularization technique that is also easier to comprehend and empirically performs better.\n\n The change in the regularization method is as follows: instead of minimizing the change in both mean and variance of the parameters distributions, we now control the gradient updates for mean of the distributions based on the predicted uncertainty we have for them. This mean that we begin with a usual constant learning rate for all parameters, and as we train for more epochs, we compute sigma (standard deviation) of the means. We simply used the STD as an indicative of their uncertainty. The more uncertain (higher STD) a parameter is computed to have, the more it should be allowed to be updated in future epochs. Therefore, we use uncertainty (STD) as a scalar to scale up or down the learning rate of all Mu parameters, The intuition behind this is that we wish to minimize any further changes on the means by simply imposing a small learning rate to them while allowing the variances to change. This results in allowing the model to learn more concepts while preserving the critical information obtained in the past. We used this intuitive regularization trick throughout the paper when we were not using pruning. \nThe key benefit from using such an approach is that we do not need to wait for a task to finish to find  the most important parameters. We do not even need to know when tasks switching occurs. By simply modifying our optimizer to adjust the learning rate based on the computed uncertainty, we regularize at every epoch, resulting in a model that is less prone to forget. \n\nPaper has been updated with all these changes and added experiments.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604841, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJMBM2RqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1264/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1264/Authors|ICLR.cc/2019/Conference/Paper1264/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers", "ICLR.cc/2019/Conference/Paper1264/Authors", "ICLR.cc/2019/Conference/Paper1264/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604841}}}, {"id": "Syl6wT6qnm", "original": null, "number": 3, "cdate": 1541229924882, "ddate": null, "tcdate": 1541229924882, "tmdate": 1541533284748, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "content": {"title": "Good motivation, minor contributions in term of algorithms", "review": "Motivated from leveraging the uncertainty information in Bayesian learning, the authors propose two algorithms to prevent forgetting: Pruning and Regularization. Experiments on several sequential learning tasks show the improved performance.\n\nQuality:  The description on the related work is comprehensive. The proposed algorithms seem easy to follow. \n \nClarity: Low \n\nThe contributions in terms of algorithms are clearly presented. However, the writing can be largely improved.\n\n(1) Some claims are improper:  I don't think it's accurate to say that most of lifelong learning is non-Bayesian (In introduction), and EWC is derived from a Bayesian perspective, and Variational Conditional Learning is a very Bayesian approach.\n\n(2) Please proofread the submission: \nTypos: e.g.,  \"Beysian\", \"citestochastic methods\";  \nStyle: x is not bold occasionally, but has the meaning given the context. \n\nOriginality: It seems to be the first work that leverages the variance in Bayesian Neural Nets (BNN) to prevent forgetting. My understanding that EWC also consider the variance, but in a different way. \n\nSignificance: \nIt is good to consider variance/uncertainty for lifelong learning, and should be encouraged.\nHowever, the comparison to the representative algorithms or state-of-the-art is missing in this submission. For example, EWC/IS, or method in [*].  Is it possible to run the experiments on more standard datasets, such as [*].\n\n[*] Overcoming Catastrophic Forgetting with Hard Attention to the Task, ICML 2018\n\n\nQuestions:\n1. In (6), there are three terms on the right side, it seems the 2nd term include the 3rd term, why do we need to add the 3rd term again?\n\n2. \"Once a task is learned, an associated binary mask is saved which will be used at inference to recover key parameters to the desired task. The overhead memory caused by saving the binary mask (less than 20MB for ResNet18), is negligible given the fact it completely eliminates the forgetting\"\n\nTo me, saving a binary mask means saving \"partial\" model. First, this is additional parameter saving. Second, in the inference stage, one can recover the corresponding best model using the mask, how close is it to cheating? (Perhaps I am not an expert in lifelong learning). \nCan you put the model size of ResNet18, so that the readers can understand 20MB is small/negligible compared to the full model. \n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "cdate": 1542234268099, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335908925, "tmdate": 1552335908925, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJx-HPCY3m", "original": null, "number": 2, "cdate": 1541166904644, "ddate": null, "tcdate": 1541166904644, "tmdate": 1541533284547, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "content": {"title": "Review", "review": "The paper addresses the problem of lifelong learning of neural networks - a setting where learning is performed on a continuously arriving new tasks without having access to previously encountered data.\nAuthors propose a method that prevents catastrophic forgetting typical for naive application of stochastic gradient descent by preventing supposedly important weights to change (in either soft of hard manner), where the weight importance is assessed by its signal to noise ratio estimated from the corresponding (approximate) posterior distribution.\nAuthors evaluate their approach on a set of image classification datasets and find it superior to the PackNet baseline as well as few simpler ones.\n\nThe idea of using uncertainty estimates obtained from Bayesian training to adjust weight updates is natural and potentially very promising. \nHowever, to me this paper does not seem to investigate the idea sufficiently deep.\n\nThe weight pruning or hard masking variant of the method depends on a very important hyperparameter p (size of the mask) which is unclear how to set beforehand. \n\nI also struggle with understanding the weight regularisation or soft masking variant. \nAuthors seem to get their inspiration in the idea of assumed density filtering, where the posterior for 1:T-1 is approximated and used a prior for task T (last sentence on page 5).\nAt the same time, in Algorithm 2, line 6 the prior is defined as the standard BBB mixture prior and not the approximate posterior from the previous task.\nQuite oddly, parameters of the _approximate posterior_ are being quadratically regularalized to not deviate from parameters of the _approximate posterior_ from the previous task. \nThis deviates from the original idea and requires additional justification.\nBesides that, I find the way this regularisation is applied potentially problematic for the variance parameter (last term in eq. 6).\nHere authors apply the regularisation to the parameter of the softplus transformation they use, but scale it with the inverse std deviation which is the \u201cclassical\u201d parametrisation. The choice of parametrisation was not discussed, however, clearly different parametrizations may lead to very different results. \n\nOn the experimental side, I have two major issues:\n1. The datasets considered are very small, authors could consider using ImageNet, especially given that they already work with 224x224 images.\n2. The only prior work used as a baseline is PackNet, while there is no reason why other established methods such as EWC are not applicable.\n\nMinor comments:\nThe middle expression in eq. 5 seems to miss the -log p(D_T | D_{1:T-1}) term which does not change the latter expression (since it does not depend on parameters theta).\nPage 3: \u201ccitestochastic methods\u201d, a citation seems to be missing.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "cdate": 1542234268099, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335908925, "tmdate": 1552335908925, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1l2REPYh7", "original": null, "number": 1, "cdate": 1541137619914, "ddate": null, "tcdate": 1541137619914, "tmdate": 1541533284340, "tddate": null, "forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "content": {"title": "Nice combination of ideas, but requires more development.", "review": "In this paper, a framework for lifelong learning based on Bayesian neural network is proposed. The key idea is to combine iterative pruning for multi-task learning along with the weight regularization. The idea of iterative pruning was first considered by Mallya et al., 2018 and weight regularization was considered for Bayesian neural network by Nguyen et al., 2018.\n\nPros: \n- Combination of two idea seems novel. I like the idea of considering the weight parameter as the \"global\" random variables and the mask parameters as the task-specific random variables. \n\nCons: \n- In general, there is lack of explanation/justification on the combination of two ideas. Especially, there is lack of explanation on how to apply the whole algorithm (e.g., text states that complete algorithm is in Algorithm 3., but there is no Algorithm 3. in the paper). \n\n- I do not understand how equation (6) is developed, and why hyper-parameters are need for \"regularization of weights\", comparing with the Variational Continual Learning (VCL, Nguyen et al., 2018). More explanation seems necessary for justification of the algorithm.\n\n- More stronger baselines need to be considered for the experiments. Why is there no comparison with the existing continual learning algorithms? At the very least, comparison with the VCL or Elastic Weight Consolidation (EWC, Kirkpatrick et al., 2017) seems necessary since one of the key idea is about regularization for weights.\n\n\nIn general, I think it is a nice idea to combine two existing approaches. However, the algorithm lacks justification in general and experimental results are not very persuasive.   ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1264/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Lifelong Learning in Bayesian Networks", "abstract": "Sequentially learning of tasks arriving in a continuous stream is a complex problem and becomes more challenging when the model has a fixed capacity. Lifelong learning aims at learning new tasks without forgetting  previously learnt ones as well as freeing up capacity for learning future tasks. We argue that identifying the most influential parameters in a representation learned for one task plays a critical role to decide on \\textit{what to remember} for continual learning.  Motivated by the statistically-grounded uncertainty defined in Bayesian neural networks, we propose to formulate a Bayesian lifelong learning framework, \\texttt{BLLL}, that addresses two lifelong learning directions: 1)  completely eliminating catastrophic forgetting using weight pruning, where a hard selection mask freezes the most certain parameters (\\texttt{BLLL-PRN}) and 2) reducing catastrophic forgetting by adaptively regularizing the learning rates using the parameter uncertainty (\\texttt{BLLL-REG}). While \\texttt{BLLL-PRN} is by definition a  zero-forgetting guaranteed method, \\texttt{BLLL-REG}, despite exhibiting some small forgetting, is a task-agnostic lifelong learner, which does not require to know when a new task arrives. This feature makes \\texttt{BLLL-REG} a more convenient candidate for applications such as robotics or on-line learning in which such information is not available. We evaluate our Bayesian learning approaches extensively on diverse object classification datasets in short and long sequences of tasks and perform superior or marginally better than the existing approaches.", "keywords": ["lifelong learning", "continual learning", "sequential learning"], "authorids": ["sayna@eecs.berkeley.edu", "elhoseiny@fb.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "TL;DR": "We formulate lifelong learning in the Bayesian-by-Backprop framework, exploiting the parameter uncertainty in two settings: for pruning network parameters and in importance weight based continual learning.", "pdf": "/pdf/0b1bdde7c924e5e8999492df6b67f2c4feac8d51.pdf", "paperhash": "ebrahimi|uncertaintyguided_lifelong_learning_in_bayesian_networks", "_bibtex": "@misc{\nebrahimi2019uncertaintyguided,\ntitle={Uncertainty-guided Lifelong Learning in Bayesian Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nyear={2019},\nurl={https://openreview.net/forum?id=SJMBM2RqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1264/Official_Review", "cdate": 1542234268099, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJMBM2RqKQ", "replyto": "SJMBM2RqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1264/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335908925, "tmdate": 1552335908925, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1264/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}