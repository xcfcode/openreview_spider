{"notes": [{"id": "BygWRaVYwH", "original": "BJg1cUWdwH", "number": 845, "cdate": 1569439177035, "ddate": null, "tcdate": 1569439177035, "tmdate": 1577168232467, "tddate": null, "forum": "BygWRaVYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ir875TV2Bj", "original": null, "number": 1, "cdate": 1576798707605, "ddate": null, "tcdate": 1576798707605, "tmdate": 1576800928743, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers agree that the technical innovations presented in this paper are not great enough to justify acceptance.  The authors correctly point out to the reviewers that the ICLR CFP states that the topics of \"implementation issues, parallelization, software platforms, hardware\u201d are acceptable.  I would point out that most papers in these spaces describe *technical innovations* that enable improvements in \"parallelization, software platforms, hardware\" rather than implementations of these improvements.   However, it is certainly true that a software package is an acceptable (although less common) basis for a publication, provided is it sufficiently unique and impactful.  After pointing this out to the reviewers and collecting opinions, the reviewers do not feel the combined technical and software contributions of this paper are enough to justify acceptance. \n ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714464, "tmdate": 1576800264187, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper845/-/Decision"}}}, {"id": "BJgg7RijtS", "original": null, "number": 1, "cdate": 1571696151880, "ddate": null, "tcdate": 1571696151880, "tmdate": 1574668480767, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The authors propose the general formulation of recent meta-learning methods and propose a good library to use.\n\nPros:\n1. The general formulation of recent meta-learning methods is reasonable.\n2. The proposed library is easy to use.\n\nCons:\n\nThe paper lacks technical novelty. I understand the goal of this paper is to build a library. However, the paper only describes a general formulation for recent meta-learning methods (e.g., MAML) and implement the formulation. It is better to clarify and some key engineering challenges and do the corresponding experiments.\n\nIn addition, in the experiment parts, the authors only compare the results with MAML++. It will be more convincing if the authors can analyze other popular meta-learning methods (e.g.. Prototypical network [1], meta-LSTM [2]). \n\nAnother suggestion is that the authors can give some examples to connect current meta-learning models with the proposed general formulation. For example, the meaning of \\phi_i^opt, \\phi_i^loss in MAML, Prototype, Reptile, etc.\n\nIt is better to explain the meaning of different colors in Figure 3.\n\n[1] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" Advances in Neural Information Processing Systems. 2017.\n[2] Ravi, Sachin, and Hugo Larochelle. \"Optimization as a model for few-shot learning.\" ICLR (2016).\n\n\n\nDecision after rebuttal: I have read the authors' responses. Like review 2, I also think the \"generalization\" is overclaimed, it only provides a general formulation. Thus, I finally decide to keep my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574989383518, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper845/Reviewers"], "noninvitees": [], "tcdate": 1570237746160, "tmdate": 1574989383530, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Review"}}}, {"id": "ByeKWBWhjS", "original": null, "number": 13, "cdate": 1573815552804, "ddate": null, "tcdate": 1573815552804, "tmdate": 1573815552804, "tddate": null, "forum": "BygWRaVYwH", "replyto": "SklXdM-3sS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Thanks for your response", "comment": "Thank you for responding before the discussion period is up.\n\nWe wish to assume good faith on the part of the reviewer, but it sounds like they are setting an impossible standard for publication. We have answered the concerns brought forth by the reviewer, by their own admission, and they have simply brought up new high-level reasons they cannot support publication (none of which is specific enough to answer). Specifically, we stress again that solving engineering problems as we do in section 4 to easily permit experiments of the sort we present in section 5 is:\na) called for by the ICLR CFP\nb) a valuable contribution to the community\nc) not specifically argued against by the reviewer\n\nWe leave it to the AC to judge whether a case for rejection stands since we have addressed the issues from the first round of comments, but we would have preferred if the reviewer had engaged with our points rather than find an excuse\u2014any excuse\u2014to reject the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "SklXdM-3sS", "original": null, "number": 12, "cdate": 1573814891453, "ddate": null, "tcdate": 1573814891453, "tmdate": 1573814891453, "tddate": null, "forum": "BygWRaVYwH", "replyto": "Hyxf3_qBsS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you for addressing my points so thoroughly.\n\nI am sorry that my review seems to be so subjective.\nI will stick to my evaluation of a weak reject though.\n\nConsidering the goal of \"mathematical generalization\" from my perspective it would have been desirable had the authors looked beyond the meta-learning literature.\nFrom a mathematical perspective meta-learning is a bilevel optimization problem.\nIn order to find a general formal framework for meta-learning it may therefore make sense to relate to results that are well-known in the broad and established fields of bilevel optimization.\nThere is also a non-negligible amount of literature on conditions that need to be present in order to differentiate parametric optimization problems.\n\nA typical approach in bilevel optimization is to utilize the implicit function theorem in order to take derivatives of a lower level optimization problem with respect to hyper parameters.\nThat is generally not possible if the Hessian of the lower level problem at the local optimum is singular, as would typically be the case of machine learning.\nBut mentioning in the context of an attempted generalization of meta-learning seems relevant to me (to a typical bilevel optimization practitioner it would seem strange to differentiate the optimizer).\n\nIf the goal is rather to present the software tools I think it would make sense to focus on software aspects."}, "signatures": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "rygmEvGisB", "original": null, "number": 10, "cdate": 1573754667182, "ddate": null, "tcdate": 1573754667182, "tmdate": 1573754667182, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BJgg7RijtS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Paper update changelog", "comment": "Dear Reviewer,\n\nWe have made modifications to the manuscript based on our discussion. For your convenience, please find, below, a log of the changes made that pertain to our discussion. Please note that the paper has still been kept under the page limit as a result of these changes.\n\nWe\u2019ve updated the manuscript to add specific examples of how select existing approaches fit the Gimli formalism in Section 3. Thank you for your suggestion.\n\nWe have clarified the purpose of the different colours in the figures at the end of the appendix.\n\nThanks again for your excellent suggestions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "SklRZwGisH", "original": null, "number": 9, "cdate": 1573754629875, "ddate": null, "tcdate": 1573754629875, "tmdate": 1573754629875, "tddate": null, "forum": "BygWRaVYwH", "replyto": "r1lteyFy9B", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Paper update changelog", "comment": "Dear Reviewer,\n\nWe have made modifications to the manuscript based on our discussion. For your convenience, please find, below, a log of the changes made that pertain to our discussion. Please note that the paper has still been kept under the page limit as a result of these changes.\n\nWe\u2019ve updated the manuscript to add specific examples of how select existing approaches fit the Gimli formalism in Section 3. Thank you for your suggestion."}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "Syx938MssH", "original": null, "number": 8, "cdate": 1573754545547, "ddate": null, "tcdate": 1573754545547, "tmdate": 1573754545547, "tddate": null, "forum": "BygWRaVYwH", "replyto": "ryeVrc4XcS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Paper update changelog", "comment": "Dear Reviewer,\n\nWe have made modifications to the manuscript based on our discussion. For your convenience, please find, below, a log of the changes made that pertain to our discussion. Please note that the paper has still been kept under the page limit as a result of these changes.\n\nReplaced \u201cThe proposed formalism provides tooling for analysing the provable requirements of the meta-optimization process, and allows for describing the process in general terms.\u201d with \u201cThe proposed formalism allows us to describe the meta-optimization process in general terms and analyse its requirements.\u201d as discussed.\n\nRemoved \u201cwithout loss of generality\u201d where it would cause confusion,\nReplaced \u201cestimate\u201d with \u201capproximate\u201d to avoid confusion, as discussed.\n\nLeft a footnote for requirement 2 to clarify the applicability of the requirement to stochastic optimizers.\n\nAdded a clarification about the relation between the Gimli update algorithm and reverse-mode differentiation as the end of section 2.\n\nThanks again for your excellent suggestions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "SJlWMtqHir", "original": null, "number": 6, "cdate": 1573394696528, "ddate": null, "tcdate": 1573394696528, "tmdate": 1573394696528, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BJgg7RijtS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Reply to Reviewer 3", "comment": "We thank the reviewer for their review and suggestions. We are happy that they found the formalization clear and the library easy to use. We note that the main (if not only?) argument in favour of rejection seems to be \u201clack of technical novelty\u201d. With all due respect, we believe the reviewer\u2019s assessment is based on a misunderstanding both of the contributions of the paper, and of the ICLR CFP. We will address both matters here before replying to the additional comments and suggestions.\n\nFirst, the goal of the paper is not specifically to build a library. The purpose of the library presented in section 4 is entirely complementary to the material presented in sections 2-3. The main deep learning frameworks provide engineering obstacles to the easy implementation of algorithms fitting the framework of section 2, including all those exemplified in section 3 (we encourage the reviewer to look at implementations thereof if they are not already authors of such implementations, or haven\u2019t seen them, to see what we mean). The purpose of the anonymized library is to make it easy to implement new and existing methods fitting the patterns discussed in sections 2-3 by addressing the engineering challenges presented at the beginning of section 4. In this respect, it is an entirely separate contribution.\n\nSecond, noting that the CFP for ICLR calls for papers describing, amongst other things, \u201cimplementation issues, parallelization, software platforms, hardware\u201d, we hope you will feel that even as a stand-alone contribution it is relevant and acceptable material for an ICLR publication. \n\nFurthermore, the two contributions in section 2 are orthogonal to the engineering contribution addressed in section 4. Our value proposition for section 2 is first that we formalize the general pattern of a variety of approaches and prove the (often implicit) requirements for such approaches to be possible, and second that we describe the general form of an algorithm covering a variety of existing (and hopefully future) approaches, some of which are discussed in section 3.\n\nIn summary, we dispute that there is not both a significant technical contribution in the engineering challenges in section 4 (which are explicitly within the scope of the call for papers), and in the complementary formalization, proofs, and algorithm from section 2. We would appreciate if the reviewer could further detail their thinking regarding this. If they agree, we would appreciate it if they reconsidered their assessment and score.\n\nWe now turn to the specific comments.\n\n> It will be more convincing if the authors can analyze other popular meta-learning methods (...)\n\nWe can certainly do this in further work, but in what way would it be \u201cmore convincing\u201d (especially since the work the reviewer refers to is significantly older)? The state of the art certainly has moved on since MAML++ (presented at ICLR\u201919 a mere 6 months ago), but the point of section 5.2 is emphatically not to produce SOTA results, but rather to showcase the sort of experimental research that is enabled and facilitated by the library presented in section 4: namely that by removing the need to reimplement models and optimizers from scratch when implementing MAML-style methods, proper ablation studies can be run, and they show improvements over reported results for MAML++ (as might be expected for other methods such as those proposed by the reviewer). If this isn\u2019t a positive contribution to the community, who will be able to freely use the library to develop and experiment with new methods, then we would be grateful if the reviewer could explain in what way it is insufficient.\n\n> Another suggestion is that the authors can give some examples to connect current meta-learning models with the proposed general formulation (...)\n\nPlease note that we do this in section 3 already (in the paragraph describing MAML), stating that \\varphi^loss = \\theta_0 (i.e. the meta-variable is the initial state of the model at the beginning of a task). We try to offer similar indications for other related work in this section. If there are any where you feel this needs expanding, please flag, and we will happily expand this section. Please note we have written the paper under the more strict 8 page limit, so details like this can be added while maintaining a paper length in line with most other submissions.\n\nRegarding methods such as reptile, or first order MAML, they do not change what the meta-variable is, but rather how the process of backpropagating through the unrolled optimization can be approximated. This is somewhat out of the scope of this paper, but if you feel this omission warrants a footnote to clarify this point, please let us know.\n\n> It is better to explain the meaning of different colors in Figure 3.\n\nWe agree. They are different parameter groups, but we will add detail here. Thank you for taking the time to read the appendix! :)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "Hyxf3_qBsS", "original": null, "number": 5, "cdate": 1573394601684, "ddate": null, "tcdate": 1573394601684, "tmdate": 1573394601684, "tddate": null, "forum": "BygWRaVYwH", "replyto": "HygYq_cHor", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 (part 3)", "comment": "> - Page 3: While this may seem like a fairly trivial formalization...\n> Yes, but also nesting this in an outer loop is fairly trivial in the sense that it is a well known approach.\n\nIf by well-known you mean that such nesting has been done with specific applications in mind, such as meta-learning the initialisation (MAML) or learning rate, then this plays precisely to the point of this section, namely that there is a commonality to all these specific approaches that fits under a fairly simple mathematical framework within which we can jointly reason about e.g. requirements. If by this you mean that there is some work providing a similar high level formalization, beyond Franceschi et al 2018 (which we compare against), which we should be mentioning here, we would be grateful if you could point us to this literature so that we can compare.\n\n> Page 4: there exist continuous hyperparam...\n> If they are not continuous then they should not even occur in this formalization so saying there exist... does not make much sense to me here\n\nYes, this is just the antecedent of an entailment which, if it doesn\u2019t hold, ensures that the consequent (the rest of the sentence) need not hold as a requirement. If this is still not clear, we can of course rephrase it.\n\n> \\alpha \\subset \\varphi^{opt} implies that \\varphi^{opt}  is a set from notation although we are treating it as a vector everywhere else\n\nYou are right that this is an abuse of mathematical notation, but we frequently talk about a set of parameters being a subset (or having overlap) with others (e.g. when doing parameter sharing), and then treat them as vectors when defining the forward pass. We can describe this linguistically rather than mathematically if you think this is genuinely confusing. Please let us know your thoughts.\n\n> Page 4: All of section 2.4 seems somewhat trivial to me, but I guess that is highly subjective.\n\nThank you for recognising the subjectivity of this value judgement. We would point out that any proof may seem trivial once presented, especially if it does not appear counter-intuitive to the reader (which evidently it does not to the reviewer). Our preference would be to leave it in the main body of the paper rather than an appendix, as some of the early readers of our paper found this section interesting or helpful (but that, too, is subjective).\n\n> Page 5: in the definition of stop operator perhaps use \u21d4\n\nWe\u2019re not sure about this: the operator has to maintain the truth of joint constraints, so if either are false it does not satisfy the definition, therefore the \u201ciff\u201d doesn\u2019t seem right. But perhaps we are misunderstanding your suggestion: if so, can you please clarify?\n\n> Page 5: Perhaps explicitly mention how your approach differs from a reverse mode differentiation of training or if it does not differ, say this.\n\nIt\u2019s a second order reverse mode. In fact, in unnamedlib, we basically rely upon pytorch\u2019s autograd.grad function to build the reverse graph, and take higher order gradients over it. The library itself deals with ensuring that gradient paths which can exist, do. We will make a specific note of this at the end of the algorithm. Thanks for your suggestion.\n\n> Page 14: When talking about _S_GD (instead of just GD) perhaps mention something about non-existence of mini-batch randomness / being deterministic\n\nThere\u2019s no requirement of determinism if the data sampled from the data distribution is not a function of the model parameters. More broadly, the sort of meta-learning Gimli covers is about optimizing the actual optimization process used (rather than the ideal one), so if in practice we train our model through iterative methods with MC estimates of the batch gradient (e.g. through SGD), then that\u2019s what we are meta-learning to improve. If this doesn\u2019t answer your question,or you think there\u2019s something specific we should clarify in the paper, please let us know an we will happily do it.\t"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "HygYq_cHor", "original": null, "number": 4, "cdate": 1573394576683, "ddate": null, "tcdate": 1573394576683, "tmdate": 1573394576683, "tddate": null, "forum": "BygWRaVYwH", "replyto": "Bkxbr_9riH", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 (part 2)", "comment": "We now turn to the detailed comments:\n\n> - Page 1: ...provides tooling for analysing the provable requirements...\n> seems like a complicated way of saying something that could be said simple\n\nWe are always happy to simplify. Would replacing \u201cThe proposed formalism provides tooling for analysing the provable requirements of the meta-optimization process, and allows for describing the process in general terms.\u201d with \u201cThe proposed formalism allows us to describe the meta-optimization process in general terms and analyse its requirements.\u201d?\n\n> - Page 2: Without loss of generality, ...\n> You are assuming a parametric model. Not sure what generality this phrase refers to.\n\nWe see what you mean, we meant without loss of generality within the space of parametric models, but this sentence evidently has caused confusion. We will cut \u201cwithout loss of generality\u201d.\n\n> - Page 2: A formalization as theta* = argmin(theta, L(theta, varphi)) is inaccurate in the sense that it does not acknowledge the existing of multiple optima. I would recommend not to use the  operator here, since  for something like a neural network would for example either return a global optimum (which no optimizer used in practice finds, and is not ment here) or would take on a set value with multiple local minima for example.\n> In the same context, the authors should mention the issues about uniqueness of optima (we are not even really finding optima when training neural networks), implicit functions / implicit differentiation\nI think there is some misunderstanding here, which perhaps we can avoid for other readers by suitably clarifying the text. argmin(theta, L(theta, varphi)) represents the optimization problem we are trying to solve when we train a parametric model. This is agnostic to whether a minimum exists (although we typically pick loss functions for which it does) or whether it is unique. Our framework works just as well when in the ideal case, argmin returns a set of equivalent optima as we would in practice just select an arbitrary element. Furthermore, we do not assume that we are actually solving this problem in practice, but rather estimating theta* through an iterative gradient-based method (which of course is susceptible to local minima, etc) and has no guarantee of converging on a global minimum. We believed that by stating that we estimate theta*, at the beginning of Section 2.2, it was made clear that equation 1 is a description of the training process under idealised conditions, as the rest of the paper deals with meta-learning to optimize the iterative optimization process (not the idealized one), but we can add a sentence or two clarifying the intent of equation 1 in section 2.1. In particular, we can say \u201capproximate\u201d instead of \u201cestimate\u201d. Would that sufficiently clarify matters, given the above? \n\n> In the context of using stochastic optimizers one should also at least mention something about the differentiability of outputs of such optimizers and how they potentially depend on randomness of mini-batches (what if different randomness is used with the same or a perturbed hyper parameter?)\n\nMost optimizers are, given data sampled from the data distribution and model parameters, deterministic (e.g. all optimizers in core pytorch except lbfgs). You are right that for stochastic optimizers, things are a little different, although that is technically catered to by requirement II of section 2.4. We can add an explicit mention of this here if you think it will make things clearer, or in Section 2.4. What do you think would be more suitable, given your reading?\n\n> - Page 3: You mention the potential statefulness of the optimizer.\n> Why not explicitly carry it in the math notation? Probably things would get cluttered but saying it should be covered within varphi does not seem reasonable to me.\n\nThe state of the optimizers is not covered within varphi^{opt}. We leave it implicit by adding a time index for notational simplicity, following the convention used for other things like loss functions throughout the paper. We are happy to add state explicitly as something like opt(theta_t, varphi^{opt}_t, G_t, S_t) instead of opt_t(theta_t, varphi^{opt}_t, G_t) if you think something is gained, but we are not sure what that adds, and it would just further clutter the equations. If you still think this would clarify ambiguity somehow, please let us know and we will amend."}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "Bkxbr_9riH", "original": null, "number": 3, "cdate": 1573394489165, "ddate": null, "tcdate": 1573394489165, "tmdate": 1573394489165, "tddate": null, "forum": "BygWRaVYwH", "replyto": "ryeVrc4XcS", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 (part 1)", "comment": "We thank the reviewer for their detailed comments. We appreciate the effort that has been put into this review, but believe there to be some fairly important misunderstandings (in addition to some very helpful comments) which we wish to discuss with the reviewer.\n\nThe reviewer makes four arguments in favour of rejection:\n\nThe only contribution is an implementation of the algorithm in part 2 in pytorch.\nNo new engineering insights in section 4.\nThe provided unifying formalization is theoretically inaccurate.\nThe provided unifying formalization does not add value compared to related literature, and just motivates the implementation.\n\nWe will address these four points below, in the hope that the reviewer is willing to engage in discussion with us regarding these issues to help improve the paper, or where relevant, willing to revise their assessment if they agree there has been a misunderstanding.\n\nRegarding point 1, with all due respect, this is simply not the case. To remind the reviewer of the main contributions of this paper: First, we propose a formalization of the process of optimizing the training process through gradient-based methods, and show that it subsumes several recent approaches. Second, we derive a general algorithm that supports the implementation of various kinds of meta-learning fitting this framework. Third, we describe (and release) a lightweight PyTorch library that enables the straightforward implementation of any meta-learning approach that fits within this framework. \n\nThe purpose of the library presented in section 4 is entirely complementary to the material presented in sections 2-3. The main deep learning frameworks provide engineering obstacles to the easy implementation of algorithms fitting the framework of section 2, including all those exemplified in section 3 (we encourage the reviewer to look at implementations thereof if they are not already authors of such implementations, or haven\u2019t seen them, to see what we mean). The purpose of the anonymized library is to make it easy to implement new and existing methods fitting the patterns discussed in sections 2-3 by addressing the engineering challenges presented at the beginning of section 4. In this respect, the library is an entirely separate contribution. Additionally, noting that the CFP for ICLR calls for papers describing, amongst other things, \u201cimplementation issues, parallelization, software platforms, hardware\u201d, we hope you will feel that even as a stand alone contribution it is relevant and acceptable material for an ICLR publication.\n\nRegarding point 2, we are not sure how to respond, and hope the reviewer will expand on their point given our rebuttal. We genuinely do address a key engineering problem, in particular in section 4.2. If this is not sufficiently novel, can you please point us to related work (including repos) which address this challenge, and permit the off-the-shelf usage of existing third-party pre-trained model for MAML, reverse-mode differentiation, or related methods? To our knowledge, there are none, and in this respect, an engineering challenge and roadblock for the community is addressed in this work.\nRegarding point 3, we believe that are some misunderstandings, which we attempt to address in more detail below. We will address the finer points of discussion below, and hope this assuages the reviewer\u2019s concerns, but if not, could the outstanding issues causing theoretical inaccuracy be flagged so the paper can be rectified?\n\nFinally, regarding point 4, we first refer back to our rebuttal to point 1: the two contributions in section 2 (a formalization of the process of optimizing the training process through gradient-based methods, and a general algorithm that supports the implementation of various kinds of meta-learning fitting this framework) are orthogonal to the engineering contribution addressed in section 4 (a discussion of engineering roadblocks, and of how we solved them in unnamedlib). Our value proposition for section 2 is first that we formalize the general pattern of a variety of approaches and prove the (often implicit) requirements for such approaches to be possible, and second that we describe the general form of an algorithm covering a variety of existing (and hopefully future) approaches, some of which are discussed in section 3. We claim to the reviewer that this adds value: we are unaware of existing work treating on a comprehensive discussion of the requirements for a wide class of meta-learning methods. We are unaware of equivalently general algorithmic formulation which covers a variety of applications, to guide their implementation. If the reviewer knows of such, we would appreciate pointers to work not covered in our related work section. If not, is the reviewer willing to reconsider their judgement in light of this?"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "BygFSwqSor", "original": null, "number": 2, "cdate": 1573394240537, "ddate": null, "tcdate": 1573394240537, "tmdate": 1573394240537, "tddate": null, "forum": "BygWRaVYwH", "replyto": "r1lteyFy9B", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for your review. We are happy to hear you found the paper interesting, especially with regard to the formalization in section 2, which forms roughly half of the content of the paper on its own. One of the main contributions of the paper is, in fact, providing the mathematical tooling to describe existing and future work within this framework. This offers several benefits, such as providing provable requirements on the optimizers, losses, and space of models for this sort of meta-learning to be possible, providing a common notation for describing new approaches, and providing a formalism within which to compare and contrast existing approaches.  We believe is, in itself, a meaningful enough contribution to justify publication, and we hope you agree.\n\nRegarding the criticisms, we understand that there are two aspects that the reviewer thinks can be improved: the first is that the related work in section 3 could give more concrete examples of how the patterns in section 2 apply; the second is that \u201ctoo much space on the unnamedlib library\u201d and another venue might be more appropriate.\n\nRegarding the first point, we agree with the reviewer that analyzing the mathematical patterns on the chosen examples in Section 3 would add value and are happy to address this. Would it be satisfactory to the reviewer if we were to address this by revising the paper during the review period, to the point where they would consider revising their assessment?\n\nRegarding the question of appropriateness of section 4 and the amount of detail therein, we respectfully disagree. The ICLR 2020 CFP specifically calls for, amongst other topics, papers discussing \u201cimplementation issues, parallelization, software platforms, hardware\u201d. We think section 4 uncontroversially fits this particular bullet point, as it describes implementation issues facing the class of meta-learning approaches fitting the formalism from section 2, and describes how we overcame them in a library (software platform) we are releasing to the public. Would the reviewer be prepared to revise their assessment in light of this, or at least give us further indication as to why this aspect of the work is nonetheless unsuitable for ICLR?"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygWRaVYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper845/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper845/Authors|ICLR.cc/2020/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165329, "tmdate": 1576860554247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper845/Authors", "ICLR.cc/2020/Conference/Paper845/Reviewers", "ICLR.cc/2020/Conference/Paper845/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Comment"}}}, {"id": "r1lteyFy9B", "original": null, "number": 2, "cdate": 1571946224812, "ddate": null, "tcdate": 1571946224812, "tmdate": 1572972544977, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work presented a general formulation of a wide class of existing meta-learning approaches, and proved the requirements that must be satisfied for such approaches to be possible.\n\nHalf of the work is focused on describing the unnamedlib library, which extends PyTorch to enable the easy\nand natural implementation of such meta-learning approaches.\n\nThe early sections are interesting, especially section 2, which gives some great insights to the existing inner loop pattern in meta-learning. However, from section 3, the paper has turned to examples and related works, where I was hoping the author would give more detailed analysis of the pattern. My concern is the authors have spent too much space on the unnamedlib library. So http://www.jmlr.org/mloss/ might be a more suitable place for publication."}, "signatures": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574989383518, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper845/Reviewers"], "noninvitees": [], "tcdate": 1570237746160, "tmdate": 1574989383530, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Review"}}}, {"id": "ryeVrc4XcS", "original": null, "number": 3, "cdate": 1572190779639, "ddate": null, "tcdate": 1572190779639, "tmdate": 1572972544889, "tddate": null, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "invitation": "ICLR.cc/2020/Conference/Paper845/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors present a PyTorch based framework for performing second-order\nreverse mode autodiff for meta-learning.\n\nFirst, the authors present a formalization of a general prototypical\nmeta-learning setting.\nThey then provide an algorithm that solves this problem via gradient based\noptimization.\nFinally, perhaps the main contribution is a specific PyTorch implementation\nof said algorithm.\n\nThe type of meta-learning setting the authors consider is one where a gradient\nbased inner loop optimizer finds $\\theta^\\star$ by performing a finite number of steps.\nThe inner loop optimizer is parameterized through $\\varphi$ that consists of\ntwo parts $\\varphi^\\text{loss}$ and $\\varphi^\\text{opt}$.\nThe parameters $\\varphi^\\text{loss}$ are somehow part of the loss used for\ntraining in the inner loop. Example: Regularization paramter.\nThe parameters $\\varphi^\\text{opt}$ do not occur in the loss but in the\noptimizer step. Example: Learning rate.\n\nExample of an inner loop step:\n$\\theta^{k+1} := \\theta^k - \\alpha (\\nabla L(\\theta^k) + \\lambda \\nabla R(\\theta^k))$\nwhere $\\theta$ are the parameters of a neural network, $L$ is the training loss,\n$R$ is the regularizer, $\\alpha$ is the learning rate, $\\lambda$ is the regularization parameter.\nIn this example we would have $\\varphi = (\\alpha \\lambda)^T$.\n\nThe authos assume $\\theta^K$, the output of the inner loop after $K$ steps to\nbe differentiable wrt $\\varphi$.\nFurthermore, the meta-learning loss is assumed to be differentiable wrt $\\theta$\nso that a gradient of the meta-learning loss wrt to $\\varphi$ can be computed.\nThe authors also assume the meta-learning loss to be sufficient smooth in $\\varphi$\nsuch that a gradient based optimization can even be used for meta learning to\na local optimum.\n\nThe authors explicitly write down the reverse mode auto differentiation of\nthe inner loop and show how to, in that way, compute the gradient of the\nmeta-learning loss wrt to $\\varphi$.\n\nThe reverse (adjoint) mode auto differentiation of the above example inner loop step\nis the following step (iterated over in reverse down from $k = K$ to $k = 1$:\n$\\bar \\theta^k := (I - \\alpha (\\nabla^2 L(\\theta^k) + \\lambda \\nabla^2 R(\\theta^k)))^T \\bar \\theta^{k+1}$\n$\\bar \\alpha := \\bar \\alpha - (\\nabla l(\\theta^k) + \\lambda \\nabla R(\\theta^k))^T \\bar \\theta^{k+1}$\n$\\bar \\lambda := \\bar \\lambda - \\alpha \\nabla R(\\theta^k)^T \\bar \\theta^{k+1}$\nwhere $\\bar \\alpha$ accumulates the gradient of $\\theta$ wrt $\\alpha$ and\n$\\bar \\lambda$ accumulates the gradient of $\\theta$ wrt $\\lambda$.\n\nThe authors give some implementation details specific to some frameworks necessary\nfor implementing such \"gradient of an inner loop\".\n\nThe authors present experiments where they show how to meta-learn learning rates\nwith their framework.\nThey also how their framework can be used to quickly implement a MAML type\nmeta-learning optimizer ablation study comparing various combinations of\narchitecture, optimizer and inner loop steps etc..\n\nRecommendation:\nI propose to reject the paper.\nIn my eyes the only contribution is the implementation of a meta-learner in\nPyTorch based on well known methods.\nThe provided unifying formalization is theoretically inaccurate (see below)\nand to me come across as merely a motivation for their framework\n(but no value added compared to existing literature).\nThere is no new insight provided on the software engineering level either as\nfar as I can see.\n\nDetailed comments:\n- Page 1: ...provides tooling for analysing the provable requirements...\n\tseems like a complicated way of saying something that could be said simple\n\n- Page 2: Without loss of generality, ...\n\tYou are assuming a parametric model. Not sure what generality this phrase\n\trefers to.\n\n- Page 2: A formalization as $\\theta^\\star = argmin(\\theta, L(\\theta, \\varphi))$\n\tis inaccurate in the sense that it does not acknowledge the existing of\n\tmultiple optima.\n\tI would recommend not to use the $argmin$ operator here, since $argmin$ for\n\tsomething like a neural network would for example either return a global\n\toptimum (which no optimizer used in practice finds, and is not ment here)\n\tor would take on a set value with multiple local minima for example.\n\n\tIn the same context, the authors should mention the issues about uniqueness\n\tof optima (we are not even really finding optima when training neural networks),\n\timplicit functions / implicit differentiation\n\n\tIn the context of using stochastic optimizers one should also at least\n\tmention something about the differentiability of outputs of such optimizers\n\tand how they potentially depend on randomness of mini-batches\n\t(what if different randomness is used with the same or a perturbed\n\thyper parameter?)\n\n- Page 3: You mention the potential statefulness of the optimizer.\n\tWhy not explicitly carry it in the math notation?\n\tProbably things would get cluttered but saying it should be covered within\n\t$\\varphi$ does not seem reasonable to me.\n\n- Page 3: While this may seem like a fairly trivial formalization...\n\tYes, but also nesting this in an outer loop is fairly trivial in the sense\n\tthat it is a well known approach.\n\n- Page 4: there exist continuous hyperparam...\n\tIf they are not continuous then they should not even occur in this\n\tformalization so saying there exist... does not make much sense to me here\n\n\t$\\alpha \\subseteq \\varphi^\\text{opt}$ implies that $\\varphi^\\text{opt}$ is\n\ta set from notation although we are treating it as a vector everywhere else\n\n- Page 4: All of section 2.4 seems somewhat trivial to me, but I guess that is\n\thighly subjective.\n\n- Page 5: in the definition of stop operator perhaps use $:\\Leftrightarrow$\n\n- Page 5: Perhaps explicitly mention how your approach differs from a reverse\n\tmode differentiation of training or if it does not differ, say this.\n\n- Page 14: When talking about _S_GD (instead of just GD) perhaps mention\n\tsomething about non-existence of mini-batch randomness / being deterministic\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper845/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Inner Loop Meta-Learning", "authors": ["Edward Grefenstette", "Brandon Amos", "Denis Yarats", "Phu Mon Htut", "Artem Molchanov", "Franziska Meier", "Douwe Kiela", "Kyunghyun Cho", "Soumith Chintala"], "authorids": ["egrefen@gmail.com", "brandon.amos.cs@gmail.com", "denisyarats@cs.nyu.edu", "pmh330@nyu.edu", "a.molchanov86@gmail.com", "fmeier@fb.com", "dkiela@fb.com", "kyunghyun.cho@nyu.edu", "soumith@gmail.com"], "keywords": ["meta-learning"], "TL;DR": "Lots of meta-learning problems follow the same general pattern, so we formalized it, proved stuff about it, turned it into an algorithm, and subsequently a pytorch library.", "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "pdf": "/pdf/d31f78dff031af9380ac12fd8f9ee631f2c57cc2.pdf", "paperhash": "grefenstette|generalized_inner_loop_metalearning", "original_pdf": "/attachment/d4739775f97bfae5599f7e510f4e035deae809eb.pdf", "_bibtex": "@misc{\ngrefenstette2020generalized,\ntitle={Generalized Inner Loop Meta-Learning},\nauthor={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\nyear={2020},\nurl={https://openreview.net/forum?id=BygWRaVYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygWRaVYwH", "replyto": "BygWRaVYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper845/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574989383518, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper845/Reviewers"], "noninvitees": [], "tcdate": 1570237746160, "tmdate": 1574989383530, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper845/-/Official_Review"}}}], "count": 15}