{"notes": [{"id": "ryl5CJSFPS", "original": "r1lcpPJFwB", "number": 2035, "cdate": 1569439698143, "ddate": null, "tcdate": 1569439698143, "tmdate": 1577168243014, "tddate": null, "forum": "ryl5CJSFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "s3mlf1QDWY", "original": null, "number": 1, "cdate": 1576798738838, "ddate": null, "tcdate": 1576798738838, "tmdate": 1576800897507, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Decision", "content": {"decision": "Reject", "comment": "This submission investigates the properties of the Jacobian matrix in deep learning setup. Specifically, it splits the spectrum of the matrix into information (large singulars) and ``nuisance (small singulars) spaces. The paper shows that over the information space learning is fast and achieves zero loss. It also shows that generalization relates to how well labels are aligned with the information space.\n\nWhile the submission certainly has encouraging analysis/results, reviewers find these contributions limited and it is not clear how some of the claims in the paper can be extended to more general settings. For example, while the authors claim that low-rank structure is suggested by theory, the support of this claim is limited to a case study on mixture of Gaussians. In addition, the provided analysis only studies two-layer networks. As elaborated by R4, extending these arguments to more than two layers does not seem straighforward using the tools used in the submission. While all reviewers appreciated author's response, they were not convinced and maintained their original ratings.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704704, "tmdate": 1576800252336, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Decision"}}}, {"id": "Syer94DpFS", "original": null, "number": 1, "cdate": 1571808396618, "ddate": null, "tcdate": 1571808396618, "tmdate": 1574709233069, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Note: The template used in this paper is of ICLR 2019, not ICLR 2020.\n\nThis paper identifies the information space and nuisance space by thresholding the singular values of the network's jacobian and shows that generally the residuals projected to the information space can be effectively optimized to zero, thus leading to efficient optimization and good generalization.\n\nI believe this paper should be rejected because its motivation and technical framework are not novel enough in that 1) the motivation of decomposition along gradient matrix is already well-founded by a series of paper related to neural tangent kernel 2) the techniques used here also fall in a similar framework. The following is the detailed comments.\n\nFirst, this paper's motivation is to employ the singular decomposition of the jacobian. Actually, the motivation is essentially the same as (Arora et al. 2019) and many other works. The neural tangent kernel matrix defined there is exactly the inner product of two jacobian (or gradient) described here and to employ the singular decomposition is actually corresponding to employing the eigendecomposition of the neural tangent kernel, which appears first in (Arora et al. 2019). The logic behind dividing the singular space into information space and nuisance space is that gradient descending along different directions has different speeds, determined by the eigenvalues.\n\nThe framework presented in this paper is based on the assumption that the parameters will not be far away from the starting point. Such an assumption further guarantees the trajectory won't be far away from the linearized trajectory, leading to an optimization guarantee. This approach is widely used by many works, and well-known for a considerably long time. Also, the paper's proof is complicated and lengthy which hinders its clarity.\n\nTo summarize, this paper definitely contains some rigorous analysis which I appreciate, but it doesn't provide new insights into optimization and generalization for deep nets. The motivation and logic behind are not novel enough, the main theorem neither. So I suggest a weak rejection to this paper in its current form.\n\n[1] Arora, Sanjeev, et al. \"Fine-grained analysis of optimization and generalization for over-parameterized two-layer neural networks.\" arXiv preprint arXiv:1901.08584 (2019).\n\n****** Post-rebuttal response ******\n\nThanks to the authors' response. I have read the rebuttal and unfortunately, I feel it is still not strong enough to  justify this paper's novelty issue and I will keep my rating unchanged.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575872728278, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Reviewers"], "noninvitees": [], "tcdate": 1570237728704, "tmdate": 1575872728295, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review"}}}, {"id": "HyxZzj92jH", "original": null, "number": 4, "cdate": 1573853960936, "ddate": null, "tcdate": 1573853960936, "tmdate": 1573853960936, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "rkgb8JyCKr", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your time and efforts in reviewing our paper. \n\nRe Point 1: Our theory yields data-dependent bounds and does not require any assumptions on the data or the Jacobian. In short: If dataset has nice properties (quantified by theory e.g. low-rankness), bounds become stronger and theory works for any cutoff value of alpha as soon as the network is sufficiently wide. However, our result shows that when the Jacobian exhibits such low-rank structure one can use a large alpha and in turn small width networks to achieve good generalization (proportional to how aligned the labels are with top eigen-vectors of the Jacobian). As case study, we rigorously proved that the Jacobian indeed becomes low-rank when the input data obeys a mixture of Gaussian model. However, we note that for low-rank behavior to happen it is not necessary for the input data to be linearly separable or low rank. Indeed, we empirically demonstrated this low-rank/bimodal structure on CIFAR-10, a dataset that is clearly not linearly separable (unlike MNIST). The low-rankedness of the Jacobian originates from the representation power of the architecture for a given data set. It seems that when the network architecture can learn good data representations, the Jacobian becomes low rank. Stated differently, if the data features at the last hidden layer are approximately separable then we expect the Jacobian to be approximately low rank. It is difficult to see how a network can generalize without such low-rank or clusterable representations. Hence, we expect low-rank behavior to be prevalent and expect these observations to hold on larger datasets such as CIFAR 100 and ImageNet for typical networks that achieve good generalization performance. In fact, preliminary simulations on a subset of this data sets confirm this.\n\nRe Point 2: The low-rank structure of the Jacobian is strongly suggested by theory as demonstrated in Sec 2.3 when the data comes from a Gaussian Mixtures. Specifically, Thm 2.5 shows that if the noise level is small, the effective rank of the M-NTK is approximately K^2 C. To verify the effective low-rank property of the Jacobian on real networks and practical datasets (not generated by GMM) we empirically show that the Jacobian has bimodal structure on CIFAR-10 and MNIST. That being said, our generalization results hold for any spectrum cut-off and we don\u2019t need any assumptions on the data distribution. Please see response to point 1 for further discussion about this. Thanks for the reference (https://arxiv.org/pdf/1910.05929.pdf). This is in line with our intuition and gives further credence to the observation that the Jacobian is low rank.\n\nRe Point 3: Great question! Cross-entropy term P will emphasize the points that are close to the decision boundary (small margin) and shrink the other ones (large margin). Hence, during training JP will be low-rank where low-rankness is not only due to J but also related to the number of points in the class boundaries. We used square loss to keep the theoretical analysis simpler, however we did make similar observations empirically on other loss functions.  In particular, we indeed found that for both squared loss with softmax and cross-entropy loss the Jacobian has also low-rank structure and observed Jacobian adaptation on CIFAR10 with cross-entropy. Roughly stated for cross-entropy theory one has to replace the residual with the derivative of the loss and we expect that the general ideas presented in this paper will hold. \n\nRe Point 4: We are not completely sure if we understand your question on modeling the derivatives w.r.t each logit. As you pointed out, in the multi-class NTK case we have to differentiate each output w.r.t. each input feature and concatenate them to obtain J. In M-NTK, we still have a single Jacobian whose dimensions grow with number of classes and this is exactly what we assumed in theory and calculated in the numerical experiments. \n\nRe Point 5: The theoretical analysis on convergence and generalization in Jacot, 2018 hold for the infinite-width limit NTK, which we agree is restrictive in practice. Our theory does not make this assumption, and provides generalization results for finite width networks, where the width k can be even constant in some cases (in particular when the Jacobian is sufficiently low-rank). In particular, our results go beyond the NTK regime as we do not require all the eigenvectors of the Jacobian to remain approximately fixed. Rather we show that it is sufficient if only the few top ones remain approximately fixed. This is exactly why we can handle rather small widths not possible in the NTK analysis. Please see the case study in Section 2.3 which clearly demonstrates the advantages of our result with respect to the NTK regime. Furthermore, in Appendix D we go even further than this and begin to demystify the more mysterious adaptation behavior observed in our experiments which is clearly outside the purview of NTK style analysis.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl5CJSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2035/Authors|ICLR.cc/2020/Conference/Paper2035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147265, "tmdate": 1576860549980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment"}}}, {"id": "S1lUTvchjH", "original": null, "number": 3, "cdate": 1573853117721, "ddate": null, "tcdate": 1573853117721, "tmdate": 1573853117721, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "Syer94DpFS", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for your time and efforts in reviewing our paper. First, many thanks for pointing out the template. Great catch! We fixed it.\n\nWe agree that a few of the high-level motivations of the paper is related to Arora et. al. and Jacot et al. However, our paper contains many new insights, studies completely new phenomena (adaptation, harnessing low-rank, rigorous early stopping analysis etc), new experiments and theoretical justifications. We have stated the similarities and differences between our paper and this work in various places in the paper. Let us point to a few (of many) of the novelties of our work w.r.t. the Arora et. al. paper.\n\n(1)\tThe central premise of this paper is how one can utilize the low-rank nature of the Jacobian to provide generalization guarantees. Arora et. al. does not utilize low-rank structure at all. In particular, as the minimum eigenvalue of the NTK kernel goes to zero the required width would go to infinity. In the limit where the Jacobian is exactly low-rank the results of Arora et at are vacuous (requires $k\\tendsto +inf) for instance when sigma tends to zero in the simple case study of Section 2.1 or the simplest binary classification problem where (x,y) samples have the discrete distribution (1,1) or (-1,-1). The advantage of this becomes clear during the network size analysis: Our network width is data-dependent, and it can be as small as constant (or logarithmic) if the data is low-dimensional. In contrast, results of Arora et al. don\u2019t even apply if the kernel matrix is rank deficient. Observe that kernel matrix is expected to have bad condition number for structured data (in contrast to random data).\n(2)\tThe concept of information and nuisance space also does not seem to appear in Arora et al.\n(3)\tAnother key aspect of the results of this paper is the use of early stopping. The results of Arora et al are based on iterating to convergence and do not have early stopping which is important for the results of this paper. In fact, earlier versions of Arora et al seemed to advocated that early stopping is completely unnecessary!\n(4)\tWe also note that as mentioned the paper in the extreme case where alpha_0=sqrt{lambda}=sqrt{lambda_min(Sigma(X))) with with K=1 we require k>= cn^4/lambda^2 whereas Arora et al requires k>= cn^8/lambda^6. So that in this very special case our results can prove the results of Arora et al with less stringent width requirements.\n(5)\tOur contributions go beyond the NTK regime as we do not require all the eigen directions of the Jacobian to be fixed across iterations rather we only need the very top ones to remain fixed. \n(6)\tWe provide a detailed experimental study of how neural networks learn better low-rank representations over time and provide theoretical justification for it (see Appendix D). Hence, our contributions go even beyond the NTK regime on the top eigen directions mentioned in (5).\n(7)\tWe also notably do not require random initialization (see Theorem 2.3). We are not familiar with any comparable generalization bound for such deterministic initialization which can be used for pretrained models.\n(8)\tFinally of minor importance we study K>1.\n\n\nIn summary we believe that some similarities in the high-level motivations should not overshadow the novel new insights, phenomena, experiments and theoretical results in this paper and we hope that you reconsider your score.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl5CJSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2035/Authors|ICLR.cc/2020/Conference/Paper2035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147265, "tmdate": 1576860549980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment"}}}, {"id": "Bklu7DqnsS", "original": null, "number": 2, "cdate": 1573852960075, "ddate": null, "tcdate": 1573852960075, "tmdate": 1573852960075, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "BygXbD0n5B", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "Thank you for your effort and time for reviewing our paper.\n\nRe \u201cThe faster convergence of the model in the information space is not surprising and was observed by Jacot 2018\u201d and \u201cThe results uncovered are not surprising and predicted by Jacot 2018 (granted, it is interesting to see that the result holds for finite width and non-continuous gradient flow).\u201d\n\nFirst let us agree that we should have discussed Jacot 2018 in further detail. We were not aware that Jacot et al. contained similar insights to Arora et al. (which we compared to). We revised the paper and now properly refer to Jacot et al (see Prior Art section). Compared to Jacot et al., paper contains many new insights (adaptation, harnessing low-rank, early stopping generalization analysis etc), provides new experiments and theoretical justifications. Below we outline some of these novelties\n\n(1)\tThe central premise of this paper (as clear by the title) is how one can utilize the low-rank nature of the Jacobian to provide generalization guarantees. Jacot et al. does not utilize low-rank structure at all. Jacot et al. does mention principal components and lower eigenvectors in one paragraph (see their page 7). This is obviously related but much closer to Arora et al.\u2019s analysis (which is also followup on Jacot et al.) than ours. In contrast this paper quantifies the low-rankness of features, demonstrates real datasets exhibit Jacobian low-rankness,  and states many provable benefits (generalization, small network width, convergence\u2026) via low-rankness.\n(2)\tThe concept of info and nuisance space (which helps quantify low-rankness) is also new to this paper and does not seem to appear in Jacot et al. \n(3)\tThe result of Jacot et al. are for gradient flow on infinite width networks. Our results hold for gradient descent with finite width neural networks. In fact, in some cases we can handle constant width thanks to carefully quantifying the benefit of low-rankness. Any simple discrete variant of Jacot et al. would require the width to scale polynomially in the size of the training data (e.g. the Arora et al. paper). It will also get much worse for badly conditioned NTK see Section 2.3 on mixture models.\n(4)\tOur contributions go beyond the NTK regime as we do not require all the eigendirections of the Jacobian to be fixed across iterations rather we only need the very top ones to remain fixed. \n(5)\tWe provide a detailed experimental study of how neural nets learn better low-rank representations over time and provide theoretical justification for it (see Appendix D). Hence, our contributions go even beyond the NTK regime on the top eigen directions mentioned in (5).\n(6)\tAs also noted by the reviewer we focus on generalization and classification unlike Jacot et. al. 2018.\n\n\nRe \u201cthe setting of theoretical contributions is too restrictive. The model exposed in section 1 is extremely simplified, as only W can be learned, and V is fixed. As a result, the model is in essence completely linear...\u201d \n\nWe respectfully disagree. First re \u201cthe model is linear\u201d. If W was fixed, then yes, the model would be linear but as a function of W the model is definitely not linear. We note that we have only focused on learning the first layer of the network for clarity of exposition. That said most of the arguments in the paper have been written in a way which extension to training of both layers is possible (including the Radamecher complexity arguments). Also, in Appendix B we already sketched how one would go about handling joint optimization of both layers.\n\nRe Nitpick \u201cour results may shed light on the generalization with pre-trained models\u201d seems stretch. While I agree that theories that require random initialization won\u2019t work for transfer learning, the results of the authors don\u2019t leverage anything about pre-training.\n\nOur contribution re pretraining is that we do not need random initialization unlike related works. Any initial Jacobian can be your NTK (applies to finite networks under smooth activations). Hence guarantees hold from arbitrary initialization. Numerical section also demonstrates network learns better low-rank representation over time. When you put these together, it is reasonable to highlight possible benefits on transfer learning. That said, we will tone down the statement.\n\nRe Model is too simple (even simpler than a standard one hidden layer network). \n\nPlease see response earlier above which demonstrates that the proof does extend to joint optimization of both layers.\n\nRe The paper is incremental, as the link between convergence rate and the projection of the desired outputs on the information space was already made in Jacot 2018\n\nPlease see response to question above clarifying the novelties with respect to Jacot et al.\n\n\nIn conclusion we believe that some similarities in the high-level motivations should not overshadow the novel new insights, phenomena, experiments and theoretical results in this paper and we hope that you reconsider your score."}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl5CJSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2035/Authors|ICLR.cc/2020/Conference/Paper2035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147265, "tmdate": 1576860549980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Authors", "ICLR.cc/2020/Conference/Paper2035/Reviewers", "ICLR.cc/2020/Conference/Paper2035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Comment"}}}, {"id": "rkgb8JyCKr", "original": null, "number": 2, "cdate": 1571839817285, "ddate": null, "tcdate": 1571839817285, "tmdate": 1572972391631, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors use the empirically supported assumption of the low-rank nature of the neural network Jacobian to provide new elements of data-dependent optimization and generalization theory. By modelling the data as a low-rank object itself, they analytically study the evolution of the train and test errors. The paper divides the space of weights and biases into the \u201cinformation\u201d and \u201cnuisance\u201d subspaces, spanned by the top largest and the remaining singular vectors of the Jacobian respectively. They use this division and its alignment with the low-rank structure of the data to talk about convergence speed. Finally, they provide numerical experiments to back their claims.\n\nI enjoyed the paper, however, there were many points where I was unclear on the precise nature of the assumptions used / the strength of the results.\n\nDisclaimer: I didn\u2019t manage to read through the proofs in the appendix and cannot therefore vouch for its correctness.\n\n-- Point 1 --\nLeveraging the data structure\n\nI am unclear on how exactly you were modelling the structure of the data. From your proofs, it seems that you have been dealing with the matrix X comprising the concatenated flatted vectors of the raw input features (e.g. pixels) of the input data [x1,x2,...,x_datasetsize]^T. In particular, the only place where I see data explicitly enter is in Definition 2.1, where you look at the X X^T and fi\u2019(w X) fi\u2019(w X)^T.\n\nIf the data is linearly separable in the raw input space on its own, then I see that the matrix X X^T will be low-rank (related to the number of classes). I also see your point about the connection of the y to the relevant (semantic) clusters. The same argument could by applied to fi\u2019(w X) fi\u2019(w X)^T -- provided that the features produced are again linearly separable, we will observe this object to have a low (number of classes - 1) rank. \n\nWhat is unclear to me is whether these assumptions are warranted. I understand that some simple datasets, e.g. MNIST, are essentially linearly separable in the raw pixels, and therefore the X X^T indeed is low rank. However, I doubt anything like that is true for big datasets, such as ImageNet. For deep networks that are used on these big datasets, such a modelling assumptions would likely not be true. I wonder how this relates to your results, since the low-rank nature of the Hessian is observed even for those networks, which is in turn related to the low rank nature of the Jacobian.\n\n-- Point 2 --\nData implicitly present in the Jacobian tensor.\n\nI wonder how you modelled the Jacobian tensor that you started using on page 3. Since the Jacobian -- the derivative of the output logits with respect to the weights, has to be evaluated at a particular input X, the assumptions you make on the data are in turn having an effect on the Jacobian, and vice versa.\n\nI am unclear on exactly what assumptions you make about the object, and whether you are actually saying that its low rank structure comes from the data, is empirical observed and therefore assumed, or due to the network regardless of the data.\n\nI recently saw a new arXiv submission that seems to be looking into this on real networks: https://arxiv.org/pdf/1910.05929.pdf Their model explicitly assumes that logit gradients cluster in the weight space in a particular way.\n\n-- Point 3 --\nSquare loss vs softmax\n\nYou are using the square loss |f(X) - y|^2 throughout your work. Many of the empirical low-rank observations of the Hessian (related to the JJ^T) are performed on real networks with the cross-entropy loss. While the Hessian with the square loss is of the form JJ^T + terms, the softmax in the cross-entropy loss introduces an additional cross-term (let us call it P for now), which in turn makes it JP(PJ)^T + terms. Do you know how this relates to your results?\n\nMore generally, does the square loss you use make the results significantly different from what we would get for a softmax?\n\n-- Point 4 --\nNeural Tangent Kernel (NTK) -- assumptions\n\nUnder the NTK assumption, you still need to model the derivatives of each logit with respect to each weight on each input in order to obtain the Jacobian matrix and in turn JJ^T. I am therefore very confused by \u201cBased on our simulations the M-NTK indeed haslow-rank structure with a few large eigenvalues and many smaller ones. \u201c on page 5. What assumptions exactly do you use in your model?\n\n-- Point 5 --\nNeural Tangent Kernel (NTK) -- validity\n\nBy assuming the NTK holding, do you limit the validity of your results? I think it is believed that NTK might not, generally speaking, be enough to capture the complexity of DNNs, and therefore assuming it might limit the range of applicability of any results derived assuming it.\n\n-- Conclusion --\nIn general, my points of confusion often stem from being unsure as to what parts of the argument were assumed and based on what empirical / theoretical evidence, and what parts were generically true. While the paper seems interesting, I am not sure what its novel contribution is and how broad the claims made actually are in their applicability.\n\nAppendix: I was not able to judge the proofs in the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575872728278, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Reviewers"], "noninvitees": [], "tcdate": 1570237728704, "tmdate": 1575872728295, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review"}}}, {"id": "BygXbD0n5B", "original": null, "number": 3, "cdate": 1572820731074, "ddate": null, "tcdate": 1572820731074, "tmdate": 1572972391534, "tddate": null, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes new (data dependent) generalization guarantees based on the Jacobian of the model. The authors suggest that if the desired outputs lie into the information space (the subspace spanned by the largest eigenvectors of the NTK), the model will train faster and better generalization will be achieved.\n\nThe faster convergence of the model in the information space is not surprising and was observed by Jacot 2018. The authors make improvements over this result:\n - They present a generalization result, whereas Jacot 2018 focuses only on the convergence on the training set. It is also formulated as a classification problem instead of a regression one.\n- It doesn\u2019t need JJ^t to stay constant during the training.\n\nHowever, the setting considered by the authors to derive their theoretical contributions is too restrictive. The model exposed in section 1 is extremely simplified, as only W can be learned and V is fixed. As a result, the model is in essence completely linear: the goal is, for a given V, to learn a \u201cgood\u201d hidden layer using a linear model and the loss L : h -> ||V phi(h) - y ||. \n\nThe experiment on cifar10 is interesting, especially the section regarding label corruption. A more extensive empirical investigation is this direction would be of great value. The results uncovered are not surprising and predicted by Jacot 2018 (granted, it is interesting to see that the result holds for finite width and non-continuous gradient flow).\n\nI think this paper in its current state is not good enough for two reasons. First, the major contribution is a generalization bound that is derived for a model that is too simple (even simpler than a standard one hidden layer network). Beside this result, the rest of the paper is  incremental, as the link between convergence rate and the projection of the desired outputs on the information space was already made in Jacot 2018\n\nNB: I did not check the derivation of the results in annexes.\n\nNitpick:\n\nPage 2: \u201cour results may shed light on the generalization capabilities of networks initialized with pre-trained models commonly used in meta/transfer learning\u201d seems like a bit of a stretch. While I agree that theories that requires random initialization won\u2019t work for transfer learning, the results presented by the authors don\u2019t really leverage anything particular about pre-training.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2035/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sametoymak@gmail.com", "zfabian@usc.edu", "mli176@ucr.edu", "msoltoon@gmail.com"], "title": "GENERALIZATION GUARANTEES FOR NEURAL NETS VIA HARNESSING THE LOW-RANKNESS OF JACOBIAN", "authors": ["Samet Oymak", "Zalan Fabian", "Mingchen Li", "Mahdi Soltanolkotabi"], "pdf": "/pdf/a115824d8aaf25a4d9fc8c8cd600b16f60733f85.pdf", "TL;DR": "We empirically demonstrate that the Jacobian of neural networks exhibit a low-rank structure and harness this property to develop new optimization and generalization guarantees.", "abstract": "Modern neural network architectures often generalize well despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. We develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. Our results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as well as how the network size affects the evolution of the train and test errors during training. Specifically, we use a control knob to split the Jacobian spectum into ``information\" and ``nuisance\" spaces associated with the large and small singular values. We show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize well. Over the nuisance space training is slower and early stopping can help with generalization at the expense of some bias. We also show that the overall generalization capability of the network is controlled by how well the labels are aligned with the information space. A key feature of our results is that even constant width neural nets can provably generalize for sufficiently nice datasets. We conduct various numerical experiments on deep networks that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.", "keywords": ["Theory of neural nets", "low-rank structure of Jacobian", "optimization and generalization theory"], "paperhash": "oymak|generalization_guarantees_for_neural_nets_via_harnessing_the_lowrankness_of_jacobian", "original_pdf": "/attachment/a43a1824b28146e2650cf2c1ad045e1304255799.pdf", "_bibtex": "@misc{\noymak2020generalization,\ntitle={{\\{}GENERALIZATION{\\}} {\\{}GUARANTEES{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETS{\\}} {\\{}VIA{\\}} {\\{}HARNESSING{\\}} {\\{}THE{\\}} {\\{}LOW{\\}}-{\\{}RANKNESS{\\}} {\\{}OF{\\}} {\\{}JACOBIAN{\\}}},\nauthor={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl5CJSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl5CJSFPS", "replyto": "ryl5CJSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575872728278, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2035/Reviewers"], "noninvitees": [], "tcdate": 1570237728704, "tmdate": 1575872728295, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2035/-/Official_Review"}}}], "count": 8}