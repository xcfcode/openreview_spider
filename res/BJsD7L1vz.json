{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124470125, "tcdate": 1518457699188, "number": 177, "cdate": 1518457699188, "id": "BJsD7L1vz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJsD7L1vz", "signatures": ["~Zhang-Wei_Hong1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.", "paperhash": "hong|diversitydriven_exploration_strategy_for_deep_reinforcement_learning", "keywords": ["Deep Reinforcement Learning", "Exploration"], "_bibtex": "@misc{\n  hong2018diversity-driven,\n  title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},\n  author={Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=BJsD7L1vz}\n}", "authorids": ["williamd4112@gapp.nthu.edu.tw", "ariel@shann.net", "at7788546@gmail.com", "s106062520@m106.nthu.edu.tw", "cylee@cs.nthu.edu.tw"], "authors": ["Zhang-Wei Hong", "Tzu-Yun Shann", "Shih-Yang Su", "Yi-Hsiang Chang", "Chun-Yi Lee"], "TL;DR": "We propose a diversity-driven exploration strategy to improve the exploration efficiency of deep reinforcement learning agents.", "pdf": "/pdf/c861598af216b2b3ee06cfbfde6d5e8bf0feff7d.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582923004, "tcdate": 1520368155963, "number": 1, "cdate": 1520368155963, "id": "H14mqu3uf", "invitation": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer3"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "# Summary\nThis paper proposes a new method for exploration in reinforcement learning. The idea is to encourage the policy to be different from its recent policies by introducing KL-divergence penalty between the current policy and the past policies. The result shows that the proposed method (applied to DQN and A2C) performs better than baselines on several Atari games.\n\n[Pros]\n- Interesting idea for improving exploration in RL.\n- A promising result on Atari games.\n\n[Cons]\n- No theoretical result.\n\n# Novelty\n- The proposed idea is novel to my knowledge. \n\n# Technical correctness\n- It might be good to have some theory that justifies the proposed approach (if possible). Though the idea looks reasonable, the policy with the proposed diversity-driven objective might oscillate without deeply exploring new policies by making a cycle of policies that are reasonably different from each other.\n\n# Quality\n- The result looks promising on some of the Atari games (e.g., Enduro). A more extensive result on many Atari games and other domains would further strengthen the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.", "paperhash": "hong|diversitydriven_exploration_strategy_for_deep_reinforcement_learning", "keywords": ["Deep Reinforcement Learning", "Exploration"], "_bibtex": "@misc{\n  hong2018diversity-driven,\n  title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},\n  author={Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=BJsD7L1vz}\n}", "authorids": ["williamd4112@gapp.nthu.edu.tw", "ariel@shann.net", "at7788546@gmail.com", "s106062520@m106.nthu.edu.tw", "cylee@cs.nthu.edu.tw"], "authors": ["Zhang-Wei Hong", "Tzu-Yun Shann", "Shih-Yang Su", "Yi-Hsiang Chang", "Chun-Yi Lee"], "TL;DR": "We propose a diversity-driven exploration strategy to improve the exploration efficiency of deep reinforcement learning agents.", "pdf": "/pdf/c861598af216b2b3ee06cfbfde6d5e8bf0feff7d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922808, "id": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper177/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer1"], "reply": {"forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922808}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582840716, "tcdate": 1520592879245, "number": 2, "cdate": 1520592879245, "id": "ryPgukgFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer2"], "content": {"title": "Interesting method, insufficient experimental validation, limited related work ", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a diversity penalty which penalises policies for being similar to previous policies. \nThe diversity penalty is formulated as a KL divergence between the previous policies and the current policy, thus encouraging the agent to explore new policies.\n\nThis seems like a reasonable objective for exploration in policy space. \n\nThere are two main concerns:\n1) Novelty: \nThe paper misses proper comparisons to other exploration methods eg [1], [2]. This raises the question whether other, more closely related publications have been missed which could substantially threaten algorithmic novelty.\n2) Experimental validation:\nThe Atari experiments are far from conclusive. The authors test on only 3 random seeds and lack statistical validation of the results. It would also be nice to have other exploration focussed baselines included in the validation. \n\n[1]: Reinforcement Learning with Deep Energy-Based Policies (Haarnoja T., Tang H., Abbeel P., Levine S. ICML 2017)\n[2]: Equivalence Between Policy Gradients and Soft Q-Learning ( Schulman, J., Abbeel, P. and Chen, X., arXiv preprint arXiv:1704.06440, 2017).\n\n \n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.", "paperhash": "hong|diversitydriven_exploration_strategy_for_deep_reinforcement_learning", "keywords": ["Deep Reinforcement Learning", "Exploration"], "_bibtex": "@misc{\n  hong2018diversity-driven,\n  title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},\n  author={Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=BJsD7L1vz}\n}", "authorids": ["williamd4112@gapp.nthu.edu.tw", "ariel@shann.net", "at7788546@gmail.com", "s106062520@m106.nthu.edu.tw", "cylee@cs.nthu.edu.tw"], "authors": ["Zhang-Wei Hong", "Tzu-Yun Shann", "Shih-Yang Su", "Yi-Hsiang Chang", "Chun-Yi Lee"], "TL;DR": "We propose a diversity-driven exploration strategy to improve the exploration efficiency of deep reinforcement learning agents.", "pdf": "/pdf/c861598af216b2b3ee06cfbfde6d5e8bf0feff7d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922808, "id": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper177/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer1"], "reply": {"forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922808}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582688533, "tcdate": 1520716290152, "number": 3, "cdate": 1520716290152, "id": "Sy9-q6ZtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer1"], "content": {"title": "An interesting exploration heuristic with some good Atari results", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a diversity-driven exploration strategy for reinforcement learning.\nThis algorithm provides a bonus whenever the policy of the algorithm differs from the \"baseline\" policy that has been recently observed.\n\nThere are several things to like about this paper:\n- The approach outlines a reasonable exploration heuristic for RL.\n- The paper outlines practical+code implementation details that may inspire people to build on this.\n- The results on Atari seem to outperform state of the art baselines.\n\nOn the other hand the paper has several shortcomings:\n- The algorithm is not clearly presented as a *heuristic* and a reader might easily be confused into thinking that this algorithm doesn't have clear shortcomings.\n- Discussion or understanding of how/why this relates to literature on \"efficient exploration\" or why they are not relevant could be improved.\n- Some of the discussion of individual Atari game behavior is confusing without video/illustration.\n\nOverall I think that this paper is reasonable, and would probably be interesting to the conference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.", "paperhash": "hong|diversitydriven_exploration_strategy_for_deep_reinforcement_learning", "keywords": ["Deep Reinforcement Learning", "Exploration"], "_bibtex": "@misc{\n  hong2018diversity-driven,\n  title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},\n  author={Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=BJsD7L1vz}\n}", "authorids": ["williamd4112@gapp.nthu.edu.tw", "ariel@shann.net", "at7788546@gmail.com", "s106062520@m106.nthu.edu.tw", "cylee@cs.nthu.edu.tw"], "authors": ["Zhang-Wei Hong", "Tzu-Yun Shann", "Shih-Yang Su", "Yi-Hsiang Chang", "Chun-Yi Lee"], "TL;DR": "We propose a diversity-driven exploration strategy to improve the exploration efficiency of deep reinforcement learning agents.", "pdf": "/pdf/c861598af216b2b3ee06cfbfde6d5e8bf0feff7d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922808, "id": "ICLR.cc/2018/Workshop/-/Paper177/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper177/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper177/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper177/AnonReviewer1"], "reply": {"forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper177/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922808}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573565110, "tcdate": 1521573565110, "number": 97, "cdate": 1521573564764, "id": "ryB6RA0tM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJsD7L1vz", "replyto": "BJsD7L1vz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.", "paperhash": "hong|diversitydriven_exploration_strategy_for_deep_reinforcement_learning", "keywords": ["Deep Reinforcement Learning", "Exploration"], "_bibtex": "@misc{\n  hong2018diversity-driven,\n  title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},\n  author={Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=BJsD7L1vz}\n}", "authorids": ["williamd4112@gapp.nthu.edu.tw", "ariel@shann.net", "at7788546@gmail.com", "s106062520@m106.nthu.edu.tw", "cylee@cs.nthu.edu.tw"], "authors": ["Zhang-Wei Hong", "Tzu-Yun Shann", "Shih-Yang Su", "Yi-Hsiang Chang", "Chun-Yi Lee"], "TL;DR": "We propose a diversity-driven exploration strategy to improve the exploration efficiency of deep reinforcement learning agents.", "pdf": "/pdf/c861598af216b2b3ee06cfbfde6d5e8bf0feff7d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}