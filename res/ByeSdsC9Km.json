{"notes": [{"id": "ByeSdsC9Km", "original": "HJxadct9FQ", "number": 350, "cdate": 1538087788771, "ddate": null, "tcdate": 1538087788771, "tmdate": 1551838775925, "tddate": null, "forum": "ByeSdsC9Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJe4GP7hgV", "original": null, "number": 2, "cdate": 1545512716143, "ddate": null, "tcdate": 1545512716143, "tmdate": 1545517947747, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Public_Comment", "content": {"comment": "This is a novel adaptive take on few-shot learning. It is great to see that it scales better and allows better generalization for < one-shot learning. I look forward to the released code.\nI have three comments:\na) This work is a novel way to combine and generalize the techniques in [1] and [2], which I think are relevant works to discuss in the paper.\nWhile out of scope currently, it would certainly be interesting\nb) to compare different notions of surprise\nc) assess the impact of stronger decoders such as in [3]\n\n1 Shankar et al. Labeled Memory Networks for Online Model Adaptation. AAAI 2018\n2 Sung et al. Learning to Compare: Relation Network for Few-Shot Learning. CVPR 2018\n3 Bertinetto et al. Learning feed-forward one-shot learners. NIPS 2016\n\n\n", "title": "Nice take on adaptive learning "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860543, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByeSdsC9Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860543}}}, {"id": "S1g6zjske4", "original": null, "number": 1, "cdate": 1544694549094, "ddate": null, "tcdate": 1544694549094, "tmdate": 1545354506763, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Meta_Review", "content": {"metareview": "All reviewers recommend acceptance. The problem is an interesting one. THe method is interesting.\nAuthors were responsive in the reviewing process.\n\nGood work. I recommend acceptance :)", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Strong paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper350/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353246083, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353246083}}}, {"id": "Hkx8p7qAA7", "original": null, "number": 1, "cdate": 1543574461902, "ddate": null, "tcdate": 1543574461902, "tmdate": 1543574461902, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Public_Comment", "content": {"comment": "First, it is a very interesting idea.\nI wonder if the Memory store can be updated? If a point stored in the Memory store, it will be deleted in the later iteration or stored in the memory forever.\nBesides, is the Memory store with/without the upper limit?\n\nThanks.", "title": "Can Memory store be updated?"}, "signatures": ["~anony_anony_anony1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~anony_anony_anony1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860543, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByeSdsC9Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860543}}}, {"id": "BJxh2pdP2X", "original": null, "number": 1, "cdate": 1541012916227, "ddate": null, "tcdate": 1541012916227, "tmdate": 1543171639961, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "content": {"title": "Interesting algorithm for a few-shot learning", "review": "Summary: the authors propose a new algorithm, APL, for a few-shot and a life-long learning based on an external memory module. APL uses a surprise-based signal to determine which data points to store in memory and an attention mechanism to the most relevant points for prediction. The authors evaluate APL on a few-shot classification task on Omniglot dataset and on a number analogy task.\n\nQuality: the authors consider interesting approach to life-long learning and I really liked the idea of a surprise-based signal to choose the data to store. However, I am not convinced by the learning setting that authors study. While a digit-symbol task from the introduction is interesting to study the properties of APL, I fail to see any real world analogy where it is useful. The same happens in a few-shot omniglot classification. The authors decided to shuffle the labels within episodes that, I guess, is supposed to represent different tasks in a typical life-long learning scenario. Again, it maybe interesting to study the behaviour of the algorithm, but I don't see any practical relevance here. It would make more sense to study the algorithm in a life-long learning setting, for example, considered in [1] and [2].\n\nClarity: the paper is well-written in general. I failed to decode the meaning behind the paragraph under Figure 3 on page 4 and would advise the authors to re-write it. The same goes to the first paragraph on page 3.\n\nOriginality: the paper builds on the prior work of Kaiser et al., 2017 and Santoro et al., 2016, but the proposed modifications are novel to my best knowledge.\n\nSignificance: below average: the paper combines interesting ideas that potentially can be used in different learning contexts and with other algorithms, however, the evaluation does not show the benefit in an obvious way.\n\nOther comments: \n* throughout the whole paper it is not clear if the embeddings are learned or not. I suppose they are, but what then happens to the ones in memory? If they are not, like in ImageNet example, where do they come from?\n* the hyperparameter \\sigma: the authors claim \"the value of \\sigma seems to not matter too much\". Matter for what? It's great if the performance is stable for a wide range of \\sigma, but it seems like it should have a great influence over the memory footprint of APL. I feel this is an important point that needs more attention.\n* it would be interesting to see how APL performs with a simple majority vote instead the decoder layer. This would count for an ablation study and could emphasize the role of the decoder.\n* Figure 4, b) plots are completely unreadable on black-and-white print, the authors might like to address that\n* In conclusion, the first claim about state-of-the-art accuracy with smaller memory footprint: I don't think that the results of the paper justify this claim.\n\n[1] Yoon et al, Lifelong Learning with Dynamically Expandable Networks, ICLR 2017\n[2] Rebuffi et al,  iCaRL: Incremental Classifier and Representation Learning, CVPR 2017\n\n********************\nAfter authors response:\n\nThanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "cdate": 1542234481248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704693, "tmdate": 1552335704693, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryehOhG2T7", "original": null, "number": 5, "cdate": 1542364276039, "ddate": null, "tcdate": 1542364276039, "tmdate": 1542364276039, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "BJxh2pdP2X", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "content": {"title": "Response to reviewer 1 (cont.)", "comment": ">> the hyperparameter \\sigma: the authors claim \"the value of \\sigma seems to not matter too much\". Matter for what? It's great if the performance is stable for a wide range of \\sigma, but it seems like it should have a great influence over the memory footprint of APL. I feel this is an important point that needs more attention.\n\nThanks for the excellent question! We have added a detailed analysis of the behavior of the model as a function of \\sigma in the supplementary information. To give you a quick summary \\sigma does not affect the memory footprint of APL as much as might be assumed a priori as the model exhibits 2 regimes: before being completely trained, the model basically writes everything to memory; while after being trained the model is either completely surprised with a new data point (has never seen it before, or is significantly different from what it\u2019s seen before), or classifies it with high accuracy. Therefore the model ends up writing roughly the same number of points to memory for a wide range of \\sigma (obviously if you make it too high or too low the model breaks).\n\n>> it would be interesting to see how APL performs with a simple majority vote instead the decoder layer. This would count for an ablation study and could emphasize the role of the decoder.\n\nMatching networks can be seen as a special case of APL without a decoder and where all points are written to memory. Therefore the performance numbers for Matching Networks serve as an upper bound to the performance of APL ablated without a decoder. We will clarify this point in the text.\n\n>> Figure 4, b) plots are completely unreadable on black-and-white print, the authors might like to address that\n\nThank you, we will strive to optimize the visual presentation of these plots in the final version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619085, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper350/Authors|ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619085}}}, {"id": "BylMPhznTQ", "original": null, "number": 4, "cdate": 1542364250430, "ddate": null, "tcdate": 1542364250430, "tmdate": 1542364250430, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "BJxh2pdP2X", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "Thank you for the useful and constructive criticism which has helped us improve the paper. We address specific concerns below.\n\n>> \u201cThe authors decided to shuffle the labels within episodes that, I guess, is supposed to represent different tasks in a typical life-long learning scenario. Again, it maybe interesting to study the behaviour of the algorithm, but I don't see any practical relevance here.\u201d\nWe would like to stress that the tasks presented in the paper are not novel and arbitrary, but rather have been the subject of an extensive body of work in the meta-learning field [c.f. 1, 2, 3, 4, 5, 6, linked below, and further references on the related work section of this paper]. The motivation behind the label-shuffling task is a scenario where the model must learn to quickly associate high-dimensional data with a particular label (e.g. an image with a class label). Note that the models are always tested on a held-out test set of classes they have never seen before, which means in a real life scenario the model would be seeing a new class for the first time and would then immediately learn to associate subsequent data of the same class with the correct class.\n\nWe emphasize that while few-shot learning is related to life-long learning, these are different research areas with different goals: few-shot learning focuses on doing well on a single task, where the model must perform well on new data not seen during training; while life-long learning focuses on adapting to *new* tasks not seen during training.\n\n>> \u201cIt would make more sense to study the algorithm in a life-long learning setting, for example, considered in [1] and [2].\u201d\n\nWhile APL was devised to perform well in the few-shot learning scenario as explained above, we thank the reviewer for suggesting another interesting research area where APL could also be applied. We performed follow-up experiments to replicate the setup described in reference [1] provided by the reviewer and compared APL to progressive networks, a well-known life-long learning algorithm. We show that APL performs as well or better than progressive networks even though it does not need to perform gradient descent steps at test time. A thorough investigation of APL in the life-long learning setting would be out of scope for this paper but very interesting as follow up work!\n\n>> \u201cI failed to decode the meaning behind the paragraph under Figure 3 on page 4 and would advise the authors to re-write it. The same goes to the first paragraph on page 3.\u201d\n\nThank you for these suggestions. We have rewritten these sections in the text in order to clarify their presentation.\n\n>> throughout the whole paper it is not clear if the embeddings are learned or not. I suppose they are, but what then happens to the ones in memory? If they are not, like in ImageNet example, where do they come from?\n\nThe embeddings are learned in the case of omniglot and the digit analogy task. Each episode is short (we start with ~40*number of classes examples and anneal the episode length as accuracy improves), and the memory is flushed between each episode, so the embeddings in the memory are never too \u2018old\u2019.\n\n[1] Vinyals, Oriol, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. \u201cMatching Networks for One Shot Learning.\u201d ArXiv:1606.04080 [Cs, Stat], June 13, 2016. http://arxiv.org/abs/1606.04080.\n[2] Ren, Mengye, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. \u201cMeta-Learning for Semi-Supervised Few-Shot Classification.\u201d ArXiv:1803.00676 [Cs, Stat], March 1, 2018. http://arxiv.org/abs/1803.00676.\n[3] Snell, Jake, Kevin Swersky, and Richard S. Zemel. \u201cPrototypical Networks for Few-Shot Learning.\u201d ArXiv:1703.05175 [Cs, Stat], March 15, 2017. http://arxiv.org/abs/1703.05175.\n[4] Finn, Chelsea, Kelvin Xu, and Sergey Levine. \u201cProbabilistic Model-Agnostic Meta-Learning.\u201d ArXiv:1806.02817 [Cs, Stat], June 7, 2018. http://arxiv.org/abs/1806.02817.\n[5] Nichol, Alex, Joshua Achiam, and John Schulman. \u201cOn First-Order Meta-Learning Algorithms.\u201d ArXiv:1803.02999 [Cs], March 8, 2018. http://arxiv.org/abs/1803.02999.\n[6] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \u201cA Simple Neural Attentive Meta-Learner,\u201d July 11, 2017. https://arxiv.org/abs/1707.03141."}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619085, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper350/Authors|ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619085}}}, {"id": "rJebKiM2pX", "original": null, "number": 3, "cdate": 1542364025499, "ddate": null, "tcdate": 1542364025499, "tmdate": 1542364025499, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "r1lzbPiPnX", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "Thanks a lot for your feedback. In the following we address some of the points you raised:\n\n- Representation alignment: Thank you for pointing this out. We have rewritten the corresponding section in the paper as this explanation could be made clearer. To quickly address your question, no gradients are calculated through the memory items. The weights of the encoder + decoder are optimized to minimize the cross-entropy loss for the current mini-batch alone, and then the embeddings produced by the encoder are stored in the memory (if the loss is high enough). Due to the nature of the classification problem, we expect embeddings for similar classes to be similar (in the euclidean distance case). Therefore the next time we see another example of that same class, the memory query should produce neighbors which share the same class. In this case, even though we never learn what to query or backpropagate through the memory, the query system should return the \u2018correct\u2019 set of neighbors. However this explanation is an intuitive hypothesis only and is not mathematically necessary! It could be the case that the encoder learns to produce very different embeddings for the same class, and therefore a k-nearest-neighbors query with euclidean distance would not return memories which are information. Which is why we needed to empirically verify whether the embeddings converge in the expected way or not. Our experimental results show that indeed this hypothesis is correct, and the query system works as we expected.\n\n- Alternative measure of surprise: We have added a discussion on this point as a general comment above.\n\n- Paper title: We agree that the title is quite broad and might lead to confusion amongst researchers from different areas. As a result we have extended it to better reflect the contents of the paper.\n\nThanks again for the useful feedback!"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619085, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper350/Authors|ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619085}}}, {"id": "rJe9NiGnpm", "original": null, "number": 2, "cdate": 1542363954292, "ddate": null, "tcdate": 1542363954292, "tmdate": 1542363954292, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "SyTTVGKnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "content": {"title": "Response to reviewer3", "comment": "Thanks a lot for your comments and suggestions. In the following we address three of the points you raised:\n\n1. Alternative measure of surprise: We have added a discussion on this point as a general comment above.\n\n3. Variational objective. In this paper the main idea was to test the effectiveness of the memory controller mechanism coupled with a relational decoder. It is definitely possible to adapt a variational objective in the architecture and it would be a very interesting avenue for future work. Thank you for the suggestion.\n\nAdditional experiments: as you suggested we have carried out more experiments to further consolidate our presentation of the model. We have applied APL to a set of continual learning experiments suggested by reviewer 3 and show that APL performs en par with progressive networks. These results are included the final version of the paper along with some pointers to the relevant literature.\n\nIn light of the positive nature of your reviews we hope that these comments and the additional experiments can sway you to increase your rating of the paper.\nThanks again for the useful comments!"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619085, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper350/Authors|ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619085}}}, {"id": "HJgNL9M2TX", "original": null, "number": 1, "cdate": 1542363723585, "ddate": null, "tcdate": 1542363723585, "tmdate": 1542363723585, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "content": {"title": "About surprise measure", "comment": "Several comments asked about alternatives to the surprise measure. We add a brief discussion below:\n\nThe proposed surprise measure uses the cross-entropy between the label prediction and the true label. This is equivalent to measuring how many more bits of information a perfect classifier would carry about the label as compared to the current model. In other words, this is the information we are missing about the label. To compress information ideally we want to store data which contains a lot of missing information, and discard redundant information. The approach we proposed simply thresholds on a provided number of bits derived from first principles. With more computational capacity it might be possible to optimize this threshold value via grid search, but the current value seems to provide good results.\nAnother option would be to use the classification accuracy as a proxy for surprise: if we make a mistake, then we store the data point. However this may not lead to optimal compression: suppose the case where two labels are very hard to distinguish and the correct posterior probabilities given an example from this vicinity would be [A:0.48, B:0.48, C:0.02, D:0.01, ...]. If the model predicts B instead of A in this case and we already have a few examples of A and B in memory, storing an additional example won\u2019t help much as we will continue to make mistakes 50% of the time regardless of whether we store it or not. On the other hand, a measure based on the number of bits will be able to distinguish this case from one in which the model mistakenly places most of the probability mass on the wrong class, or simply outputs a uniform probability distribution.\n\nIt is also possible to learn a metric for surprise, for example by training a separate model which can tell us whether this input is surprising or not. This might be particularly useful in the case of unsupervised learning, where we don\u2019t know how similar or dissimilar each data point is to the rest of the dataset. However, it is unclear whether this would help in the case of supervised learning, where there is already a natural low-dimensional representation of how examples relate to one another (i.e. the classes). Such an exploration would be interesting as future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper350/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619085, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeSdsC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper350/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper350/Authors|ICLR.cc/2019/Conference/Paper350/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers", "ICLR.cc/2019/Conference/Paper350/Authors", "ICLR.cc/2019/Conference/Paper350/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619085}}}, {"id": "SyTTVGKnQ", "original": null, "number": 3, "cdate": 1541117124516, "ddate": null, "tcdate": 1541117124516, "tmdate": 1541534068866, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "content": {"title": "Review for adaptive posterior learning", "review": "The paper proposes a novel model that reads in information, decide whether this information is surprising and hence whether or not to keep it in memory and also utilizing information in the memory to quickly adapt or reason. The authors experimented with few-shot Omniglot classification and meta learning reasoning tasks. \n\nNovelty:\n\nThe authors introduced a novel self-contained model that decides what to write to the external memory and making use of the external memory for different tasks.\n\nMy comments are mostly as follows: \n\n1. The paper is well written, the problems are clearly stated, the solution is presented in a clear way, overall very easy to follow.\n\n2. This is an interesting paper that combines a novel technique for writing to external memory based on surprisal and using it for more difficult tasks such as deductive reasoning.  I really like the surprisal mechanism, there are cognitive/ neuroscience materials that supports this approach (that the brain tends to write to memory things that are surprising). This also makes total sense from a machine learning perspective. \n\n3. Could another objective  be used for surprisal? Also, instead of a determinstic encoder, decoder, is it possible to use a variational objective?\n\n4. The experiments look convincing.\n\nOverall a very nice paper, nice idea, could show more resul", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "cdate": 1542234481248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704693, "tmdate": 1552335704693, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lzbPiPnX", "original": null, "number": 2, "cdate": 1541023482144, "ddate": null, "tcdate": 1541023482144, "tmdate": 1541534068608, "tddate": null, "forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "content": {"title": "New idea in using memory for generalization task, more clarification and experiments are needed.", "review": "In this paper, authors present an algorithm to generalize learned properties from few observations by using a memory store and a memory controller. The experiments show comparable results on few-shot classification task and better performance and scalability for when the number of labels is unknown .\n\n- The paper is well-written and easy to follow in general. The notations and model specifications are clear. \n\n- The idea of incorporating an external memory store to save previous experiences is interesting especially without the need to backpropagate through the memory at each step. It is done by alignment of a query with the embeddings that are stored in the memory using k-nearest neighbor with Euclidean distance measures.  However, I am not quiet sure about how this is done in practice. It is stated in the paper that this alignment needs to emerge as a byproduct of training which is achieved by getting optimized to be as class-discriminative as possible. Isn't this implicitly optimizing part of the memory? I think more clarification would help a lot in understanding of this part.\n\n-  I liked using a memory controller that decides whether a point is 'surprising'. Authors defined surprise to be negative log of prediction for label. I was wondering if they considered other measures, and investigated the effects that they might have. I think a brief discussion would be helpful.\n\n- I am not an expert in this area but the experiments look convincing in general. Results in table one corresponding to 423-way are convincing since the proposed algorithm is the only candid that is able to perform the task with relatively good performance. On imagenet data set, the results are comparable to Inception-ResNet-v2 for fixed label case. However, more in-depth experiments or settings such as top-5 accuracy are needed to justify the performance of algorithm on this data set.  For the number analogy task the algorithm performs well in achieving high accuracy.\n\n- Title of the paper is too generic. From the looks of it, adaptive posterior learning should cover wider set of tasks or probabilistic models, but it does not. So to avoid confusion (and the expectation that comes with this name), I strongly suggest that the authors change the title or make it more specific to actually represent what is discussed in the paper.\n\n- In figure 4 c, I think x label should be \"class number\" not \"number of classes\". ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper350/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint.  In addition, its memory compression allows it to scale to thousands of unknown labels.  Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.", "keywords": ["metalearning", "memory", "few-shot", "relational", "self-attention", "classification", "sequential", "reasoning", "working memory", "episodic memory"], "authorids": ["tiago.mpramalho@gmail.com", "garnelo@google.com"], "authors": ["Tiago Ramalho", "Marta Garnelo"], "TL;DR": "We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.", "pdf": "/pdf/956c46c6a073db95ea165145a217984e60e5d2fb.pdf", "paperhash": "ramalho|adaptive_posterior_learning_fewshot_learning_with_a_surprisebased_memory_module", "_bibtex": "@inproceedings{\nramalho2018adaptive,\ntitle={Adaptive Posterior Learning: few-shot learning with a surprise-based memory module},\nauthor={Tiago Ramalho and Marta Garnelo},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeSdsC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper350/Official_Review", "cdate": 1542234481248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeSdsC9Km", "replyto": "ByeSdsC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper350/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704693, "tmdate": 1552335704693, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper350/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}