{"notes": [{"id": "HygtiTEYvS", "original": "BklaVN1dvB", "number": 751, "cdate": 1569439136702, "ddate": null, "tcdate": 1569439136702, "tmdate": 1577168218809, "tddate": null, "forum": "HygtiTEYvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wL6Kx4bkX4", "original": null, "number": 1, "cdate": 1576798705005, "ddate": null, "tcdate": 1576798705005, "tmdate": 1576800931080, "tddate": null, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "invitation": "ICLR.cc/2020/Conference/Paper751/-/Decision", "content": {"decision": "Reject", "comment": "The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain. \n\nThe reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well-motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine-tuning would not be sufficient. \n\nThe recommendation is to reject the paper at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728641, "tmdate": 1576800281085, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper751/-/Decision"}}}, {"id": "HJgTpHmniH", "original": null, "number": 1, "cdate": 1573823940630, "ddate": null, "tcdate": 1573823940630, "tmdate": 1573823940630, "tddate": null, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "invitation": "ICLR.cc/2020/Conference/Paper751/-/Official_Comment", "content": {"title": "Rebuttal Comment", "comment": "We would like to thank the Reviewers for their feedback. Unfortunately, we were not able to perform additional experiments and address all comments to the extent and depth we would like to within the rebuttal period, but we will use the feedback as a guideline for improving on our paper and results and resubmit in the future.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygtiTEYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper751/Authors", "ICLR.cc/2020/Conference/Paper751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper751/Reviewers", "ICLR.cc/2020/Conference/Paper751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper751/Authors|ICLR.cc/2020/Conference/Paper751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166775, "tmdate": 1576860559613, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper751/Authors", "ICLR.cc/2020/Conference/Paper751/Reviewers", "ICLR.cc/2020/Conference/Paper751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper751/-/Official_Comment"}}}, {"id": "BylWnJiXtH", "original": null, "number": 1, "cdate": 1571168168723, "ddate": null, "tcdate": 1571168168723, "tmdate": 1572972556854, "tddate": null, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "invitation": "ICLR.cc/2020/Conference/Paper751/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper investigates a setting in which the observation function changes while the underlying environmental dynamic stays the same. \nIn order to re-use the policy which was trained on the old observation function, they propose to learn a mapping function to map the new observations to the old ones.\n\nI believe the work is interesting as generalization and reducing the sample complexity of learning policies, for example through re-use of old policies, is of high current interest.  However, I believe this paper requires more work to show the feasibility of the proposed approach. In particular:\n- The proposed method has the problem that matching is done 'locally' and without any guarantee that the mapping function will converge to the correct mapping. This is not a problem of the method itself but of the challenging problem setup. The authors discuss this and propose two approaches to alleviate this. However, no experimental evidence is shown whether and to what extend this prevents wrong local minima, as I don't believe the mountain car has such problems?\n- Evaluation is only done on the mountain-car experiment, which is not sufficient to show feasibility in general\n- The learning curves in the experiment seem highly volatile and unstable. I'm not sure why this is the case, maybe wrong hyperparameters or a bug in the code? \n- Lastly, the paper is over the recommended limit of 8 pages and could, in my opinion, made more concise at many points to shorten it and also make it easier to read.\n\nIn summary: I believe this interesting work, but requires more experiments in different environments and additional ablation studies to show the feasibility of the proposed method. Making the writing more concise would, in my opinion, not only shorten the paper but also make it easier to read. "}, "signatures": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575446353012, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper751/Reviewers"], "noninvitees": [], "tcdate": 1570237747620, "tmdate": 1575446353023, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper751/-/Official_Review"}}}, {"id": "SygsfAGpKB", "original": null, "number": 2, "cdate": 1571790355436, "ddate": null, "tcdate": 1571790355436, "tmdate": 1572972556821, "tddate": null, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "invitation": "ICLR.cc/2020/Conference/Paper751/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a means to adapt to new state representations during reinforcement learning. The method works by learning a translation model that translates new state representations to old state representations. The authors evaluate the method on the MountainCar environment and show that the adaptation model is more efficient than training a new policy from scratch.\n\nMy concerns with this work are as follows:\n\n- I don't find the type of changes to state representations very useful (e.g. Section 6, velocity *=2, position /= 2) nor practical. Moreover, learning a translation model to recover the old representation seems easy given this type of simple perturbations. This perturbation is fundamentally different than those used to motivate the problem (e.g. \"a robot whose sensors break down\" or \"whose sensor outputs degrade\", in the introduction).\n- The authors only evaluate with one type of simple perturbation on one environment, hence I am skeptical regarding the generalizability of this work.\n- The premise of this work is to not store old transitions (Section 1, the paragraph \"the most simplistic idea is to...\"), however this model does store old transitions because it uses prioritized experience replay. In this case there is no argument against using this data to train the translation model.\n- A comparison that is missing from the paper is to fine-tune the existing model. I believe this is a more fair comparison in terms of sample-efficiency than training from scratch.\n\n\nOther comments:\n- For the title, to call the adaptation to slightly different state representations \"policy adaptation\" is a stretch.\n- There are a lot of tangential information in the introduction on things like sample efficiency and model-free vs. model-based RL. This is distracting.\n- The paper is excessively long."}, "signatures": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575446353012, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper751/Reviewers"], "noninvitees": [], "tcdate": 1570237747620, "tmdate": 1575446353023, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper751/-/Official_Review"}}}, {"id": "Byx9n-9WqS", "original": null, "number": 3, "cdate": 1572082097842, "ddate": null, "tcdate": 1572082097842, "tmdate": 1572972556776, "tddate": null, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "invitation": "ICLR.cc/2020/Conference/Paper751/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "NOTE: This paper is 10-pages long which requires a higher bar according to the guidelines.\n\nSummarize what the paper claims to do/contribute.\n* This paper claims to propose the first method to translate an environment representation to a different representation when that changes. \n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\nReject.\n* The results were not adequate. I suggest exploring more environments and more complex ones than MountainCar. \n* I do not believe this is the first attempt in translating an environment represention to a different one. Other techniques in domain adaptation have been working on this for quite some time. It might be the case that the technique proposed here is better eg to pixel-based adaptation or adaptation of features, but it would need to be shown experimentally.\n* Given the higher number of pages, i would have expected more thorough experimentation. "}, "signatures": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper751/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christopher.mutschler@iis.fraunhofer.de", "pokutta@zib.de"], "title": "Self-Supervised Policy Adaptation", "authors": ["Christopher Mutschler", "Sebastian Pokutta"], "pdf": "/pdf/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "TL;DR": "Greedy State Representation Learning (GSRL) translates a given policy when the environment representation changes ", "abstract": "We consider the problem of adapting an existing policy when the environment representation changes. Upon a change of the encoding of the observations the agent can no longer make use of its policy as it cannot correctly interpret the new observations. This paper proposes Greedy State Representation Learning (GSRL) to transfer the original policy by translating the environment representation back into its original encoding. To achieve this GSRL samples observations from both the environment and a dynamics model trained from prior experience. This generates pairs of state encodings, i.e., a new representation from the environment and a (biased) old representation from the forward model, that allow us to bootstrap a neural network model for state translation. Although early translations are unsatisfactory (as expected), the agent eventually learns a valid translation as it minimizes the error between expected and observed environment dynamics. Our experiments show the efficiency of our approach and that it translates the policy in considerably less steps than it would take to retrain the policy.", "keywords": ["reinforcement learning", "environment representation", "representation learning", "model mismatch"], "paperhash": "mutschler|selfsupervised_policy_adaptation", "original_pdf": "/attachment/254e7d62ed5c1d7a84735b6dbc0c8a42ec398b85.pdf", "_bibtex": "@misc{\nmutschler2020selfsupervised,\ntitle={Self-Supervised Policy Adaptation},\nauthor={Christopher Mutschler and Sebastian Pokutta},\nyear={2020},\nurl={https://openreview.net/forum?id=HygtiTEYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygtiTEYvS", "replyto": "HygtiTEYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575446353012, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper751/Reviewers"], "noninvitees": [], "tcdate": 1570237747620, "tmdate": 1575446353023, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper751/-/Official_Review"}}}], "count": 6}