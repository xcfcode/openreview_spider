{"notes": [{"id": "ysti0DEWTSo", "original": "CYNXXW0WOhN", "number": 1098, "cdate": 1601308123497, "ddate": null, "tcdate": 1601308123497, "tmdate": 1614985758677, "tddate": null, "forum": "ysti0DEWTSo", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QJDjSmFlu4", "original": null, "number": 1, "cdate": 1610040375892, "ddate": null, "tcdate": 1610040375892, "tmdate": 1610473968099, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers found the construction is very clever and the empirical results are interesting. However, a more thorough theoretical explanation is needed for acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040375878, "tmdate": 1610473968082, "id": "ICLR.cc/2021/Conference/Paper1098/-/Decision"}}}, {"id": "BfV-ovvpCR", "original": null, "number": 8, "cdate": 1606203495218, "ddate": null, "tcdate": 1606203495218, "tmdate": 1606203495218, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "FauptPX7Al", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Mean-square loss", "comment": "In the revised paper, we added experimental results for the mean-square loss and commented on how our experimental results reflect the results in the references mentioned by the reviewer. \n\nOur main result does not change, but we found quantitative difference in test errors. Overall, the cross-entropy loss yields better performance than the mean-square loss. We also found that test errors for finite networks tend to be closer to those for the NTK, compared with those for the mean-square loss. This indicates that a network trained with the cross-entropy loss more easily gets out of the lazy learning regime compared with that with the mean-square loss, which is consistent with the fact that \"cross-entropy encourages the weights to go to infinity after all training samples are correctly classified (Lyu & Li, (arXiv:1906.05890)), while the whole training path for square loss is in the NTK regime (Arora et al., (arXiv:1904.11955))\" as mentioned by the reviewer. However, we would like to emphasize that even for the mean-square loss, finite networks do better for local labels unless the width is too small.\n\nWe also found that test errors for the mean-square loss show stronger dependences on the width of the network. This result is consistent with rigorous results obtained in Zou et al., (arXiv:1811.08888), Nitanda et al., (arXiv:1905.09870), Ji & Telgarsky, (arXiv:1909.12292), and Chen et al., (arXiv:1911.12360).\n\nWe thank the reviewer for insightful comments and useful information, which help us to improve our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "C-NriC51j2", "original": null, "number": 7, "cdate": 1606202660352, "ddate": null, "tcdate": 1606202660352, "tmdate": 1606202660352, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "BXxP3TcpqY7", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Further revision: Results for the mean-square loss", "comment": "We further revised our paper.\n* We added experimental results for the mean-square loss in Appendix D, according to the suggestion by the reviewer 3. The main result does not change (deeper is better for local labels and shallower is better for global labels), but we find quantitative differences in test errors for the mean-square loss and the cross entropy loss. We also discussed the relation to some previous studies mentioned by the reviewer 3.\n* We realized that not the Euclidean distance but the Manhattan distance should be used in section 4, so we fixed it. The reason is as follows: In the previous work by De Palma et al. (NeurIPS 2019, arXiv:1812.10156) on typical functions generated by random neural networks, binary inputs and the Hamming distance are used. In continuous inputs, the Hamming distance should be replaced by the Manhattan distance since the latter naturally reduces to the former for binary inputs. On the other hand, the Euclidean distance does not reduce to the Hamming distance for binary inputs."}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "FauptPX7Al", "original": null, "number": 5, "cdate": 1605682781667, "ddate": null, "tcdate": 1605682781667, "tmdate": 1605771921324, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "5otdIN6fMW", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for carefully reading our paper and pointing out some important aspects of the NTK. We try to address reviewer's concerns below.\n\n1. *First of all, the definition of NTK in this paper in equation (10) may be wrong. If I understand it correctly, the definition of NTK in Jacot et al., 2018 uses gradient with respect to the W\u2019s instead of the w\u2019s. Therefore by chain rule, the definition in this paper differs from the correct definition by a large factor. This can be essential and may be the reason why the experimental performance of NTK is so bad.*\n\nWe thank the reviewer for pointing out that equation (10) in the previous version was wrong. We agree that gradients should be taken with respect to the scaled weights ($\\tilde{w}$ in the revised version), so we modified Section 2.4. However, we did not use equation (10) directly for our experiments on the NTK, and this modification does not affect our numerical results. Experiments are properly performed by using correct formulae summarized in Appendix A.\n\n\n2. (1) *Moreover, the comparison done in this paper is between finite neural networks trained with cross-entropy loss and NTK trained with mean square loss. The authors commented that replacing cross-entropy loss with mean square loss does not make much difference. However, in the NTK literature there is indeed some difference in landscape properties, over-parameterization requirement, etc (Zou et al., (arXiv:1811.08888), Nitanda et al., (arXiv:1905.09870), Ji & Telgarsky, (arXiv:1909.12292), Chen et al., (arXiv:1911.12360)).*\n\nWe thank the reviewer for pointing out the importance of the choice of the loss function. In our paper, we commented that the mean-square loss yields similar results, but what we wanted to indicate is that qualitative results (deeper is better for local labels, whereas shallower is better for global labels) do not change by replacing the cross-entropy loss by the mean-square loss. Quantitatively, we actually observed the difference. Therefore, our result is consistent with the papers the reviewer cited.\n\n\n2. (2) *Moreover, for classification losses, if the network is trained for a very long time, it will eventually escape from the NTK regime since cross-entropy encourages the weights to go to infinity after all training samples are correctly classified (Lyu & Li, (arXiv:1906.05890)), while the whole training path for square loss is in the NTK regime (Arora et al., (arXiv:1904.11955)). Therefore I suggest that the authors should add training results for finite-width NN using square loss as well, and comment on how their experimental results reflect the results in the references mentioned above.*\n\nAccording to reviewer's suggestion, we are now preparing experimental results using the mean-square loss. When all the numerical results are obtained, we will add them to our paper and make a brief comment on the relation to the references the reviewer pointed out.\n\n\n3. *My third concern is closely related to the second one above. Throughout the paper, the authors discussed finite width NNs as if they are not in the NTK regime or lazy training regime (for example the 4th paragraph on page 2). This is not true, as almost all results in NTK regimes indeed study wide, but finite NNs. In fact, it is shown in Ji & Telgarsky, (arXiv:1909.12292) and Chen et al., (arXiv:1911.12360) that NNs with (almost) constant\nwidth can still fall in the NTK regime with classification losses (unless, of course, when trained for too long). It has been studied that whether the training is in NTK regime is not determined by the width, but the scaling of the network (Chizat et al., (arXiv:1812.07956), Mei et al., (arXiv:1902.06015), Chen et al., (arXiv:2002.04026)).*\n\nWe agree that wide but finite networks can fall in the NTK regime when the learning rate is small enough. Indeed, in Figure 4, we find that the performance of a finite network is similar to that of the NTK in the small learning-rate regime. In order to avoid a misleading statement, we added a sentence \"it is known that a wide but finite network can still be in the lazy learning regime for sufficiently small learning rates\" in the last paragraph of section 1.\n\n\n4. *There have been some slight differences in the definition of NTK. Particularly, it is discussed in Cao and Gu, (arXiv:1905.13210) that different definitions of NTK may differ in a 2^L factor for ReLU networks. I briefly checked Section A of the submission and it seems that the calculation of this paper matches Cao and Gu, (arXiv:1905.13210). Since the network depth is the focus of this paper, the authors may consider clarifying it.*\n\nWe thank the reviewer for this comment. At the beginning of section 2.4 in our revised paper, we mentioned that our formulation follows Cao and Gu (and Arora et al.)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "9EjVujeDgVT", "original": null, "number": 6, "cdate": 1605682997664, "ddate": null, "tcdate": 1605682997664, "tmdate": 1605682997664, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "wTNh0pyt-cG", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the constructive review. We reply to the comments below.\n\n*The connection between depth and local labels, and between shallowness and global label intuitively makes sense in the explanation of the authors, but I am left wondering how widely generalizable are these conclusions? What is the effect of the data generation procedure?*\n\nAs the reviewer2 also mentioned, it is not so trivial to connect our result with real dataset. However, in some cases, we can gain an insight from our result. In the revised version, we pointed out that there exist realistic problems in which k-global features are considered to be important, i.e., machine learning of thermodynamic systems in physics. \n\nSince thermodynamic variables (such as the energy) are usually written as a global sum of local quantities, they are regarded as global features. Our main result then suggests that shallower network is better for such a problem. We added Appendix C, in which we reported experimental results on a simple classification problem of the snapshots of the Ising-spin configurations according to their temperatures. We found that a shallow network is indeed better than a deep one. Applications of machine learning to physics has been an important topic of research, and this additional result implies that our result has a connection to real data.\n\n\n*Similarly the comparison with NTK is interesting, even if it seems to me more confirming NTK theory than providing clear support for the argument of the paper about the effects of depth; the conclusion of section 3.3 agrees with general NTK, but maybe it would have been helpful a longer discussion on how this relate to feature learning.*\n\nThe main purpose of showing Figure 4 is not to show that a finite network performs similarly to the NTK in the small learning-rate regime, but to show that the feature learning regime (i.e. large learning rate regime) should be investigated to understand the effects of depth. In the revised version, we clearly state it at the end of section 3.3.\n\n\n*It would definitely gain more value if a rigorous theoretical discussion of the results may be offered, or, if the same behavior may be observed on other, possibly real-world datasets.*\n\nAt present, we do not have a rigorous theoretical framework, but instead, we added a potential explanation on why deeper (shallower) is better for local (global) labels in section 4. We explained our empirical finding by the chaotic signal propagation through a deep neural network. \n\nAs for real datasets, we point out that global features are important in physics datasets on thermodynamic systems, and our result offers a nontrivial prediction: shallower is better for such a dataset. In Appendix C, it is experimentally demonstrated that it is indeed the case.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "1oh9MWU2Tf_", "original": null, "number": 4, "cdate": 1605682100892, "ddate": null, "tcdate": 1605682100892, "tmdate": 1605682100892, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "t_ioJsQf1dS", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the insightful review. Below we reply to your comments/concerns.\n\n1. *I think my biggest concern is whether this robust effect relates to global vs. local features in real data in the way that the authors suggest.*\n\nWe also think that the relation to real dataset is quite important, and we agree with reviewer's comment \"*I think that this is not completely obvious*\". Since real datasets are too complicated, at this stage, we cannot establish a precise relationship between our \"k-locality\" and locality in a generic real dataset.\n\nOn the other hand, we point out that there exist realistic problems in which k-global features are considered to be important, i.e., machine learning of thermodynamic systems in physics. Since thermodynamic variables (such as the energy) are usually written as a global sum of local quantities, they are regarded as global features. Our main result then suggests that shallower network is better for such a problem. We added Appendix C, in which we reported experimental results on a simple classification problem of the snapshots of the Ising-spin configurations according to their temperatures. We found that a shallow network is indeed better than a deep one. This additional result implies that our result has a connection to real data.\n\n\n2. *One question I had for the authors is why they think that shallow networks can lean global features better than deeper networks. I don't think the authors try to offer an explanation for this effect at any point*\n\nThanks for this comment. In the previous version, we did not provide any explanation at all. In our revised paper, we argued a possible explanation on why deeper is better for local labels and shallower is better for global labels in Section 4.\nWe argued that the chaotic signal propagation through a deep network may play a key role. An important insight is that learning k-local label requires a finer scale of resolution than a typical scale of resolution realized by a randomly initialized pre-trained neural network.\nIt means that two similar inputs should result in distinct outputs when the two inputs have different labels. A deep network can implement such a situation by utilizing chaoticity of signal propagation. This is our intuitive picture on why deeper is better for local features. On the other hand, the k-global label is much more stable under perturbations to an input, and hence the chaoticity brings about unnecessary high resolution and can be disadvantageous. That is why shallower is better for global labels.\nWe also added a related additional experiment in Appendix B.\n\n\n3. *Additionally, I was wondering whether the authors did any experiments for much larger k. Clearly k=d is not that interesting, but were there experiments in a middle regime rather than k << d?*\n\nWe did experiments in a middle regime, but we found that a trained network on the k-local or k-global label with k comparable with d hardly generalizes for not too small d and N up to 50000. If N is further increased, the trained network might start to generalize, but due to its large computation cost, we did not try to access much larger N.\nAlternatively, we can consider a much smaller d, e.g. d=10 and k=5, but we think it is not so interesting.\n\n\n4. *Finally, could the authors comment on the relationship of their results to that of arxiv2007.15801? In that paper, for MLPs, the authors seem to find that infinite width networks broadly perform better than finite width networks. Inspection of your Figure 2(c) suggests one possibility is that real data is more akin to having global information? On the other hand, in that paper the authors find that finite networks outperform infinite networks for convolution networks (which obviously care about features that are local in space, and not the less-restrictive k-local notion of locality). Did the authors consider or make any experiments on convolutional architectures?*\n\nIn our setting, the NTK sometimes outperforms finite networks for the k-global label. Therefore, it might be plausible to consider that real dataset contains global features. However, real dataset will contain both local and global features and the comparison is not trivial, and it is difficult to give a definite interpretation.\n\nAs for the convolutional architecture, we have not made any experiment so far since we wanted to first understand the behavior in the simplest setting. We think that it is an important future problem to investigate CNNs and understand the role played by the spatial locality in the data.\n\n\n5. *Did the authors consider any other initializations other than what's discussed in the paper and footnote 2?*\n\nNo, we have not tried other initializations.\n\n6. *A minor comment: I think footnote 1 is a little misleading. The \"k-global\" label type would still be considered \"k-local\" from a quantum information perspective.*\n\nThanks. We corrected the footnote 1."}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "bP87TqruHwL", "original": null, "number": 3, "cdate": 1605681114174, "ddate": null, "tcdate": 1605681114174, "tmdate": 1605681114174, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "iYBWx-4Xeyi", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the detailed review. \n\nFirst of all, we want to respond to your concern about the terminology of \"local\" and \"global\".\n\nThe k-local label is determined by the sign of a product of k entries among d compoenents, whereas the k-global label is determined by a sum of such products over all the components.\nThe label is determined by a small subset of the components in the k-local label (we assume that k is much smaller than d), whereas all the components contribute to the label in the k-global label.\n \nAs the reviewer wrote, local here implies a function with a restricted domain to compute the output, and hence this definition of locality is quite intuitive.\nThe difference between local and global labels is not the operation type.\n\nWe hope that the above explanation clarifies reviewer's concerns about the terminology. \n\n\nNext, we want to reply to your comment:\n\n*On the other hand, I am not sure what Figure 4 is supposed to bring to the table*\n\nAs the reviewer pointed out, we have to admit that the meaning of Figure 4 was not well explained in the previous version. Figure 4 is important because it shows the necessity to investigate the feature learning regime (large learning-rate regime) to understand the benefit of depth in learning local features. Although our understanding on the lazy-learning regime has advanced owing to recent important works, the feature learning regime remains less well understood. Our result would give some hint to gain more insights into the feature learning regime.\n\n\nFinally, we want to address your following comments:\n\n*Authors present good empirical analysis but there is no potential explanation of their conclusion that suggests that global features can be approximated better with shallow networks, and local features with deep networks.*\n*... but I have wished that the paper ended on a higher note with regards to, why is k-locality approximated better with deeper networks, and k-global labels with shallow ones*\n\nIn our revised paper, we added a new section \"Discussion on the observed depth dependence\", where we gave a potential explanation on our result. We argued that the chaotic signal propagation through a deep neural network may play a key role. Learning the k-local label requires a relatively high resolution in the sense that the k-local label easily changes by adding local perturbations. It means that similar inputs must result in distinct outputs, which is naturally implemented by utilizing chaotic property of deep signal propagation. On the other hand, the k-global label is much more stable under local perturbations to an input, and hence the chaoticity due to depth can be rather disadvantageous. That is why shallower is better for the k-global label.\nWe also performed a new experiment that is related to the above argument, and reported its result in Appendix B."}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "BXxP3TcpqY7", "original": null, "number": 2, "cdate": 1605680683379, "ddate": null, "tcdate": 1605680683379, "tmdate": 1605680683379, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment", "content": {"title": "To all the reviewers", "comment": "We thank the reviewers for their constructive comments. We revised our paper according to reviewers' comments. According to the suggestion by the reviewer3, we are now preparing experimental results for the mean-square loss. Although we have not yet completed this job, at this stage we would like to upload the revised paper and reply to each reviewer.\n\nThe major change in this revision is summarized below:\n\n* We added a new section 4, in which we give a potential explanation of our empirical finding, i.e. why deeper is better for local labels and shallower is better for global labels. We explained our result via the chaotic signal propagation through a deep neural network.\n\n* We added Appendix B, in which an additional experimental result on the local stability of learned features with random labels is demonstrated, which supplements an intuitive argument in section 4. This experimental result suggests that a deeper network tends to learn more local features even for a completely structureless dataset.\n\n* We added Appendix C, in which we show that our result is relevant for a real dataset: data generated by thermodynamic systems in physics. Since a thermodynamic quantity is regarded as a global feature, our empirical result implies that a shallow network is better than a deep one. We demonstrated that it is indeed the case.\n\n* According to the comment by the reviewer3, we corrected equation (10) and a paragraph above it.\n\n* According to the comment by the reviewer3, we mentioned in the last paragraph of section 1 that a wide but finite network can be in the lazy learning regime for sufficiently small learning rates."}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ysti0DEWTSo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1098/Authors|ICLR.cc/2021/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Comment"}}}, {"id": "t_ioJsQf1dS", "original": null, "number": 1, "cdate": 1603121275607, "ddate": null, "tcdate": 1603121275607, "tmdate": 1605024531709, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review", "content": {"title": "Clever artificial dataset with artificial notion of local vs. global features to study infinite NTK and finite networks performance.", "review": "This paper aims to empirically explore the depth dependence of overparameterized networks. The authors study fully-connected networks trained on a synthetic dataset consisting of random Gaussian inputs, with the label a simple function of the input. In one case, for \"local\" labels, the label is the parity of a product of a subset of the components. In the other case, for \"global\" labels, the label is a sum of such products of subsets with coverage over all the components. Broadly, the authors find that deeper MLPs are better able to learn the local labels, but shallower MLPs are better able to learn the global labels. Finally, the authors compare these results to the infinite width NTK and show that the NTK does not at all capture the behavior of the finite networks.\n\nI think the strongest point of the paper is that the authors are able to find such a robust effect. The design of the synthetic dataset is very clever, and the results on depth and finite vs. infinite width are very compelling.\n\nI think my biggest concern is whether this robust effect relates to global vs. local features in real data in the way that the authors suggest. I agree that the functions considered by the authors have a nice notion of \"local\" and \"global\" but I'm not sure how good a representation this is of the types of features that are present in real data. At the very least, it would have been nice to have some kind of discussion of the relationship, and at best the authors could have provided some kind of evidence that the features in an exemplar real dataset can be organized this way. I think that this is not completely obvious since the features considered here are multiplicative in components and the label is based on parity. (Do local features in images have this property? What do the authors have in mind for a global feature in an image? I highly doubt that things are arranged in sums of products over the whole image, with the parity of the result being important.) I understand that this is a toy model and I appreciate the nice result; I just would like to understand how representative this toy is.\n\nI think that this is a good paper, and I recommend that ICLR accepts.\n\nI think the experiments are clever, the results are robust, and -- despite my concerns about how to relate this dataset to \"real data\" -- the discrepancy between local and global information and depth is really interesting. I also find the comparison with the infinite width NTK to be very useful; it's really important to understand the ways in which infinite networks are good models for realistic finite networks, and here the authors demonstrate that such infinite networks completely fail on this task.\n\nOne question I had for the authors is why they think that shallow networks can learn global features better than deeper networks. I don't think the authors try to offer an explanation for this effect at any point.\n\nAdditionally, I was wondering whether the authors did any experiments for much larger k. Clearly k=d is not that interesting, but were there experiments in a middle regime rather than k << d? \n\nFinally, could the authors comment on the relationship of their results to that of arxiv2007.15801? In that paper, for MLPs, the authors seem to find that infinite width networks broadly perform better than finite width networks. Inspection of your Figure 2(c) suggests one possibility is that real data is more akin to having global information? On the other hand, in that paper the authors find that finite networks outperform infinite networks for convolution networks (which obviously care about features that are local in space, and not the less-restrictive k-local notion of locality). Did the authors consider or make any experiments on convolutional architectures?\n\nDid the authors consider any other initializations other than what's discussed in the paper and footnote 2?\n\nA minor comment: I think footnote 1 is a little misleading. The \"k-global\" label type would still be considered \"k-local\" from a quantum information perspective. (Of course the authors are welcome to describe their invented dataset however they like.)", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127044, "tmdate": 1606915765708, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1098/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review"}}}, {"id": "wTNh0pyt-cG", "original": null, "number": 2, "cdate": 1603811151938, "ddate": null, "tcdate": 1603811151938, "tmdate": 1605024531641, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review", "content": {"title": "Insightful experiments, a little preliminary ", "review": "\nThis paper analyzes overparametrized networks evaluating how depth and width affect the generalization performance of the network. A set of experiments is designed in which labels are determined either by local or global interactions among the features, and generalization is observed for different values of width and depth of the network. NTK is also considered as a limit case of a network with infinite width.\n\nThe paper is well-written, and the experiments properly designed and explained. I appreciate the rigorous study, and the observations drawn, although theoretical arguments in support of the conclusion would have made them stronger. The connection between depth and local labels, and between shallowness and global label intuitively makes sense in the explanation of the authors, but I am left wondering how widely generalizable are these conclusions? What is the effect of the data generation procedure? Similarly the comparison with NTK is interesting, even if it seems to me more confirming NTK theory than providing clear support for the argument of the paper about the effects of depth; the conclusion of section 3.3 agrees with general NTK, but maybe it would have been helpful a longer discussion on how this relate to feature learning.\n\nIt seems to me that this work is good, but quite preliminary. It devised well-designed experiments, but all the contributions are empirical observations. It would definitely gain more value if a rigorous theoretical discussion of the results may be offered, or, if the same behavior may be observed on other, possibly real-world datasets.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127044, "tmdate": 1606915765708, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1098/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review"}}}, {"id": "iYBWx-4Xeyi", "original": null, "number": 3, "cdate": 1603867066622, "ddate": null, "tcdate": 1603867066622, "tmdate": 1605024531578, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review", "content": {"title": "Highly controlled experiments that provide insight but confusing terminology", "review": "This paper proposes an empirical study of the nature of `\"local\" and \"global\" features and how well they can be approximated via fully connected networks under a highly controlled experimental setup.\n\nPros\u2019:\nThe experiments are highly controlled and the research seems fairly reproducible\nThe paper is self-contained\nThe paper -- though containing a mathematical motivation linked to approximation theory -- is written in a very accessible way for non-expert researchers in the field (myself)\nThe overall question and motivation of the problem the authors are studying is of great importance in the fields of approximation theory, local vs global (Theory of Deep Learning, Applied Vision Science & Representation Learning) and learning capacity via SGD as well. [I am willing to raise my score if authors convince me what I have missed, or where I was confused -- see below].\n\nCons\u2019:\nThere seems to be a misnomer on the definition of what local and global means. I am confused, and these two concepts that should be quite intuitive, are actually defined to mean different things (section 2.1 -- see critical observation below).\nAuthors present good empirical analysis but there is no potential explanation of their conclusion that suggests that global features can be approximated better with shallow networks, and local features with deeper networks. Arguments potentially linked to receptive field size and hierarchy like in vision science (Neyshabur, 2020; Deza, Liao, Banburki & Poggio 2020) would be interesting, but naturally authors cannot make such claims given that all networks trained/tested were fully connected with a R.F. size of 1.\n\n----\n\nCritical Observation [Section 2.1]\n\nK-local label vs K-global label: Why does each k-local label depend on a multiplication of all k entries, while k-global depends on a sum of all k-entries? There seems to be a misnomer on what local and global means here. It is as if in both cases the label is determined by \u201cglobal\u201d properties (as all the inputs are used to compute the label), but the only difference is the operation type: multiplication vs sum -- but I could have missed something. This is quite confusing even though the authors state in the paper that locality need not mean spatial locality.\n\nThe problem with the confusing definition of locality and global properties is that it permeates into the introduction and conclusion of the paper and other relevant work. For example the claim in the introduction: \u201cdeeper is better for local labels, while shallow is better for global labels\u201d under this new definition is confusing as it suggests something that is not what it seems. Unless local here literally means spatially local or implies that a function ignores/zeroes out other inputs (as it would be for functions with restricted -- hence local -- domains to compute the output), then the claim of the paper is misleading/confusing with the relevant literature. Authors should do a better job in arguing why they choose multiplication as a proxy for a local operation and summation as a proxy for a global one. If anything it would make more sense that a k-local label is essentially a subset of the k-global label (example a partial sum, vs a different operation altogether). \n\nAgain, at a higher level: I would have expected a sort of definition where there is only a single \u201cglobal\u201d label (vs k-global label) computed from a feature vector, and `multiple\u2019 k-local labels, depending on what entries in the feature vector are sampled, such that as k reaches the total input size k-local is equal to \u201cglobal\u201d -- but this does not seem to be the case.\n\n---\nSections 2.2 and onwards seem more clear.\n\nOther observations: I really like the dissociation for k-local (continuing with the definition proposed by authors) and k-global labels expressed in Figure 2. This is a neat result, but again I think the argument made should be pushed towards the fundamentally different nature of the operation (multiplication vs addition -- and not local vs global)\n\nThe rest of the paper Figure 3 seems quite interesting to find the point in depth such that there is equalized performance for the 2-local and 2-global cases. On the other hand, I am not sure what Figure 4 is supposed to bring to the table (it seems like comparisons to performance with NTK) -- but, so what? Why does this matter?\n\n---\n\nI think the paper overall proposes a good first step via empirical analysis of the local vs global problem (under the authors definitions), but I would have wished that the paper ended on a higher note with regards to, *why* is k-locality approximated better with deeper networks, and k-global labels with shallow ones. Not that a proof would be necessary, but at least an intuition given the mathematical structure of the network (stacked dot products + non-linearity), their approximation power, and the nature of the label (multiplication vs addition). \n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127044, "tmdate": 1606915765708, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1098/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review"}}}, {"id": "5otdIN6fMW", "original": null, "number": 4, "cdate": 1603970352040, "ddate": null, "tcdate": 1603970352040, "tmdate": 1605024531513, "tddate": null, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "invitation": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review", "content": {"title": "Important topic, but the conclusions are not convincing", "review": "Recent theoretical study on the training of neural networks has introduced an important kernel function called neural tangent kernel. This paper studies the training of deep ReLU networks and compares it with the training directly using NTK by conducting experiments on synthetic data. Based on the experimental results, the authors conclude that deeper networks perform better on certain datasets whose labels are more \u201clocal\u201d, while shallower networks are better at more \u201cglobal\u201d labels. Moreover, the authors observed that finite-width networks have better generalization than NTK. \n\nI am not convinced by the claims of this paper, for the following reasons:\n\n1. First of all, the definition of NTK in this paper in equation (10) may be wrong. If I understand it correctly, the definition of NTK in Jacot et al., 2018 uses gradient with respect to the W\u2019s instead of the w\u2019s. Therefore by chain rule, the definition in this paper differs from the correct definition by a large factor. This can be essential and may be the reason why the experimental performance of NTK is so bad. For the same reason, it may also be possible that not all layers of the network has been trained with the same importance (some layers change faster than other layers due to having a different chain-rule factor), and this can potentially lead to performance differences for NNs and NTKs with different depths.\n\n2. Moreover, the comparison done in this paper is between finite neural networks trained with *cross-entropy loss* and NTK trained with *mean square loss*. The authors commented that replacing cross-entropy loss with mean square loss does not make much difference. However, in the NTK literature there is indeed some difference in landscape properties, over-parameterization requirement, etc (Zou et al., (arXiv:1811.08888), Nitanda et al., (arXiv:1905.09870), Ji & Telgarsky, (arXiv:1909.12292), Chen et al., (arXiv:1911.12360)). Moreover, for classification losses, if the network is trained for a very long time, it will eventually escape from the NTK regime since cross-entropy encourages the weights to go to infinity after all training samples are correctly classified (Lyu & Li, (arXiv:1906.05890)), while the whole training path for square loss is in the NTK regime (Arora et al., (arXiv:1904.11955)). Therefore I suggest that the authors should add training results for finite-width NN using square loss as well, and comment on how their experimental results reflect the results in the references mentioned above.\n\n3. My third concern is closely related to the second one above. Throughout the paper, the authors discussed finite width NNs as if they are not in the NTK regime or lazy training regime (for example the 4th paragraph on page 2). This is not true, as almost all results in NTK regimes indeed study wide, but finite NNs. In fact, it is shown in Ji & Telgarsky, (arXiv:1909.12292) and Chen et al., (arXiv:1911.12360) that NNs with (almost) constant width can still fall in the NTK regime with classification losses (unless, of course, when trained for too long). It has been studied that whether the training is in NTK regime is not determined by the width, but the scaling of the network (Chizat et al., (arXiv:1812.07956), Mei et al., (arXiv:1902.06015), Chen et al., (arXiv:2002.04026)). \n\nBecause of the above two points 2 and 3, I am particularly skeptical about the authors\u2019 explanation of the performance difference between finite NN and NTK.\n\n4. There have been some slight differences in the definition of NTK. Particularly, it is discussed in Cao and Gu, (arXiv:1905.13210) that different definitions of NTK may differ in a 2^L factor for ReLU networks. I briefly checked Section A of the submission and it seems that the calculation of this paper matches Cao and Gu, (arXiv:1905.13210). Since the network depth is the focus of this paper, the authors may consider clarifying it.\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1098/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1098/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Is deeper better? It depends on locality of relevant features", "authorids": ["~Takashi_Mori1", "ueda@phys.s.u-tokyo.ac.jp"], "authors": ["Takashi Mori", "Masahito Ueda"], "keywords": ["deep learning", "generalization", "overparameterization"], "abstract": "It has been recognized that a heavily overparameterized artificial neural network exhibits surprisingly good generalization performance in various machine-learning tasks. Recent theoretical studies have made attempts to unveil the mystery of the overparameterization. In most of those previous works, the overparameterization is achieved by increasing the width of the network, while the effect of increasing the depth has been less well understood. In this work, we investigate the effect of increasing the depth within an overparameterized regime. To gain an insight into the advantage of depth, we introduce local and global labels as abstract but simple classification rules. It turns out that the locality of the relevant feature for a given classification rule plays an important role; our experimental results suggest that deeper is better for local labels, whereas shallower is better for global labels. We also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and an infinitesimal learning rate. It is shown that the NTK does not correctly capture the depth dependence of the generalization performance, which indicates the importance of the feature learning, rather than the lazy learning.", "one-sentence_summary": "It depends on locality of relevant features whether the depth is beneficial in deep learning for classification tasks.", "pdf": "/pdf/3f11d8d7de8244c32a7de84da6a61799d4f50620.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mori|is_deeper_better_it_depends_on_locality_of_relevant_features", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XPH_algUmV", "_bibtex": "@misc{\nmori2021is,\ntitle={Is deeper better? It depends on locality of relevant features},\nauthor={Takashi Mori and Masahito Ueda},\nyear={2021},\nurl={https://openreview.net/forum?id=ysti0DEWTSo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ysti0DEWTSo", "replyto": "ysti0DEWTSo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1098/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127044, "tmdate": 1606915765708, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1098/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1098/-/Official_Review"}}}], "count": 13}