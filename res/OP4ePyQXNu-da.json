{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1395184620000, "tcdate": 1395184620000, "number": 9, "id": "DDB3rJ8n3-9zV", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Nicolas Boulanger-Lewandowski"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Overall interesting paper. I just have some reservation about your claim that:\r\n\r\n'[...] RNN-NADE (Boulanger-Lewandowski et al., 2013), makes speci\ufb01c assumptions about the data (i.e. binary observables). RNNs do not attach any assumptions to the inputs.'\r\n\r\nThe RNN-RBM/RNN-NADE actually function equally well with real-valued inputs and outputs by using Gaussian RBMs or RNADE, as was done with motion capture data in the original RNN-RBM paper (ICML 2012). In fact, your paper assumes binary variables following the Bernoulli distribution (first equation of section 3.1.1) which, if anything, makes stronger assumptions about the data."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393875240000, "tcdate": 1393875240000, "number": 8, "id": "N1E31iPUSc7Um", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We regret to inform you that we made mistakes in the experimental procedure regarding the Penn Treebank corpus.  Due to these, our submitted results on that benchmark are *invalid*.\r\n\r\nThe experimental results on the midi tasks are not affected by this. We will shortly submit a version to arxiv from which the relevant sections are removed. Sadly, due to the size of the benchmark we are not be able to rerun the experiments within a reasonable time to be considered for the final decision about the paper.\r\n\r\nTo allow you to consider the case, we decided to inform you as soon as possible about this issue.  We hope that the chairs and reviewers will still consider our work for the conference.  At the same time, however, we realize that this issue alone may lead to a rejection of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392563640000, "tcdate": 1392563640000, "number": 7, "id": "LL10tMhg0dY_p", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have just uploaded a new version of the paper to arxiv, which will be available at Tue, 18 Feb 2014 01:00:00 GMT.\r\n\r\nWe address various concerns of the reviewers. Here is a break down of the improvements:\r\n\r\n- added figures and text showing the behaviour of fast dropout units (especially delta_k) in various situations,\r\n- added an analysis of the evolution of the recurrent weight matrix during training, which supports our theoretical findings,\r\n- calculation in section 2.2.2 'sampling' corrected,\r\n- added hyper parameters used for the experiments in the appendix,\r\n- included results from [1] for completeness,\r\n- numerous typos and errors fixed.\r\n\r\nWe want to thank the anonymous reviewers, since we believe their comments to have improved the work.\r\n\r\n\r\n[1] Pascanu, Razvan, et al. 'How to Construct Deep Recurrent Neural Networks.'arXiv preprint arXiv:1312.6026 (2013).\r\nAPA"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392297000000, "tcdate": 1392297000000, "number": 1, "id": "PPMEr2u9DTn9i", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "OP4ePyQXNu-da", "replyto": "OOmozKPb_SzGw", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Good day reviewer fc8d,\r\n\r\nWe agree that our work is not groundbreakingly novel. It is just the transfer of a regulariser to a different model; the justification, however, is new to the best of our knowledge.\r\n\r\nWe did not experiment with AWN-RNNs since to the best of our knowledge these models are rather hard to train. E.g. you need to pretrain with non-AWN, then apply AWN, and use the MAP solution subsequently for optimal results (compare [1]). We fear that not being experts with this method due to the lack of practical experience might lead to results that are not representative and would not be fair for AWN-RNN.\r\n\r\nWe stand corrected--AWN is not an MCMC method. We will fix this in the next iteration of the work.\r\n\r\nThank you for the typos, we will correct them.\r\n\r\n\r\nBest,\r\n-Justin\r\n\r\n\r\n[1] Graves, Alex. 'Generating sequences with recurrent neural networks.' arXiv preprint arXiv:1308.0850 (2013)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392219900000, "tcdate": 1392219900000, "number": 6, "id": "OOmozKPb_SzGw", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["anonymous reviewer fc8d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On Fast Dropout and its Applicability to Recurrent Networks", "review": "This paper is a rather straightforward extension of fast dropout to standard RNNs. Maybe too straightforward, it is not very groundbreakingly novel. However, I like the results, and the application of the method to RNNs is useful to know about for other researchers. The authors remark that a comparison for standard RNNs (rather than LSTMs) with adaptive weight noise is lacking -- why have the authors not tried this comparison I wonder, as it seems highly appropriate in this context and doable in their setup.\r\nThere are a few points in the text that should be improved. For example, calling Graves' adaptive weight noise method MCMC is questionable in my view. Also, there are quite a few typos:\r\nabstract: absence -> absence\r\nfirst paragraph: sensitive initialisations -> sensible initialisations\r\nfirst paragraph: remove 'so-called'\r\nsecond paragraph: Contrary -> In contrast\r\nlast par of sec 1: reason -> discuss\r\nfirst par sec 2.2.2: functionc ->function.\r\nsec 2.2.2: 'an subsequently' -> 'and subsequently'\r\npage six, line 11: 'were *** is defined as' -> 'where *** is defined as'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392130620000, "tcdate": 1392130620000, "number": 1, "id": "mATGdlDSLbdSv", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "OP4ePyQXNu-da", "replyto": "gX7L54shLjXYb", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Hello Reviewer fe58,\r\n\r\nThank you for your comments. We will reply to certain points raised below.\r\n\r\n> Right now, the analysis says, it all boils down to delta_a. But how  can we relate delta_a to situations that occur in practice? I.e., can you say, delta_a is positive in situtaion a, and negative in situations b? Perhaps the analysis already allows for such conclusions, but I don't see them, and so it would be nice to inculde something like that.\r\n\r\nWe only gave an informal notion in the bulleted list in section 2.2.2, which we will illustrate with qualitative plots in an upcoming version later this week.\r\nFurther analysis of delta_a requires assumptions about the network architecture (outgoing units, transfer functions, loss function), which we believe to be beyond the scope of this work--we already are beyond the soft page limit and do not know what to cut instead.\r\n\r\n> Also, it is worth stating that a global regularizer may not exist, and the contribution of dropout to the update may not be the derivative of any gradient.\r\n\r\nCan you expand on what you have in mind?\r\n\r\n\r\nAlso, thanks for the typos. The second typo actually was not only a typo but an error; luckily, the consequences of that are minor and the results of that section still stand. We will correct this soon.\r\n\r\n\r\nThanks for your comments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392130560000, "tcdate": 1392130560000, "number": 1, "id": "RlNmIbKrEuIrj", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "OP4ePyQXNu-da", "replyto": "22-aeRonaO2n8", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear reviewer b50c,\r\n\r\nWe will address your points one by one.\r\n\r\n> This paper rederives the fast dropout method which is a deterministic alternative to the originally proposed stochastic dropout regularization for neural networks. Then the authors apply fast dropout to recurrent neural networks (RNN) on two different datasets.\r\n\r\nThe derivation is actually a minor part of the paper (about a single page out of nine) which is necessary to prepare common ground for the understanding of section 2.2.2, which is novel.\r\n\r\n> The main focus of the paper is the derivation of the fast dropout training with almost no link to RNNs until the experiment sections, which makes the paper consists of two disconnected components.\r\n\r\nWhile it is true that the analysis (Section 2.2.2.) is more general than RNNs, the consequence of it is particularly important for RNNs: the regulariser does not bias the dynamics. This is mentioned in the abstract, main text and partially in the conclusion, but probably not prominently enough.\r\n\r\n> The authors could have presented experiments on feedforward NN without affecting the smoothness of the paper at all.\r\n\r\nWe did not include any experiments with standard feedforward MLPs since this has been done extensively in [1]; we do not feel that there is something that we could do better and do not see any novel hypothesis for MLPs we could test.\r\n\r\n> The results are expected. Dropout, on small datasets, does better than baseline systems which doesn\u2019t use any stochastic regularization and is as good as ones that does (Graves 2013). \r\n\r\nWithout the analysis one cannot know about a bias of dynamics. Indeed, recent work [4] has shown that non-fast dropout might bias the dynamics due to its relation to L2 regularization. Thus we believe that the results are far form expected.\r\nFD-RNNs improve upon the state of the art on Penn Treebank character prediction set in [5], but the result table probably needs to be clearer.\r\n\r\n> In table 2, it is mentioned that assumptions made in RNN-NADE system is also applicable to FD. The authors are encouraged to add these assumptions to get better results.\r\n\r\nApplying FD to RNN-NADE is certainly interesting by itself, but well beyond the scope of this paper.\r\n\r\n> The results sections (3.1.2 and 3.2.2) still need more work. There are no analysis or discussion about the achieved results. Analysis of the effects of network structure, dropout rate, and other hyper-parameters are missing. The tables' order is reversed (table 3 is referenced first then table 2 and table 1).\r\n\r\nWe will add the actual hyper parameters obtaining the results to the appendix of an upcoming version. We will also add a few sentences on characteristics of the estimated parameters, such as the Eigenvectors of the transition matrix.\r\n\r\nMore specifically, we will show empirically that the transition matrices found during optimisation inhibit rather big Eigenvalues (>10) which is unlikely when classic regularisers such as weight decay are used.\r\n\r\n> For the results in 3.1.2, please clarify if there is a test data on which you measure NLL or you measure performance on the training data only (there is no training/test division menitoned for the music datasets).\r\n\r\nThe results are obtained on the same splits as in the previous works by [2, 3]. We mention the use of test data for evaluation twice (in the table caption and the beginning of the section) but the information is probably not well placed. We will update the experimental section to be more clear.\r\n\r\n\r\nThanks for your efforts; we will address the issues in an upcoming version later this week.\r\n\r\n\r\n\r\n[1] Wang, Sida, and Christopher Manning. 'Fast dropout training.' Proceedings of the 30th International Conference on Machine Learning (ICML-13). 2013.\r\n[2] Bengio, Yoshua, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. 'Advances in optimizing recurrent networks.' arXiv preprint arXiv:1212.0901(2012).\r\n[3] Boulanger-Lewandowski, Nicolas, Yoshua Bengio, and Pascal Vincent. 'Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription.' arXiv preprint arXiv:1206.6392 (2012).\r\n[4] Wager, Stefan, Sida Wang, and Percy Liang. 'Dropout training as adaptive regularization.' Advances in Neural Information Processing Systems. 2013.\r\n[5] Graves, Alex. 'Generating sequences with recurrent neural networks.' arXiv preprint arXiv:1308.0850 (2013)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392130560000, "tcdate": 1392130560000, "number": 2, "id": "_Ifoh30XojiqN", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Sorry, the reply above was intended for the other review."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392130500000, "tcdate": 1392130500000, "number": 5, "id": "yE_Ay6FimTX09", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewer b50c,\r\n\r\nWe will address your points one by one.\r\n\r\n> This paper rederives the fast dropout method which is a deterministic alternative to the originally proposed stochastic dropout regularization for neural networks. Then the authors apply fast dropout to recurrent neural networks (RNN) on two different datasets.\r\n\r\nThe derivation is actually a minor part of the paper (about a single page out of nine) which is necessary to prepare common ground for the understanding of section 2.2.2, which is novel.\r\n\r\n> The main focus of the paper is the derivation of the fast dropout training with almost no link to RNNs until the experiment sections, which makes the paper consists of two disconnected components.\r\n\r\nWhile it is true that the analysis (Section 2.2.2.) is more general than RNNs, the consequence of it is particularly important for RNNs: the regulariser does not bias the dynamics. This is mentioned in the abstract, main text and partially in the conclusion, but probably not prominently enough.\r\n\r\n> The authors could have presented experiments on feedforward NN without affecting the smoothness of the paper at all.\r\n\r\nWe did not include any experiments with standard feedforward MLPs since this has been done extensively in [1]; we do not feel that there is something that we could do better and do not see any novel hypothesis for MLPs we could test.\r\n\r\n> The results are expected. Dropout, on small datasets, does better than baseline systems which doesn\u2019t use any stochastic regularization and is as good as ones that does (Graves 2013). \r\n\r\nWithout the analysis one cannot know about a bias of dynamics. Indeed, recent work [4] has shown that non-fast dropout might bias the dynamics due to its relation to L2 regularization. Thus we believe that the results are far form expected.\r\nFD-RNNs improve upon the state of the art on Penn Treebank character prediction set in [5], but the result table probably needs to be clearer.\r\n\r\n> In table 2, it is mentioned that assumptions made in RNN-NADE system is also applicable to FD. The authors are encouraged to add these assumptions to get better results.\r\n\r\nApplying FD to RNN-NADE is certainly interesting by itself, but well beyond the scope of this paper.\r\n\r\n> The results sections (3.1.2 and 3.2.2) still need more work. There are no analysis or discussion about the achieved results. Analysis of the effects of network structure, dropout rate, and other hyper-parameters are missing. The tables' order is reversed (table 3 is referenced first then table 2 and table 1).\r\n\r\nWe will add the actual hyper parameters obtaining the results to the appendix of an upcoming version. We will also add a few sentences on characteristics of the estimated parameters, such as the Eigenvectors of the transition matrix.\r\n\r\nMore specifically, we will show empirically that the transition matrices found during optimisation inhibit rather big Eigenvalues (>10) which is unlikely when classic regularisers such as weight decay are used.\r\n\r\n> For the results in 3.1.2, please clarify if there is a test data on which you measure NLL or you measure performance on the training data only (there is no training/test division menitoned for the music datasets).\r\n\r\nThe results are obtained on the same splits as in the previous works by [2, 3]. We mention the use of test data for evaluation twice (in the table caption and the beginning of the section) but the information is probably not well placed. We will update the experimental section to be more clear.\r\n\r\n\r\nThanks for your efforts; we will address the issues in an upcoming version later this week.\r\n\r\n\r\n\r\n[1] Wang, Sida, and Christopher Manning. 'Fast dropout training.' Proceedings of the 30th International Conference on Machine Learning (ICML-13). 2013.\r\n[2] Bengio, Yoshua, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. 'Advances in optimizing recurrent networks.' arXiv preprint arXiv:1212.0901(2012).\r\n[3] Boulanger-Lewandowski, Nicolas, Yoshua Bengio, and Pascal Vincent. 'Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription.' arXiv preprint arXiv:1206.6392 (2012).\r\n[4] Wager, Stefan, Sida Wang, and Percy Liang. 'Dropout training as adaptive regularization.' Advances in Neural Information Processing Systems. 2013.\r\n[5] Graves, Alex. 'Generating sequences with recurrent neural networks.' arXiv preprint arXiv:1308.0850 (2013)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391902260000, "tcdate": 1391902260000, "number": 4, "id": "gX7L54shLjXYb", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["anonymous reviewer fe58"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On Fast Dropout and its Applicability to Recurrent Networks", "review": "The paper applies the fast dropout of Wang and Manning to RNNs, obtaining excellent results\r\non two standard RNN datasets.  This work also provieds an interesting analysis of fast droupt,\r\npresenting it as an explicit regularization.\r\n\r\nPros:  The results are good.    The paper convincingly shows that fast dropout is effective\r\nfor training RNNs.   I like the equation E[dJ/da * da/dw_i] = dJ/dE[a] * dE[a]/dw_i + dJ/dV[a] * dV[a]/dw_i.\r\nIt helps reason about stochastic systems such as this.  \r\n\r\nCons:  The novelty lies almost entirely in the new analysis of dropout.   I also wish the analysis could tell\r\nus more about dropout. The analysis establishes that the variance may increase or decrease in response to delta_a.\r\nBut what is it?  That's the most interesting part.  We need to know what happens to delta_a in order to be able\r\nto say nontrivial things about dropout.  Right now, the analysis says, it all boils down to delta_a.  But how \r\ncan we relate delta_a to situations that occur in practice?  I.e., can you say, delta_a is positive in situtaion a,\r\nand negative in situations b?   Perhaps the analysis already allows for such conclusions, but I don't\r\nsee them, and so it would be nice to inculde something like that.  \r\n\r\nAlso, it is worth stating that a global regularizer may not exist, and the contribution of dropout to the update\r\nmay not be the derivative of any gradient.\r\n\r\nThere are a few typos:\r\nPage 6, sampling:   hat a = E[a] + s * sqrt{V[a]}, and not E[a] + s * V[a] as written in the text.  I worry\r\nthat this may have caused an error in the remanider of the sampling section, but I am not sure because I didn't\r\nunderstand it fully.\r\n\r\nLikewies, Eq. 1 should be E[a], not V[a]."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391844060000, "tcdate": 1391844060000, "number": 3, "id": "22-aeRonaO2n8", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["anonymous reviewer b50c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On Fast Dropout and its Applicability to Recurrent Networks", "review": "This paper rederives the fast dropout method which is a deterministic alternative to the originally proposed stochastic dropout regularization for neural networks. Then the authors apply fast dropout to recurrent neural networks (RNN) on two different datasets. \r\n\r\nThe main focus of the paper is the derivation of the fast dropout training with almost no link to RNNs until the experiment sections, which makes the paper consists of two disconnected components. The authors could have presented experiments on feedforward NN without affecting the smoothness of the paper at all.\r\n\r\nThe results are expected. Dropout, on small datasets, does better than baseline systems which doesn\u2019t use any stochastic regularization and is as good as ones that does (Graves 2013). In table 2, it is mentioned that assumptions made in RNN-NADE system is also applicable to FD. The authors are encouraged to add these assumptions to get better results.\r\n\r\nThe results sections (3.1.2 and 3.2.2) still need more work. There are no analysis or discussion about the achieved results. Analysis of the effects of network structure, dropout rate, and other hyper-parameters are missing. The tables' order is reversed (table 3 is referenced first then table 2 and table 1).\r\n\r\nFor the results in 3.1.2, please clarify if there is a test data on which you measure NLL or you measure performance on the training data only (there is no training/test division menitoned for the music datasets)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388779560000, "tcdate": 1388779560000, "number": 1, "id": "sDhns359fRsUF", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "OP4ePyQXNu-da", "replyto": "OP4ePyQXNu-da", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have added a new version recently.\r\n\r\nThis version reports new experimental results from FD-RNNs on the music data sets where the dropout rates were chosen by crossvalidation. This leads to improved results."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387398600000, "tcdate": 1387398600000, "number": 7, "id": "OP4ePyQXNu-da", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "OP4ePyQXNu-da", "signatures": ["bayer.justin@googlemail.com"], "readers": ["everyone"], "content": {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "decision": "submitted, no decision", "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients.\r\nThe control of overfitting has seen considerably less attention.\r\nThis paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task, on which we achieve state of the art results.", "pdf": "https://arxiv.org/abs/1311.0701", "paperhash": "bayer|on_fast_dropout_and_its_applicability_to_recurrent_networks", "keywords": [], "conflicts": [], "authors": ["Justin Bayer", "Christian Osendorfer", "Sebastian Urban", "Nutan Chen", "Daniela Korhammer", "Patrick van der Smagt"], "authorids": ["bayer.justin@googlemail.com", "osendorf@gmail.com", "surban@tum.de", "ntchen86@gmail.com", "korhammd@in.tum.de", "smagt@tum.de"]}, "writers": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 13}