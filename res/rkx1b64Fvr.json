{"notes": [{"id": "rkx1b64Fvr", "original": "SyeeNW8IDS", "number": 361, "cdate": 1569438967391, "ddate": null, "tcdate": 1569438967391, "tmdate": 1577168238330, "tddate": null, "forum": "rkx1b64Fvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification", "keywords": ["Natural Language Processing", "Text Classification", "Densent", "Multi-input Model", "Attention Mechanism"], "pdf": "/pdf/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "authors": ["Junhao Qiu", "Ronghua Shi", "Fangfang Li (the corresponding author)", "Jinjing Shi", "Wangmin Liao"], "TL;DR": "We propose a new multi-input  model with a novel attention mechanism, can effectively solve the issues of the shallow text classification model such as  doing not extract long-range associations, global representations, and hierarchical features.", "authorids": ["qiujunhao@csu.edu.cn", "shirh@csu.edu.cn", "lifangfang@csu.edu.cn", "shijinjing@csu.edu.cn", "0909123117@csu.edu.cn"], "paperhash": "qiu|a_new_multiinput_model_with_the_attention_mechanism_for_text_classification", "original_pdf": "/attachment/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "_bibtex": "@misc{\nqiu2020a,\ntitle={A New Multi-input Model with the Attention Mechanism for Text Classification},\nauthor={Junhao Qiu and Ronghua Shi and Fangfang Li (the corresponding author) and Jinjing Shi and Wangmin Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkx1b64Fvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "bPP9sDslEY", "original": null, "number": 1, "cdate": 1576798694218, "ddate": null, "tcdate": 1576798694218, "tmdate": 1576800941293, "tddate": null, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "invitation": "ICLR.cc/2020/Conference/Paper361/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a CNN-based text classification model that uses words, characters, and labels as its input. It also presents an attention block to replace the pooling operation that is typically used in a CNN. The proposed method is evaluated on six benchmark classification datasets, achieving reasonably good results.\n\nWhile the proposed method performs reasonably well compared to baselines in the papers, all reviewers pointed out that there is no discussion or comparison with existing SotA based on pretrained models (e.g., BERT, XLNet), which would strengthen the main claim of the paper. All three reviewers also suggested that the writing of the paper could be improved. The authors did not respond to these reviews, so there was little discussion needed to arrive at a consensus.\n\nI agree with all reviewers and recommend to reject the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification", "keywords": ["Natural Language Processing", "Text Classification", "Densent", "Multi-input Model", "Attention Mechanism"], "pdf": "/pdf/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "authors": ["Junhao Qiu", "Ronghua Shi", "Fangfang Li (the corresponding author)", "Jinjing Shi", "Wangmin Liao"], "TL;DR": "We propose a new multi-input  model with a novel attention mechanism, can effectively solve the issues of the shallow text classification model such as  doing not extract long-range associations, global representations, and hierarchical features.", "authorids": ["qiujunhao@csu.edu.cn", "shirh@csu.edu.cn", "lifangfang@csu.edu.cn", "shijinjing@csu.edu.cn", "0909123117@csu.edu.cn"], "paperhash": "qiu|a_new_multiinput_model_with_the_attention_mechanism_for_text_classification", "original_pdf": "/attachment/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "_bibtex": "@misc{\nqiu2020a,\ntitle={A New Multi-input Model with the Attention Mechanism for Text Classification},\nauthor={Junhao Qiu and Ronghua Shi and Fangfang Li (the corresponding author) and Jinjing Shi and Wangmin Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkx1b64Fvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729756, "tmdate": 1576800282413, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper361/-/Decision"}}}, {"id": "HJl-jdkaKB", "original": null, "number": 1, "cdate": 1571776665401, "ddate": null, "tcdate": 1571776665401, "tmdate": 1572972605138, "tddate": null, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "invitation": "ICLR.cc/2020/Conference/Paper361/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\n\nThis paper introduces a new model architecture for doing text classification. The main contribution is proposing a deeper CNN approach, using both word and character embeddings (as well as label embeddings with attention). The paper claims improved performance over baselines. \n\nDecision\n\nI reject the paper for 3 main reasons:\n1) Very misleading claims regarding establishing a new state of the art. The baselines used for comparison don't include any of the best existing published results. \n2) Lack of positioning within the literature. In particular, no mention nor discussion of Transformers (self-attention) networks, including BERT and XLNet approaches, which are the state of the art in text classification. \n3) Lack of justification/explanation for the proposed architecture. One key argument made is that current models are shallow, but it appears that only CNN models are considered for that comparison. More discussion is needed to understand why the new aspects of the proposed network are importantly different from other existing approaches.\n\nAdditional details for decision\n\nThe results from this paper are significantly inferior to the best results published. With a few quick searches, I found that there are several approaches performing better than the proposed model on every dataset considered in the analysis, as you can see below.\n\nhttp://nlpprogress.com/english/text_classification.html\nhttps://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md\nhttps://paperswithcode.com/sota/text-classification-on-yahoo-answers\n\nExtra notes (not factoring in decision)\n\n- Consider spacing out the 3 rightmost blocks in Figure 1, I found the layout confusing and there's space available.\n- In section 3, I would have liked more explanation for the motivation of the various design choices."}, "signatures": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification", "keywords": ["Natural Language Processing", "Text Classification", "Densent", "Multi-input Model", "Attention Mechanism"], "pdf": "/pdf/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "authors": ["Junhao Qiu", "Ronghua Shi", "Fangfang Li (the corresponding author)", "Jinjing Shi", "Wangmin Liao"], "TL;DR": "We propose a new multi-input  model with a novel attention mechanism, can effectively solve the issues of the shallow text classification model such as  doing not extract long-range associations, global representations, and hierarchical features.", "authorids": ["qiujunhao@csu.edu.cn", "shirh@csu.edu.cn", "lifangfang@csu.edu.cn", "shijinjing@csu.edu.cn", "0909123117@csu.edu.cn"], "paperhash": "qiu|a_new_multiinput_model_with_the_attention_mechanism_for_text_classification", "original_pdf": "/attachment/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "_bibtex": "@misc{\nqiu2020a,\ntitle={A New Multi-input Model with the Attention Mechanism for Text Classification},\nauthor={Junhao Qiu and Ronghua Shi and Fangfang Li (the corresponding author) and Jinjing Shi and Wangmin Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkx1b64Fvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574873235617, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper361/Reviewers"], "noninvitees": [], "tcdate": 1570237753269, "tmdate": 1574873235632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper361/-/Official_Review"}}}, {"id": "BJg7Gw_0KB", "original": null, "number": 2, "cdate": 1571878666722, "ddate": null, "tcdate": 1571878666722, "tmdate": 1572972605094, "tddate": null, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "invitation": "ICLR.cc/2020/Conference/Paper361/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a multi-input model for text classification.  The presentation of this paper is very poor.  The novelty is very limited.  The justification of the proposed architecture is not persuasive.  The experiment design has many flaws.  All the compared baselines are extremely weak.  The authors need to follow the experiment setup in more recent text classification papers.  Authors need at least to compare their methods to BERT.  The current version of this paper is definitely not an ICLR publication.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification", "keywords": ["Natural Language Processing", "Text Classification", "Densent", "Multi-input Model", "Attention Mechanism"], "pdf": "/pdf/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "authors": ["Junhao Qiu", "Ronghua Shi", "Fangfang Li (the corresponding author)", "Jinjing Shi", "Wangmin Liao"], "TL;DR": "We propose a new multi-input  model with a novel attention mechanism, can effectively solve the issues of the shallow text classification model such as  doing not extract long-range associations, global representations, and hierarchical features.", "authorids": ["qiujunhao@csu.edu.cn", "shirh@csu.edu.cn", "lifangfang@csu.edu.cn", "shijinjing@csu.edu.cn", "0909123117@csu.edu.cn"], "paperhash": "qiu|a_new_multiinput_model_with_the_attention_mechanism_for_text_classification", "original_pdf": "/attachment/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "_bibtex": "@misc{\nqiu2020a,\ntitle={A New Multi-input Model with the Attention Mechanism for Text Classification},\nauthor={Junhao Qiu and Ronghua Shi and Fangfang Li (the corresponding author) and Jinjing Shi and Wangmin Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkx1b64Fvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574873235617, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper361/Reviewers"], "noninvitees": [], "tcdate": 1570237753269, "tmdate": 1574873235632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper361/-/Official_Review"}}}, {"id": "SygNcwUaqH", "original": null, "number": 3, "cdate": 1572853643841, "ddate": null, "tcdate": 1572853643841, "tmdate": 1572972605049, "tddate": null, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "invitation": "ICLR.cc/2020/Conference/Paper361/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper described a multi-input model for text classification. This paper is a bit hard to read compared to other submissions. I highly recommend authors to have native speaker to proofread this before submission. The paper mentioned \"labels\" as input to embedding layer as well as attention mechanism. However, it will be more helpful if authors can provide examples of labels. Moreover, the experiment results (e.g., Table 3 and 5) did not include label. I wonder if the methods used as baselines are still the state-of-the-art as language model pre-training might have better results. It will be more convincing if authors include additional baselines such as fine-tuning BERT.\n\nMinor issue: the references in Table 3 (in brackets) are hard to find. \n\nBased on above reason, I would reject this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper361/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification", "keywords": ["Natural Language Processing", "Text Classification", "Densent", "Multi-input Model", "Attention Mechanism"], "pdf": "/pdf/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "authors": ["Junhao Qiu", "Ronghua Shi", "Fangfang Li (the corresponding author)", "Jinjing Shi", "Wangmin Liao"], "TL;DR": "We propose a new multi-input  model with a novel attention mechanism, can effectively solve the issues of the shallow text classification model such as  doing not extract long-range associations, global representations, and hierarchical features.", "authorids": ["qiujunhao@csu.edu.cn", "shirh@csu.edu.cn", "lifangfang@csu.edu.cn", "shijinjing@csu.edu.cn", "0909123117@csu.edu.cn"], "paperhash": "qiu|a_new_multiinput_model_with_the_attention_mechanism_for_text_classification", "original_pdf": "/attachment/546e7056149308ffedc64a4ef2c883fb6b94a2be.pdf", "_bibtex": "@misc{\nqiu2020a,\ntitle={A New Multi-input Model with the Attention Mechanism for Text Classification},\nauthor={Junhao Qiu and Ronghua Shi and Fangfang Li (the corresponding author) and Jinjing Shi and Wangmin Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkx1b64Fvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkx1b64Fvr", "replyto": "rkx1b64Fvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574873235617, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper361/Reviewers"], "noninvitees": [], "tcdate": 1570237753269, "tmdate": 1574873235632, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper361/-/Official_Review"}}}], "count": 5}