{"notes": [{"id": "B1g-X3RqKm", "original": "ryln1AP5Ym", "number": 1329, "cdate": 1538087960527, "ddate": null, "tcdate": 1538087960527, "tmdate": 1545355390254, "tddate": null, "forum": "B1g-X3RqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Proposed Hierarchy of Deep Learning Tasks", "abstract": "As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see analogies to the Chomsky Hierarchy in computational linguistics and time and space complexity in theoretical computer science.\n\nAs a complement to prior theoretical work on the data and computational requirements of learning, this paper presents an empirical approach. We introduce a methodology for measuring validation error scaling with data and model size and test tasks in natural language, vision, and speech domains. We find that power-law validation error scaling exists across a breadth of factors and that model size scales sublinearly with data size, suggesting that simple learning theoretic models offer insights into the scaling behavior of realistic deep learning settings, and providing a new perspective on how to organize the space of  problems. \n\nWe measure the power-law exponent---the \"steepness\" of the learning curve---and propose using this metric to sort problems by degree of difficulty.  There is no data like more data, but some tasks are more effective at taking advantage of more data.  Those that are more effective are easier on the proposed scale. \n\nUsing this approach, we can observe that studied tasks in speech and vision domains scale faster than those in the natural language domain, offering insight into the observation that progress in these areas has proceeded more rapidly than in natural language.", "keywords": ["Deep learning", "scaling with data", "computational complexity", "learning curves", "speech recognition", "image recognition", "machine translation", "language modeling"], "authorids": ["joel@baidu.com", "sharan@baidu.com", "ardalaninewsha@baidu.com", "junheewoo@baidu.com", "hassankianinejad@baidu.com", "patwarymostofa@baidu.com", "yangyang62@baidu.com", "zhouyanqi@baidu.com", "gregdiamos@baidu.com", "kennethchurch@baidu.com"], "authors": ["Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou", "Gregory Diamos", "Kenneth Church"], "TL;DR": "We use 50 GPU years of compute time to study how deep learning scales with more data, and propose a new way to organize the space of problems by difficulty.", "pdf": "/pdf/cf2d05da49f06b82d0f3d7def5639e560da0e8ab.pdf", "paperhash": "hestness|a_proposed_hierarchy_of_deep_learning_tasks", "_bibtex": "@misc{\nhestness2019a,\ntitle={A Proposed Hierarchy of Deep Learning Tasks},\nauthor={Joel Hestness and Sharan Narang and Newsha Ardalani and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou and Gregory Diamos and Kenneth Church},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g-X3RqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJlyV2agg4", "original": null, "number": 1, "cdate": 1544768551433, "ddate": null, "tcdate": 1544768551433, "tmdate": 1545354520643, "tddate": null, "forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1329/Meta_Review", "content": {"metareview": "This paper attempts at ranking of tasks handled by deep learning methods based on learning curves.  A main premise of the paper is \"fitting learning curves to a power law, and then sorting tasks by empirical estimates of exponents\".   The idea of the paper is quite interesting.\n\nHowever, the paper makes some bold claims which are a bit distant from the empirical study it conducts.  It is hard to line up the order in Table 2 with the Chomsky hierarchy.  Also, for various tasks, various different deep models are used (ResNets for image classification, LSTMs for LM, and so on).  I was not convinced that the beta parameter is model-agnostic.\n\nSimilar concerns are expressed by the reviewers, and they agree that the paper should address the criticism that they express in their feedback.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Meta Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1329/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1329/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proposed Hierarchy of Deep Learning Tasks", "abstract": "As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see analogies to the Chomsky Hierarchy in computational linguistics and time and space complexity in theoretical computer science.\n\nAs a complement to prior theoretical work on the data and computational requirements of learning, this paper presents an empirical approach. We introduce a methodology for measuring validation error scaling with data and model size and test tasks in natural language, vision, and speech domains. We find that power-law validation error scaling exists across a breadth of factors and that model size scales sublinearly with data size, suggesting that simple learning theoretic models offer insights into the scaling behavior of realistic deep learning settings, and providing a new perspective on how to organize the space of  problems. \n\nWe measure the power-law exponent---the \"steepness\" of the learning curve---and propose using this metric to sort problems by degree of difficulty.  There is no data like more data, but some tasks are more effective at taking advantage of more data.  Those that are more effective are easier on the proposed scale. \n\nUsing this approach, we can observe that studied tasks in speech and vision domains scale faster than those in the natural language domain, offering insight into the observation that progress in these areas has proceeded more rapidly than in natural language.", "keywords": ["Deep learning", "scaling with data", "computational complexity", "learning curves", "speech recognition", "image recognition", "machine translation", "language modeling"], "authorids": ["joel@baidu.com", "sharan@baidu.com", "ardalaninewsha@baidu.com", "junheewoo@baidu.com", "hassankianinejad@baidu.com", "patwarymostofa@baidu.com", "yangyang62@baidu.com", "zhouyanqi@baidu.com", "gregdiamos@baidu.com", "kennethchurch@baidu.com"], "authors": ["Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou", "Gregory Diamos", "Kenneth Church"], "TL;DR": "We use 50 GPU years of compute time to study how deep learning scales with more data, and propose a new way to organize the space of problems by difficulty.", "pdf": "/pdf/cf2d05da49f06b82d0f3d7def5639e560da0e8ab.pdf", "paperhash": "hestness|a_proposed_hierarchy_of_deep_learning_tasks", "_bibtex": "@misc{\nhestness2019a,\ntitle={A Proposed Hierarchy of Deep Learning Tasks},\nauthor={Joel Hestness and Sharan Narang and Newsha Ardalani and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou and Gregory Diamos and Kenneth Church},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g-X3RqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1329/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352879066, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1329/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1329/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1329/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352879066}}}, {"id": "S1gHjNOq3Q", "original": null, "number": 3, "cdate": 1541207196991, "ddate": null, "tcdate": 1541207196991, "tmdate": 1541533229486, "tddate": null, "forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "content": {"title": "Interesting approach for a relatively unexplored issue", "review": "The paper proposes an empirical solution to coming up with a hierarchy of deep learning tasks or in general machine learning tasks. They propose a two-way analysis where power-law relations are assumed between (a) validation loss and training set size, and (b) the number of parameters of the best model and training set size. The first power-law exponent, \\beta_g, indicates how much can more training data be helpful for a given task and is used for ordering the hardness of problems. The second power-law exponent, \\beta_p, indicates how effectively does the model use extra parameters with increasing training set (can also be thought of as how good the model is at compression). From experiments across a range of domains, the authors find that indeed on tasks where much of the progress has been made tend to be ones with smaller \\beta_g (and \\beta_p). It's arguable as to how comparable these power-law exponents are across domains because of differences in losses and other factors, but it's definitely a good heuristic to start working in this direction.   \n\nClarifications needed:\n(a) Why was full training data never used? The plots/analysis would have looked more complete if the whole training data was used, wondering why certain thresholds for data fraction were chosen.\n(b) What exactly does dividing the training set into independent shards mean? Are training sets of different sizes created by random sampling without replacement from the whole training set?\n(c) How exactly is the \"Trend\" line fitted? From the right sub-figure in Figure 1, it seems that fitting a straight line in only the power-law region makes sense. But that would require determining the start and end points of the power-law region. So some clarification on how exactly is this curve fitting done? For the record, I'm satisfied with the curve fitting done in the plots but just need the details.\n\nMajor issues: \n(a) Very difficult to know in Figure 3 (right) what s(m)'s are associated with which curve, except the one for RHNs maybe.\n(b) Section 4.1: In the discussion around Figure 2, I found some numbers a little off. Firstly, using the left plot, I would say that at even 8 images the model starts doing better than random instead of <25 that's stated in the text.  Secondly, the 99.9% classification error rate for top-5 is wrong, it's 99.5% for top-5 (\"99.9% classification error rate for top-1 and top-5\").\n(c) Section 5: The authors use the phrase \"low dimensional natural language data\" which is quite debatable, to say the least. The number of possible sentences of K length with vocabulary |V| scale exponentially |V|^K where |V| is easily in 10K's most of the time. So to say that this is low dimensional is plain wrong. Just think about what is the (input, output) space of machine translation compared to image classification.\n\nTypos/Suggestions:\n(a) Section 3: \"Depending the task\" -> \"Depending on the task\"\n(b) Section 4.3: \"repeatably\" -> \"repeatability\"\n(c) Figure 4: Specify number of params in millions. The plot also seems oddly big compared to other plots. Also, proper case the axis labels, like other plots. \n(d) Section 4.2 and 4.2.1 can be merged because the character LM experiments are not discussed in the main text or at least not clearly enough for me.  The values of \\beta_g seem to include the results of character LM experiments. So either mention the character LM experiments in more detail or just point to results being in appendix. \n ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1329/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proposed Hierarchy of Deep Learning Tasks", "abstract": "As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see analogies to the Chomsky Hierarchy in computational linguistics and time and space complexity in theoretical computer science.\n\nAs a complement to prior theoretical work on the data and computational requirements of learning, this paper presents an empirical approach. We introduce a methodology for measuring validation error scaling with data and model size and test tasks in natural language, vision, and speech domains. We find that power-law validation error scaling exists across a breadth of factors and that model size scales sublinearly with data size, suggesting that simple learning theoretic models offer insights into the scaling behavior of realistic deep learning settings, and providing a new perspective on how to organize the space of  problems. \n\nWe measure the power-law exponent---the \"steepness\" of the learning curve---and propose using this metric to sort problems by degree of difficulty.  There is no data like more data, but some tasks are more effective at taking advantage of more data.  Those that are more effective are easier on the proposed scale. \n\nUsing this approach, we can observe that studied tasks in speech and vision domains scale faster than those in the natural language domain, offering insight into the observation that progress in these areas has proceeded more rapidly than in natural language.", "keywords": ["Deep learning", "scaling with data", "computational complexity", "learning curves", "speech recognition", "image recognition", "machine translation", "language modeling"], "authorids": ["joel@baidu.com", "sharan@baidu.com", "ardalaninewsha@baidu.com", "junheewoo@baidu.com", "hassankianinejad@baidu.com", "patwarymostofa@baidu.com", "yangyang62@baidu.com", "zhouyanqi@baidu.com", "gregdiamos@baidu.com", "kennethchurch@baidu.com"], "authors": ["Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou", "Gregory Diamos", "Kenneth Church"], "TL;DR": "We use 50 GPU years of compute time to study how deep learning scales with more data, and propose a new way to organize the space of problems by difficulty.", "pdf": "/pdf/cf2d05da49f06b82d0f3d7def5639e560da0e8ab.pdf", "paperhash": "hestness|a_proposed_hierarchy_of_deep_learning_tasks", "_bibtex": "@misc{\nhestness2019a,\ntitle={A Proposed Hierarchy of Deep Learning Tasks},\nauthor={Joel Hestness and Sharan Narang and Newsha Ardalani and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou and Gregory Diamos and Kenneth Church},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g-X3RqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "cdate": 1542234253667, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1329/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923181, "tmdate": 1552335923181, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1329/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJezimi_n7", "original": null, "number": 2, "cdate": 1541088153668, "ddate": null, "tcdate": 1541088153668, "tmdate": 1541533229248, "tddate": null, "forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "content": {"title": "ambitious goal and lack of approach", "review": "The paper provides empirical evidence that the generalization error scales inversely proportional to the log of number of training samples.\n\nThe motivation of the paper is well explained. A large amount of effort is put into experiments. The conclusion is consistent throughout.\n\nIt is a little unclear about the definition of s(m). From the definition at the end of Section 2, it is unclear what it means to fit the training data. It can mean reaching zero on the task loss (e.g., the zero-one loss) or reaching zero on the surrogate loss (e.g., the cross entropy). I assume models larger than a certain size should have no trouble fitting the training set, so I'm not sure if the curve, say in Figure 2., is really plotting the smallest model that can reach zero training error or something else.\n\nVarying the size of the network is also tricky. Most papers, including this one, seem to be confined by the concept of layers. Increasing the number of filters and increasing the number of hidden units are actually two very structured operations. We seldom investigate cases to break the symmetry. For example, what if the number of hidden units is increased in one layer while the number is decreased in another layer? What if the number of hidden units is increased for the forward LSTM but not the backward? Once we break the symmetry, it becomes unclear whether the size of the network is really the right measure.\n\nSuppose we agree on the measure of network size that the paper uses. It is nice to have a consistent theory about the network size and the generalization error. However, it does not provide any reason, or at least rule out any reason, as to why this is the case. For example, say if I have a newly proposed model, the paper does not tell me much about the potential curve I might get.\n\nThe paper spends most of the time discussing the relationship between the network size and the generalization error, but it does not have experiments supporting the hypothesis that harder problems are more difficult to fit or to generalize (in the paper's terminology, large beta_g and large beta_p). For example, a counter argument would be that the community hasn't found a good enough inductive bias for the tasks with large beta_g and beta_p. It is very hard to prove or disprove these statements from the results presented in the paper.\n\nThis paper also sends a dangerous message that image classification and speech recognition are inherently simpler than language modeling and machine translation. A counter argument for this might be that the speech and vision community has spent too much optimizing models on these popular data sets to the point that the models overfit to the data sets. Again these statements can be argued either way. It is hard to a scientific conclusion.\n\nAs a final note, here are the quotes from the first two paragraphs.\n\n\"In undergraduate classes on Algorithms, we are taught how to reduce one problem to another, so we can make claims about time and space complexity that generalize across a wide range of problems.\"\n\n\"It would be much easier to make sense of the deep learning literature if we could find ways to generalize more effectively across problems.\"\n\nAfter reading the paper, I still cannot see the relationships among language modeling, machine translation, speech recognition, and image classification.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1329/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proposed Hierarchy of Deep Learning Tasks", "abstract": "As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see analogies to the Chomsky Hierarchy in computational linguistics and time and space complexity in theoretical computer science.\n\nAs a complement to prior theoretical work on the data and computational requirements of learning, this paper presents an empirical approach. We introduce a methodology for measuring validation error scaling with data and model size and test tasks in natural language, vision, and speech domains. We find that power-law validation error scaling exists across a breadth of factors and that model size scales sublinearly with data size, suggesting that simple learning theoretic models offer insights into the scaling behavior of realistic deep learning settings, and providing a new perspective on how to organize the space of  problems. \n\nWe measure the power-law exponent---the \"steepness\" of the learning curve---and propose using this metric to sort problems by degree of difficulty.  There is no data like more data, but some tasks are more effective at taking advantage of more data.  Those that are more effective are easier on the proposed scale. \n\nUsing this approach, we can observe that studied tasks in speech and vision domains scale faster than those in the natural language domain, offering insight into the observation that progress in these areas has proceeded more rapidly than in natural language.", "keywords": ["Deep learning", "scaling with data", "computational complexity", "learning curves", "speech recognition", "image recognition", "machine translation", "language modeling"], "authorids": ["joel@baidu.com", "sharan@baidu.com", "ardalaninewsha@baidu.com", "junheewoo@baidu.com", "hassankianinejad@baidu.com", "patwarymostofa@baidu.com", "yangyang62@baidu.com", "zhouyanqi@baidu.com", "gregdiamos@baidu.com", "kennethchurch@baidu.com"], "authors": ["Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou", "Gregory Diamos", "Kenneth Church"], "TL;DR": "We use 50 GPU years of compute time to study how deep learning scales with more data, and propose a new way to organize the space of problems by difficulty.", "pdf": "/pdf/cf2d05da49f06b82d0f3d7def5639e560da0e8ab.pdf", "paperhash": "hestness|a_proposed_hierarchy_of_deep_learning_tasks", "_bibtex": "@misc{\nhestness2019a,\ntitle={A Proposed Hierarchy of Deep Learning Tasks},\nauthor={Joel Hestness and Sharan Narang and Newsha Ardalani and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou and Gregory Diamos and Kenneth Church},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g-X3RqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "cdate": 1542234253667, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1329/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923181, "tmdate": 1552335923181, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1329/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgUXa2QhQ", "original": null, "number": 1, "cdate": 1540767006348, "ddate": null, "tcdate": 1540767006348, "tmdate": 1541533229004, "tddate": null, "forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "content": {"title": "Interesting paper, but requires more work", "review": "The authors propose to measure the power-law exponent to sort natural language processing, speech and vision problems by the degree of their difficulty. The main idea is that, while in general model performance goes up for most tasks if more training data becomes available or for bigger model sizes, some tasks are more effective at leveraging more data. Those tasks are supposed to be easier on the proposed scale.\n\nThe main idea of the paper is to consider the bigger picture of deep learning research and to put different results on different tasks into an overall context. I think this is an exciting direction and I strongly encourage the authors to continue their research. However, the paper in its current state seems not quite ready to me. The write-up is repetitive at times, i.e., 'There is not data like more data.' appears 7 times in the paper. Also, some parts are very informal, e.g., the use of 'can't' instead of 'cannot'. Also, the sentence 'It would be nice if our particular proposal is adopted, but it is more important to us that the field agree on a satisfactory solution than that they adopt our particular proposal.', though probably correct, makes the reader wonder if the authors do not trust their proposal, and it would better be replaced by alternative suggestions or deleted. Also, the claim 'This may help explain why there is relatively more excitement about deep nets in speech and vision (vs. language modeling).' seems strange to me - deep nets are the most commonly used model type for language modeling at the moment.\n\nFurthermore, I believe that drawing conclusions about tasks with the proposed approach is an over-simplification. The authors should probably talk about difficulties of datasets, since even for the same task, datasets can be of varying difficulty. Similarly, it would have been nice to see more discussion on what conclusions can be drawn from the obtained results; the authors say that they hope that 'such a hierarchy can serve as a guide to the data and computational requirements of open problems', but, unless I missed this, it is unclear from the paper how this should be done.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1329/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proposed Hierarchy of Deep Learning Tasks", "abstract": "As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see analogies to the Chomsky Hierarchy in computational linguistics and time and space complexity in theoretical computer science.\n\nAs a complement to prior theoretical work on the data and computational requirements of learning, this paper presents an empirical approach. We introduce a methodology for measuring validation error scaling with data and model size and test tasks in natural language, vision, and speech domains. We find that power-law validation error scaling exists across a breadth of factors and that model size scales sublinearly with data size, suggesting that simple learning theoretic models offer insights into the scaling behavior of realistic deep learning settings, and providing a new perspective on how to organize the space of  problems. \n\nWe measure the power-law exponent---the \"steepness\" of the learning curve---and propose using this metric to sort problems by degree of difficulty.  There is no data like more data, but some tasks are more effective at taking advantage of more data.  Those that are more effective are easier on the proposed scale. \n\nUsing this approach, we can observe that studied tasks in speech and vision domains scale faster than those in the natural language domain, offering insight into the observation that progress in these areas has proceeded more rapidly than in natural language.", "keywords": ["Deep learning", "scaling with data", "computational complexity", "learning curves", "speech recognition", "image recognition", "machine translation", "language modeling"], "authorids": ["joel@baidu.com", "sharan@baidu.com", "ardalaninewsha@baidu.com", "junheewoo@baidu.com", "hassankianinejad@baidu.com", "patwarymostofa@baidu.com", "yangyang62@baidu.com", "zhouyanqi@baidu.com", "gregdiamos@baidu.com", "kennethchurch@baidu.com"], "authors": ["Joel Hestness", "Sharan Narang", "Newsha Ardalani", "Heewoo Jun", "Hassan Kianinejad", "Md. Mostofa Ali Patwary", "Yang Yang", "Yanqi Zhou", "Gregory Diamos", "Kenneth Church"], "TL;DR": "We use 50 GPU years of compute time to study how deep learning scales with more data, and propose a new way to organize the space of problems by difficulty.", "pdf": "/pdf/cf2d05da49f06b82d0f3d7def5639e560da0e8ab.pdf", "paperhash": "hestness|a_proposed_hierarchy_of_deep_learning_tasks", "_bibtex": "@misc{\nhestness2019a,\ntitle={A Proposed Hierarchy of Deep Learning Tasks},\nauthor={Joel Hestness and Sharan Narang and Newsha Ardalani and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou and Gregory Diamos and Kenneth Church},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g-X3RqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1329/Official_Review", "cdate": 1542234253667, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1g-X3RqKm", "replyto": "B1g-X3RqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1329/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923181, "tmdate": 1552335923181, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1329/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}