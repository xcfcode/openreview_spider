{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457722821433, "tcdate": 1457722821433, "id": "q7kOZk5zrc8LEkD3t7r7", "invitation": "ICLR.cc/2016/workshop/-/paper/157/unofficial_review", "forum": "q7kqBkL33f8LEkD3t7X9", "replyto": "q7kqBkL33f8LEkD3t7X9", "signatures": ["~Andrew_Rabinovich1"], "readers": ["everyone"], "writers": ["~Andrew_Rabinovich1"], "content": {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "rating": "7: Good paper, accept", "review": "The paper shows good results and has interesting insights. It is a bit raw in presentation. Finally, and most importantly, we've moved away from feature engineering with deep learning, but are not doing model engineering. For example:\n1) formulated principles of construction nets\n   - avoid bottlenecks on early layers, \n   - spatial aggregation (convolutions) can be done over lower dimensional embedding because adjacent units highly correlated => why bottlenecks inside inceptions.\n\n2) factorization\n  - 5x5 factorized with two 3x3, 3x3 everywhere\n  - 3x3 factorized with 1x3 and 3x1 in the middle of network\n  - added inception with conv 7x7 and factorized to series of 1x7 and 7x1. It behaves well only for low-dim grids in the end of network.\n\nIdeally, we would have algorithms that help design models for a specific problem and make these decisions from data. Furthermore, inception was engineered for ImageNet, and is likely starting to overfit. It is not clear if all of the design decisions at this stage are actually generalizable to other problems. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "abstract": "Very deep convolutional networks have been central to the\nlargest advances in image recognition performance in recent years.\nOne example is the Inception architecture that has been shown to achieve\ngood performance at relatively low computational cost.\nRecently, the introduction of residual connections in conjunction with a more\ntraditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar\nto the latest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly,\nhowever, when fully trained, the final quality of the non-residual Inception\nvariants seem to be close to those of residual versions.\nWe present several new streamlined architectures for both residual and\nnon-residual Inception networks. With an ensemble of three residual and\none pure Inception-v4, we achieve 3.08\\% top-5 error on the test set of\nthe ImageNet classification (CLS) challenge", "pdf": "/pdf/q7kqBkL33f8LEkD3t7X9.pdf", "paperhash": "szegedy|inceptionv4_inceptionresnet_and_the_impact_of_residual_connections_on_learning", "conflicts": ["google.com"], "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "authorids": ["szegedy@google.com", "sioffe@google.com", "vanhoucke@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455830029237, "ddate": null, "super": null, "final": null, "tcdate": 1455830029237, "id": "ICLR.cc/2016/workshop/-/paper/157/unofficial_review", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"pdf": null, "writers": {"values-regex": "~.*"}, "forum": "q7kqBkL33f8LEkD3t7X9", "replyto": "q7kqBkL33f8LEkD3t7X9", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["szegedy@google.com", "vanhoucke@google.com", "sioffe@google.com"]}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457708161310, "tcdate": 1457708161310, "id": "OM0Wyn6kAcp57ZJjtN5R", "invitation": "ICLR.cc/2016/workshop/-/paper/157/review/12", "forum": "q7kqBkL33f8LEkD3t7X9", "replyto": "q7kqBkL33f8LEkD3t7X9", "signatures": ["~Tom_Sercu1"], "readers": ["everyone"], "writers": ["~Tom_Sercu1"], "content": {"title": "Significant and inevitable work, a bit sloppily and prematurely presented.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents a combination of the inception architecture\nwith residual networks. This is done by adding a shortcut connection\nto each inception module. This can alternatively be seen as a resnet where\nthe 2 conv layers are replaced by a (slightly modified) inception module.\nThe paper (claims to) provide results against the hypothesis that adding residual\nconnections improves training, rather increasing the model size is what makes the difference.\n\nPros:\nFirst off, a combination of inception & resnets is kind of incontournable.\nThis work is presenting an interesting combination of two strong and impactful models,\nand is in that sense quite significant and (moderately) novel.\nThe \"fair\" comparison in terms of computational budget is insightful and I\ntend to agree with the author's claims. However see the first 2 cons below.\n\nMy remarks (count them as cons or suggestions for improvement):\n+ The resnet-151 is (probably?) still quite a lot deeper than inception-resnet-v2,\n   so it feels too early to conclude that res connetions wouldnt give an edge\n   when making these inception-resnets even deeper.\n+ I find Figure 3 hard to believe / worthy of more dicussion:\n   in inception-v3 vs inception-resenet-v1 the shortcut connections\n   make a massive difference in terms of convergence time.\n   However for the scaled-up version there's almost no gain from residual\n   connections! That just looks so *very* strange.\n+ No explanation how inception-resnet-v2 looks like, how big is it exactly?\n+ A bit more argumentation for the inception-resnet design would be good.\n  To me it seems like several inception blocks for one shortcut would also be\n  an option.\n+ Not very well-structured, \"Model\" contains arguments that might be better\n   in \"Results\" or \"Discussion\" sections.\n+ Paper appears to be hastily written, the citation style is confusing, the\n   section \"4. Results\" seems to be written on a phone?\n    \"Firs we compare\", \"Figure 3 This graph\", \"newline introduced\",\n+ All combined, I'd say that the paper is worthy of publication but it is a bit\n  premature and looks a bit like \"marking territory\". At the other hand,\n  this is a workshop paper so this might be fine.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "abstract": "Very deep convolutional networks have been central to the\nlargest advances in image recognition performance in recent years.\nOne example is the Inception architecture that has been shown to achieve\ngood performance at relatively low computational cost.\nRecently, the introduction of residual connections in conjunction with a more\ntraditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar\nto the latest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly,\nhowever, when fully trained, the final quality of the non-residual Inception\nvariants seem to be close to those of residual versions.\nWe present several new streamlined architectures for both residual and\nnon-residual Inception networks. With an ensemble of three residual and\none pure Inception-v4, we achieve 3.08\\% top-5 error on the test set of\nthe ImageNet classification (CLS) challenge", "pdf": "/pdf/q7kqBkL33f8LEkD3t7X9.pdf", "paperhash": "szegedy|inceptionv4_inceptionresnet_and_the_impact_of_residual_connections_on_learning", "conflicts": ["google.com"], "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "authorids": ["szegedy@google.com", "sioffe@google.com", "vanhoucke@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580026845, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580026845, "id": "ICLR.cc/2016/workshop/-/paper/157/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "q7kqBkL33f8LEkD3t7X9", "replyto": "q7kqBkL33f8LEkD3t7X9", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/157/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455830025091, "tcdate": 1455830025091, "id": "q7kqBkL33f8LEkD3t7X9", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "q7kqBkL33f8LEkD3t7X9", "signatures": ["~Christian_Szegedy1"], "readers": ["everyone"], "writers": ["~Christian_Szegedy1"], "content": {"CMT_id": "", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "abstract": "Very deep convolutional networks have been central to the\nlargest advances in image recognition performance in recent years.\nOne example is the Inception architecture that has been shown to achieve\ngood performance at relatively low computational cost.\nRecently, the introduction of residual connections in conjunction with a more\ntraditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar\nto the latest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly,\nhowever, when fully trained, the final quality of the non-residual Inception\nvariants seem to be close to those of residual versions.\nWe present several new streamlined architectures for both residual and\nnon-residual Inception networks. With an ensemble of three residual and\none pure Inception-v4, we achieve 3.08\\% top-5 error on the test set of\nthe ImageNet classification (CLS) challenge", "pdf": "/pdf/q7kqBkL33f8LEkD3t7X9.pdf", "paperhash": "szegedy|inceptionv4_inceptionresnet_and_the_impact_of_residual_connections_on_learning", "conflicts": ["google.com"], "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "authorids": ["szegedy@google.com", "sioffe@google.com", "vanhoucke@google.com"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}