{"notes": [{"id": "Bkeb7lHtvH", "original": "rJgx2mxKPr", "number": 2199, "cdate": 1569439768693, "ddate": null, "tcdate": 1569439768693, "tmdate": 1583912052232, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Z9Ja6s6vb", "original": null, "number": 1, "cdate": 1576798743013, "ddate": null, "tcdate": 1576798743013, "tmdate": 1576800893188, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays. The authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum.\n\nAll reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721567, "tmdate": 1576800272677, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Decision"}}}, {"id": "BJxe-k_0Kr", "original": null, "number": 2, "cdate": 1571876599811, "ddate": null, "tcdate": 1571876599811, "tmdate": 1574040087954, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The authors introduce a theoretical model for delayed gradients in asynchronous training. It is a very nice model and solving the corresponding differential equation allows to study its stability. Authors derive stability bounds for pure SGD (learning rate needs to decrease with delay) and for SGD with momentum, where they introduce a nice momentum formulation that improves stability. These are nice insights and good results and they are validated by experiments. More experiments and practical analysis would be welcome though. Some example questions: would introducing some sychronization help? Is the lower learning rate hurting training speed when measures as wall-clock time to accuracy?\n\nI am very grateful for the authors' response. It would still be good to see more experiments, but I hope this paper gets accepted.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575811334520, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Reviewers"], "noninvitees": [], "tcdate": 1570237726283, "tmdate": 1575811334542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review"}}}, {"id": "BygmdOItoB", "original": null, "number": 3, "cdate": 1573640298781, "ddate": null, "tcdate": 1573640298781, "tmdate": 1573640298781, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "SkgYLuIwjr", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment", "content": {"title": "Reply to Reviewer #4", "comment": "We thank the reviewer for the positive and helpful feedback. Below we address the questions the reviewer raised.\n    \n(1) \"In Fig. 3, we can clearly see a threshold of $\\eta$. I notice that when $\\tau=16$ the fluctuation is more significant than other three cases. Can you explain why this appears?\"\n    \nIndeed, the standard deviation is somewhat larger when $\\tau=16$. We suspect it is a finite-sample effect, and will run more repetitions to verify (it will take more than a few days to check thoroughly, but these results will be ready for the camera ready version).\n    \n(2) \"In Sec. 3.1, do you consider any kind of learning rate scheduling to change learning rate over epochs, like you did in Sec. 3.2?\"\n    \nThe theoretical analysis in Section 2 focused on a fixed learning rate for simplicity. Therefore, in Section 3.1 which goal was to support our theoretical findings with empirical evidence, we chose to also focus on a fixed learning rate regime. Particularly, we investigated the interaction between the delay, a fixed learning rate, and momentum and how this interaction affects stability and generalization.\n    \nIt is interesting to explore the relation between the scheduling of the learning rate and the generalization from the perspective of dynamical stability. To do this, we need to consider different methods of practical learning rate scheduling regimes and analyze each scheduling method separately.    \n    \n(3) \"It would be great to evaluate on more tasks, as it has been shown that some may be more robust than others (Dai et al., 2019).\"\n    \nOur findings of the relation between the momentum and asynchrony align with (Dai et al., 2019). In (Dai et al., 2019), the authors demonstrate that momentum based algorithms, e.g. Adam and RMSProp, are more sensitive to staleness. It is intriguing to analyze such algorithms in the asynchronous setting from the perspective of dynamical stability. We will aim to examine the behaviour of other tasks until the camera ready version (again, it will take us more than a few days to check it thoroughly). Thank you for the suggestion."}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeb7lHtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2199/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2199/Authors|ICLR.cc/2020/Conference/Paper2199/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144883, "tmdate": 1576860560269, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment"}}}, {"id": "SkgYLuIwjr", "original": null, "number": 3, "cdate": 1573509201487, "ddate": null, "tcdate": 1573509201487, "tmdate": 1573509237322, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper studies how asynchrony affects model training by investigating dynamic stability of minimum points that A-SGD can access. They point out that not all local minimum points are accessible, and asynchrony can affect which minimum points can be accessed, and thus helps to explain why models trained by A-SGD have higher generalization gap. The authors also propose shifted-momentum that utilize momentum for asynchronous training.\n\nOverall, this paper provides nice insights and thorough theoretical analysis. Experiments are carefully designed to validate their results. I think this paper is well written and its novelty is significant.\n\nStrength:\n- Theoretical formulation and analysis in this paper is nice and elegant.\n- Provide theoretical insights of A-SGD with momentum, which is important.\n- Experiments of minima selection are carefully designed. I like the idea to observe trajectories ``leaving minimum''.\n\nSome quick questions:\n- In Fig. 3, we can clearly see a threshold of \\eta. I notice that when \\tau=16 the fluctuation is more significant than other three cases. Can you explain why this appears?\n- In Sec. 3.1, do you consider any kind of learning rate scheduling to change learning rate over epochs, like you did in Sec. 3.2?\n- It would be great to evaluate on more tasks, as it has been shown that some may be more robust than others (Dai et al., 2019).\n\nWei Dai, Yi Zhou, Nanqing Dong, Hao Zhang, and Eric Xing. Toward Understanding the Impact of Staleness in Distributed Machine Learning. In Proc. International Conference on Learning Representations (ICLR), 2019.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575811334520, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Reviewers"], "noninvitees": [], "tcdate": 1570237726283, "tmdate": 1575811334542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review"}}}, {"id": "BygeIZlDoB", "original": null, "number": 2, "cdate": 1573482824046, "ddate": null, "tcdate": 1573482824046, "tmdate": 1573482824046, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "SyeTfjp3YH", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment", "content": {"title": "Reply to Reviewer #3", "comment": "We thank the reviewer for the helpful feedback. We added a conclusions section as suggested. Below we address the questions the reviewer raised.\n        \n(1a) \"I think the authors should attempt to make a stronger case for the practical implications of their analysis: in particular, in the most practical setting (where we don't have a minimum obtained from synchronous training), what does the provided analysis allow us to do?\"\n        \nOur analysis is true for steady state, i.e., at the proximity of a minimum. As mentioned correctly, in practical cases, the training doesn't always end in a minimum. However, we observe that the training modifications we derive from our analysis help improve stability during training, even before reaching a minimum. We added empirical evidence that support this claim in appendix section G. In this section, we used the experiment in Fig. 1 and sampled the validation error in different epochs during training (see Figure 13). As can be seen, for learning rate = 0.01, which is the optimal learning for the steady state (and matches our proposed modification of the learning rate), the validation error stays near optimal throughout training.\n        \n(1b) \"Part of this might involve being more explicit about the results in Table 1: what exactly was the procedure for selecting the learning rates? Is it meaningfully different than just lowering the learning rate?\"\n        \nOur procedure for determining the A-SGD learning rate is to take the learning rate used for large batch training and divide it by the delay. In Table 1, the learning rate we used for the large batch is as suggested in [1]: the baseline (small batch) learning rate, multiplied by the square root of the ratio between the sizes of the small batch and the large batch. \n\n(2) \"Equation (3) is rather obscure without the Appendix, especially since unbolded x hasn't been introduced anywhere. I think the authors should try to convey more of what's going on in this equation in the main text.\"\n        \nWe added more details on this equation in the main text. In particular, below equation (3) we added the definition of unbolded x: \"$x_t$ is the projection of the expectation of the perturbation of $\\mathbf{x}_t - \\mathbf{x^{*}}$ on the eigenvector which corresponds to the maximal eigenvalue $a$\".\n\n(3a) \"Minor: 'looses' should be 'loses' throughout\" \n    \nWe fixed this typo through the paper.\n    \n(3b) \"it might be good to include a conclusion section.\"\n        \nWe added a discussion section, which includes conclusions, and a discussion of implications (such as 1a above).\n        \n[1] - Hoffer, E., Hubara, I.,  Soudry, D. (2017). Train longer, generalize better: closing the generalization gap in large batch training of neural networks."}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeb7lHtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2199/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2199/Authors|ICLR.cc/2020/Conference/Paper2199/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144883, "tmdate": 1576860560269, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment"}}}, {"id": "HJlV0elDir", "original": null, "number": 1, "cdate": 1573482699833, "ddate": null, "tcdate": 1573482699833, "tmdate": 1573482699833, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "BJxe-k_0Kr", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment", "content": {"title": "Reply to Reviewer #1", "comment": "We thank the reviewer for the positive and helpful feedback which allowed to improve the paper: we added a discussion section with two paragraphs elaborating on the Reviewer's questions, and how they lead into interesting directions for future research. We address the reviewer questions below:\n        \n(1) \"Would introducing some sychronization help?\" \n        \nThere are several methods to incorporate some synchronization, e.g. [1,2].  We agree it would be an interesting research direction to investigate how our stability analysis changes for such synchronization methods. Generally, based on our analysis for asynchronous methods with stochastic delay (Appendix C), we expect that,  synchronization methods that reduce the delay in expectation will also help stability - enabling the use of larger learning rates. However, an exact answer would require choosing a specific method and calculating the stability threshold. \n        \n(2) \"Is the lower learning rate hurting training speed when measures as wall-clock time to accuracy?\"\n        \nIn our experiments we did not find a degradation in convergence speed when using the optimal learning rate scaling for the steady state (naturally, the situation might change in other datasets or models). For example, in Figure 1 right, we see that the learning rate at steady state (after 2000 epochs) which achieves optimal generalization is 0.01. The same learning rate achieves optimal convergence, as can be seen in Figure 1 left: its validation error curve is almost always lower then all other learning rates. To see this more clearly, we also added an additional figure to the paper appendix G (Fig. 13) . In this new figure, we use the experiment introduced in Fig. 1 and show the validation error during training sampled at different epochs. As can be seen, with learning rate 0.01 (which matches the proposed modification of the learning rate), the validation error stays near optimal through training, i.e. at the different epochs sampled training we observe that the lower learning rate (chosen according to our suggested modification) achieves smaller validation error compared with higher or lower learning rates. \n        \nNote that, although this is not wall-clock measuring, as there is no degradation in convergence speed in terms of iterations (or epochs), this implies that using the proposed learning rate in A-SGD will be beneficial in terms of run time performance as well - in comparison to other learning rates. If, instead, we compare A-SGD (with the proposed learning rate) vs. S-SGD (synchronous training), then, when measuring performance with wall-clock time, one should consider the system hardware, its heterogeneously, the network bandwidth and etc. There are settings in which training asynchronously will greatly benefit the training time compared to synchronous training. In such settings, using lower learning rate might improve wall-clock time to reach accuracy compared to synchronous training, as suggested by our results in section 3.2. \n        \n[1] - Assran, M., Loizou, N., Ballas, N., Rabbat, M. (2018). Stochastic Gradient Push for Distributed Deep Learning.\n        \n[2] - Chen, J., Pan, X., Monga, R., Bengio, S. (2016). Revisiting Distributed Synchronous SGD."}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeb7lHtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2199/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2199/Authors|ICLR.cc/2020/Conference/Paper2199/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144883, "tmdate": 1576860560269, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Authors", "ICLR.cc/2020/Conference/Paper2199/Reviewers", "ICLR.cc/2020/Conference/Paper2199/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Comment"}}}, {"id": "SyeTfjp3YH", "original": null, "number": 1, "cdate": 1571769108570, "ddate": null, "tcdate": 1571769108570, "tmdate": 1572972370149, "tddate": null, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "invitation": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors model A-SGD as a dynamical system, where parameters are updated with delayed gradients. The authors analyze the stability of this system, and they first derive that the learning rate must scale linearly with the inverse of the delay around a minimum to remain stable. Using a similar analysis they show that the standard way of incorporating momentum into A-SGD requires small learning rates for high momentum values, and they propose \"shifted momentum,\" which allows for stability under higher momentum values. Experimentally, the authors show that around minima the learning rate needed to retain stability scales linearly with the inverse of the delay, that there appears to be an analogous threshold when training models from scratch, that shifted momentum allows for higher momentum values, and finally that on several datasets A-SGD with an appropriate learning rate is able to generalize at least as well as large batch synchronous training.\n\nThis is a nice paper with a large number of interesting theoretical and experimental results, and I believe it should be accepted. I think there are some largely presentational issues that should be addressed, however:\n\n- I think the authors should attempt to make a stronger case for the practical implications of their analysis: in particular, in the most practical setting (where we don't have a minimum obtained from synchronous training), what does the provided analysis allow us to do? Part of this might involve being more explicit about the results in Table 1: what exactly was the procedure for selecting the learning rates? Is it meaningfully different than just lowering the learning rate?\n\n- Equation (3) is rather obscure without the Appendix, especially since unbolded x hasn't been introduced anywhere. I think the authors should try to convey more of what's going on in this equation in the main text.\n\nMinor: 'looses' should be 'loses' throughout, and it might be good to include a conclusion section. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2199/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "authors": ["Niv Giladi", "Mor Shpigel Nacson", "Elad Hoffer", "Daniel Soudry"], "authorids": ["giladiniv@gmail.com", "mor.shpigel@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "keywords": ["implicit bias", "stability", "neural networks", "generalization gap", "asynchronous SGD"], "TL;DR": "How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings.", "pdf": "/pdf/8908d2b62bb681ad54ffe8d0a00e8c5d5a64f67c.pdf", "code": "https://github.com/paper-submissions/delay_stability", "paperhash": "giladi|at_stabilitys_edge_how_to_adjust_hyperparameters_to_preserve_minima_selection_in_asynchronous_training_of_neural_networks", "_bibtex": "@inproceedings{\nGiladi2020At,\ntitle={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},\nauthor={Niv Giladi and Mor Shpigel Nacson and Elad Hoffer and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeb7lHtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8f220cc33ccda4faa32407ea0ae047c4745ad029.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeb7lHtvH", "replyto": "Bkeb7lHtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2199/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575811334520, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2199/Reviewers"], "noninvitees": [], "tcdate": 1570237726283, "tmdate": 1575811334542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2199/-/Official_Review"}}}], "count": 8}