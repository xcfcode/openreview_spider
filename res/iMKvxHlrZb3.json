{"notes": [{"id": "iMKvxHlrZb3", "original": "Qr-vKTns4YO", "number": 955, "cdate": 1601308108487, "ddate": null, "tcdate": 1601308108487, "tmdate": 1614985628215, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gy43aB4wZP3", "original": null, "number": 1, "cdate": 1610040531995, "ddate": null, "tcdate": 1610040531995, "tmdate": 1610474141631, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposed an extension of the SIGN model as an efficient and scalable solution to handle prediction problems on heterogeneous graphs with multiple edge types.  The approach is quite simple: (1) sample subsets of edge types, then construct graphs with these subsets of edge types and (2) compute node features on each such graph as if they have only a single edge type, (3) then aggregate the representations from multiple graphs into one using an attention mechanism, and (4) train MLPs on node representations as in SIGN.  Results show that such a simple method can produce quite good results, and is very efficient and scalable.\n\nThe reviewers of this paper put it on the borderline, with 3 out of 4 leaning toward rejection.  The most common criticism is the lack of novelty.  Indeed this paper is an extension of prior work SIGN, and the proposed approach is simple.  However, I personally think the simplicity and the great empirical results is rather the strength of this paper.\n\nThe authors also did a good job addressing reviewers\u2019 comments and concerns in the discussions, but a few reviewers unfortunately didn\u2019t actively engage in the process.\n\nI'd really encourage the authors to improve and highlight the strength of this paper more and submit to the next venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040531982, "tmdate": 1610474141614, "id": "ICLR.cc/2021/Conference/Paper955/-/Decision"}}}, {"id": "1tWiuOT9FQi", "original": null, "number": 4, "cdate": 1604038443698, "ddate": null, "tcdate": 1604038443698, "tmdate": 1606782681610, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Review", "content": {"title": "This paper aims to propose a new GNN for heterogeneous graphs, which is scalable to large-scale graphs. The proposed idea is to leverage an existing model called SIGN, which simplifies GCN by dropping the non-linear transformation from intermediate layers, and extend it to heterogeneous graphs. The results on several benchmark datasets show the proposed approach is better and faster than baselines.", "review": "This paper aims to propose a new GNN for heterogeneous graphs, which is scalable to large-scale graphs. The proposed idea is to leverage an existing model called SIGN, which simplifies GCN by dropping the non-linear transformation from intermediate layers, and extend it to heterogeneous graphs. The results on several benchmark datasets show the proposed approach is better and faster than baselines.\n\nAlthough the proposed idea seems interesting, there are several concerns about the paper.\n\n1.\tThe description of the methodology is very vague. For example, Fig. 1 is presented without detailed explanation. It is unclear how the features computed by different subgraphs can be aggregated, especially consider nodes only appear in a subset of those subgraphs. The formula in Eq. (2) and (3) do not help due to their simplicity.\n2.\tThe novelty of the paper is also limited, consider it is extending an existing algorithm SIGN to heterogeneous version.\n3.\tI am not fully convinced the simplified GNN works better than some other GNNs designed for heterogeneous networks, such as HAN and HGT, which contains attention scheme and carefully models the message type in the GNN framework. According to other simplified GNN paper, their performance is just comparable to their counterpart but not better, and usually the results are worse than the attention-based one.\n4.\tIt seems the numbers in Table 4 is higher than the ones in the original paper, such as HGT. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538130740, "tmdate": 1606915808983, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper955/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Review"}}}, {"id": "IeoLVfoXnob", "original": null, "number": 8, "cdate": 1606260694648, "ddate": null, "tcdate": 1606260694648, "tmdate": 1606260983383, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "4dilifakSE5", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the detailed response.\n\nI do appreciate the authors' results with a very simple model design. But at the current stage, I'm not that confident whether the improved performance is achieved with the informative TransE input, as the authors admit overfitting might happen for the other baseline models. And also, I'm more willing to see the evaluation on datasets with different types of input features, which would definitely make the paper more convincing and useful for heterogeneous graph mining.\n\n\nThe faster inference speed and the simple model design is a good part of the paper. Due to the authors' clarification, I can raise the score to 5. Looking forward to the complete experimental results (e.g., inference time comparison, consistent evaluation on OGBN-MAG)\n\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "zXybr9FEmON", "original": null, "number": 3, "cdate": 1603930562606, "ddate": null, "tcdate": 1603930562606, "tmdate": 1606260710232, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Review", "content": {"title": "Review #3", "review": "This paper extends from SIGN (https://arxiv.org/abs/2004.11198) model to heterogeneous graphs.\n\nThe SIGN model argues that simply applying MLP on graph-smoothed node features (concatenating k-th hop neighbor features, k $\\in$ [1-L]) can achieve similar results compared with learnable aggregation applied in GNNs. To extend to the heterogeneous graph, this paper proposes to sample relation graphs, by:\n(1) sample several subsets $R_i$ of relations; \n(2) sample relation subgraphs whose edges belong to $R_i$; \n(3) treat each subgraph as homogeneous graphs and perform neighbor aggregation (simply average). \n(4) Apply MLP on each node for node classification.\n\nI have several questions about the proposed approaches:\n\n(1) The difficulty of heterogeneous graphs is that each node might have different types of features. For example, in a social network, nodes can be associated with image, text, or some discrete profiles. Thus, the neighborhood smoothing only works when the input features are both 1) already very informative, and don't need too much transformation; 2) features from different node types are projected to the same space. Therefore, I'm afraid the authors' proposed aggregation might not generalize to more complicated heterogeneous graphs. (It seems that during experiments, the authors utilize TransE to pre-train embeddings for all the nodes, so that they are naturally within the same space, making the problem simpler. One evidence is that when using other feature initialization strategies, such as simple average, the performance of this model drops significantly)\n\n(2) Also, it's confusing to me why can we fuse all the subgraphs with different subgraph schema. Intuitively, with different relation set, the semantic of the relation subgraph should be very different, but the authors seem to treat them equally. It would be better if the authors can provide some analysis on this, for example, for a given node, what is the variance of final node embeddings calculated with subgraphs of different relation sets.\n\n(3) How to get the inference results for large graphs? It seems that the proposed method should get a different predictions for each node with a different relation set. So which set the authors to use? Complete set or average over random sampling? (If it's random, the reported variance, which is close to 0, seems to be very strange).\n\nAlso, though the authors show superior experimental results, I have several concerns about experiment settings:\n\n(1) About feature initialization. From Table 5, we can see that the proposed NARS method highly relies on the TransE\n embedding initialization. When using a standard feature initialization method (such as average neighbors), the results are much lower than HGT and R-GCN. However, the authors didn't provide implementation details about how to train such TransE embedding (normally it's weird to use TransE for heterogeneous graph, as the node number can be much larger than the knowledge graph and we don't have that much relation type. For example, two papers published by the same author and on the same venue might have exactly the same TransE embedding, if we don't consider text input. So it's confusing to me why the results with TransE embedding is better). The authors should better elaborate on this part or release the code for clarity.\n\n(2) About baseline results. Since the utilization of TransE embedding, the experimental settings of the baseline are different from the original papers. But there's still some confusing part. For example, the HGT model's implementation on OGB-MAG uses neighbor average strategy, and the accuracy result is 0.5, while the result reported in table 5 is 0.489. Also, the model parameter is not matched with the reported number. \n\n(3) About inference time. As discussed above, I'm not sure how the proposed method can efficiently get accurate inferences for all the nodes in the test set. If the authors want to claim their method is more scalable, it would be better to include the inference time comparison.\n\n\nOverall, I think the simplified procedure (direct neighbor average) over heterogeneous graph limits the usage of this model, and there's also some unclear part in experimental settings.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538130740, "tmdate": 1606915808983, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper955/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Review"}}}, {"id": "7b3DDZ5lxR", "original": null, "number": 7, "cdate": 1606256702537, "ddate": null, "tcdate": 1606256702537, "tmdate": 1606256702537, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "ES6usP2sRTx", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Re: Re: Response to Reviewer #2 ", "comment": "We appreciate the encouragement and constructive feedback. We are working on comparing alternative metagraph sampling approaches and will add these results to the next version of the paper. Regarding some of the more complex sampling strategies you propose, we don't think the existing benchmarks have complex enough multi-relational structure to provide meaningful results, but we'd be curious to look at them for node classification on knowledge graphs, for example."}, "signatures": ["ICLR.cc/2021/Conference/Paper955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "4dilifakSE5", "original": null, "number": 4, "cdate": 1605758469383, "ddate": null, "tcdate": 1605758469383, "tmdate": 1605822890196, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "zXybr9FEmON", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the detailed feedback. \n\nThe reviewer\u2019s summary description of the method is correct but misses a crucial step which may be responsible for some misunderstandings:\nAfter step (3), we *aggregate* the neighbor-aggregated features using a learned 1D convolution into a single feature set, which is then fed to the MLP for node classification. You can think of this as learned attention weights over the different subgraph features.\n\nPart A: for reviewer\u2019s questions regarding the proposed approaches:\n1. \u201cGeneralization to more complex graphs with multiple feature types:\u201d\nWe agree with the reviewer that it\u2019s not clear how well this technique will generalize to graphs that are more complex than any published benchmark datasets, especially when different entities have different types of features. One promising sign is that NARS is successful on these benchmark academic graph tasks when a different type of feature is used for paper nodes (text features) and non-paper nodes (graph embedding features). Alternatively, multiple types of features can be treated as a single feature type that is the concatenation of all feature types (padding zeros when a node is missing features of a type), but of course that doesn\u2019t scale to large numbers of distinct features types.\nRegarding initialization with TransE, we only do this for nodes that do not come with input features, which is consistent with what prior work has done (although they used metapath2vec rather than TransE embeddings). The reviewer is correct that NARS is more sensitive to missing input features than competing methods, presumably because it can\u2019t learn structural information. But TransE features are cheap to generate relative to HGNNs, and you can generate them once in advance and use them for multiple tasks, so we feel that this is a good tradeoff. We view this comparison of different input embedding types as a contribution of our work, since it represents an effective way to improve (especially) neighbor-averaging approaches without sacrificing scalability.\n2. \u201cHow to aggregate subgraphs?\u201d: The aggregation of features from different relation subgraph schema is done by a learned convolution, and the idea here is to give the model the power to choose which relation subgraphs are more meaningful. This is similar to the semantic-level attention mechanism of Heterogeneous Graph Attention Network where features generated by different semantic meta-paths are aggregated by an attention weight. Even though different features have different semantic meanings, they can be aggregated together as long as they are in the same embedding space.\n3. \u201cHow to do inference?\u201d: At inference time, we compute neighbor-averaged features for all of the subgraphs sampled during training and aggregate them with the learned 1D convolution. Figure 4 illustrates how sensitive the performance is to the particular set of subgraphs sampled; as you increase the number of subgraphs it becomes less sensitive.\n\nPart B: for reviewer\u2019s questions regarding the experiment settings\n1. \u201cFeature initialization\u201d: As we mention in response A(1), we only use graph embeddings for nodes without input features, which is consistent with prior work (e.g. HGT). We use the existing DGL-KE package (https://github.com/awslabs/dgl-ke) to train TransE with no modifications. A comparison of different input embeddings is provided in Table 5; we find that metapath2vec embeddings work okay as well. We have released code so that all of these steps can be replicated.\n2. \u201cBaselines reported performance:\u201d Thanks for pointing this out. The reason for the inconsistency is that we used 5 rather than 4 layers for HGT on the MAG datasets, which leads to overfitting when using neighbor-averaged features. This overfitting mostly goes away once we add embedding features, but we will update Tables 4 and 5 to reflect the improved HGT performance.\n3. \u201cInference\u201d: Thanks for mentioning this omission, we will add inference compute time comparisons to the paper. For the OGB-MAG dataset, NARS takes about 5 seconds to predict labels for all nodes, compared to ~30 minutes for the HGT reference implementation. In general, however, properly implemented GNN inference is typically very cheap compared to training because message passing can be performed a single time to compute labels for all nodes; NARS\u2019 training speedup comes from its ability to do this at training time as well."}, "signatures": ["ICLR.cc/2021/Conference/Paper955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "ES6usP2sRTx", "original": null, "number": 6, "cdate": 1605775790965, "ddate": null, "tcdate": 1605775790965, "tmdate": 1605775790965, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "ka7kO2gUK8y", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Re: Response to Reviewer #2", "comment": "Dear authors,\n\nThank you for responding to my review. I have also read the other reviews. First, I want to encourage you to continue this work irrespective of the outcome here. I disagree with some other reviewers who've simply stated that the paper should be rejected because of \"a lack of novelty.\" What matters is that a paper provides novel understanding of a problem and not necessarily a novel method. To strengthen your paper, however, I would really encourage you to explore more than one way to induce (sample) subgraphs. You proposed one such way. A stronger paper would set out to explore and evaluate several ways to sample subgraphs.  A really interesting (and to me exciting) question would also be to relate your sampling method to the types of (implicit) rules your method can learn. It is known that rule-based methods work really well on multi-relational graphs. Rules usually involve a small number of relation types (at most 2 for Horn rules with 2 elements in the body) and your method might make it easier for the model to implicitly learn such rules from the sampled subgraphs. This might be an interesting \"qualitative\" study: looking through attribution methods (e.g. integrated gradients or such) at the types of relation type combinations the model is leveraging and to what rules this might correspond. \n\nOverall, I will not lower my score but I also don't want to increase it as for this I would want to see a more thorough evaluation of more sampling strategies. "}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "xFG5YV_yBm1", "original": null, "number": 3, "cdate": 1605758163143, "ddate": null, "tcdate": 1605758163143, "tmdate": 1605766965618, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "1tWiuOT9FQi", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for the feedback. \n1. \u201cHow to aggregate features\u201d: Thanks for pointing out the lack of clarity. The features are aggregated by a 1D convolution (Sec 3.1); i.e. each output feature is a linear combination of the features from the different subgraphs with learned weights. Every node is in every subgraph (subgraphs are subsets of edges); if a node has no neighbors then its features in that subgraph will be a vector of 0s.\n2. \u201cNovelty\u201d: NARS is a new SotA method for HG node classification that is also simpler and more scalable than existing approaches. This task is important enough to justify substantial prior research published at similar venues (e.g. HGT, HAN). We don\u2019t think it was obvious a priori how neighbor-averaging approaches like SIGN could be extended to HGs, or that an approach like NARS would work (in fact, several reviewers are surprised that this approach should work). We see value to the ICLR community for this work since it produces SotA performance on HG classification, and is simpler and more scalable than existing approaches.\n3. \u201cPerformance vs. HAN and HGT\u201d: We agree with the reviewer; we were also surprised that NARS achieves superior performance to existing methods, which are much more complex. As a result, we were very careful with our baseline comparisons; in fact, we were able to improve the performance of some of the baselines relative to those reported (as the reviewer points out). We\u2019re not sure what would convince the reviewer besides our reported results, but to that end we have open sourced the code for these experiments and are submitting the results to the OGB leaderboard so they can be verified by the community.\n4. \u201cHigher number than reported in the original paper\u201d: We ran the official implementation of HGT on the same datasets used in the original paper. Since we were surprised by the performance of our model, we spent some time improving the baselines from the reported results. Specifically, for HGT OAG venue and L1-field prediction, at inference time we sample 10 times for target nodes and use the average of predictions to reduce sampling variance of HGT, as we mentioned in footnote 3 on page 6 of our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "Z6ZuH-1lEV5", "original": null, "number": 5, "cdate": 1605758794159, "ddate": null, "tcdate": 1605758794159, "tmdate": 1605758794159, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "WJVB599S8lG", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the response.\n\nRegarding novelty, we do not believe NARS is an obvious extension of prior work, and indeed the fact that the reviewers are surprised (as we were) that this model achieves such strong performance suggests that it\u2019s an interesting and non-obvious result. We agree that NARS is a *simpler* approach than competing methods like HGT and HAN, but we see that as a benefit of this approach, considering that it achieves superior performance to these methods while being simpler, faster and more scalable. \n\nThe reviewer also asks why NARS works when the randomly-sampled relation subgraphs are not semantically meaningful. Our goal with NARS was to allow the model to learn which random \u201cmetapaths\u201d are semantically meaningful (through the learned attentions) rather than providing them as an input as in prior approaches, although in principle NARS certainly allows the subgraphs/metapaths to be provided by the user. We were also a bit surprised that we were able to achieve SotA accuray by randomly selecting subgraphs and learning an attention over them, thus removing the need for hand-tuning metapaths.\n\nFor the small standard deviation the reviewer mentioned, we are not sure which papers the reviewer is referring to. The standard deviation we got has similar scale to our baselines except HGT, which performs sampling during testing. Also, the scale of std of our results is consistent with those on node prediction leaderboard of OGB benchmark: https://ogb.stanford.edu/docs/leader_nodeprop/"}, "signatures": ["ICLR.cc/2021/Conference/Paper955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "ka7kO2gUK8y", "original": null, "number": 2, "cdate": 1605757900048, "ddate": null, "tcdate": 1605757900048, "tmdate": 1605757900048, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "3PImdhRId5K", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the positive and constructive feedback.\n\nWe agree with the reviewer that it would be interesting to explore different approaches to subgraph sampling, although these alternative strategies may sacrifice some of the simplicity of NARS.\nThe proposed experiment with R-GCN on subgraphs is interesting and we will add it to the camera-ready. From our perspective, avoiding the need to do message passing during training is a primary benefit of NARS, which would not be the case for R-GCN; however, it would be informative to see if the benefits of subgraph sampling still apply when relational operators can be learned. Regarding Gaifman graphs, are you proposing first generating the Gaifman graph for the multi-relational graph and then sampling some subgraphs of that (homogeneous) graph? Which ones?\n\nRe related work, thanks for pointing us at this earlier instance of negative sampling, we weren\u2019t aware of it. We\u2019ll mention it in the prior work on sampling methods for large graphs."}, "signatures": ["ICLR.cc/2021/Conference/Paper955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iMKvxHlrZb3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper955/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper955/Authors|ICLR.cc/2021/Conference/Paper955/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865368, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Comment"}}}, {"id": "WJVB599S8lG", "original": null, "number": 2, "cdate": 1603903085403, "ddate": null, "tcdate": 1603903085403, "tmdate": 1605027076237, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Review", "content": {"title": "Interesting Problem, but the novelty of the technical depth is marginal", "review": "This paper studied heterogeneous graph embedding with graph neural networks. The authors proposed an approach which samples multiple different subgraphs containing a subset of relations and then aggregate the node representations from different subgraphs with attentions. Experimental results on some big data sets prove the effectiveness and efficiency of the proposed approach. \n\nOverall, heterogenous graph embedding with graph neural network is a very important and interesting problem with a variety of applications. However, the novelty of the proposed approach seems to be quite marginal, and I am not quite convinced by the proposed approach. Why are we able to get better results by first sampling a subset of relations to get the subgraph and then aggregating the node representations from different subgraphs? It seems weird to be as a randomly selected subset of relations do not convey any specific semantic meanings as traditional metapath based approaches. I am also surprised by the very small standard deviation in Table 4, which is not consistent with results reported in existing literature on the tasks of node classification. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538130740, "tmdate": 1606915808983, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper955/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Review"}}}, {"id": "3PImdhRId5K", "original": null, "number": 1, "cdate": 1603875673995, "ddate": null, "tcdate": 1603875673995, "tmdate": 1605024565842, "tddate": null, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "invitation": "ICLR.cc/2021/Conference/Paper955/-/Official_Review", "content": {"title": "Broadening the scope of SIGN to multi-relational graphs", "review": "The authors propose a method to broaden the scope of SIGN, a technique recently introduced for single-relational graphs. The method allows SIGN to also be applied to multi-relational graphs (often called heterogeneous or knowledge graphs in different communities). In SIGN, various powers of the Laplacian are precomputed. For each power, the features of the nodes of a node\u2019s neighborhood are averaged and (e.g., with an MLP) projected into a node vector representation. This is then used to classify the node. \n\nOne way to apply SIGN to multi-relational graphs would be to conflate all relation types into a single relation type. The authors show that this doesn\u2019t work well. \n\nThe authors then propose the following: take the set of all relation types R and sample a subset R\u2019 from it. Construct the subgraph induced by R\u2019 by only keeping edges of relation types in R\u2019. Then treat all of these relation types as one (turn into a single-relational subgraph) and apply (essentially) SIGN to this subgraph. Do this several times, that is, sample subsets of R several times. Finally aggregate the representations coming from each of the application of SIGN to the sampled subgraphs.\n\nThe authors then also discuss what to do if the nodes do not have attributes (features) and ways to improve the memory efficiency of the approach. \n\nThe authors then proceed to empirically evaluate their method. They compare to existing approaches (such as R-GCNs) and show that NARS (the name of their approach) is competitive with these existing methods. Often it is significantly better. They also perform several types of ablation studies, analyzing the impact of choices such as the number of subgraphs sampled. Generally, I would say that the experiments are well done and look at various different important questions. \n\nIn summary, I don\u2019t have much to criticise. The only shortcoming is that the method is more or less an adaptation of SIGN to the multi-relational setting. I want to be careful to lament a \u201clack of novelty\u201d here, because it is still a good contribution to broaden the scope of an existing method to a larger class of problems. However, one thing I\u2019m missing is a more thorough analysis of the sampling strategies one could use. What is analyzed is the number of subgraphs sampled. But this is based on one way of sampling subgraphs (based on first sampling relation types and inducing the graph based on this). What I\u2019m missing are: sampling subgraphs from the Gaifman graph; and sampling multi-relational graphs and applying R-GCN on each of those and aggregating. The latter is probably less efficient, but I would conjecture also much better than running one R-GCN. Again, see related work below. It has been shown before that sampling multi-relational subgraphs can add robustness of the model to overfitting. Overall, I would tend towards accepting the paper but I would really like to see more analysis on different sampling strategies. \n\nI would encourage the authors to consider as related work a paper from 2017 which is one of the first to propose neighborhood sampling for GNNs (much earlier than most papers you mention in this context) and which is one of the first ones who simply average (learned) features but with an unsupervised objective:\n\nGarcia-Duran and Niepert, Learning Graph Representations with Embedding Propagation, NeurIPS 2017\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper955/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper955/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scalable Graph Neural Networks for Heterogeneous Graphs", "authorids": ["~Lingfan_Yu1", "~Jiajun_Shen1", "~Jinyang_Li1", "~Adam_Lerer1"], "authors": ["Lingfan Yu", "Jiajun Shen", "Jinyang Li", "Adam Lerer"], "keywords": ["Graph Neural Networks", "Large Graphs", "Heterogeneous Graphs"], "abstract": "Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the \u2018metagraph\u2018 of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|scalable_graph_neural_networks_for_heterogeneous_graphs", "pdf": "/pdf/6629950ac7502dd98951eca805fe36efada80845.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zqLVEiSF8H", "_bibtex": "@misc{\nyu2021scalable,\ntitle={Scalable Graph Neural Networks for Heterogeneous Graphs},\nauthor={Lingfan Yu and Jiajun Shen and Jinyang Li and Adam Lerer},\nyear={2021},\nurl={https://openreview.net/forum?id=iMKvxHlrZb3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iMKvxHlrZb3", "replyto": "iMKvxHlrZb3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538130740, "tmdate": 1606915808983, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper955/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper955/-/Official_Review"}}}], "count": 13}