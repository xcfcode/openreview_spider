{"notes": [{"id": "HyeJmlrFvH", "original": "rJgYtmgKwr", "number": 2194, "cdate": 1569439766576, "ddate": null, "tcdate": 1569439766576, "tmdate": 1577168274942, "tddate": null, "forum": "HyeJmlrFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aNi8w9akuD", "original": null, "number": 1, "cdate": 1576798742898, "ddate": null, "tcdate": 1576798742898, "tmdate": 1576800893315, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a communication-efficient data-parallel SGD with quantization. The method bridges the gap between theory and practice. The QSGD method has theoretical guarantees while QSGDinf doesn't, but the latter gives better result. This paper proves stronger results for QSGD using a different quantization scheme which matches the performance of QSGDinf.\n\nThe reviewers find issues with the approach and have pointed some of them out. During the discussion period, we did discuss if reviewers would like to raise their scores. Unfortunately, they still have unresolved issues (see R1's comment). \nR1 made another comment recently that they were unable to add to their review:\n\"The proposed algorithm and the theoretical analysis does not include momentum. However, in the experiments, it is clearly stated that momentum (with a factor of 0.9) is used. Thus, it is unclear whether the experiments really validate the theoretical guarantees. And, it is also unclear how momentum is added for both NUQSGD and EF-SGD, since momentum is not mentioned in Algorithm 1 in this paper, or the paper of QSGD, or the paper of EF-SignSGD. (There is a version of SignSGD with momentum *without* error feedback, called SIGNUM).\"\n\nWith the current score, the paper does not make the cut for ICLR, but I encourage the authors to revise the paper based on reviewers' feedback. For now, I recommend to reject this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707492, "tmdate": 1576800255712, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Decision"}}}, {"id": "BkxWwBzniS", "original": null, "number": 12, "cdate": 1573819737265, "ddate": null, "tcdate": 1573819737265, "tmdate": 1573829186334, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "We will be posting a new version of the paper momentarily. This note summarizes the changes:\n\n1. We now report results comparing NUQSGD with error-corrected methods, notably EF-SIGNSGD, on ImageNet. We find that our techniques are superior. In particular, we had to perform significant hyperparameter tuning to even get the error corrected methods (EF-SIGNSGD) to converge. Once we got them to converge, the communication benefits had largely disappeared. We emphasize that our methods achieve full accuracy and speedup under the baseline hyperparameter settings, and do not require additional tuning. This is essential on data sets like ImageNet where tuning is extremely expensive. We also include learning curves when \u2018time\u2019 is the x-axis.\n\n\n2. In the appendix, we prove that, for any given set of levels, there exists a distribution of points with dimension d such that the variance is in Omega(sqrt{d}), and so our bound is tight in d. \n\n3. Regarding our upper bound and its dependence on s: In the appendix, we now derived the optimal worst-case variance upper bounds expressed as an integer QP. We present several relaxations of this bound and plot its dependence on s and d in the appendix. \n\n4.  In the appendix, we now state the implications of our work for convergence on nonconvex problems. As stated in the paper, these results are standard. The important work in this setting is control of the variance and communication cost.\n\n5. We've made various other minor improvements to notation, explanations, etc.\n\nWe would welcome suggestions as to what material we might promote to the main body (rather than the appendix). We have left most changes in the appendix to ease the reviewers job in finding these new contributions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeJmlrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2194/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2194/Authors|ICLR.cc/2020/Conference/Paper2194/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144957, "tmdate": 1576860537578, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment"}}}, {"id": "rygJKdFDjr", "original": null, "number": 6, "cdate": 1573521526991, "ddate": null, "tcdate": 1573521526991, "tmdate": 1573521526991, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "SJxLkbwatH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment", "content": {"title": "Response to Review 1", "comment": "Thanks for your feedback. Below is our specific feedback to your review. We have also posted a general response (see our top-level comment) to all reviewers addressing high level points.\n\n>In this paper, a very important reference and baseline is missing, which is call error-feedback SGD [1]. Although the title of [1] focuses on SignSGD, it provides a general algorithm for arbitrary compressor with an error/variance bound similar to Theorem 2 in this paper, no matter the compressor is unbiased or not. Since [1] provides the SOTA results for quantized SGD, the proposed algorithm should be compared to it in the experiments.\n\nWe agree that it is interesting to compare NUQSGD with error-corrected methods (although we feel this comparison is orthogonal to the problem of closing the performance--theory gap between QSGD and QSGDinf.) We are running experiments comparing NUQSGD with error-corrected methods and hope they are finished before the rebuttal period. Regardless, we will include them in the final paper. One important note is that error-corrected signSGD, sparsified methods, and TernGrad require non-trivial additional parameter tuning to reduce accuracy loss (learning rate. momentum, and warmup tuning---see e.g. \"Deep Gradient Compression\"). By contrast, our experiments target the setting where training is performed with standard hyperparameters as the full-precision version, and we are able to recover full accuracy in this regime. This is the standard set by QSGD, which is closer to practical applications.\n\n>This paper claims to have strong theoretical guarantees. However, the theoretical analysis only works for convex functions. Note that the theoretical analysis in [1] also works for non-convex functions. \n\nUsing standard arguments, NUQSGD does provide guarantees in the non-convex case as well, since the quantized stochastic gradients are still unbiased. (This is the same for QSGD.) We mention this in the paper, after Theorem 4: \"On nonconvex problems, convergence guarantees can be established along the lines of, e.g., (Ghadimi and Lan, 2013, Theorem 2.1).\" In particular, this results gives convergence to a second-order stationary point. These are virtually the same guarantees as error-corrected signSGD.  We will include the nonconvex convergence statement in the updated paper.\n\n>Regardless of the convergence guarantees (which is weak considering the existing theorems in [1]). the proposed algorithm, NUQSGD, does not show improvement on the convergence, compared to the baseline QSGDinf. \n\nThe goal of the paper was to close the gap between QSGD and QSGDinf. QSGD provides theoretical guarantees but is empirically worse than QSGDinf. QSGDinf has no theoretical guarantees. NUQSGD matches the empirical performance of QSGDinf and has slightly stronger asymptotic guarantees than QSGD. We think this progress is worth reporting.\n\nSince submission, we have also improved our understanding of the variance bounds for NUQSGD. \n\nWe have proven that, for any given set of levels, there exists a distribution of points with dimension d such that the variance is in Omega(sqrt{d}), and so our bound is tight in d. We will include this proof in the updated version (forthcoming).  \n\nRegarding our upper bound and its dependence on s: We have now derived the optimal worst-case variance upper bound for a fixed set of arbitrary levels, expressed as the solution to an integer program with quadratic constraints. We can relax the program to obtain a quadratic program. A coarser analysis yields an upper bound expressed as the solution to a linear program, which is more amenable to analysis.\n\nWe are now using these numerical tools to build insight, and will include some plots in the updated draft. For an exponentially spaced collection of levels of the form ((0,p^s, ... , p^2 ,p,1) for p in (0,1) and an integer number of levels, s, we have a numerical method for finding the value p that minimizes the worst-case variance, for any given s and d. We know that our current scheme is near optimal (in worst case) according to the LP bound in some cases. Using these techniques we can get slightly tighter bounds numerically.\n\n>In Figure 3, the experiments only show loss vs. # of iterations, which does not show the actual training time.\n\nRegarding simulation-based learning curves with respect to time, if different compression schemes are run on the same gpu, there will be no difference between any quantization method. This does not hold for error-corrected methods though, since they require additional storage for the error. We will add convergence-versus-time bounds to the updated version.\n\n>In Definition 1, in some cases s is a constant integer, and in some other case become a function, which is very confusing. I also hope the authors can highlight the definition of and , which are essential for understanding the nonuniform quantization mechanism. \n\nIn the revision, we clarify these definitions. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeJmlrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2194/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2194/Authors|ICLR.cc/2020/Conference/Paper2194/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144957, "tmdate": 1576860537578, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment"}}}, {"id": "HJeK8dYwir", "original": null, "number": 5, "cdate": 1573521489080, "ddate": null, "tcdate": 1573521489080, "tmdate": 1573521489080, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "r1xsybtTFB", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment", "content": {"title": "Response to Review 3", "comment": "Thanks for your feedback. Below is our specific feedback to your review. We have also posted a general response (see our top-level comment) to all reviewers addressing high level points.\n\n>It would be great to include more theoretical analysis which demonstrates the importance of variance upper bound for convergence speed guarantee.\n\nWe have improved our understanding of the variance bounds for NUQSGD. \n\nWe have proven that, for any given set of levels, there exists a distribution of points with dimension d such that the variance is in Omega(sqrt{d}), and so our bound is tight in d. We will include this proof in the updated version (forthcoming).  \n\nRegarding our upper bound and its dependence on s: We have now derived the optimal worst-case variance upper bound for a fixed set of arbitrary levels, expressed as the solution to an integer program with quadratic constraints. We can relax the program to obtain a quadratic program. A coarser analysis yields an upper bound expressed as the solution to a linear program, which is more amenable to analysis.\n\nWe are now using these numerical tools to build insight, and will include some plots in the updated draft. For an exponentially spaced collection of levels of the form ((0,p^s, ... , p^2 ,p,1) for p in (0,1) and an integer number of levels, s, we have a numerical method for finding the value p that minimizes the worst-case variance, for any given s and d. We know that our current scheme is near optimal (in worst case) according to the LP bound in some cases. Using these techniques we can get slightly tighter bounds numerically.\n\n>In the experimental part, they control the hyperparameters including batch-size, base learning rate, momentum, and weight decay to be identical with each method. This may cause tuning biases (the setting may favor one method but hurt others' performance).\n\nWe agree that the performance of each method might slightly improve if we tune hyperparameters for that specific method. However, we are interested in a setting where training is performed with the same standard hyperparameters as those for the full-precision version. We would like to recover full accuracy in this regime. This is the standard set by the original work on QSGD, which is closer to practical applications where hyperparameter tuning is expensive. Again, the goal was to close the empirical performance gap with QSGDinf (we did) and the theoretical gap with QSGD (we did).\n\n>Although the paper mainly focuses on comparing with QSGD, there are several relative communication efficient training algorithms which I think are worth to compare empirically (at least one of them)\n\nAmong unbiased schemes, QSGDinf is state-of-the-art but it does not come with theoretical guarantees. QSGD has guarantees but worse performance. Our goal was to close this gap, and we achieved this goal. We think this progress is worth reporting. \n\nWe agree that it is interesting to compare NUQSGD with signed-based methods (although we feel this comparison is orthogonal to the problem of closing the performance--theory gap between QSGD and QSGDinf). Recently, error-feedback SGD has been shown to outperform signSGD. We are running experiments comparing NUQSGD with error-corrected methods and hope they are finished before the rebuttal period. Regardless, we will include them in the final paper. One important note is that error-corrected signSGD, sparsified methods, and TernGrad require non-trivial additional parameter tuning to reduce accuracy loss (learning rate. momentum, and warmup tuning---see e.g. \"Deep Gradient Compression\"). By contrast, our experiments target the setting where training is performed with standard hyperparameters as the full-precision version, and we are able to recover full accuracy in this regime.\n\n>In figure 4, the encoding cost is significantly increased from 4-bit to 8-bit NUQSGD. Any reason why it happens? Is it due to inefficient encoding implementation?\n\nIt is because the cost of the compression is proportional to the number of quantization points used, i.e., # quantization points for 8bit = #quantization points for 4bit^2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeJmlrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2194/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2194/Authors|ICLR.cc/2020/Conference/Paper2194/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144957, "tmdate": 1576860537578, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment"}}}, {"id": "rkgOQdKDjS", "original": null, "number": 4, "cdate": 1573521439666, "ddate": null, "tcdate": 1573521439666, "tmdate": 1573521439666, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "Bkxbah715H", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment", "content": {"title": "Response to Review 2", "comment": "Thanks for your feedback. Below is our specific feedback to your review. We have also posted a general response (see our top-level comment) to all reviewers addressing high level points.\n\n>NUQSGD does not provide significant improvements in terms of the variance and communication cost.\n\nThe goal of the paper was to close the gap between QSGD and QSGDinf. QSGD provides theoretical guarantees but is empirically worse than QSGDinf. QSGDinf has no theoretical guarantees. NUQSGD matches the empirical performance of QSGDinf and has slightly stronger asymptotic guarantees than QSGD, and so we don't see the fact that the improvement is \"minor\" as undermining the significance. In practice, it's much better than QSGD.\n\n>We would expect NUQSGD to improve the dependence on the dimension d, which is more significant\n\nWe have proven that, for any given set of levels, there exists a distribution of points with dimension d such that the variance is in Omega(sqrt{d}), and so our bound is tight in d. We will include this proof in the updated version (forthcoming). \n\n>It would be great to add learning curves with the \u2018time\u2019 being the x-axis as well. Also, I would suggest the authors to record the time needed to proceed one iteration for each parallel algorithm to compare the communication cost.\n\nRegarding simulation-based learning curves with respect to time, if different compression schemes are run on the same gpu, there will be no difference between any quantization method. This does not hold for error-corrected methods though, since they require additional storage for the error. We will add convergence-versus-time bounds to the updated version. In addition, we will record the time needed to proceed one iteration for each parallel algorithm. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeJmlrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2194/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2194/Authors|ICLR.cc/2020/Conference/Paper2194/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144957, "tmdate": 1576860537578, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment"}}}, {"id": "SklTsSGviH", "original": null, "number": 3, "cdate": 1573492133122, "ddate": null, "tcdate": 1573492133122, "tmdate": 1573504070887, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment", "content": {"title": "General response to all reviewers", "comment": "Dear reviewers, \n\nThank you for your reviews. In summary, we received the following feedback (key issues):\n\n1. [Variance upper bound; R2,3]. The theoretical improvement over QSGD seems minor. Can stronger theoretical guarantees be obtained? In particular, can you tighten the variance bound in terms of d? \n\n2. [Nonconvexity; R1]. Can convergence results be obtained for nonconvex problems? \n\n3. [Sign-based methods; R1,3]. Is NUQSGD interesting if its performance is comparable to QSGDinf? How does NUQSGD compare with sign-based methods? \n\n4. [Loss vs time; R1,2] How do learning curves look if \u2018time\u2019 is the x-axis? \n\nWe agree these are important questions. We have a plan to address each of them. We describe that plan below. We hope that if we indeed succeed in executing this plan, you will raise your scores to 8!\n\nWe plan to make the following four changes to address the key issues/questions above. If you would require further changes to update your score to 8, please let us know!\n\n\n*****\n**1**\n*****\nThe goal of the paper was to close the gap between QSGD and QSGDinf. QSGD provides theoretical guarantees but is empirically worse than QSGDinf. QSGDinf has no theoretical guarantees. NUQSGD matches the empirical performance of QSGDinf and has slightly stronger asymptotic guarantees than QSGD, and so we don't see the fact that the improvement is \"minor\" as undermining the significance. In practice, it's much better than QSGD.\n\nThat said, we have improved our understanding of the variance bounds for NUQSGD. \n\nRegarding tightness of our variance bounds: Reviewer 1 asks whether we can beat the O(sqrt{d}) dimension dependence in the variance bound. We have proven that, for any given set of levels, there exists a distribution of points with dimension d such that the variance is in Omega(sqrt{d}), and so our bound is tight in d. We will include this proof in the updated version (forthcoming). \n\nRegarding our upper bound and its dependence on s: We have now derived the optimal worst-case variance upper bound for a fixed set of arbitrary levels, expressed as the solution to an integer program with quadratic constraints. We can relax the program to obtain a quadratic program. A coarser analysis yields an upper bound expressed as the solution to a linear program, which is more amenable to analysis.\n\nWe are now using these numerical tools to build insight, and will include some plots in the updated draft. For an exponentially spaced collection of levels of the form ((0,p^s, ... , p^2 ,p,1) for p in (0,1) and an integer number of levels, s, we have a numerical method for finding the value p that minimizes the worst-case variance, for any given s and d. We know that our current scheme is near optimal (in worst case) according to the LP bound in some cases. Using these techniques we can get slightly tighter bounds numerically.\n\n\n*****\n**2**\n*****\nNUQSGD does provide guarantees in the non-convex case as well, since the quantized stochastic gradients are still unbiased. (This is the same for QSGD.) We mention this in the paper, after Theorem 4: \"On nonconvex problems, convergence guarantees can be established along the lines of, e.g., (Ghadimi and Lan, 2013, Theorem 2.1).\" In particular, this results gives convergence to a second-order stationary point. These are virtually the same guarantees as error-corrected signSGD.  We will include the nonconvex convergence statement in the updated paper.\n\n\n*****\n**3**\n*****\nAmong unbiased schemes, QSGDinf is state-of-the-art but it does not come with theoretical guarantees. QSGD has guarantees but worse performance. Our goal was to close this gap, and we achieved this goal. We think this progress is worth reporting. \n\nWe agree that it is interesting to compare NUQSGD with error-corrected methods (although we feel this comparison is orthogonal to the problem of closing the performance--theory gap between QSGD and QSGDinf.) We are running experiments comparing NUQSGD with error-corrected methods and hope they are finished before the rebuttal period. Regardless, we will include them in the final paper. One important note is that error-corrected signSGD, sparsified methods, and TernGrad require non-trivial additional parameter tuning to reduce accuracy loss (learning rate. momentum, and warmup tuning---see e.g. \"Deep Gradient Compression\"). By contrast, our experiments target the setting where training is performed with standard hyperparameters as the full-precision version, and we are able to recover full accuracy in this regime. This is the standard set by QSGD, which is closer to practical applications.\n\n\n*****\n**4**\n*****\nRegarding simulation-based learning curves with respect to time, if different compression schemes are run on the same gpu, there will be no difference between any quantization method. This does not hold for error-corrected methods though, since they require additional storage for the error. We will add convergence-versus-time bounds to the updated version. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeJmlrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2194/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2194/Authors|ICLR.cc/2020/Conference/Paper2194/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144957, "tmdate": 1576860537578, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Authors", "ICLR.cc/2020/Conference/Paper2194/Reviewers", "ICLR.cc/2020/Conference/Paper2194/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Comment"}}}, {"id": "SJxLkbwatH", "original": null, "number": 1, "cdate": 1571807454156, "ddate": null, "tcdate": 1571807454156, "tmdate": 1572972370833, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a new gradient compression method, which is called nonuniform quantization. The algorithm is a reasonable variant of SGD with uniform quantization. The paper is well written. The experiments show good performance.\n\nHowever, there are several weakness in this paper:\n\n1. In this paper, a very important reference and baseline is missing, which is call error-feedback SGD [1]. Although the title of [1] focuses on SignSGD, it provides a general algorithm for arbitrary compressor with a error/variance bound similar to Theorem 2 in this paper, no matter the compressor is unbiased or not. Since [1] provides the SOTA results for quantized SGD, the proposed algorithm should be compared to it in the experiments.\n\n2. This paper claims to have strong theoretical guarantees. However, the theoretical analysis only works for convex functions. Note that the theoretical analysis in [1] also works for non-convex functions.\n\n3. Regardless of the convergence guarantees (which is weak considering the existing theorems in [1]). the proposed algorithm, NUQSGD, does not show improvement on the convergence, compared to the baseline QSGDinf.\n\n4. In Figure 3, the experiments only show loss vs. # of iterations, which does not show the actual training time. In Figure 4, training time is only shown for NUQSGD, which ignores the other baselines including QSGD and QSGDinf. What I really what to see is training loss (or testing accuracy) vs. training time (or communication overhead, such as number of bits), so that we can evaluate the trade-off between communication overhead and the convergence, compared to the baselines.\n\n\n\nMinor issue (I hope the authors can consider the following suggestions in a revised version. However, since the issue is minor, it doesn't affect the score):\n\n!. In Definition 1, in some cases $s$ is c constant integer, and in some other case $s$ become a function, which is very confusing and not friendly to the readers. I also hope the authors can highlight the definition of $r$ and $p$, which are essential for understanding the nonuniform quantization mechanism. \n\n\n\n\n--------------\nReference\n\n[1] Karimireddy, Sai Praneeth et al. \u201cError Feedback Fixes SignSGD and other Gradient Compression Schemes.\u201d ICML (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575513092374, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Reviewers"], "noninvitees": [], "tcdate": 1570237726355, "tmdate": 1575513092388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review"}}}, {"id": "r1xsybtTFB", "original": null, "number": 2, "cdate": 1571815651509, "ddate": null, "tcdate": 1571815651509, "tmdate": 1572972370788, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new scheme for quantizing gradients which are followed by the previous work QSGD [1]. They show that it yields stronger theoretical guarantees than QSGD while showing a great empirical performance. \nThe main difference between their scheme NUQSGD and QSGD is that they use nonuniform quantization (0, 1/2^{s},  \u2026., 2^{s-1}/2^{s}, 1) instead of uniform quantization (0, 1/s, \u2026, (s-1)/s,1).  Intuitively, by the way, it could reduce quantization error and variance by better matching the properties of normalized vectors.\nThe results are in 2 parts. First comparing with QSGD, they establish stronger convergence guarantees for NUQSGD, under standard assumptions. They also establish theoretical results for the variance upper bound and expected communication cost of their scheme. Second, they show strong empirical performance on deep models and a large dataset, with an efficient implementation in PyTorch.\n\nHowever, there are several issues and questions that if fixed or illustrated could be a great paper.\n\n\t1) The author claim NUQSGD achieves stronger convergence guarantees comparing with QSGD but hasn't illustrated the point in detail. On page 6, the paragraph named 'NUQSGD vs QSGD' mainly claims that variance upper bound controls the guarantee on the convergence speed by empirically showing the results of variance upper bound. It would be great to include more theoretical analysis which demonstrates the importance of variance upper bound for convergence speed guarantee.\n\t2) In the experimental part, they control the hyperparameters including batch-size, base learning rate, momentum, and weight decay to be identical with each method. This may cause tuning biases (the setting may favor one method but hurt others' performance).\n\t3) Although the paper mainly focuses on comparing with QSGD, there are several relative communication efficient training algorithms which I think are worth to compare empirically (at least one of them). For example:\n\t\ta. Deep Gradient compression [2]\n\t\tb. signSGD [3]\n\t\tc. TernGrad [4]\n\t4) In figure 4, the encoding cost is significantly increased from 4-bit to 8-bit NUQSGD. Any reason why it happens? Is it due to inefficient encoding implementation? \n\nI agree with the authors' point that it's worth to explore the interaction between NUQSGD with more complex reduction patterns like ring-based. Since the ring-based algorithm like all-reduce is more popular in practice nowadays, interacting with it would have a better practical meaning. \n\n[1] D. Alistarh, D. Grubic, J. Z. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-ef\ufb01cient SGD via gradient quantization and encoding. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017.\n\n[2] Lin Y, Han S, Mao H, Wang Y, Dally WJ. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887. 2017 Dec 5.\n\n[3] Bernstein J, Zhao J, Azizzadenesheli K, Anandkumar A. signSGD with majority vote is communication efficient and fault-tolerant. arXiv. 2018 Oct 11.\n\n[4] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. TernGrad: Ternary gradients to reduce communication in distributed deep learning. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575513092374, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Reviewers"], "noninvitees": [], "tcdate": 1570237726355, "tmdate": 1575513092388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review"}}}, {"id": "Bkxbah715H", "original": null, "number": 3, "cdate": 1571925177391, "ddate": null, "tcdate": 1571925177391, "tmdate": 1572972370731, "tddate": null, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Brief summary of the paper:\nThis paper studies data-parallel SGD that K processors work together to minimize an objective function. Each processor computes a stochastic gradient and broadcasts to other peers. In this distributed system, there is a trade-off between the *communication cost* from sharing the stochastic gradient and the *variance* from gradient quantization. This paper is a follow-up of Alistarh et al.\u00a0(2017). It proposes a non-uniform (logarithmic) quantization scheme (NUQSGD). This paper provides theoretical analysis of the variance and communication cost of NUQSGD. Then the paper analyzes the convergence rate of NUQSGD for convex and smooth objective function. At the end, this paper empirically evaluates NUQSGD for image classification problem. \n\n\nOriginality and significance:\nThis paper follows up on the parallel SGD framework proposed by Alistarh et al.\u00a0(2017), where the authors proposed QSGD using a uniform quantization. This paper proposes NUQSGD using a non-uniform quantization method. The quantization of the stochastic gradient amplifies the stochastic variance, which influences the rate of convergence of SGD. Thus, on one hand, it is important to design a quantization method to improve the variance, for the sake of convergence rate. On the other hand, it is also important to decrease the communication cost. NUQSGD does not provide significant improvements in terms of the variance and communication cost. \n\nTheorem 2 and Theorem 3: QSGD has a variance of min {d/s^2, \\sqrt{d}/s} and NUQSGD has a variance of min{O(d/2^{-2s}), O(\\sqrt{d/2^{-2s}})}. QSGD has communication cost of \\tilde O(s(s+\\sqrt{d})) and NUQSGD has communication cost of \\tilde O(2^{2s}\\sqrt{d} ). Compared to QSGD, we can see that NUQSGD improves the dependence on s for the variance term, but it has a worse (exponential) dependence on s for the communication cost. Usually s is a small number and it serves as a hyper-parameter to be tuned. We would expect NUQSGD to improve the dependence on the dimension d, which is more significant. However, NUQSGD has the same dependence on d as QSGD in terms of both variance and communication cost. \n\nExperiments: Figure 3 compares NUQSGD with other parallel SGD algorithms and vanilla SGD. Figure 3 shows how fast the training loss decreases with respect to iterations. It would be great to add learning curves with the \u2018time\u2019 being the x-axis as well. Also, I would suggest the authors to record the time needed to proceed one iteration for each parallel algorithm to compare the communication cost. \n\nQuality and clarity:\nThis paper is well-written. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2194/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["alir@vectorinstitute.ai", "faghri@cs.toronto.edu", "droy@utstat.toronto.edu", "dan.alistarh@ist.ac.at", "markovilya197@gmail.com", "vitalii.aksenov@ist.ac.at"], "title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "authors": ["Ali Ramezani-Kebrya", "Fartash Faghri", "Ilya Markov", "Vitalii Aksenov", "Dan Alistarh", "Daniel M. Roy"], "pdf": "/pdf/e6fe72b209054a2cc13eba7a3649d629ccfca64b.pdf", "TL;DR": "NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.", "keywords": [], "paperhash": "ramezanikebrya|provably_communicationefficient_dataparallel_sgd_via_nonuniform_quantization", "original_pdf": "/attachment/366fff301232dc261fd0eadd66eb4dd134492f79.pdf", "_bibtex": "@misc{\nramezani-kebrya2020provably,\ntitle={Provably Communication-efficient Data-parallel {\\{}SGD{\\}} via Nonuniform Quantization},\nauthor={Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeJmlrFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeJmlrFvH", "replyto": "HyeJmlrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575513092374, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2194/Reviewers"], "noninvitees": [], "tcdate": 1570237726355, "tmdate": 1575513092388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2194/-/Official_Review"}}}], "count": 10}