{"notes": [{"id": "HJlt7209Km", "original": "HkxQDfAqtQ", "number": 1377, "cdate": 1538087968704, "ddate": null, "tcdate": 1538087968704, "tmdate": 1545355376284, "tddate": null, "forum": "HJlt7209Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJxEw0-zeE", "original": null, "number": 1, "cdate": 1544851035576, "ddate": null, "tcdate": 1544851035576, "tmdate": 1545354532432, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Meta_Review", "content": {"metareview": "The paper proposes a feature smoothing technique as a new and \"cheaper\" technique for training adversarially robust models. \n\nPros:\n\n* the paper is generally well written and the claimed results seem quite promising\n\n* the theory contribution are interesting\n\nCons:\n\n* the main technique is fairly incremental\n\n* there were concerns regarding the comprehensiveness of evaluations and baselines used", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Interesting proposal but requires more comprehensive evaluation and comparison"}, "signatures": ["ICLR.cc/2019/Conference/Paper1377/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1377/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352862235, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlt7209Km", "replyto": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1377/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1377/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1377/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352862235}}}, {"id": "HylC0e-C2m", "original": null, "number": 3, "cdate": 1541439701819, "ddate": null, "tcdate": 1541439701819, "tmdate": 1541533184817, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "content": {"title": "Some interesting proposals, with weak justification and experimental verification.", "review": "In this paper the authors introduce a novel method to defend against adversarial attacks that they call feature smoothing. The authors then discuss feature smoothing and related \u201ccheap\u201d data augmentation-based defenses against adversarial attacks in a nice general discussion. Next, the authors present empirical data comparing and contrasting the different methods they introduce as a means of constructing models that are robust to adversarial examples on MNIST and CIFAR10. The authors close by attempting to theoretically motivate their strategy in terms of reducing variance of the decision boundary.\n\nOverall, I found this paper pleasant to read. However, it is unclear to me exactly how novel its contributions are. As discussed by the authors, there are strong similarities between feature smoothing and mixup although I did enjoy the unifying exposition presented in the text. It also seems as though the paper suffers from some simplifying assumptions considered by the authors. For example, in sec. 2 the authors claim that \\tilde x will be closer to the decision boundary than x. However, this is only true if the decision boundary is convex. \n\nI appreciated the extensive experiments run by the authors. However, I wish they had included results from adversarial training. It seems (looking at Madry\u2019s paper) that the defense offered by these cheap methods is still significantly worse than adversarial training. I feel that some discussion of this is warranted even if the goal is to reduce computational complexity.\n\nFinally, I am not sure what to make of the theory presented. While it is nice to see that the variance of the decision boundary is reduced by regularization in the case of 1-dimensional linear regression, I am not at all convinced by the authors generalization to neural networks. In particular, their discussion seems to only hold for one-hidden-layer networks. Although the authors don\u2019t offer much clarity here. For example eq. 2 is literally just a statement that ReLU is a convex function. However, it is clearly the case that multiple layers of the network will violate this hypothesis. Overall, I did not find this discussion particularly compelling. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1377/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "cdate": 1542234243003, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlt7209Km", "replyto": "HJlt7209Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335933590, "tmdate": 1552335933590, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgRZ_Yd3Q", "original": null, "number": 2, "cdate": 1541081094106, "ddate": null, "tcdate": 1541081094106, "tmdate": 1541533184612, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "content": {"title": "An interesting paper whose novelty seems incremental to the reviewer", "review": "The authors proposed a feature smoothing method without adding any computational burden for defensing against adversarial examples. The idea is that both feature smoothing and Gaussian noise can help extend the range of data. Moreover, the authors combined these methods together to gain a better test and adversarial accuracy. They further proved 3 theorems to try to analyze the biases and variances of decision boundary based on the fisher information and delta method.  \n\nIn my opinion, the main contribution of this paper is to prove that the boundary variance will decrease due to adding one additional regularization term to the loss function. \n\nMain comments:\n1.\tThe proposed feature smoothing method seems less novel to me. In contrast to the mixup method, the proposed method appears to remove the label smoothing part, so it is better to explain or justify why this could be better theoretically.  Moreover, in the PGD and PGD-cw results, the performance is not as good as the Gaussian random noise method. Can the authors offer any discussion or comments on the possible reasons?\n2.\tSome details of the proof of Theorem 4.1 seemed to be omitted. I am a bit confused about this. \na.\t\u201cWithout loss of generality, we further assume b = 0 and w > 0.\u201d  With smaller magnitude, b=0 is reasonable, but why to assume w>0?\nb.\tCould you present the derivation details or the backing theory of the approximation of var(b), when one more regularization term are added?  \n3.\tIn addition, a method of modifying the network is proposed to adapt to the feature smoothing method. However, no experimental results are reported to support its effectiveness. I would believe some empirical evaluations may further strengthen the paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1377/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "cdate": 1542234243003, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlt7209Km", "replyto": "HJlt7209Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335933590, "tmdate": 1552335933590, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eWYFtDnQ", "original": null, "number": 1, "cdate": 1541015928638, "ddate": null, "tcdate": 1541015928638, "tmdate": 1541533184403, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "content": {"title": "review", "review": "The paper proposes a feature smoothing technique, which generates virtual data points by interpolating the input space of two randomly sampled examples. The aim is to generate virtual training data points that are close to adversarial examples. Experimental results on both MNIST and Cifar10 datasets show that the proposed method augmented with other regularization techniques are robust to adversarial attacks and obtain higher accuracy when comparing with some testing baselines. Also, the paper presents some theoretical analyses showing that label smoothing, logit squeezing, weight decay, Mixup and feature smoothing all produce small estimated variance of the decision boundary when regularizing the networks. \n\nThe paper is generally well written, and the experiments show promising results. Nevertheless, the proposed method is not very novel, and the method is not comprehensively evaluated with experiments.\n\nMajor remarks:\n\n1.\tThe experiments show that feature smoothing has to combine with other regularizers in order to outperform other testing methods. In this sense the contribution of the feature smoothing along is not clear. For example, without integrating other regularizers, Mixup and feature smoothing obtain very close results for BlackBox-PGD, BlackBoxcw and Clean, as shown in Table 1. In addition, in the paper, the feature smoothing along is only validated on the MNIST (not even tested on Cifar10 in Table2). Consequently, it is difficult to evaluate the contribution of the proposed smoothing technique. \n2.\tExperiments are conducted on datasets MNIST and Cifar10 with small number of target classes. Empirically, it would be useful to see how it performs on more complex data set such as Cifar100 or ImageNet.\n3.\tThe argument for why the proposed feature smoothing method works is presented in Theorem4.3 in Section 4.2, but the theorem seems to rely on the assumption that one can add data around the true decision boundary. However, how we can generate samples near the true decision boundary and how we should chose the mixing ratio to attain this goal is not clear to me in the paper. In addition, how we can sure that the adding synthetic data from one class does not collide with manifolds of other classes as suggested in AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization)? This is particular relevant if the proposed feature smoothing strategy prefers to create virtual samples close to the true decision boundary.\n4.\tAt the end of page4, the authors claim that both feature smoothing and Mixup generate new data points that are closer to the true boundary. I wonder if the authors could further justify or show that either theoretically or experimentally. \n5.\tThe proposed method is similar to SMOTE (Chawla et al., SMOTE: Synthetic Minority Over-sampling Technique). In this sense, comparison with SMOTE would be very beneficial.\n\nMinor remarks:\n\n1.\tIn the paper Mixup, value 1 was carefully chosen as the mixing policy Alpha for Cifar10 (otherwise, underfitting can easily occur as shown in AdaMixUp), and it seems in the paper the authors used a very large value of 8 for Mixup\u2019s Beta distribution, and I did not see the justification for that number in the paper.\n2.\tTypo in the second paragraph of page2: SHNV should be SVHN\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1377/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Official_Review", "cdate": 1542234243003, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlt7209Km", "replyto": "HJlt7209Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335933590, "tmdate": 1552335933590, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgewee0hQ", "original": null, "number": 7, "cdate": 1541435480277, "ddate": null, "tcdate": 1541435480277, "tmdate": 1541435480277, "tddate": null, "forum": "HJlt7209Km", "replyto": "BkxDE0KTnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "Hello,\nPlease refrain from making sweeping generalizations. Is there a proof that \"Cheap methods are unlikely to be robust under stronger PGD attack\"? The analysis in your paper that you cite is limited to logit squeezing.. why does this imply that \"any\" cheap method is likely to not be robust? ", "title": "Sweeping generalizations"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "BkxDE0KTnX", "original": null, "number": 6, "cdate": 1541410350522, "ddate": null, "tcdate": 1541410350522, "tmdate": 1541410350522, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "Regarding your results on MNIST, I would like to point you to Table 1 in our paper https://arxiv.org/abs/1810.12042 where we show that logit squeezing (combined with gaussian noise), as proposed by Harini et al., does not provide actual robustness. We could successfully break it not only on MNIST but also CIFAR10 and Tiny ImageNet. Further, we find that the robustness of logit squeezing mainly comes from the fact that it makes gradient based optimization in the input space significantly more difficult by introducing many local maxima near the clean inputs. This can be seen as gradient masking. Crucial for our evaluation was the fact that we performed many random restarts when performing PGD (up to 10000) and additionally performed a proper grid search over the step size used during optimization. \n\nTherefore, it would be interesting to see the robustness of your models against a PGD attack with large number of iterations, large step size, and many random restarts. Based on our experiments, we would expect that this should reduce the adversarial accuracy of \"cheap methods\" (logit squeezing + noise, label smoothing + noise, feature smoothing + noise)  down to (almost) 0%.", "title": "Cheap methods are unlikely to be robust under stronger PGD attack"}, "signatures": ["~Marius_Mosbach1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["~Marius_Mosbach1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "BklBpKu6tm", "original": null, "number": 5, "cdate": 1538259389284, "ddate": null, "tcdate": 1538259389284, "tmdate": 1538259389284, "tddate": null, "forum": "HJlt7209Km", "replyto": "r1lVAt8TtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "Their LS-PGA seems specific to their thermometer method. When I do experiments on other defenses, I always refer to PGD on CW_\\infty as Logit space PGD(PGA).  My personal habit, this is my bad, I should explain it clearly.  \nI also agree with your other comments.", "title": "My bad (I did not explain all the things clearly)"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "r1lVAt8TtQ", "original": null, "number": 4, "cdate": 1538251212493, "ddate": null, "tcdate": 1538251212493, "tmdate": 1538251212493, "tddate": null, "forum": "HJlt7209Km", "replyto": "rJgUHOmTtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "This previous ICLR paper (https://openreview.net/pdf?id=S18Su--CW) proposes an attack called LS-PGA, which is what I thought you were referring to. If you just mean PGD on the logits, then yes, agreed.", "title": "LS-PGA terminology"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "rJgUHOmTtX", "original": null, "number": 3, "cdate": 1538238526281, "ddate": null, "tcdate": 1538238526281, "tmdate": 1538246835791, "tddate": null, "forum": "HJlt7209Km", "replyto": "Byxwh9zpYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "PGD-CW use the CW_\\infty loss, which is actually similar to LS-PGA (PGD using logits, PGD in logit space => PGD using a CW_\\infty loss => PGD-CW (l_\\infty-bounded))", "title": "Exactly, what I mean is the solution in the appendix."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "Hkl3Eh-TFm", "original": null, "number": 1, "cdate": 1538231348211, "ddate": null, "tcdate": 1538231348211, "tmdate": 1538239556767, "tddate": null, "forum": "HJlt7209Km", "replyto": "HJlt7209Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "An interesting method. But have you noticed this work (https://arxiv.org/abs/1705.07204), which proposes a very simple binarization solution on MNIST? Therefore, achieving robustness against l_\\infty attacks on MNIST is very simple. And the results on CIFAR10 are not good enough. It seems the accuracy against PGD-cw (LS-PGA) is only 9.03%? \nLS-PGA means Logit Space Projected Gradient Ascent (PGD in logit space => PGD using a CW_\\infty loss => PGD-CW (l_\\infty-bounded))(Just an explanation for the next comment)\nFor comparison, MadryLab's model achieves 44.71% under DAA and 45.21 under 10 random start PGD. Besides, as you mentioned, this method is more efficient. So have you ever tested on large datasets like ImageNet? ", "title": "Interesting but the results seem not good enough"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}, {"id": "Byxwh9zpYQ", "original": null, "number": 2, "cdate": 1538235055320, "ddate": null, "tcdate": 1538235055320, "tmdate": 1538235055320, "tddate": null, "forum": "HJlt7209Km", "replyto": "Hkl3Eh-TFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "content": {"comment": "I don't disagree with your general sentiment, but two minor notes:\n- Ensemble Adversarial Training itself does not claim to solve MNIST in the white-box setting. It is in Appendix C.1 where the authors note that binarization for L_infinity MNIST is effective.\n- LS-PGA doesn't have anything to do with their attack method (and I don't think they claim it does). PGD-CW is (if I understand correctly, becuase this paper doesn't explain it) PGD from Madry et al. (2018) with the loss function from Carlini & Wagner (2017).\n\nOne more observation about the paper: BlackBox-CW on CIFAR-10 accuracy (17%) should not be a stronger attack and have have lower accuracy than than White Box PGD (32%).", "title": "EAT does not solve MNIST"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1377/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical and Empirical Study of Adversarial Examples", "abstract": "Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead.  In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point.  The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.", "keywords": ["Adversarial examples", "Feature smoothing", "Data augmentation", "Decision boundary"], "authorids": ["fuchenl@andrew.cmu.edu", "shanghongwei@oath.com", "hongz@oath.com"], "authors": ["Fuchen Liu", "Hongwei Shang", "Hong Zhang"], "pdf": "/pdf/e87e9d3d8197d4a8799ef382016a6bc5ba67c87b.pdf", "paperhash": "liu|theoretical_and_empirical_study_of_adversarial_examples", "_bibtex": "@misc{\nliu2019theoretical,\ntitle={Theoretical and Empirical Study of Adversarial Examples},\nauthor={Fuchen Liu and Hongwei Shang and Hong Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlt7209Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1377/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311611440, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJlt7209Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1377/Authors", "ICLR.cc/2019/Conference/Paper1377/Reviewers", "ICLR.cc/2019/Conference/Paper1377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311611440}}}], "count": 12}