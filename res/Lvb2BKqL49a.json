{"notes": [{"id": "Lvb2BKqL49a", "original": "nmWwmbdLe0", "number": 826, "cdate": 1601308095472, "ddate": null, "tcdate": 1601308095472, "tmdate": 1614985626988, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4E7IKMGHpu5", "original": null, "number": 1, "cdate": 1610040533029, "ddate": null, "tcdate": 1610040533029, "tmdate": 1610474142824, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper is a study in optimizing the Donsker-Varadhan lower bound on mutual information focusing on a \"drift\" problem.  The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together.  They propose a fix for this problem. The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance. The paper does not address the variance (convergence) issues with the DV bound.\n\nWe have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection.  Other reviews are not very enthusiastic.  I will side with rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040533016, "tmdate": 1610474142806, "id": "ICLR.cc/2021/Conference/Paper826/-/Decision"}}}, {"id": "9gqOLUXduKo", "original": null, "number": 1, "cdate": 1603862961004, "ddate": null, "tcdate": 1603862961004, "tmdate": 1607055258325, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Review", "content": {"title": "Novel mutual information lower bound to regularized the drifting problem", "review": "### EDIT:\n\n I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft. Unfortunately, I'm still not very happy with the motivation of attacking the drifting phenomenon on MINE. The main reason for removing the drifting effect is for moving average of history outputs. However, there are various ways for tackling this ( as pointed out in my original reviews, like using a non-drifted mutual information estimator with moving average or plugin some robust density estimators). Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. But actually, the stability of other MI estimators also allows them to avoids having exploding network outputs.\n\n Also, in practice, people don't usually run moving average on MI for representation learning. I encourage the author to explore the importance of moving average of MI estimators further.\n\nR3 suggests the author take some non-parametric estimators as baselines. But I think it's fine to only compare to some parametric(variational) methods on high dimension setting, where most non-parametric estimators fail. Nonetheless, it's always good to have additional experiments compared to some non-parametric methods in low dimension settings.\n\nOverall, I lean toward rejection given current concerns.\n\nSummary\n\nThe paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges. Also, the optimization of MINE will result in the bimodal distribution of the outputs, in which the statistical network has very distinct values for joint and non-joint samples. The paper presents a theoretical explanation for the drifting phenomenon. In light of this, the author's approach is to add a regularized term to prevent the drifting phenomenon. They impose a $L_2$ regularization on the logsumexp term to enforce the network to find a single solution. Further, the authors make use of the historical estimation for better performance. Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain\n\nContributions\n\ni) Proposal of a novel regularized MINE objective for solving the drifting phenomenon. The new objective successfully finds a single solution and exhibits a lower variance.\n\nii) Provide interesting insights, such as drifting phenomenon and the instability due to small batch size, out of experiments. \n\niii) Experimental validation of the proposed method for solving the drifting problem. Achieve better performance for mutual information estimation in the continuous domain. \n\nIssues:\n\ni) The drifting phenomenon of MINE is a feature but not a bug, since the drifting term has no effect on the final MINE estimated value. The motivation and benefits of solving drifting problems are unclear to me.\n\nii) Is the drifting phenomenon of the statistical network ubiquitous among the density ratio estimators? For example, does it exist in the density ratio estimator in logistic regression or in JS dual lower bound? If not, we can directly plug these non-drifting density estimators in MINE, instead of regularized MINE. Another apparent remedy is to make the output of statistical network $T$ zero-meaned by subtracting the online sample mean of $T$.\n\niii) The proposed ReMINE is motivated by the drifting phenomenon. But it can also alleviate the exploded outputs / bimodal distribution of the outputs since ReMINE explicitly imposes $L_2$ constraint on its output. The connection between the exploded outputs / bimodal distribution of the outputs and ReMINE is weak in the paper.\n\niv) The paper states that MINE must have a batch size proportional to the exponential of true MI to control the variance. The statement is wrong. Yes, the variance of some mutual information estimator, like NWJ,  is proportional to the exponential of true MI, as proved in [1]. However, the variance of MINE is not proportional to the exponential of true MI in finite sample case (in asymptotic maybe), due to the log function. \n\nMinors:\na) I wonder whether the SMILE estimator (cited in the paper) implicitly solves the drifting problem. Since the optimal statistical network cannot drift freely in SMILE.\n\n[1]A Theory of Usable Information Under Computational Constraints, Xu et al, ICLR20. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134145, "tmdate": 1606915809291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper826/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Review"}}}, {"id": "AunbqRjBJpZ", "original": null, "number": 4, "cdate": 1604378877850, "ddate": null, "tcdate": 1604378877850, "tmdate": 1606775677439, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Review", "content": {"title": "Major revisions needed", "review": "Response to authors: After reading the authors' response, I have decided to maintain my original rating. The authors have not adequately addressed my main concerns.\n\nNovelty: The work here, as indicated by the authors, is largely an incremental improvement over an existing work MINE. The authors' response did not alleviate this concern and in fact reinforced it.\n\nCitations and comparisons to other work: The authors did not agree to even include citations to important literature in this area. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions.\n\nTheoretical work: The authors simply pointed to the theoretical work for MINE. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). More theoretical work is needed in this area to justify the use of these estimators over others.\n\nSome responses to other comments that may help the authors with further revisions:\n\n(1) The presentation of MINE should occur in the main paper as this is crucial for understanding the paper.\n\n(2) This was not clear. Perhaps the authors could include similar pointers in the paper with each of these issues.\n\n(3) The bias I'm referring to here is the actual statistical bias of the estimator. From Theorem 6, it seems that the drift problem does seem to create some bias but it would be useful to quantify that, which could then lead to a bias correction approach.\n\n(7) The way this is currently worded, it sounds like you are saying that training with a larger batch size is bad. This part should be clarified to avoid this. \n\nOriginal Review:\n\nThis paper presents a modified version of a neural network-based MI estimator. They investigate a few of the issues of this specific estimator and propose a regularization to help with one of them.\u00a0MI estimation is an important and difficult topic. Improvements in this area are of definite interest.\n\nPros:\nThe paper appears to be technically correct. The experiments are somewhat supportive of including the regularization, especially when the MI is higher which is a known issue with some MI estimators.\n\nCons:\nThere are some interesting ideas here but the paper feels unpolished. The presentation of the ideas is somewhat unconventional. Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them. The paper could benefit from a bit more focus in this regard. In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement. But it's not clear how much of a problem this drift really is. The authors show that it causes a bias but they do not present how much bias it adds.\u00a0 \u00a0\n\nIn addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons (see the references below for some examples, which all have strong theoretical results).\u00a0\n\nThe theoretical work is also weak with regards to the convergence rates of the proposed estimator as well. While empirical results can confirm that an estimator can be useful in practice, they are easy to cherry-pick and ultimately theoretical guarantees are needed to know an estimator's general performance. Thus the results would be a lot stronger if convergence rate guarantees were given.\u00a0\n\nOther comments/questions:\nThe authors should define the MINE estimator in this paper.\n\nThe second bullet point on page 2 says that \"training with larger batch size reduces the variance of the MI estimate\". Isn't this a good thing? That would lead to better convergence.\n\nIn Section 3.1 the notation is technically incorrect. Instead of stating $I(X;X)$ it should be written as $I(X_1;X_2)$ where $X_1$ and $X_2$ are i.i.d. The former suggests that you're comparing the same random variables.\u00a0\n\nOn page 4, it's suggested that joint samples are sparse with reduced sample size. Why aren't joint samples simply included together during training?\n\nDoes regular L2 regularization help with the drift problem?\u00a0\n\n[R1]\u00a0Moon et al.\"Ensemble estimation of mutual information,\" ISIT, 2017.\n[R2]\u00a0Moon et al., \"Information theoretic structure learning with confidence,\" ICASSP, 2017.\n[R3]\u00a0Moon et al., \"Ensemble Estimation of Information Divergence,\" Entropy, 2018.\n[R4] Singh and Poczos, \"Exponential concentration of a density functional estimator,\" NeurIPS, 2014.\n[R5] Kandasamy et al., \"Nonparametric von Mises estimators for entropies, divergences, and mutual informations,\" NeurIPS. 2015.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134145, "tmdate": 1606915809291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper826/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Review"}}}, {"id": "IvERaFx2Zb", "original": null, "number": 4, "cdate": 1605452552454, "ddate": null, "tcdate": 1605452552454, "tmdate": 1605454333363, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "AunbqRjBJpZ", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part II) ", "comment": "(6) While empirical results can confirm that an estimator can be useful in practice, they are easy to cherry-pick \u2026\n\nAs it can be seen in the provided source code, we simply added our ReMINE implementation to the existing code base of (Poole et al., 2019). We did not modify the provided benchmark experiments.\nFor experiments of SMILE (Self-consistency test, Song & Ermon, 2020), we couldn\u2019t find the relevant source code except for the implementation of the SMILE loss itself. We re-implemented the experiment (which is also included in the provided source code) and reproduced the similar results on SMILE, InfoNCE, and JS with 10 repeated runs.\nFor experiments of CMI (Mukherjee et al., 2020), we also conduct 10 repeated runs from the source code the authors provided. We added our implementation to the provided code.\n\n(7) The second bullet point on page 2 says that \"training with larger batch size reduces the variance of the MI estimate.\" Isn't this a good thing? That would lead to better convergence.\n\nYes, it is a good thing. Following the work of (McAllester & Stratos, 2018) and (Song & Ermon, 2020), we experimentally observe the relationship between the batch size and the estimation variance.\nHowever, the problems are when batch size is small: \u201cExploding network outputs, where smaller batch sizes cause the network outputs to explode.\u201d\n\n(8) In Section 3.1, the notation is technically incorrect. ... The former suggests that you're comparing the same random variables. \n\nAs you have pointed out, it is not i.i.d., rather, we are indeed comparing exactly the same random variable. Namely, $\\frac{dP}{dQ} = 1$  for joint samples and $\\frac{dP}{dQ} = 0$ for non-joint samples.\n\n(9) On page 4, it's suggested that joint samples are sparse with reduced sample size. Why aren't joint samples simply included together during training?\n\nBecause it will change the distribution of $Q$ (namely, to $Q\u2019$), hence the optimal network converges to the different $\\frac{dP}{dQ}$ (namely, to $\\frac{dP}{dQ'}$). Also, this gets much harder to provide a method that works in a general sense if samples weren\u2019t discrete (e.g. Experiment shown in Figure 6 and Figure 9).\n\n(10) Does regular L2 regularization help with the drift problem? \n\nYes. We provided proofs (Section 4), experiments (Figure 4, 5), analysis on the gradients (Subsection 5.1) and the experimental results using L1 regularization (Figure 11).\n\n(Poole et al., 2019): Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pp. 5171\u20135180, 2019.\n(Jiao et al., 2018): Jiantao Jiao, Weihao Gao, and Yanjun Han. The nearest neighbor information estimator is adaptively near minimax rate-optimal. In Advances in neural information processing systems,  pp. 3156\u20133167, 2018.\n(Gao et al., 2017): Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath.  Estimating mutual information for discrete-continuous mixtures. In Advances in neural information processing systems, pp.5986\u20135997, 2017\n(Song & Ermon, 2020): Jiaming Song and Stefano Ermon.  Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2020.  URL https://openreview.net/forum?id=B1x62TNtDS.\n(Mukherjee et al., 2020): Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan.  Ccmi:  Classifier based conditional mutual information estimation.  In Uncertainty in Artificial Intelligence, pp. 1083\u20131093. PMLR, 2020.\n(McAllester & Stratos, 2018): David McAllester and Karl Stratos.  Formal limitations on the measurement of mutual information. arXiv preprint arXiv:1811.04251, 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "RT9-Il4loBW", "original": null, "number": 7, "cdate": 1605452777533, "ddate": null, "tcdate": 1605452777533, "tmdate": 1605454114344, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "9gqOLUXduKo", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (Part I)", "comment": "We sincerely appreciate everyone\u2019s comments, which were indeed helpful in improving this paper. R2\u2019s reviews were very helpful, especially in a theoretical sense. We address the concerns raised by R2 in order of appearances.\n\n(1)  The drifting phenomenon of MINE is a feature but not a bug, since the drifting term has no effect on the final MINE estimated value. The motivation and benefits of solving drifting problems are unclear to me.\n\nTo produce a single estimate with the history of network outputs, both macro- and micro-averaging strategies require the outputs to be not drifting. (Paragraph below Algorithm 1) We also provide the proof for Theorem 6 in the appendix.\nAlso, removing the constant $C$ of the network output enables us to directly interpret the network output values across different batches. (Figure 4 caption) \n\n(2) Is the drifting phenomenon of the statistical network ubiquitous among the density ratio estimators? \u2026 If not, we can directly plug these non-drifting density estimators in MINE, instead of ReMINE.\n\nAs R1 suggested, other bounds such as JS or MINE-$f$ might be used to avoid the drifting problem. However, the drifting phenomenon is not the only problem that our method solves. Three bullet points in Section 3 shows the advantages of our method:\n* Solves the drifting phenomenon: Section 4\n* Avoids having exploding network outputs: By implicitly adjusting the gradient step size by using L2 regularization. (Section 5)\n* Interpretable network outputs: \u201cWe can now directly interpret the network outputs, and $E(j)$ directly converges to ideal MI $\\log N$, thanks to the regularization of $C$.\u201d (Section 5 Figure 4)\n\n(3) Is the drifting phenomenon of the statistical network ubiquitous among the density ratio estimators? \u2026 Another apparent remedy is to make the output of statistical network $T$ zero-meaned by subtracting the online sample mean of $T$.\n\nNote that for the optimal $T$, first term $E_{P}[T] \\to I(X; Y)+ C$ and second term $\\log(E_{Q}[e^T])  \\to C$. There are two apparent problems when subtracting the online sample mean.\n\nFirst, as seen in Figure 3 (d), the second term output tends to get quite noisy. The reason behind it is in Subsection 3.2 \u201cExploding network outputs\u201d. The blue line in Figure 3 (d) can be viewed as the output of the online sample mean strategy, which fails to be stabilized. Also in Figure 6, the light blue line is the estimate of each batch. In other words, getting a stable online estimate of C from a single batch is hard. This naturally leads to unstable gradients, which can be another answer to the above question (2).\n\nSecondly, C changes for every batch. This is why the original MINE also had to use heuristics like the exponential moving average (Subsection 3.2 from Belghazi et al., 2018) to avoid the inherent bias generated by the averaging strategies (Theorem 6). Hence, the exponential moving average used in other literature such as (Poole et al., 2019) has limitations, as seen in Figure 1(c). Note that the red line is the exponential moving average, and the axis for figure 1(c) is logarithmic.\nBesides, since we constrained the estimate of the second term in DV to have the same value for each batch, we relaxed the constraint on sample complexity from a single batch to the number of samples in the history (Algorithm 1).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "T2HxFrrwzd", "original": null, "number": 8, "cdate": 1605452812033, "ddate": null, "tcdate": 1605452812033, "tmdate": 1605453906909, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "9gqOLUXduKo", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (Part II)", "comment": "(4) The proposed ReMINE is motivated by the drifting phenomenon. But it can also alleviate bimodal distribution of the outputs since ReMINE explicitly imposes a constraint on its output. The connection between the exploded outputs / bimodal distribution of the outputs and ReMINE is weak in the paper.\n\nExploding outputs:\nWe analyze that ReMINE mitigates the exploding outputs in the gradient descent-based optimization by implicitly controlling the step size of the gradient (Subsection 5.1).\n\nBimodal distribution of the outputs:\nFirst of all, as the statistics network of ReMINE converges to $\\log \\frac{dp}{dq}$ (log-likelihood ratio), it is natural to be having bimodal outputs. ($\\frac{dp}{dq}=1$ for joint cases, and $\\frac{dp}{dq}=0$ for non-joint cases, as our toy dataset is discrete.) (see Theorem 5.) So, we don\u2019t think that ReMINE suppresses the bimodal distribution of the outputs, at least in a theoretical sense.\n\nHowever, in a gradient-descent perspective, large lambda (regularizer weight) is indeed not helpful, as the non-joint sample network outputs converge to $-\\frac{1}{2 \\lambda}$ in the discrete case. (Explanation in the subsection 5.1, and Figure 4(b) also shows that non-joint sample network outputs converge to $-\\frac{1}{2 * 0.1} = -5$)\nWe additionally show the performance of ReMINE on our toy dataset in a range of lambda in Figure 8(c). Empirically, we found that using values near 0.1~1.0 helped performance. We used 0.1 or 1.0 for all our experiments except Figure 8(c).\n\nAdditionally, without removing $C$ with ReMINE, we believe that we shouldn\u2019t directly interpret the network output values, but it is interpreted as a relative size. We believe Fig 2(b) and Fig 4(b) will also clarify this.\n\n(5) The paper states that MINE must have a batch size proportional to the exponential of true MI to control the variance. The statement is wrong. Yes, the variance of some mutual information estimator, like NWJ, is proportional to the exponential of true MI, as proved in [1]. However, the variance of MINE is not proportional to the exponential of true MI in finite sample cases (in asymptotic maybe) due to the log function.\n\nBy reflecting R2's comment, the sentence was revised like this:\n\u201cwhere the batch size correlates with the variance of the MI estimates.\u201d\n\n(6) I wonder whether the SMILE estimator (Song & Ermon, 2020) implicitly solves the drifting problem since the optimal statistical network cannot drift freely in SMILE.\n\nWe found that SMILE (without JS) behaves quite similarly with MINE, as the gradient gets clipped off whenever network outputs are not inside the fixed bound.\n\n\n(Belghazi et al., 2018): Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531\u2013540, 2018.\n(Poole et al., 2019): Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pp. 5171\u20135180, 2019.\n(Song & Ermon, 2020): Jiaming Song and Stefano Ermon.  Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2020.  URL https://openreview.net/forum?id=B1x62TNtDS."}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "Hv5sCJ0smv8", "original": null, "number": 5, "cdate": 1605452642391, "ddate": null, "tcdate": 1605452642391, "tmdate": 1605452693801, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "qdhcOU5Y9C", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We sincerely appreciate everyone\u2019s comments, which were indeed helpful on improving this paper. R4\u2019s detailed review helped improve the readability of this paper. We address the concerns raised by R4 in order of appearances.\n\n(1) I found the writing to be difficult to understand and follow. For example, the paper is not self-contained: the authors use terms like \"statistical network\" without providing definitions, and begin simulations without introducing MINE or giving a high-level discussion of how it estimates the DV representation, so it would be very difficult to read this paper without having first read the original MINE paper.\n\nTo improve the readability of our paper, we added the definition of the statistic network in Section 2, paragraph \u201cVariational Mutual Information Estimation\u201d. We also added another section \u201cAppendix: Comparison with Mutual Information Neural Estimator\u201d in the appendix.\n\n(2) The paper lacks intuition. For example, there is little to no discussion of why the synthetic dataset proposed is a good choice for studying the underlying properties of MINE.\n\nThe ultimate reason behind the synthetic dataset is \u201ceasily discerning samples of joint distribution from marginal distribution (Subsection 3.1 \u201cDataset\u201d)\u201d. Without this, we cannot visualize Figure 1 and 2, and clearly show the drifting problem.\n\n(3) Some additional typos/comments:\n\nAll comments are reflected in the revised version except the figure. We are going to re-draw all the graphs in the camera-ready phase. Thank you for your careful review.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "MumJMF4g3MV", "original": null, "number": 6, "cdate": 1605452673340, "ddate": null, "tcdate": 1605452673340, "tmdate": 1605452673340, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Y1gfqVSXPL_", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank R1 for recognizing our efforts made in this work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "TXx3rgMeQcL", "original": null, "number": 3, "cdate": 1605452513405, "ddate": null, "tcdate": 1605452513405, "tmdate": 1605452576877, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "AunbqRjBJpZ", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part I)", "comment": "We sincerely appreciate everyone\u2019s comments, which were indeed helpful in improving this paper. Especially, we added another section based on R3\u2019s comments, which made this paper more readable. We address the concerns raised by R3 in order of appearances.\n\n(1) The presentation of the ideas is somewhat unconventional.\n\nAs both R3 and R4 have noted, our paper did not provide the explanations on MINE clearly enough. To address the concerns, we added another section \u201cAppendix: Comparison with Mutual Information Neural Estimator\u201d in the appendix. We hope the added section eases the readability of the paper.\n\n(2) Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them.\n\nWe did not discard any issues. In this work, we raise 4 issues in total (Bullet points in the Introduction):\n\nQ. How does the neural network inside MINE behave when estimating MI?\nA. We analyze the network indirectly by analyzing the network outputs, as shown in Section 3.2, Figure 1, and 2.\n\nQ. Why does MINE loss diverge in some cases? (oscillates, and converges to infinity,  etc.) Where does the instability originate from?\nA. \u201cExploding network outputs\u201d in subsection 3.2 points out the problematic gradients in MINE, and the first paragraph in subsection 5.1 \u201cEffectiveness of L2 regularization\u201d analyzes the gradient of ReMINE, which helps avoiding the output explosion problem.\n\nQ. Can we make a more stable estimate on small batch size settings?\nA. We point out the drifting problem in Figure 1 (a), first paragraph in 3.2 \u201cObservation\u201d, Section 4, and we propose a novel strategy (micro-averaging strategy) in Algorithm 1, which cannot be used when the drifting problem occurs (Theorem 6).\n\nQ. Why does the value of each term in MINE loss are shifting even after the estimated MI\nconverges? Are there any side effects of this phenomenon?\nA. Same as above. (Section 4) Hence MINE can only rely on single batch estimates (Paragraph after Algorithm 1).\n\n(3) It's not clear how much of a problem this drift really is. The authors show that it causes a bias but they do not present how much bias it adds. \n\nWe did not fully understand what \u201cbias\u201d the R1 was pointing out. Note that the drifting phenomenon does not cause a biased MI estimate for a single-batch estimate.\n\n1) For the bias in the estimated MI from a single batch:\nFigure 1 (a) experimentally shows the drifting, and Section 4 is dedicated to understanding the phenomena. However, MI estimate for the single batch is not biased, as shown in Figure 1 (a) and proven in the DV representation.\n\n2) For the bias in the estimated MI from multiple batches:\nTheorem 6 shows that both macro- and micro-averaging strategies produce a biased estimate when the drifting problem occurs. (e.g. Averaging the batchwise estimate of MI leads to a biased estimate.)\n\n3) For the bias in the gradient estimates:\nMINE tries to mitigate the gradient bias by exponential moving average of mini-batches. (Paragraph below Lemma 1, Appendix: Comparison with Mutual Information Neural Estimator) ReMINE does not solve the biased gradient problem, but it mitigates the gradient explosion problem by implicitly controlling the step size of the gradient (Subsection 5.1).\n\n(4) In addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons.\n\nSimilar to (Poole et al., 2019), we concentrate on the variational bounds of mutual information so that it can be used on representation learning or generative models (as noted in the first paragraph of the introduction). We believe that our paper is an incremental study from MINE, hence fair to compare with neural-network based MI estimators with our method. Nonetheless, we compared our method with non-neural network MI estimators such as (Jiao et al., 2018) and (Gao et al., 2017) which has strong theoretical guarantees with the benchmark experiment provided in (Poole et al., 2019) in the appendix.\n\n(5) The theoretical work is also weak with regards to the convergence rates of the proposed estimator \u2026 The authors should define the MINE estimator in this paper.\n\nConvergence proofs are already provided in the MINE paper, and we wrongly thought the extension was obvious. To address the reviewers\u2019 concerns on the issue of self-containment of our paper (R2, R3), we add \u201cAppendix: Comparison with Mutual Information Neural Estimator\u201d in the appendix."}, "signatures": ["ICLR.cc/2021/Conference/Paper826/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Lvb2BKqL49a", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper826/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper826/Authors|ICLR.cc/2021/Conference/Paper826/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Comment"}}}, {"id": "Y1gfqVSXPL_", "original": null, "number": 2, "cdate": 1603932400474, "ddate": null, "tcdate": 1603932400474, "tmdate": 1605024596783, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Review", "content": {"title": "Good paper with new insights for the community", "review": "This paper attempts to answer the four questions raised from the mutual information estimator. To this end, this paper investigates why the MINE succeeds or fails during the optimization on a synthetic dataset. Based on the observations and discussions, the paper then proposes a novel lower bound to regularize the neural networks and alleviate the problems of MINE.\n\nOverall, the paper is easy to follow and new insights have been brought for the MI estimator and the downstream tasks.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134145, "tmdate": 1606915809291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper826/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Review"}}}, {"id": "qdhcOU5Y9C", "original": null, "number": 3, "cdate": 1603988383101, "ddate": null, "tcdate": 1603988383101, "tmdate": 1605024596718, "tddate": null, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "invitation": "ICLR.cc/2021/Conference/Paper826/-/Official_Review", "content": {"title": "Regularized MINE review", "review": "The work studies a neural-network based estimator, referred to as MINE, for approximating the mutual information between two variables.  By designing a synthetic dataset, the work studies properties of MINE and, based on these findings, proposes a new method incorporating regularization, called ReMINE, that they empirically demonstrate has nice performance. \n\nStrengths:\n-- The proposed ReMINE algorithm combats drifting of the two terms in MINE's approximation of the DV representation through regularization that forces the network to find a single solution (as opposed to a family of solutions).\n-- The proposed method is novel and performs well compared to state-of-the-art.\n\nWeaknesses:\n-- I found the writing to be difficult to understand and follow. For example, the paper is not self-contained: the authors use terms like \"statistical network\" without providing definitions, and begin simulations without introducing MINE or giving a high-level discussion of how it estimates the DV representation, so it would be very difficult to read this paper without having first read the original MINE paper. \n-- The paper lacks intuition. For example, there is little to no discussion of why the synthetic dataset proposed is a good choice for studying the underlying properties of MINE.\n\nSome additional typos/comments:\n\nPg 1: \"...the value of each term in MINE loss IS shifting even...\"\nPg 2: I believe equation (3) should be a lower bound not an equality.\nPg 2: \"...where estimates of  __ and __ DRIFT in parallel...\"\nPg 4: mu and sigma in Figure 2 were never defined.\nWhen printed, the black-and-white figures are too small to read and interpret.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper826/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper826/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularized Mutual Information Neural Estimation", "authorids": ["~Kwanghee_Choi1", "~Siyeong_Lee1"], "authors": ["Kwanghee Choi", "Siyeong Lee"], "keywords": ["Information Theory", "Regularization"], "abstract": "With the variational lower bound of mutual information (MI), the estimation of MI can be understood as an optimization task via stochastic gradient descent. In this work, we start by showing how Mutual Information Neural Estimator (MINE) searches for the optimal function $T$ that maximizes the Donsker-Varadhan representation. With our synthetic dataset, we directly observe the neural network outputs during the optimization to investigate why MINE succeeds or fails: We discover the drifting phenomenon, where the constant term of $T$ is shifting through the optimization process, and analyze the instability caused by the interaction between the $logsumexp$ and the insufficient batch size. Next, through theoretical and experimental evidence, we propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. We also introduce an averaging strategy that produces an unbiased estimate by utilizing multiple batches to mitigate the batch size limitation. Finally, we show that $L^2$ regularization achieves significant improvements in both discrete and continuous settings.", "one-sentence_summary": "We propose a novel lower bound that effectively regularizes the neural network to alleviate the problems of MINE. ", "pdf": "/pdf/962f87fffea11ce16712635669a979d1f75054d5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choi|regularized_mutual_information_neural_estimation", "supplementary_material": "/attachment/9487689522a21e9f72ec956eb9a851156f2d1e30.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=HIauhIkd4z", "_bibtex": "@misc{\nchoi2021regularized,\ntitle={Regularized Mutual Information Neural Estimation},\nauthor={Kwanghee Choi and Siyeong Lee},\nyear={2021},\nurl={https://openreview.net/forum?id=Lvb2BKqL49a}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Lvb2BKqL49a", "replyto": "Lvb2BKqL49a", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper826/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134145, "tmdate": 1606915809291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper826/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper826/-/Official_Review"}}}], "count": 12}