{"notes": [{"id": "BJge3TNKwH", "original": "H1xmNhkODB", "number": 769, "cdate": 1569439144264, "ddate": null, "tcdate": 1569439144264, "tmdate": 1583912022424, "tddate": null, "forum": "BJge3TNKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Y8RaLJ2XJ4", "original": null, "number": 1, "cdate": 1576798705550, "ddate": null, "tcdate": 1576798705550, "tmdate": 1576800930579, "tddate": null, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "invitation": "ICLR.cc/2020/Conference/Paper769/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711396, "tmdate": 1576800260591, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper769/-/Decision"}}}, {"id": "SkxaLppiiS", "original": null, "number": 3, "cdate": 1573801300950, "ddate": null, "tcdate": 1573801300950, "tmdate": 1573801300950, "tddate": null, "forum": "BJge3TNKwH", "replyto": "BylVcusk5r", "invitation": "ICLR.cc/2020/Conference/Paper769/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "Thank you very much for a positive evaluation of our work and for setting time aside to carefully read our paper. We appreciate your valuable time spent on serving the community. Below please find our responses. \n\n\n1) \"I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method.\"\n\nWe agree with the reviewer that our experimental section heavily relied on the MNIST dataset. Reviewer #3 also raised the same point, however, as you mentioned the main strength of the paper is certainly not in our empirical evaluation. To that end,  we added the experiments requested by Reviewer #3 on MNIST-to-SVHN and sequential learning on CIFAR-100. We demonstrate that the results on the new datasets are consistent with the ones reported in the original submission. Please see the extended supplementary material in the revised manuscript.\n\n2) \"I think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community.\"\n\nThis is a great point. Although we are restricted by the page limit of the conference, we did our best to revise the manuscript to further clarify the characteristics of these metrics and made additional connections to integral probability metrics (IPMs).\n\nYet again we thank the reviewer for a candid evaluation of our work. "}, "signatures": ["ICLR.cc/2020/Conference/Paper769/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJge3TNKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference/Paper769/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper769/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper769/Reviewers", "ICLR.cc/2020/Conference/Paper769/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper769/Authors|ICLR.cc/2020/Conference/Paper769/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166502, "tmdate": 1576860544350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference/Paper769/Reviewers", "ICLR.cc/2020/Conference/Paper769/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper769/-/Official_Comment"}}}, {"id": "Byg4_nTisr", "original": null, "number": 2, "cdate": 1573801068193, "ddate": null, "tcdate": 1573801068193, "tmdate": 1573801068193, "tddate": null, "forum": "BJge3TNKwH", "replyto": "SJlTfBCMqH", "invitation": "ICLR.cc/2020/Conference/Paper769/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3", "comment": "Thank you for the positive evaluation of our work and your insightful comments. We appreciate your valuable time spent on serving the community. Below please find our responses.  \n\nWe agree with the reviewer that the neuro-scientific terms used in our paper might make our manuscript, unnecessarily, hard to follow for many readers. To that end, we have revised the Introduction and Preliminaries sections and added rigorous definitions of these terms to provide a more enjoyable read for the readers.\n\nWe agree with both reviewers that the permuted-MNIST task is artificial and overly simple, which was precisely the reason for us to include the results on the SYNTHIA dataset in the paper. We also agree with the reviewer that our article could benefit from additional experiments; hence, we performed the requested experiments on MNIST-to-SVHN and sequential learning on CIFAR-100. Please see the extended supplementary materials in the revised manuscript. \n\nRegarding the comparison with IMM and PGMA, we first note that we have added these methods to our references. Next, we would like to point out the fundamental differences between the methods included in our work (i.e., online-EWC, MAS, and SCP) and IMM and PGMA. Below we enumerate these differences: \n\nThe core idea behind IMM [2] is to learn a new model for the new task using the old optimized parameters as an initialization and then consolidate the old and new models into a single model via moment matching (mean-IMM or mode-IMM). However, in our case, we only utilize one model and use the concept of synaptic plasticity, where we selectively rigidify the network parameters to preserve the previously learned representation while learning a new task. As you can see, there is a fundamental difference between the two approaches, which makes the direct comparison challenging. \n\nPGMA [3] is yet another fascinating approach that is fundamentally different from, and arguably significantly more complicated than our proposed method. At its core, PGMA uses the concept of memory replay, which is an orthogonal approach to selective synaptic plasticity that is the focus of our work. In short, PGMA learns an autoencoder-based generative model for old tasks. To avoid catastrophic forgetting, PGMA then replays samples from old tasks using its generative model while learning the new task. \n\nGiven: 1) the significant differences between IMM and PGMA and our proposed method, 2) the short period for the response, and 3) the unfortunate overlap of the ICLR rebuttal deadline and the CVPR2020 submission deadline, we could not provide the comparison to these methods at this time.   \n\nFinally, we have updated the caption of Figure 6 to make it easier to follow. In particular, we clarify that the blue and red shadings on the plots indicate the durations in which the models were trained on ``summer'' and ``winter'' data, respectively.\n\n[1] Aljundi et al. Memory Aware Synapses: Learning what (not) to forget, ECCV 2018.\n\n[2] Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n\n[3] Hu et al. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation, ICLR 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper769/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJge3TNKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference/Paper769/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper769/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper769/Reviewers", "ICLR.cc/2020/Conference/Paper769/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper769/Authors|ICLR.cc/2020/Conference/Paper769/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166502, "tmdate": 1576860544350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper769/Authors", "ICLR.cc/2020/Conference/Paper769/Reviewers", "ICLR.cc/2020/Conference/Paper769/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper769/-/Official_Comment"}}}, {"id": "BylVcusk5r", "original": null, "number": 1, "cdate": 1571956876167, "ddate": null, "tcdate": 1571956876167, "tmdate": 1572972554557, "tddate": null, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "invitation": "ICLR.cc/2020/Conference/Paper769/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "I think the paper is written quite well, and the approach makes a lot of sense. I think the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. The authors put in some effort in explaining why they propose this alternative distance between distributions. \n\nI think overall this is a great paper, very informative. If I need to nitpick, I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method. \n\nI think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community. "}, "signatures": ["ICLR.cc/2020/Conference/Paper769/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper769/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576378267508, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper769/Reviewers"], "noninvitees": [], "tcdate": 1570237747358, "tmdate": 1576378267519, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper769/-/Official_Review"}}}, {"id": "SJlTfBCMqH", "original": null, "number": 2, "cdate": 1572164884958, "ddate": null, "tcdate": 1572164884958, "tmdate": 1572972554509, "tddate": null, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "invitation": "ICLR.cc/2020/Conference/Paper769/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "[Summary]\nThis paper proposes a new method for overcoming catastrophic forgetting in continual learning, based on distribution-based regularization using the sliced Cramer distance, i.e. Sliced Cramer Preservation (SCP). Unlike previous work on catastrophic forgetting, this paper tackles unsupervised learning scenarios as well as supervised learning. They evaluate the proposed SCP on permutated MNIST, sequential learning in autoencoder task, and sequential learning for segmentation. \n\n[Pros]\n- This paper tackles unsupervised learning scenarios beyond the classification on benchmark datasets.\n- This paper employes sliced Cramer distance with theoretical justification.\n- The analysis on EwC and MAS in terms of geometric view\n- Experimental results look promising.\n\n[Cons]  \n- Even if MAS[1] describes the details on synaptic concept and Hebbian rule, many readers might be not familiar with synaptic or neuro-science terms. So, in prelimiary session, more explanation can help readers to understand.\n- In addition to permute-MNIST, it is required to be evaluated on more conventional tasks such as MNIST->SVHN or CIFAR-10, 100 datasets. Also, more recent work should be compared such as IMM [2] and PGMA [3] for supervised learning scenarios.\n- All graph result figures can be improved for enhancing legibility. In Figure 6, In particular, the subtitle of Summer case confuses me.\n \n[1] Aljundi et al. Memory Aware Synapses: Learning what (not) to forget, ECCV 2018.\n[2] Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n[3] Hu et al. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation, ICLR 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper769/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper769/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"], "TL;DR": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "pdf": "/pdf/94dc267f1eb2462a0e0706040c156c440b3a27ae.pdf", "paperhash": "kolouri|sliced_cramer_synaptic_consolidation_for_preserving_deeply_learned_representations", "_bibtex": "@inproceedings{\nKolouri2020Sliced,\ntitle={Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations},\nauthor={Soheil Kolouri and Nicholas A. Ketz and Andrea Soltoggio and Praveen K. Pilly},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJge3TNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b8575645ec0eb906c5bf32145d733f11bdc7cb52.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJge3TNKwH", "replyto": "BJge3TNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper769/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576378267508, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper769/Reviewers"], "noninvitees": [], "tcdate": 1570237747358, "tmdate": 1576378267519, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper769/-/Official_Review"}}}], "count": 6}