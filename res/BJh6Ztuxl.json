{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486460203554, "tcdate": 1478164932387, "number": 60, "id": "BJh6Ztuxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJh6Ztuxl", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "content": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396332773, "tcdate": 1486396332773, "number": 1, "id": "B1H2oG8_e", "invitation": "ICLR.cc/2017/conference/-/paper60/acceptance", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396333298, "id": "ICLR.cc/2017/conference/-/paper60/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396333298}}}, {"tddate": null, "tmdate": 1482744812947, "tcdate": 1482744788311, "number": 8, "id": "Bk3RXvREx", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "HkHqRoIEe", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Reviewer - comment", "comment": "Hey,\nThanks for the positive review and helpful comments,\nWe\u2019ve added the missing references and updated the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1482744580142, "tcdate": 1482744430539, "number": 7, "id": "S18uGDCEl", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "H1rEX6WNl", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Experimental analysis of unsupervised sentence embeddings", "comment": "Hey,\nThanks for the positive review,\nWe agree that more analysis of the effect of natural language properties in the encoded sentences is important\nWe will further explore it in future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1482239629073, "tcdate": 1482239629073, "number": 3, "id": "HkHqRoIEe", "invitation": "ICLR.cc/2017/conference/-/paper60/official/review", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer2"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "\nThe authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.\n\nExploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.\n\nWhile this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).\n\nThe word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is \"Generating Text with Recurrent Neural Networks\" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as \"debagging.\"\n\nAlthough this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512712132, "id": "ICLR.cc/2017/conference/-/paper60/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer1", "ICLR.cc/2017/conference/paper60/AnonReviewer2"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512712132}}}, {"tddate": null, "tmdate": 1481917229297, "tcdate": 1481917229297, "number": 2, "id": "H1rEX6WNl", "invitation": "ICLR.cc/2017/conference/-/paper60/official/review", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer1"], "content": {"title": "Experimental analysis of unsupervised sentence embeddings", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).\n\nThere are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper bound which can be deduced from natural language statistics?' This is investigated from one angle in Section 7, but I would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a 'surprising' observation. I had a similar reaction to the word-order experiments.\n\nMost of the interesting results, in my opinion, are about the ED model. It is fascinating that the LSTM encoder does not seem to rely on natural-language ordering statistics -- it seems like doing so should be a big win in terms of per-parameter expressivity. I also think that it's strange that word content accuracy begins to drop for high-dimensional embeddings. I suppose this could be investigated by handicapping the decoder.\n\nOverall, this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings. I recommend acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512712132, "id": "ICLR.cc/2017/conference/-/paper60/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer1", "ICLR.cc/2017/conference/paper60/AnonReviewer2"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512712132}}}, {"tddate": null, "tmdate": 1481556280278, "tcdate": 1481556280274, "number": 6, "id": "Sklrbr3Qx", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "rkqq9Mime", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Interesting analytic results on unsupervised sentence encoders", "comment": "Hey,\nThanks for the positive review,\n\nRe. CBOW drop: we tried different configurations, different seeds and different hyper parameters, and the results still hold. You are right that it is surprising but we don't have a good explanation for it at the moment. We will be happy to discuss new ideas on how to investigate this.\n\nRe. CBOW encoding order, and the comparison of order vs content: you are right, we changed our wording to be more precise about these issues and updated the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1481480849926, "tcdate": 1481480849920, "number": 1, "id": "rkqq9Mime", "invitation": "ICLR.cc/2017/conference/-/paper60/official/review", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer3"], "content": {"title": "Interesting analytic results on unsupervised sentence encoders", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.\n\nEffective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.\n\nOne minor red flag: \n- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.\n\nTwo writing comments:\n- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.\n- Saying that \"LSTM auto-encoders are more effective at encoding word order than word content\" doesn't really make sense. These two quantities aren't comparable. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512712132, "id": "ICLR.cc/2017/conference/-/paper60/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer1", "ICLR.cc/2017/conference/paper60/AnonReviewer2"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512712132}}}, {"tddate": null, "tmdate": 1481034520945, "tcdate": 1481034520940, "number": 5, "id": "H1-XjBN7l", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "ByAHmvJml", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Encoding of sentence length in embedding norm", "comment": "Hey,\nThanks for the comment.\n\nWe are not sure what you mean by \u201cquantifying our explanation\u201d, but we did perform two additional experiments in which we simulate the CBOW case using random vectors. We created 50k synthetic \u201csentences\u201d of different lengths, using the average of random vectors, first from a uniform distribution in (-1,1) and second from a normal distribution with N(0,1). Then, we plot the average norm of these vectors the same way we did in the CBOW case and both of the curves are very similar to the curve we got while using CBOW embeddings. \n\nIn the case of ED models, instead of decreasing, the norm is increasing as the sentence length grows higher. This makes intuitive sense as an RNN accumulates more information as it processes the sentence, and does not have the averaging property of the CBOW representation. Justifying this theoretically is less straightforward."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1481034383240, "tcdate": 1481034383234, "number": 4, "id": "S1vq9SE7x", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "rkZF4HJ7e", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Hey", "comment": "Hey,\nThanks for the comment.\n\nWe agree that quantifying generalization ability more directly is important and interesting, but we also find it beyond the scope of this work. However, we would like to stress that we do not view higher scores for any of the tasks as reflecting of the corresponding model being *better*. A high score on an aux task for a given model implies in our view only that this model encodes more of the given property. If this is not a desired behavior for your end task, then take the high score as an indicator of a bad thing. We do note that both skip-thoughts and ED with dimension size 1000 achieve very similar results on the word content task.\n\nWe\u2019re not sure we understood the overfitting question, but we try to answer the best we can. In order to avoid overfitting, we used a dropout layer after the non-linear function with a high dropout rate of 0.8 (smaller dropout rates indeed overfit), and implemented an early stopping criterion, meaning that we stopped training after 5 epochs in which we observed no loss improvement on a dedicated validation set."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1480713030363, "tcdate": 1480713030359, "number": 3, "id": "ByAHmvJml", "invitation": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer1"], "content": {"title": "Encoding of sentence length in embedding norm", "question": "Figure 2(b) shows the norm of CBOW embeddings decreasing very smoothly as the sentence length increases. Have you tried quantifying your explanation of this behavior based on the central limit theorem? If so, how well does it fit the curve?\n\nWhat does the analogue of Figure 2(b) look like for the ED embeddings?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959485846, "id": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer2", "ICLR.cc/2017/conference/paper60/AnonReviewer1"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959485846}}}, {"tddate": null, "tmdate": 1480705144717, "tcdate": 1480705144713, "number": 2, "id": "rkZF4HJ7e", "invitation": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer2"], "content": {"title": "Questions", "question": "Often it's desirable that a representation have some invariance to irrelevant changes in the input -- skip-thought vectors in particular aim to factor out certain types of phrasing variations that are irrelevant to prediction of surrounding context. In this sense, the ability to exactly reconstruct the input sentence could be seen as a detriment for a representation, since it is keeping around information that may not be helpful for future tasks. In this work, the ability to perfectly encode content seems to be presented as a mostly unqualified win. Have the authors considered auxiliary tasks to measure this sort of beneficial invariance to e.g. paraphrasing?\n\nI'm very surprised that the auxiliary classifiers can not be overfit on the training set since they are so nonlinear, especially when training on e.g. skip-thoughts which do not explicitly throw away any information -- can the authors give some intuition as to why this is the case?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959485846, "id": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer2", "ICLR.cc/2017/conference/paper60/AnonReviewer1"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959485846}}}, {"tddate": null, "tmdate": 1479837146587, "tcdate": 1479837146582, "number": 3, "id": "ry7JLbGzl", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "BkO3ZUA-e", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "Detail questions", "comment": "Hey,\nThanks for the comment.\n\n- Regarding the motivation for testing on training examples, in this work we are interested in analyzing to what extent a model can encode basic properties of sentences (content, order, length). Hence, we wanted to reduce all the other \u201cnoise\u201d and analyze the models on their \u201cideal environment\u201d: encoding the sentences they were trained on encoding. This gives us an upper-bound on the model\u2019s tendency to encode such values in its representation, without mixing in the effects of generalization.\nIt is true that additional experiments on unobserved sentences are also of interest, but we believe that the current setup is the more informative one. Note that we do also have experiments in which we encode sentences that the models were not trained on (all of the synthetic examples in which we used permuted sentences), and we observe similar behaviors. \n\n- Yes, we did significance tests (using paired t-test and Wilcoxon test), and for all the results that we report in the summery of findings the score differences are all highly significant (p << 0.0001). We updated the appendix to include the paired t-test p-values for the pairs we tested.\n\n- We are also puzzled by this, but do not currently have a good explanation. If you have convincing hypotheses we\u2019ll be happy to test them. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1479594415837, "tcdate": 1479594415834, "number": 1, "id": "BkO3ZUA-e", "invitation": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["ICLR.cc/2017/conference/paper60/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper60/AnonReviewer3"], "content": {"title": "Detail questions", "question": "\u2013 Could you explain your motivation for testing your models only on their training examples? You explain it briefly at the start of 3.1, but I can't reconstruct the argument.\n\n\u2013 Have you done significance testing of any kind on any of your results? The slight dip in ED performance in Figure 1a, for example, look like it could be a random fluctuation, but you mention it in your summary of the results.\n\n\u2013 What do you think might cause the massive drop in performance for CBOW in Figure 1b?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959485846, "id": "ICLR.cc/2017/conference/-/paper60/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper60/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper60/AnonReviewer3", "ICLR.cc/2017/conference/paper60/AnonReviewer2", "ICLR.cc/2017/conference/paper60/AnonReviewer1"], "reply": {"forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper60/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959485846}}}, {"tddate": null, "tmdate": 1479548763937, "tcdate": 1479548763932, "number": 2, "id": "ByEDyja-l", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "B1Xod9qWg", "signatures": ["~Yossi_Adi1"], "readers": ["everyone"], "writers": ["~Yossi_Adi1"], "content": {"title": "applications of task (c)", "comment": "Hey,\nWe agree that task (c) is related to the semantic meaning of the sentences, however that semantic is not restricted to named entities. While it is true that in \"john loved mary\", you get a different meaning if you switch the entities (\"mary loved john\") and it does not make sense to change the relative position of the verb (\"loved john mary\" is not a valid English sentence), this does not generally hold. In longer sentences, i.e \"john loved mary and hated jane\", the relative position of non-entity words (\"loved\" and \"mary\", or \"loved\" and \"hated\") also matters.  In addition, we believe that order information is a basic property of language, (which is what we are trying to analyze) and should be crucial for other tasks that relay on on that property, such as syntactic ones. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}, {"tddate": null, "tmdate": 1479350427543, "tcdate": 1479350427536, "number": 1, "id": "B1Xod9qWg", "invitation": "ICLR.cc/2017/conference/-/paper60/public/comment", "forum": "BJh6Ztuxl", "replyto": "BJh6Ztuxl", "signatures": ["~Jiaqi_Mu1"], "readers": ["everyone"], "writers": ["~Jiaqi_Mu1"], "content": {"title": "applications of task (c)", "comment": "We (J. Mu and P. Viswanath) enjoyed the tour-de-force comparison of a vast variety of sentence representation algorithms all in one compact manuscript. Of particular interest to us were the three  \"synthetic\" tasks introduced here:  (a) to what extent the sentence representation encodes its length; (b) to what extent the sentence representation encodes the identities of words within it and (c) to what extent the sentence representation encodes word order.  \n\nThe best part of these tasks is that they are very well defined and labeling does not need any (expert) supervision at all and can be done over the entire corpus too. We have a reasonable intuition on why tasks (a) and (b) might be interesting/relevant for downstream tasks: the length of a sentence could be a proxy for the amount of content in the sentence; testing of a word within a sentence could be a type of test for the topic embedded in the sentence. \n\nBut we aren't so clear as to what might be the sense in which task (c) could be useful for downstream applications. One instance where this matters seems to be cause-effect relationships. For example,  the order of 'Mary' and 'John' is critical in 'Mary stole an apple from John.' Such a pair of words tend to be  named entities, however. \n\nThe tests presented in this manuscript worked with a random pair of words (and not just named entities or scenarios where cause-effect relationship mattered). We would love to hear what the authors think about the use cases of task (c) and our conjecture that they are particularly relevant in cause-effect scenarios. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed\nlength vectors, in a way that preserves the sentence meanings. Two common\nmethods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.\nThe sentence vectors are used as features for subsequent machine learning tasks\nor for pre-training in the context of deep learning. However, not much is known\nabout the properties that are encoded in these sentence representations and about\nthe language information they capture.\nWe propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when\nusing the representation as input. We demonstrate the potential contribution of the\napproach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded\nvector\u2019s dimensionality on the resulting representations.", "pdf": "/pdf/20c136f446b546356902d50e2364756217c88326.pdf", "TL;DR": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "paperhash": "adi|finegrained_analysis_of_sentence_embeddings_using_auxiliary_prediction_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["biu.ac.il", "mit.edu", "ibm.com"], "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "authorids": ["yossiadidrum@gmail.com", "einatke@il.ibm.com", "belinkov@mit.edu", "oferl@il.ibm.com", "yoav.goldberg@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287745392, "id": "ICLR.cc/2017/conference/-/paper60/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJh6Ztuxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper60/reviewers", "ICLR.cc/2017/conference/paper60/areachairs"], "cdate": 1485287745392}}}], "count": 16}