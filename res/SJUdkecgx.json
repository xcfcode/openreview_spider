{"notes": [{"tddate": null, "ddate": null, "tmdate": 1512844109805, "tcdate": 1512844109805, "number": 12, "cdate": 1512844109805, "id": "rJULiitbz", "invitation": "ICLR.cc/2017/conference/-/paper163/public/comment", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["~Madhura_Gandhi1"], "readers": ["everyone"], "writers": ["~Madhura_Gandhi1"], "nonreaders": [""], "content": {"title": "Question for author: Time Limit in Reinforcement Learning ", "comment": "We are students of University of Michigan registered for the ICLR Reproducibility Challenge 2018. As a part of that, we are trying to replicate the results stated in the paper 'Time Limits in Reinforcement Learning'. The authors have mentioned that the code would be made publicly available on https://sites.google.com/view/time-limits-in-rl . However, the code is not up yet. Would it be possible to get access to the code? That would greatly help us verifying our implementation of the algorithms. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705259, "id": "ICLR.cc/2017/conference/-/paper163/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJUdkecgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper163/reviewers", "ICLR.cc/2017/conference/paper163/areachairs"], "cdate": 1485287705259}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488245156814, "tcdate": 1478258541801, "number": 163, "id": "SJUdkecgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJUdkecgx", "signatures": ["~Masoud_Faraki1"], "readers": ["everyone"], "content": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396401392, "tcdate": 1486396401392, "number": 1, "id": "HJYx3zLOg", "invitation": "ICLR.cc/2017/conference/-/paper163/acceptance", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper proposes two extensions of the KISSME metric learning method: (i) learning dimensionality reduction together with the metric; (ii) incorporating it into deep neural networks. The contribution is rather incremental, and the results are at the level of the prior art in deep metric learning, so in its current form the paper is not ready to be accepted."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396403451, "id": "ICLR.cc/2017/conference/-/paper163/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396403451}}}, {"tddate": null, "tmdate": 1484919717722, "tcdate": 1484919717722, "number": 2, "id": "SkCj75JPx", "invitation": "ICLR.cc/2017/conference/-/paper163/official/comment", "forum": "SJUdkecgx", "replyto": "rJftYFcGe", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "content": {"title": "comment on author response", "comment": "Dear Authors, \nthank you for your responses and updates on the paper. \nThe modifications on the first experiment are useful, so that comparisons can be made using the same underlying features.\nThe second part of the paper, integration of metric learning in deep net, is still not quite convincing to me. The main problem is that the method is disconnected from the first, since a different cost function is used (12), and that the contribution of this paper wrt directly optimizing (12) is an over parametrization as compared to simply integrating the metric in the fully connected layers of the deep net. The observed improvement here is interesting, but not well understood and analyzed in my opinion. \nMy recommendation remains negative for acceptance of this paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705084, "id": "ICLR.cc/2017/conference/-/paper163/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers", "ICLR.cc/2017/conference/paper163/areachairs"], "cdate": 1485287705084}}}, {"tddate": null, "tmdate": 1481921085929, "tcdate": 1481920872520, "number": 3, "id": "rkg_ZA-Ng", "invitation": "ICLR.cc/2017/conference/-/paper163/official/review", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "content": {"title": "Low-rank Mahalanobis metric learning with the KISMME objective function", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents extensions of the KISSME metric learning method.\nFirst, a subspace learning component is integrated, which avoids the need of ad-hoc pre-processing in the case of high-dimensional data.\nSecond, the approach is integrated with a CNN model that derives the features from an input image before it is entered into the Mahalanobis distance computation.\n\nThe related work section would benefit from a disucssion of previous work that also learns low-rank Mahalanobis metrics, such as e.g. [A]. One might argue that the referenced work, and similar ones, solves a non-convex objective and is therefore of less interest. This argument, however, seems weak at the moment that CNN training is integrated, since this also involves hihgly non-convex optimisation, yet leads to excellent results. A discussion on this point would be useful to include in the paper.\n\nIn the first experiment, the authors compare to related metric learning work. They  conclude that the proposed approach is superior, but this seems unjustified as the CNNs used to generate the features by the authors is different from the one used in related work. Thus it seems to me that no conclusions on the quality of the metric learning approach can be drawn from these experiments.\n\nIn the second experiment the authors show that a baseline where a factorisation of the PSD matrix M is absorbed in the low-rank projection matrix W leads to worse results than the proposed over-parametrised method. It would be useful if the authors could provide an analysis of why this might be the case; the current text does not seem to provide an explanation or hyposthesis. Is the same cost function used by the proposed method and the pairwise and triplet baseline ?\n\nI found the title of the paper slightly misleading, as the contribution of the paper is not to provide an extension to deep feature training: this has been done extensively in the past. In my understanding, the contribution lies in the proposed (overparameterized) formulation of learning a low-rank Mahalanobis metric for the KISSME objective function. Certainly, the extension to deep feature training is interesting, but relatively straightforward as compared to the main contribution. The title could reflect this more accurately.\n\nThe main motivation of the paper to base itself on KISSME is scalability, see introduction. However, for the case where a CNN is trained it seems that it is likely to be in a large-scale data and iterative training regime, and it is not obvious that KISSME would be more efficient than other metric learning objectives based on a pairwise or triplet loss. Therefore, comparison with other metric learning  objective functions in both performance and run time would be very useful to get a complete picture of which methods are most effective under what conditions. \n\n\n[A] Guillaumin, M.; Mensink, T.; Verbeek, J. & Schmid, C. Face recognition from caption-based supervision IJCV, 2012, 96, 64-82\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678408, "id": "ICLR.cc/2017/conference/-/paper163/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678408}}}, {"tddate": null, "tmdate": 1481905474598, "tcdate": 1481905474598, "number": 2, "id": "HyqrH9ZEl", "invitation": "ICLR.cc/2017/conference/-/paper163/official/review", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer2"], "content": {"title": "reviving a promising metric learning approach through the lens of deep learning", "rating": "7: Good paper, accept", "review": "KISSME was a promising metric learning approach that got lost in the deep learning flood. This is a shame since it had very good results at the time, but it was hindered by the quirks of the PCA pre-processing stage. In a nutshell, this paper reimagines this approach using deep convnets, gets good results on modern benchmark datasets and does not require meticulous preprocessing tricks. It therefore deserves to be published, since it rescues a promising approach from obscurity and does a competent job of illustrating its potential in a modern context, with all the necessary discussion of related work on old (LMNN, MMC, PCCA) and new (deep, triplet-loss based) metric learning approaches.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678408, "id": "ICLR.cc/2017/conference/-/paper163/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678408}}}, {"tddate": null, "tmdate": 1481899838513, "tcdate": 1481899838513, "number": 1, "id": "HkIrkKbNg", "invitation": "ICLR.cc/2017/conference/-/paper163/official/review", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer3"], "content": {"title": "A potentially useful contribution in metric learning with deep nets, but the paper lacks clarity and polish", "rating": "7: Good paper, accept", "review": "The paper modifies the KISSME algorithm in two ways: by incorporating a dimensionality reduction step and by integrating it in a loss function for learning deep networks. Experiments show that the latter solution is more stable than standard approaches to metric learning with deep nets.\n\nThe paper contribution could be good, but the paper could use some polish. After carefully reading the authors' response to my original questions, I have to say that I am still confused about several details.\n\nLemma 1 states that \"log(delta) identifies a Mahalanobis metric ... as M = Proj(...)\". I still think that the wording of the Lemma is imprecise; the authors should probably simply say that log(delta) results in an approximation of a certain metric, up to a constant term.  The word \"identifies\" definitely does not mean \"approximates\", which, since the word \"approximation\" is contained in the proof of the Lemma, is what is happening here. The authors have promised to fix this issue in the final version by changing the wording to \"log(\\delta) determines a Mahalanobis metric\", but \"determines\" means that log(\\delta) is a metric, whereas it seems to only *approximate* a metric.\n\nNote also that the proof contains the sentence \"the Mahalanobis distance should approximate\". It is quite odd to find the verb \"should\" in a mathematical proof. Ultimately, we are not sure what is being proved since the statement of the lemma is imprecise.\n\nFollowing up on this discussion, about the relationships between Eq. 4 and 6, the authors comment that \"While, the form of Eq.6 may suggest that it is an approximation to Eq.4, as discussed above, it is indeed the form of metric obtained in the latent space\".  I agree that, taken in isolation, this is a valid definition of some metric, but then, at the very least, the paper links this up with the rest of the discussion in a very confusing manner. Note that Eq. 6 is obtained \"using lemma 1\", which contains the world \"approximation\" in the proof.\n\nWhile none of these issues necessarily invalidates the *method* and the *results* of the paper, they make the *premises* a little tenuous and confusing.\n\nA second contribution of the paper is to incorporate KISSME in metric learning using deep networks and pairwise loss. Here the authors clarify satisfactorily the difference between standard metric learning in deep networks and their approach in their answers. I would recommend putting the comparison with the baselines upfront to make this message clearer to the reader (e.g. by moving Section 4.2 before Section 4.1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEq. (5) and (6) are supposed to \"reflect better\" Eq. (1) than Eq. (4). In fact, Eq. (6) is a regularised version of Eq. (4) (by the low-dimensional bottleneck), so it can only be a worse approximation of Eq. (1). Probably the authors mean that the regularisation can make Eq. (6) preferable to Eq. (4) on the test data.\n\nI would also like to understand better how the method is integrated in deep learning.  As noted by the authors,  with CNNs one can simply learn a projection matrix L as the last FC layer and define the metric to be L'L. Learning can use the usual Siamese scheme with pairwise or triplet losses. I am rather confused how the proposed algorithm relates to this approach according (Section 3). My understanding is that one first fixes L and trains the rest of the network optimising the pairwise loss Eq. (14). Once this is done, one updates L as the square root of the matrix M obtained by the modified KISSME algorithm. Is that correct? If so, the authors should compare to the obvious baseline of optimising L directly as part of back-propagation while training the network, as this is a much more direct approach.\n\nI am also confused about the dimensionality reduction role of matrix W. In Eq. (5) W^T is used to take the data down from dimension D to dimension d. However, when applied to a CNN, the dimensionality reduction is assumed to be incorporated in the network itself, such that the dimension of the CNN representation is d from the outset. Hence, in this case the modified KISSME algorithm does not need to perform any dimensionality reduction. Is that so? Can the authors give us an intuition of what their algorithm brings on top of standard Siamese learning in this case?\n\nExperiments:\n\nComparison with state of the art on the Car dataset is not very meaningful as every method appear to use a different underlying deep net. The paper uses GoogLeNet, which is significantly better than VGG-M used by Liu et al (2016). Hence it is not surprising that the authors obtain a better performance.\n\nI am more interested in understanding the comparison with the KISSME baseline. How is this baseline applied to this data? Is there any fine tuning of the CNN at all? If not, this may be enough to explain the difference with the proposed JDR-KISSME.\n\nThere is one last experiment on CIFAR after conclusion and references. Should we consider that as well?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678408, "id": "ICLR.cc/2017/conference/-/paper163/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678408}}}, {"tddate": null, "tmdate": 1481485268363, "tcdate": 1481485268357, "number": 3, "id": "BJhRjQoQe", "invitation": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer1"], "content": {"title": "Experimental comparison and scalability", "question": "Hi, \n\nit would be very useful if you could comment on the following two points, sorry for posting these only late.\nMany thanks\n\nIn the first experiment, the authors compare to related metric learning work. They  claim that the proposed approach \"achieves the state-of-the-art verification accuracies\", but this seems unjustified as the CNNs used to generate the features by the authors is different from the one used in related work. Thus it seems to me that no conclusions on the quality of the proposed approach as compared to others can be drawn from these experiments.\n\nThe main motivation of the paper to base itself on KISSME is scalability, see introduction. However, for the case where a CNN is trained it seems that it is likely to be in a large-scale data and iterative training regime, and it is not obvious that KISSME would be more efficient than other metric learning objectives based on a pairwise or triplet loss.\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481485268864, "id": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481485268864}}}, {"tddate": null, "tmdate": 1481252337320, "tcdate": 1481252337313, "number": 2, "id": "SkKgRcwXl", "invitation": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer2"], "content": {"title": "hard negative mining", "question": "As you describe in \u00a7 4, this isn't the focus of your work, but I'm wondering how you might incorporate best mining practices into your approach."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481485268864, "id": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481485268864}}}, {"tddate": null, "tmdate": 1480075851421, "tcdate": 1480075851416, "number": 1, "id": "SyQ8coBMg", "invitation": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "forum": "SJUdkecgx", "replyto": "SJUdkecgx", "signatures": ["ICLR.cc/2017/conference/paper163/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper163/AnonReviewer3"], "content": {"title": "Some questions on technical content and experiments", "question": "Technical content:\n\nThere are several imprecisions in the paper which I would like the authors to clarify:\n\nLemma 1 states that \"log(delta) identifies a Mahalanobis metric ... as M = Proj(...)\". While clearly the projection operator results, by definition of its range, in a metric, there is no clear definition of what \"identifies\" means. My best interpretation of \"identify\" is that log(delta) results in a quantity that is roughly similar to M.\n\nEq. (5) and (6) are supposed to \"reflect better\" Eq. (1) than Eq. (4). In fact, Eq. (6) is a regularised version of Eq. (4) (by the low-dimensional bottleneck), so it can only be a worse approximation of Eq. (1). Probably the authors mean that the regularisation can make Eq. (6) preferable to Eq. (4) on the test data.\n\nI would also like to understand better how the method is integrated in deep learning.  As noted by the authors,  with CNNs one can simply learn a projection matrix L as the last FC layer and define the metric to be L'L. Learning can use the usual Siamese scheme with pairwise or triplet losses. I am rather confused how the proposed algorithm relates to this approach according (Section 3). My understanding is that one first fixes L and trains the rest of the network optimising the pairwise loss Eq. (14). Once this is done, one updates L as the square root of the matrix M obtained by the modified KISSME algorithm. Is that correct? If so, the authors should compare to the obvious baseline of optimising L directly as part of back-propagation while training the network, as this is a much more direct approach.\n\nI am also confused about the dimensionality reduction role of matrix W. In Eq. (5) W^T is used to take the data down from dimension D to dimension d. However, when applied to a CNN, the dimensionality reduction is assumed to be incorporated in the network itself, such that the dimension of the CNN representation is d from the outset. Hence, in this case the modified KISSME algorithm does not need to perform any dimensionality reduction. Is that so? Can the authors give us an intuition of what their algorithm brings on top of standard Siamese learning in this case?\n\nExperiments:\n\nComparison with state of the art on the Car dataset is not very meaningful as every method appear to use a different underlying deep net. The paper uses GoogLeNet, which is significantly better than VGG-M used by Liu et al (2016). Hence it is not surprising that the authors obtain a better performance.\n\nI am more interested in understanding the comparison with the KISSME baseline. How is this baseline applied to this data? Is there any fine tuning of the CNN at all? If not, this may be enough to explain the difference with the proposed JDR-KISSME.\n\nThere is one last experiment on CIFAR after conclusion and references. Should we consider that as well?\n\n\n\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "", "abstract": "  ", "pdf": "/pdf/e4c133068cf851617ae25510b7a65f93df9c31e5.pdf", "TL;DR": "We propose: ", "conflicts": [""], "authors": [""], "authorids": [""], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481485268864, "id": "ICLR.cc/2017/conference/-/paper163/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper163/AnonReviewer3", "ICLR.cc/2017/conference/paper163/AnonReviewer2", "ICLR.cc/2017/conference/paper163/AnonReviewer1"], "reply": {"forum": "SJUdkecgx", "replyto": "SJUdkecgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper163/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481485268864}}}], "count": 10}