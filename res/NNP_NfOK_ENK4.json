{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1394240040000, "tcdate": 1394240040000, "number": 1, "id": "9hrpQySeBbhGc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "NNP_NfOK_ENK4", "replyto": "7hdD9NYlqQhbJ", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your feedback.  In response to your suggestions, we intend to make the following changes:\r\n  - reworking the abstract to clarify the problem and our contribution.\r\n  - expanding the number of non-linear function approximators used in simulation to demonstrate the generality of the problem (e.g., MLP and CART)\r\n  - the idea to use a neural network with a logistic output is a good one and we will incorporate this rather than derive the rLMS model, which distracts from the main point made in the paper\r\n  - also, the idea to simply break the reward prediction into positive and negative parts rather than predict all of the different rewarding and punishing outcomes makes a great deal of sense -- it will still eliminate the confusion (e.g., between reward predictors and omission of punishment predictors) and concurrently address one of the classical conditioning related concerns raised in another review.   This will move the paper away from the state-prediction focus, though some of this will still be relevant to discuss and relate to what is being done.\r\n - we will enhance the simulations to contain real values instead of binary values and vary the percentage of data points in the training set which expose the non-linear nature of the problem.  This makes the simulation a little more general, allowing us more data points to work with and the ability to do a few stats. This will also give a graph showing how the different approaches perform as we move from the ``partial'' (linear relationships only) training data set toward a ``full'' data set."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394239920000, "tcdate": 1394239920000, "number": 1, "id": "UUcR5oVeRuZb4", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "NNP_NfOK_ENK4", "replyto": "kkTqd3NaFyZaP", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your feedback.  In response to your comments, we offer the following:\r\n  1) I am unaware of the similar cases you speak of, where positive and negative contributions are divided and used with standard predictors.  Unfortunately, I have likely not responded in time to request references, though if possible, a reference or two would be much appreciated. We are enhancing our toy simulation based on your feedback and that of another reviewer (see response to the third anonymous review below).  Based on a suggestion from another reviewer, we will be replacing the MLE of a rectified linear function with a simple neuron having a logistic function output.  This will avoid making the derivation, which might distract readers from the main point, which is to highlight a problem we expect reinforcement learners will begin experiencing when providing high-level object representations of the world as input to a VFA where both significantly rewarding and punishing outcomes occur.\r\n   2) Your feedback on blocking when using different USs was quite insightful.  Based on the reference you provided I also discovered a few other relevant papers.  It would seem that there is significant empirical evidence (but see Blaisdell, Denniston, and Miller, L&M 28, 268-279 from 1997) to indeed suggest that changing the US in the second phase of a blocking procedure does not eliminate blocking.  The implication is that breaking up a reward prediction into separate predictions of individual rewarding outcomes is not the right thing to do.  However, in all of the experiments I found, the different USs within each are of the same valence (either both rewards or both punishments) to avoid counterconditioning effects.  A solution that better suits the evidence in these experiments and still makes our point is to break up the reward value prediction into two separate predictors instead of many:  a predictor for all rewards and a predictor for all punishments, where each predictor is rectified.  This would also seem a little easier to justify biologically, since we no longer need error signals for predictions of individual outcomes but rather only for two -- one for rewards and one for punishments.  In short, we plan to change our two-stage approach to group rewarding and punishing outcomes in the first stage instead of predicting individual outcomes and merely subtracting the punishment prediction from the reward prediction in the second stage.  This prevents our ability to allow satiety to directly influence the output value, however, and means that we will have to remove such discussion."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394239800000, "tcdate": 1394239800000, "number": 1, "id": "TvLXT3OmycZnz", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "NNP_NfOK_ENK4", "replyto": "bbRiuo3neGbJT", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your feedback.  We welcome the requested references and offer responses below to the specific issues raised in the review.  Briefly, we did not mean to focus on our specific approach to predicting future states.  Rather, we tried to show a specific problem faced by conventional value-function approximators (which learn to predict reward value rather than state) and how it can be avoided by incorporating state prediction in a straightforward way.  The simplicity of our simulations is deliberate, to clearly demonstrate the problem, showing that even non-linear value-function approximators cannot avoid it entirely.  From two other reviews of this paper, we have chosen to adjust our model away from state-prediction in general, more clearly demonstrating our main point, which I will discuss further below.  Nevertheless, I would like to address your comments specifically.\r\n\r\nFirst, it seems appropriate to briefly describe the contents of several papers mentioned in the review and how they relate to the present work. Unfortunately, I was not able to retrieve the two Schmidhuber papers that refer to the 'vector-valued adaptive critic' mentioned in the review.  I was, however, kindly pointed to another paper by Schmidhuber discussing the subject:\r\n\r\nJ.  Schmidhuber. Networks adjusting networks. In J. Kindermann and A. Linden, editors, Proceedings of `Distributed Adaptive Neural Information Processing', St.Augustin, 24.-25.5. 1989, pages 197-208. Oldenbourg, 1990. Extended version: TR FKI-125-90 (revised), Institut f\u00fcr Informatik, TUM.\r\n\r\nThe typical critic (i.e., a value-function approximator) learns to predict a single reward value.  In this Schmidhuber paper, a critic is presented that learns to predict multiple reward/punishment types and the associated prediction errors are backpropagated through a controller module to improve action selection.  Applied to a pole-balancing task, the critic's predictive targets reflect different types of pain, or ways in which the task can fail.  Schmidhuber found that using these multiple predictive targets allowed the controller to balance the pole with substantially fewer training episodes.  In Nguyen and Widrow (1989), the world state is represented by six variables reflecting position and orientation information of a truck backing up.  In their paper, a model is trained to predict the next state values with the ultimate goal of informing a steering controller that will minimize the final error (difference between the ideal final position and orientation of the truck and its location at the end of a series of backup cycles).  In Schmidhuber and Huber (1991), the state is represented by pixel intensity sums organized to coarsely emulate a fovea.  Here, a model is trained to predict the next state, that is, the pixel sums after a simulated eye movement.  Also, in this paper, the model is used to inform a controller and help direct movement effectively.  How do these three papers relate to the present work?  Neural networks such as those used in these papers to predict the future state could also be used in our approach as the first stage of our two-stage architecture, albeit with some minor changes.  In particular, the vector-valued adaptive critic learns to predict the same kinds of targets as our first stage.\r\n\r\nBut, the main point of the paper is not really about using a particular approach to predicting future states.  To clarify this, it may be helpful to highlight that our work assumes that the world state provided as input is represented as a vector of features, where each feature represents the degree of presence, or salience of a particular real-world object or stimulus.  This representation comes from the idea that cortical neurons become active in the presence of specific stimuli and to a lesser degree for others within a neighbourhood of similarity.  It also suits the way classical conditioning modeling conventionally represents stimuli.  A crucial aspect of this representation is that the features have a positive value.  This is a little different from the value-vector adaptive critic in the first paper, which had linear output units (no logistic activation function).  The importance of this becomes apparent when the reinforcement learning task, as in our simulations involves predicting both rewarding and punishing types of targets.\r\n\r\nThe primary novelty or contribution of our work is that the two-stage architecture which incorporates a state-prediction instead of a reward-prediction avoids a problem value-function approximators have in the world or state representation we have assumed.  Here it is:  the conventional 'state in, value out' value-function approximator or critic can mistake a feature that is strongly predictive of the omission of a reward for a punishment when it is presented alone, whereas the omission of an unexpected reward is really indicative of a neutral outcome (see paper text and previous comments for details).  Similarly, a feature predictive of omission of punishment can be mistaken to predict a rewarding outcome.  In contrast to value prediction, the architecture in the paper (in its present form) learns to predict specific rewarding or punishing outcomes (a portion of the total state).  The difference between this and the vector-valued adaptive critic is that our approach truncates predictions below zero, recognizing that rewarding or punishing outcomes may occur or not but never anti-occur (e.g., some food or no food, but not less than zero food).  Perhaps the vector-valued adaptive critic could be given logistic output units instead of linear ones to provide a similar truncation, but even so would only serve to function similarly.  The bottom line is that our work demonstrates with simple models that value-function approximators have this problem and important architectural features (truncated prediction of motivational outcomes rather than reward-prediction) that can resolve it. \r\n\r\nI hope this clarifies the focus of the paper.  Although we do presently introduce a linear state predictor that truncates negative predictions (soon to be replaced by a simple summation node with logistic activation function), it is mostly meant to assist in getting to our main point and could be replaced by other state-prediction techniques (and should be replaced if highly non-linear relationships are to be learned). Again, the main point is to highlight a problem seen in straightforward value-function approximation and offer a simple solution that involves truncated state prediction rather than reward prediction.  Our simplistic simulations (soon to be made more general) were designed to demonstrate this clearly."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394239680000, "tcdate": 1394239680000, "number": 7, "id": "YTciTSQEOoj4l", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The reviewers brought up very good comments so that we previously considered withdrawing the paper to have more time to work on it.  We now feel we that we are able to address the reviewer's concerns, especially given your recent indication of acceptance.  Therefore, we now retract our statement of withdrawal. Below we answer each of the reviews and will soon incorporate the associated changes into our paper.  We apologize for the lateness of our responses, but have only come to this decision recently.\r\n \r\nTo summarize the most significant proposed changes:\r\n - Adjusting the abstract and opening paragraphs to avoid implying that reinforcement learning does something in general that it does not do (see David's review and my response above)\r\n - Replace the partial state-prediction of the first stage of our architecture with a reward-prediction and punishment prediction learners, which are rectified.\r\n - Enhancing the simulations to include real values instead of binary only and to vary the number of data points that expose the non-linear nature of the problem.  Also, we will add two non-linear VFAs (MLP and CART) to show that the problem with VFAs is not specific to support vector regression.\r\n - Strengthening the discussion by providing greater distinction between the present work and previous work (as noted in the first anonymous review) and address the classical conditioning comments of the second anonymous review.\r\n\r\nThis represents a significant change in the substance of the paper, but the ultimate conclusions do not change.  The changes will result in focusing more squarely on our main point and strengthening the support for it through more generalized simulations.\r\n\r\nTake care,\r\n  - Patrick Connor (and Thomas Trappenberg)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392151320000, "tcdate": 1392151320000, "number": 6, "id": "nnW2J9gBITnGA", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "A sincere thank you to all of the anonymous reviewers, who have collectively raised several significant issues.  While, I believe that some of these issues could be addressed with little impact to the paper, a couple of them would require substantial enough additions that we have chosen to withdraw the paper at this time.  Your feedback has been very insightful and is appreciated.\r\n\r\nTake care,\r\n  - Patrick Connor (and Thomas Trappenberg)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391931300000, "tcdate": 1391931300000, "number": 5, "id": "7hdD9NYlqQhbJ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["anonymous reviewer 9fb5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "review": "Summary\r\nThis paper proposes a value function approximation architecture which explicitly models inhibition effects for reinforcement learning. In this context, inhibition refers to a stimulus that eliminates a reward when presented along with a stimulus which usually produces a reward. For example, the stimulus pair \u2018NP\u2019 does not produce a reward whereas \u2018P\u2019 alone does, and \u2018N\u2019 alone produces a reward of 0. The authors address the issue of value function approximators not correctly modeling \u2018N\u2019 in isolation having reward 0, as opposed to a negative reward. The authors propose decomposing the value approximation problem into two halves, one to estimate negative rewards and another to estimate positive rewards. The outputs of these functions are then combined to form a final value estimate. The authors derive a linear regression function approximator which rectifies its output to properly fit into the proposed two-stage scheme. The method is evaluated on a toy dataset of 16 examples which illustrates the problem.\r\n\r\nReview\r\nThe abstract is very hard to follow. Please rewrite it to more clearly describe the problem and your contribution.\r\n\r\nThe problem of confusing negative reward stimuli with inhibitors does not seem inherent to the reinforcement learning problem. Instead it seems to be a property of the chosen value function approximation algorithm. The authors should discuss, and perhaps test, whether a more expressive class of value function approximators exhibit this property. Given the conference, a neural network function approximator could be appropriate. Additionally, why not decompose the estimator into positive / negative reward estimates but use neural nets to approximate each half? They can naturally output rectified rewards without additional derivation. \r\n\r\nThe derivation equations are easy to follow, but their motivation is unclear. Please add more text discussing why it is necessary to make such derivations instead of choosing a different estimator\r\n\r\nThe experimental evaluation leaves much to be desired. The toy example is nice as a control experiment, but far from comprehensive. I would like to see a larger problem in which your approach performs better or discovers inhibition structure other value function approximators can not discover.\r\n\r\nKey points\r\n+ Proposed approach of decomposing value function approximation seems interesting and could lead to approximating functions which are easier to understand or debug\r\n- Unclear whether a more expressive class of value function approximators would fail to learn about inhibition stimuli automatically\r\n- Toy dataset evaluation\r\n- Writing is okay but does not clearly introduce the problem and motivate the approach"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391818740000, "tcdate": 1391818740000, "number": 4, "id": "kkTqd3NaFyZaP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["anonymous reviewer 1a31"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "review": "This paper proposes to break value prediction into prediction of a specific outcome (e.g., food) and a value for that outcome allows reinforcement learning to make the correct predictions for inhibitors -- as opposed to standard RL which predicts negative value for a reward inhibitor. In practice, this is implemented by separately predicting positive (reward) and negative (punishment) values using rectified predictors. Tests on a toy problem with 16 training points show that this setting indeed avoids the usual prediction of opposite valence for an inhibitor.\r\n\r\nInhibitors are a difficulty for RL and present an interesting puzzle, however this paper fails to make a good case:\r\n\r\n1) There is no real technical contribution here: separating positive from negative values while using standard predictors has been done often (e.g. in computer vision); the toy test is exceedingly simple.\r\nThe bulk of the technical part of the paper is a lengthy explanation of simple maximum likelihood estimation of a thresholded linear function.\r\n\r\n2) The paper is motivated as 'tak[ing] a cue from biological systems', but there are problems with this too:\r\n- 'we train function approximators to predict specific primary reinforcements (e.g., food, charging station, etc.).' This makes the job of an inhibitor easier indeed, but predicts that a function approximator should not be able to produce trans-reinforcer blocking, where a predictor trained on one specific primary reinforcement (e.g., shock) blocks conditioning to another reinforcement (e.g., loud noise). See:\r\nBakal, C. W., Johnson, R. D. & Rescorla, R. A. The effect of change in US quality on the blocking effect. Pavlov. J. Biol. Sci. 9, 97\u2013103 (1974).\r\n- 'we might take a cue from biological systems, where it seems that the dynamic revaluation of a primary reinforcer due to satiety is second nature': while this is true of primary reinforcers, this is not always true of secondary reinforcers, that RL needs to train as well. There is a lot of literarture on sensitivity to outcome devaluation, e.g.:\u00a0\r\nMotivational control of goal-directed action\r\nAnthony Dickinson, Bernard Balleine\r\nAnimal Learning & Behavior\r\nMarch 1994, Volume 22, Issue 1, pp 1-18\r\n\r\nIn its current form, the paper does not seem to offer enough for acceptance, in terms of either technical novelty or insight into natural behavior."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391444040000, "tcdate": 1391444040000, "number": 3, "id": "bbRiuo3neGbJT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["anonymous reviewer 9906"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "review": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning\r\nPatrick C. Connor, Thomas P. Trappenberg\r\n\r\nSummary: The authors discuss benefits of reinforcement learners with a supervised module that predicts primary reinforcements and a reinforcement learner which sums their values.\r\n\r\nQuite a few references to relevant previous work seem to be missing, e.g., see below.\r\n\r\np 2: 'The notion of predicting future observations or states is not new to RL [9, 10].'\r\n\r\nThese references are of 2004 and 2005. Two much older references from 1990 on this:\r\n\r\nR. S. Sutton. First Results with DYNA, an Integrated Architecture for Learning, Planning and Reacting. Proceedings of the AAAI Spring Symposium on Planning in Uncertain, Unpredictable, or Changing Environments, 1990.\r\n\r\nJ.  Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Proc. IEEE/INNS International Joint Conference on Neural Networks, San Diego, volume 2, pages 253-258, 1990.\r\n\r\np 6: 'the architecture we propose is not trained using a scalar reward prediction error, but rather a vector of state feature-speci\ufb01c prediction errors'\r\n\r\nSo it is like the vector-valued adaptive critic for multi-dimensional reinforcement signals of the old system from 1990 below - please discuss the differences: \r\n\r\nJ. Schmidhuber. Recurrent networks adjusted by adaptive critics. In Proc. IEEE/INNS International Joint Conference on Neural Networks, Washington, D. C., volume 1, pages 719\u2013722, 1990\r\n\r\nJ. Schmidhuber. Additional remarks on G. Lukes\u2019 review of Schmidhuber\u2019s paper \u2018Recurrent networks adjusted by adaptive critics\u2019. Neural Network Reviews, 4(1), 1990.\r\n\r\nBelow other old predictors of state feature-speci\ufb01c prediction errors. RL is based on training a recurrent system that combines the world model and the actor. Which are the relative advantages or drawbacks of the approach of the authors?:\r\n\r\nN. Nguyen and B. Widrow. The truck backer-upper: An example of self learning in neural networks. Proceedings of the International Joint Conference on Neural Networks, 357-363, IEEE Press, Piscataway, NU, 1989\r\n\r\nJ. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2(1 & 2):135-141, 1991\r\n\r\nExperiments: this is a very simple RL task. \r\n\r\nGeneral recommendation: It is not quite clear to this reviewer how this work goes beyond the previous work from 20 years ago mentioned above. At the very least, the authors should make the differences very clear."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390610280000, "tcdate": 1390610280000, "number": 2, "id": "yyQCyIciZgy_J", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["Patrick Connor"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hi, David.  It's wonderful to have this venue for receiving feedback and being able to discuss our work.  I think I understand your feedback and that some of the early statements of our paper have unfortunately distracted from its main theme.  We agree with you that Reinforcement Learning in general is not confined to attributing a positive or negative value to each contributing feature of a state, such that the state's apparent value is always the linear sum of their effects.  Rather, it encompasses more than this.  We did not intend the received meaning, though we can see the value of making minor textual changes to clear this up.\r\n\r\nThe problem we find, though, is that given data which suggests linear contributions of individual features (the 'partial' dataset in our simulations), value function approximators (VFAs), whether linear (LR-P results in the paper) or non-linear (SV-P results), treat the features as linear, which comes as no surprise.  Essentially we want to highlight a real world case where making this 'linear assumption' is actually inappropriate, since it will add to prediction errors.  \r\n\r\nWhereas features/predictors may be associated with either reward, punishment or neither (whether linearly as individuals or in non-linear combinations as in XOR), features can also be associated with cancelling these otherwise predicted outcomes.  That is, some features can predict outcomes and others can cancel those predictions.  We show that linear and non-linear VFAs will see canceling features as having the opposite effect (say, value=-1) on the state-value as do the features which predict the associated outcomes.  Then, when a predictor (value=1) and a cancelling feature (value=-1) are presented together, a neutral outcome (1-1=0) is expected.  But what happens when these canceling features are presented alone, having no prediction to cancel?  According to the VFAs, they are not neutral but still have their opposing effect if they have not been trained on this case (i.e., value = 0 - 1 = -1).  This mistaken sense of opposing value adds to the prediction error.  Ideally, one would have the 'full' dataset, which tells us the non-linear nature of such cancelling features, but we are unlikely to have it all (or most?) of the time.\r\n\r\nWe bring all of this up in the paper because we see a simple way to avoid the problem when only the 'partial' (i.e., 'linear') dataset is available.  We change the representation or the prediction target from reward-value to the outcomes of interest (e.g. food, shock, etc.).  Then, when a canceling feature of an outcome is presented alone, it's opposing (-1) contribution can be truncated (i.e., -1 -> 0) since the outcome will never be less than 0 (e.g., there will never be less than zero shock).\r\n\r\nIt is true that we do not show evidence for other ways to correct for or weaken the 'linear assumption' here, because we have not seen any previous acknowledgement of this issue, and thus no attempts at solving it as of yet.  I wonder if this might be because having enough data (i.e., the 'full' dataset) makes the problem moot, just as having enough supervised data (i.e., an exhaustive mapping) can make the use of a deep learning representation moot, for example.  Instead, I think the most important evidence we show is that even non-linear VFAs are subject to treating the partial, linear data in a linear fashion, which is inappropriate in this case.\r\n\r\nIn summary, we recognize the need to do some rephrasing in the early part of the paper regarding the linearity of feature contributions to state-value.   Yet, we focus on showing that when the data does suggest linear contributions, they sometimes represent a specific non-linear case that VFAs fail to recognize but that can be easily accommodated without error by changing the prediction target from reward-value to outcome.  We are the first to acknowledge this as far as we know, and thus offer the only existing way to avoid accruing the associated VFA prediction errors.  \r\n\r\nTake care,\r\n   - Patrick"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389919920000, "tcdate": 1389919920000, "number": 1, "id": "lQb2ljSA2gMmE", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NNP_NfOK_ENK4", "replyto": "NNP_NfOK_ENK4", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I believe this paper mis-characterizes the Reinforcement Learning (RL) problem and the problem it purports to solve is, in fact, not a problem at all in a general reinforcement learning context.  \r\n\r\nIn particular, the first sentence of the abstract states: \r\n'Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value', but this is simply not true in general for reinforcement learning.\r\n\r\nIn the most general setting, the reward at each step t+1 (r_{t+1}) is calculated as a function of the state at time t, the state at time t+1, and the action at time t.  So r_{t+1} = f(s_t,a_t,a_{t+1}).  Even if we specify the problem somewhat so that reward is only a function of the state (r_{t+1}  = f(s_{t+1}), this is still a general enough framework to incorporate arbitrary interactions between different variables that compose the state.  \r\n\r\nIn contrast, this paper assumes linear interactions between at least some subset of variables that specify the state.\r\n\r\nSuccinctly, it is not each input, feature, or stimulus that has a positive or negative reward, rather, it is the totality of the environment's state that has an associated reward.  \r\n\r\nI am not aware of common practices in RL, so it may be that a linearity assumption such as the authors have made is commonly used in practice.  And it may be that their approach to weakening this assumption could be useful in practice, or theoretically.  But I find the evidence presented extremely unconvincing on this point.  Primarily this is because there is no presentation of other approaches to weakening the imposed linearity constraint."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387563120000, "tcdate": 1387563120000, "number": 17, "id": "NNP_NfOK_ENK4", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "NNP_NfOK_ENK4", "signatures": ["patrick.connor@dal.ca"], "readers": ["everyone"], "content": {"title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "decision": "submitted, no decision", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "pdf": "https://arxiv.org/abs/1312.5714", "paperhash": "connor|an_architecture_for_distinguishing_between_predictors_and_inhibitors_in_reinforcement_learning", "keywords": [], "conflicts": [], "authors": ["Patrick C. Connor", "Thomas P. Trappenberg"], "authorids": ["patrick.connor@dal.ca", "tt@cs.dal.ca"]}, "writers": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 11}