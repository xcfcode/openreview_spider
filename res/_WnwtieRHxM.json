{"notes": [{"id": "_WnwtieRHxM", "original": "LG3oPplWXSW", "number": 2906, "cdate": 1601308322333, "ddate": null, "tcdate": 1601308322333, "tmdate": 1616045816975, "tddate": null, "forum": "_WnwtieRHxM", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1EWneJj9_7", "original": null, "number": 1, "cdate": 1610040491520, "ddate": null, "tcdate": 1610040491520, "tmdate": 1610474097441, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The paper studies the effect of importance weighting schemes on the implicit bias of gradient descent in deep learning models. It provides several theoretical results which give important insights on the effect of the importance weighting scheme on the limit of the convergence, as well as convergence rates. Results are presented for linear separators and deep learning models. A covariate shift setting is also studied. The theoretical results are supported with empirical demonstrations, and also lead to useful insights regarding which weighting schemes are expected to be more helpful. They also explain some previously observed empirical phenomena. \n\n\nPros:\n- New theoretical results which provide important insights on an important topic\n- Empirical demonstrations which support the theoretical results\n\n\nCons:\n- No significant issues. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491506, "tmdate": 1610474097425, "id": "ICLR.cc/2021/Conference/Paper2906/-/Decision"}}}, {"id": "x_YPOW0q0Qb", "original": null, "number": 11, "cdate": 1606040342408, "ddate": null, "tcdate": 1606040342408, "tmdate": 1606055765659, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "1jTxPHb1H2", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "Good answer from the reviewers", "comment": "I closely looked at reviewer's comments and it is more clear now. \nConsequently, I think the paper deserves to be accepted for publication and I change my score from 5 to 7. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer4"], "readers": ["ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer4", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "Rg68wP-ai3V", "original": null, "number": 1, "cdate": 1603351959185, "ddate": null, "tcdate": 1603351959185, "tmdate": 1606055736418, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review", "content": {"title": "Interesting problem but the paper is not clea", "review": "It is now well-understood that when the data are linearly separable,  gradient descent over the linear class of functions converges toward the hard margin solution. It highlights the implicit bias of gradient descent. Among all solutions interpolating the dataset, gradient descent selects the one with larger margin, partly explaining why over-parametrized models may generalize.  The picture for non-linear class is a little bit more complicated. \nIn this paper, the authors study the impact of importance weighting on the implicit bias of gradient descent for both linear and non linear predictors. The problem is interesting and non trivial because importance weighting may affect the geometry of the gradient descent. In particular, a natural question is the following: does the importance weighting affect the margin of the solution (and thus change its generalization properties ?). Note that the authors does not clearly define what is the goal of the paper and to which problem there are trying to answer\n\n\nHere are some questions and remarks about the paper:\n\n- You study importance weighting for neural-networks: is it used in practice for these class of functions ? Could you give some references ? \n\n- The introduction is not clear. For example the first paragraph aims to define importance sampling. Introduce a formal definition. You sentence \"Importance weighting is a standard tool used to estimate a quantity under a target distribution while\nonly the source distribution is accessible\" is not clear at all.  Then, you present exploratory ideas before clearly presenting the contributions of the paper. We are completely lost ... You should clearly define what is the goal of the paper and how you achieve it. I had to read 3 times before understanding what you really wanted to do \n\n- Do you focus only on the binary case ? It is one again not clear \"For the sake of notation, we mostly study the binary setting\", if so just say it. \n\n- You use concepts that you do not define clearly: what does mean that $\\mathcal D$ is separated by $f(\\theta^{t},x)$ at some point $x$ ? Do you mean that there exists $\\theta^{t}$ such that  $y_if(\\theta^{t},x_i) \\geq 0$ ? \n\n- The paragraph after claim 1 is not clear at all (and you forgot the exponent $\\alpha$ on the norm of $\\theta$). I really don't get what you are trying to say here ... \n\n- There are problems with your bibliography (page 3 paragraph beginning section 3, page 12 paragraph beginning A.1.3 for examples)\n\n- Sometimes you write $\\|\\cdot \\|$ and sometimes $\\|\\cdot \\|_2$. Keep the same notation along the paper \n\n\nThe proofs of the paper are a little bit hard to follow but seem to be correct. I also have an other question. Would you be able to generalize the analysis without the condition $w_i \\in [1/M,M]$ ? This condition is restrictive and we cannot use $w_i = 0$ which can be very useful for robust purposes. So is it possible to look at the condition $w_i \\leq M$ instead ? It is, in my opinion, an interesting question.\n\n\nTo summarize, the paper tackles an interesting problem (but in the restrictive setting $w_i \\in [1/M,M]$)? However it paper is poorly written and not clear at all. I had the feeling that the authors were completely in a rush and did not organize the paper. Instead they put ideas together, without a clear common thread. I think you should focus on the form of the paper and then resubmit it for publication. There are too many problems right now. Work on the clarity of your ideas. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086239, "tmdate": 1606915797859, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2906/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review"}}}, {"id": "KOjbqOgKq1b", "original": null, "number": 10, "cdate": 1605992281284, "ddate": null, "tcdate": 1605992281284, "tmdate": 1605992585593, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "DM9vBTPa9pf", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "Thanks for the clarifications", "comment": "I have read the other reviews and responses. I recommend for acceptance and did not change the score."}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "m-bC9FP51ty", "original": null, "number": 5, "cdate": 1605638958247, "ddate": null, "tcdate": 1605638958247, "tmdate": 1605638958247, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "v4xWZR4rFyu", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "We thank the reviewer for careful reading and providing feedback", "comment": "We want to thank the reviewer for the careful reading of our manuscript and providing the feedback!\n\nWe have made minor modifications to the manuscript to address the issues pointed out by the other reviewers."}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Paper2906/Reviewers", "ICLR.cc/2021/Conference/Paper2906/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "SUHSnLE6HLS", "original": null, "number": 4, "cdate": 1605638907987, "ddate": null, "tcdate": 1605638907987, "tmdate": 1605638907987, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "y14gOftjhT", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "We thank the reviewer for careful reading and providing feedback to our manuscript", "comment": "We want to thank the reviewer for the careful reading of our manuscript and providing the feedback! \nWe apologize for the typos and unclear writings, and we have corrected them in this version of the paper.\n\nWe have also made minor modifications to the manuscript to address the issues pointed out by the other reviewers."}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Paper2906/Reviewers", "ICLR.cc/2021/Conference/Paper2906/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "1jTxPHb1H2", "original": null, "number": 3, "cdate": 1605638761004, "ddate": null, "tcdate": 1605638761004, "tmdate": 1605638761004, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "Rg68wP-ai3V", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "We hope that we address the concerns of the reviewer", "comment": "We thank the reviewer for providing feedback to our manuscript. We apologize for the typos and clear writing, and we have corrected them in this version of the manuscript.\n\nWe wish first to clarify that our research questions (Q1, Q2 and Q3) are defined clearly in Section 2. We believe that for the sake of readability to the general readers, it is necessary to first provide the background (on implicit bias, NN generalization, importance weighting, etc.) before stating our key research questions.\n\nWe summarize our response to the individual remarks as below:\n\nRemark 1. \"You study importance weighting for neural-networks: is it used in practice for these class of functions? Could you give some references ?\"\n\nWe kindly remind the reviewer that we have provided a number of references that use importance weighting for deep learning models, in the first two paragraphs in Section 1, and Section 5. \n\nRemark 2. \"The introduction is not clear.\"\n\nWe think the first sentence in the first graph of Section 1 is quite standard to introduce importance sampling, see [3].\n\nOur story line is as follows: \na. Introduce the background of importance weighting, and demonstrate it is an active research topic.\nb. Introduce the existing importance weighting methods.\nc. Emphasize there lacks theoretical understanding of importance weighting:\n    Introduce the first result that studies the impact of importance weighting [3] from empirical observations;\n    Introduce implicit bias of GD, to which [3] connects the impact of importance weighting.\nd. State that we fill in the theoretical gap to understand importance weighting. Also provide a few directions that are worthy of further investigations.\n\nWe think this story line is clear and we have already stated in the third and fourth paragraphs of Section 1 that our goal is to give a theoretical understanding of importance sampling in terms of the implicit bias of gradient descent as well as the generalization performance. The concrete research goals are defined as Q1, Q2, Q3 in Section 2.\n\nRemark 3. \"Do you focus only on the binary case? It is one again not clear \"For the sake of notation, we mostly study the binary setting\", if so just say it.\"\n\nExtending our results to the multi-class case using the cross-entropy loss is straightforward consequence following the arguments in [1] and [2]. However, denoting the problem setup and providing visual illustration for multi-class classification require adding more notations that are unnecessary for our purpose. We do not think there is any overclaim in this case.\n\nRemark 4. \"You use the concept \"separable\" that you do not define clearly.\"\n\nWe kindly remind the reviewer that we define \"separable\" clearly in assumption A.1. \n\nRemark 5. \"The paragraph after Claim 1 is not clear at all.\"\n\nThe paragraph after Claim 1 is essential for the general audience to understand our work because it reveals the connection between norm divergence (Claim 1) and the max-margin solution. We believe our statement and message are clear. \n\nRemark 6. \"There are problems with your bibliography\"\n\nWe thank the reviewer for pointing out this issue. They have been corrected.\n\nRemark 7. \"The $\\|\\cdot\\|_2$ norm notation is not consistent.\"\n\nWe kindly remind the reviewer that we mentioned in the very beginning of the Preliminary section that we use $\\|\\cdot\\|$ to denote the $\\|\\cdot\\|_2$ norm when no confusion arises. \n\nFinally, we point out that our results do not depend on the lower bound of the weights, so technically speaking, we only require the weights to be bounded in [0,M]. However, when the importance weights are defined by the target and source distribution such as in the sample bias correction and domain adaptation, i.e. $w(x) = P_t(x) / P_s(x)$, the meaning of $w=0$ is unclear. Also, in the counterfactual machine learning where the importance weights are defined by the inverse of propensity score, i.e. $w(x)=1/\\text{propensity}(x)$, having $w \\in [1/M,M]$ is a common regularity condition. We decode to keep our conditions consistent with the above applications of importance weighting.\n\nWe hope we have addressed the concerns of the reviewer, and again we thank the reviewer for the effort and time. \n\n[1]. Soudry, Daniel, et al. \"The implicit bias of gradient descent on separable data.\" The Journal of Machine Learning Research 19.1 (2018): 2822-2878.\n\n[2]. Wei, Colin, et al. \"Regularization matters: Generalization and optimization of neural nets vs their induced kernel.\" Advances in Neural Information Processing Systems. 2019.\n\n[3]. Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In\nInternational Conference on Machine Learning, pp. 872\u2013881, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Paper2906/Reviewers", "ICLR.cc/2021/Conference/Paper2906/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "DM9vBTPa9pf", "original": null, "number": 2, "cdate": 1605638524852, "ddate": null, "tcdate": 1605638524852, "tmdate": 1605638524852, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "GohpxQSVEHd", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment", "content": {"title": "We thank the reviewer for careful reading and providing valuable feedback", "comment": "We want to thank the reviewer for the careful reading and providing valuable feedback, in particular for the comments which we believe will help us bring a better version of this paper.\nWe summarize our response to the review's comments as below.\n\nComment 1. \"Under which condition is it better to use weighted ERM and not ERM. How can this be derived from the analysis?\"\n\nThere should be no doubt that the weighted ERM is not worse than ERM if appropriate weights are used. The goal of this work is to understand how importance weighting impacts ERM. To this end, we wish first to distinguish the two scenarios:\n(a). the weights are defined by the problem, e.g. the propensity-weighted learning and domain adaptation;\n(b). the weights are the artifact introduced to improve the optimization (training) and generalization performance, e.g. we give the corrupted samples smaller weights. \n\nFor scenario (a), our results establish the optimization and generalization guarantee, in terms of to which local minima will the GD converge, and how that specific local minimum can generalize. \n\nFor scenario (b), our results suggest that (using the linear classifier as an example):\n(1). When the data is separable, by selecting the weights according to the (inverse) margin, the convergence to the max-margin solution can be made faster. \n(2). When the data is not separable, for the separable part of the data, the arguments in (1) apply; and for the non-separable part, the weights uniquely determine the intercept (shift) of the final solution on the non-separable data. \nIn the second case and the first case where the convergence is not reached by finite-step optimization, our arguments in Sec 4.2 characterize the tradeoffs in terms of the weights selection.  \nHence, the potential benefit of weighted ERM can depend how the weights are selected, e.g. if they can represent the hard-to-classify degree. \n\nComment 2. \"The discussion of the generalization results in Section 4 is not clear.\"\n\nHere, the generalization performance depends on both the margin on the training data (sampled from the source distribution) and the discrepancy between the source and target distribution. Therefore, Prop 3 does not account for the discrepancy part, so it cannot imply that weighted ERM and non-weighted ERM have the same generalization performance.  Also, Prop 3 shows the invariance of margin only after convergence in the asymptotic sense, while in practice, the finite-step optimization is more realistic. In the finite-step setting, the weights can affect the margin of the solution so the generalization can differ from the non-weighted outcome. \nWe are also very interested in the connection between Fig 2c and 2d pointed out by the reviewer. According to our preliminary study, explaining the phenomenon thoroughly would require notions other than margin to characterize other aspects of the solution. Unfortunately, we do not find a good way to incorporate our findings to this paper because most of our arguments are related to margin. Hence we decide to pursue it as a new research topic.  \n\nComment 3. \"In the introduction, the contribution of the paper is not so clear.\" \n\nWe agree that the contributions can be summarized more clearly, and the relation to Byrd et al. (2019) can be strengthened. We will add them to the final version of the paper. In terms of the exploratory topics, we hope that they could connect to the audience from other domains and motivate follow up work.\n\nComment 4. \"In Section 3, it is claimed that the exponential loss is used WLOG. Does this mean that the results also hold for other losses?\"\n\nOur results hold for all the loss functions that have the exponential-tail behavior we summarized in Appendix A.1.1. We understand it is not mandatory for the reviewer to check the appendix, so we point it out here that they include: exponential loss, log loss and cross-entropy loss.\n\nComment 5. \"It is claimed that Fig 2b shows better performance but this figure only shows the margin. Is the test loss also better?\"\n\nIn our experiments, the test loss is indeed better when the solution has a smaller margin. However, we do not expect the relationship to hold in the general cases, so we do not present the results in our manuscript to avoid confusion.\n\nComment 6. \"In some cases, it seems that the informal claims made by the authors are too strong.\"\n\nWe agree that we only study the major scenarios and certain cases are not covered, so we change the statement of \"complete understanding\" in our paper to \"in-depth understanding\" to describe our work more precisely. As for the claim that ``$\\theta_{sep}$ in the separable region does not depend on w'', we would like to argue that it is indeed a straightforward consequence of Prop 1. Finally, we thank the reviewer for pointing out that the matching lower bound for NN complexity is still under active research, so we change the statement to \"the role of margin for NN generalization is being explored actively\". "}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Paper2906/Reviewers", "ICLR.cc/2021/Conference/Paper2906/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_WnwtieRHxM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2906/Authors|ICLR.cc/2021/Conference/Paper2906/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Comment"}}}, {"id": "GohpxQSVEHd", "original": null, "number": 2, "cdate": 1603892692167, "ddate": null, "tcdate": 1603892692167, "tmdate": 1605024106558, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review", "content": {"title": "Novel results for weighted ERM and nonlinear models. The main messages of the paper can be improved.", "review": "\n### Summary\n\nThis paper studies the inductive bias of gradient descent (GD) on smooth non-linear models when optimizing a weighted ERM. The authors provide several novel results for the linear and non-linear model cases. For linear models and linearly separable data, they show that GD converges to the hard-margin SVM solution and the convergence rate upper bound is lower for weighted ERMs that have higher weight on low margin points. They further characterize the inductive bias for non-linearly separable data, on a unique non-linearly separable subspace defined by Ji and Telgarsky (2018). For nonlinear models they consider a weak regularization setup. They show that asymptotically GD converges to a max margin predictor, which is similar to the non-weighted ERM case. They prove a generalization bound for weighted ERM and together with experiments provide insights on the generalization performance of GD in this case.\n\n### Reason for score\n\nThe paper provides several novel theoretical results in a practical setting. The proof techniques might be useful for analyzing non-linear models in other settings. Although the writing and main \"take-home\" messages of the paper can be improved, overall I recommend for acceptance.\n\n### Pros\n1.\tNovel theoretical results in several challenging settings (linear models with non-separable data and non-linear models).\n2.\tAnalysis in a practical setting of weighted ERM.\n3.\tNovel theoretical insights that are corroborated with experiments.\n\n\n### Cons\n\n1.\tAlthough the authors provide several insights on weighted ERMs, it is difficult to understand the main takeaways from the analysis. Specifically, under which conditions is it better to use weighted ERM and not ERM. How can this be derived from the analysis?\n2.\tThe discussion of the generalization results in Section 4 is not clear. The first result shows that asymptotically GD obtains the same margin in the case of non-weighted and weighted ERMS. Therefore, this *may* hint that they have the same generalization performance. In contrast, the discussion after Theorem 1 suggests that weighted ERM can have better generalization performance. This should be clarified. I think that Figure 2c and Figure 2d are very interesting because they show the following: for unbalanced data, GD on weighted ERM and ERM obtains roughly the same margin, but it has different generalization performance in these cases. I think that the authors should discuss this and it may strengthen the insights provided by the analysis.\n3.\tIn the introduction, the contribution of the paper is not so clear. For example, \"characterize the impact of importance weighting\u2026\" is not clear. \"We propose several exploratory topics\u2026\" \u2013 this seems like a minor contribution. I think that the authors should summarize the theoretical results and say how they relate to the observations of Byrd et al. (2019).\n4.\tIn Section 3, it is claimed that the exponential loss is used WLOG. Does this mean that the results also hold for other losses? This is not clear.\n5.\tIt is claimed that Figure (b) shows better performance but this figure only shows the margin. Is the test loss also better?\n6.\tIn some cases, it seems that the informal claims made by the authors are too strong. For example, in page 5 it is said that \" we provide a complete understanding\", but the inductive bias of GD is not shown on the separable space in the case of non-linearly separable data (only for the non-separable space). In page 5, it is said that theta_sep in the separable region does not depend on w, although this is not shown formally. In page 4, it is said that the pivotal role of the margin for generalization of NNs is well understood. However, we are currently far from understanding generalization in NNs and the cited papers provide only *upper* bounds on the generalization error and might be very loose.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086239, "tmdate": 1606915797859, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2906/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review"}}}, {"id": "y14gOftjhT", "original": null, "number": 3, "cdate": 1603895491292, "ddate": null, "tcdate": 1603895491292, "tmdate": 1605024106485, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review", "content": {"title": "Theoretical and empirical explanations of the role of importance weighting for (deep) learning models", "review": "Summary:\n\nThis paper proposes a theoretical explanation of the role of importance weighting with regards to the implicit bias of gradient descent (convergence to the same direction as the maximum-margin solution) and the generalization ability of the model. \n\nThe authors first extend the norm divergence result to a general setting where weak regularization is used. \n\nFor linear predictor and separable data, they show that importance weighting affects the convergence speed during gradient descent but not the convergence result or the $1 / log(t)$  rate. In particular, they find an expression of the constant term resulting from the importance weighting. Thus, using an \u201cinverse margin weighted\u201d method, the authors are able to accelerate gradient descent in a finite-step optimization setting. \n\nFor linear predictor and non-separable data, the authors show that importance weighting uniquely defines the solution on the non-separable subset, which can be seen as an intercept/constant term. The weights only control how the constant shifts on the non-separable data subset. \n\nFor non-linear predictor, they show that the optimal margin is reached regardless of the importance weighting choice under the infinitesimal weak regularization setting. They also show that in a finite-sample setting, importance weighting affects the generalization bound via the empirical risk and a term depending on the model complexity and the deviation between target and source distributions. The impact of importance weighting on the generalization ability of the model is also shown empirically. \n\nOverall, the authors show that importance weighting can affect how fast the model separates the data and how fast the model converges to the max-margin solution for linear and non-linear predictor in some settings. They also deduce that giving more weights to the hard-to-classify points, corresponding to the small-margin samples, when used in importance weighting, is very important for the acceleration of the optimization. The authors conjecture and show empirically that the results still holds if the importance weights are jointly learned with the model\n\n\nPros:\nThe paper seems to offer important theoretical results and empirical validations regarding the role of importance weighting on the implicit bias of gradient descent and the generalization ability of linear and non-linear models in some settings.\nThe paper is clear and well written. There is a good balance of theoretical findings and empirical validations.\n\nCons: \nThis paper does not seem to have any major weaknesses. \n\n\nMinor Comments:\n-\tThere seems to be a typo in the 3rd sentence of the 2nd section: \u201cfrom the which\u201d should probably be \"from which\"\n-\tThere seems to be a typo in the 2nd paragraph of page 4: \u201cto understand the role of importance weighting\u201d (\"of\" is probably missing)\n-\tThere seems to be a typo at the top of page 5: \u201cis able to accelerate\u201d (\"to\" is probably missing)\n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086239, "tmdate": 1606915797859, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2906/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review"}}}, {"id": "v4xWZR4rFyu", "original": null, "number": 4, "cdate": 1604095747565, "ddate": null, "tcdate": 1604095747565, "tmdate": 1605024106423, "tddate": null, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "invitation": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review", "content": {"title": "Useful insights into importance weighting ", "review": "The paper studies the effect of importance weighting schemes in deep learning models. A deep learning setting is considered where the empirical loss of labeled training data is weighted with importance weights and regularization on the network parameters is also included in the objective, which is optimized with gradient descent. \n\nTwo main results are presented in the paper. The first result focuses on a linear prediction scheme, in which case the convergence is shown to be faster if the weights of the samples are matched with the inverse of their SVM margin. An extension of this result is also proposed for the case where the data is not linearly separable.\n\nThe other main result considers multiple-layer feedforward networks in a covariate shift setting. A generalization bound is presented, which relates the probability of error to the agreement between the importance weights and the deviation between the source and the target distributions. The interesting takeaway message from this analysis is that aligning the weights with the distribution deviation reduces the error.\n\nOverall, the paper is clear and well written, and it provides some interesting insights into several common learning settings from the perspective of importance weighting. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2906/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2906/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the role of importance weighting for deep learning", "authorids": ["~Da_Xu2", "yeyt@berkeley.edu", "~Chuanwei_Ruan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan"], "keywords": ["Importance Weighting", "Deep Learning", "Implicit Bias", "Gradient Descent", "Learning Theory"], "abstract": "The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.", "one-sentence_summary": "We study the theoretical properties of importance weighting for deep learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|understanding_the_role_of_importance_weighting_for_deep_learning", "supplementary_material": "/attachment/3029f9503041428cf47848d65aaa18abf8751c2d.zip", "pdf": "/pdf/c52af0c839abd3f9cd52515da1042deed9eb2692.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxu2021understanding,\ntitle={Understanding the role of importance weighting for deep learning},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_WnwtieRHxM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_WnwtieRHxM", "replyto": "_WnwtieRHxM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2906/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086239, "tmdate": 1606915797859, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2906/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2906/-/Official_Review"}}}], "count": 12}