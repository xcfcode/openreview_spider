{"notes": [{"id": "D4A-v0kltaX", "original": "ssXXgcy3ctw", "number": 2897, "cdate": 1601308321291, "ddate": null, "tcdate": 1601308321291, "tmdate": 1614985754512, "tddate": null, "forum": "D4A-v0kltaX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mgQF7117qbn", "original": null, "number": 1, "cdate": 1610040380272, "ddate": null, "tcdate": 1610040380272, "tmdate": 1610473973209, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The objective of the paper is to develop a framework for solving PDES with reduced model size and for scarce observation settings. It proposes to use functional input dependent convolutions for learning spatio-temporal differential operators together with a non linear numerical scheme (Picard solver). Training makes use of an adjoint formulation.\n\nAll the reviewers agree that the authors improved the initial version but opt for a reject. In its present form, the technical description is still incomplete with missing explanations. The experiments should be reinforced and the results are partly unexplained."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040380259, "tmdate": 1610473973190, "id": "ICLR.cc/2021/Conference/Paper2897/-/Decision"}}}, {"id": "FhVjYOZu-bp", "original": null, "number": 3, "cdate": 1603882004402, "ddate": null, "tcdate": 1603882004402, "tmdate": 1607034179813, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review", "content": {"title": "Paper needs rewriting to be understandable & reproducable", "review": "Summary:\nThe paper proposes a neural network based solver for PDEs based on the Picard Iteration. In the numerical experiments section the paper applies the method to solve 1d or 2d PDEs.\n\nDisclaimer:\nI am not an expert in PDE solvers but I am researching topics related to your frequently cited works of Kipf et. al., Battaglia et. al. & Raissi et. al.. Therefore, my knowledge on PDE solver might be a limiting factor to understand the complete work.\n\nReview:\nFor me personally it is really hard to parse the paper and get a rough understanding of the paper. I cannot fully understand the application, why it should be better to numerical solvers, how it roughly works, what kind of training data is used. By now I have reconstructed the genreal idea from the details presented but I am uncertain about whether my understanding is correct and it is so limited that reproduction of the algorithm is not possible. Furthermore, the reader should not be required to reconstruct the idea from a paper from the details.\nTherefore, an evaluation of the contribution and significance is not really possible for me. As I cannot **even** get a rough understanding from reading the paper and googling related work / textbooks, I think the paper needs to be rewritten to make it suitable for the ICLR community. If I understood the approach partially correctly, I think that the idea is really neat, interesting and novel and gave me a new perspective. However, the experiments also focus on well posed standard problems and not on the more complex problems of multi-body contact right mechanics which is targeted with many of the referred works.\n\nIn the following, I will state my main questions regarding the understanding.\n\n\nSummary & Questions for each section:\n\n(1) Introduction:\nFor me the introduction is missing to state the clear problem statement and the available information to solve the problem. What kind of application do you have in mind? The paper cites a lot of graph networks from Battaglia et. al. or Kipf et. al. but I cannot relate your problem to the works of these authors. I guess you also want to learn a simulator but then the experiments just focus on solving very small PDEs. Besides the main motivation I am missing an exact mathematical problem statement. My guess is that you want to solve for x(p) given the differential equation described by \\partial x / \\partial p = f(p, x(p)). Now the question for me is, what is known / unknown about the differential equation? E.g.,\n* Do I assume to know the structure of f but not the exact parameters?\n* Do I know f but not the solution x(p)?\n* Do I know x(p) at some points and nothing about f?\n\nMy guess is the last, but I am uncertain. Following a precise problem statement, I would like to see the general approach how to solve the problem and how this is currently done. For this paper my guess is that the standard approach to obtain the PDE solutions is to derive the filter C manually from the analytical f. Then one can solve for the solution of x(p), by convolving C with the mesh of x_k obtaining A and obtaining new x_{k+1} by solving A x_{k+1} = b. Is that correct? Furthermore, I am uncertain about the definitions of b and x. I guess x is a vectorized mesh of the domain of x. Such an short theoretical overview of standard approaches would be very helpful as the google results of picard iteration lead to very different symbolic approaches.\n\n(2) Method:\nI would like to understand the general algorithm first and then deep dive into the details. The current version just presents the details but not the overall approach. Hence the reader has to reconstruct the main approach from the details, which is really cumbersome. My current understanding of your approach is that you start with a random neural network for C and perform the picard iteration with the random C. At the end of the picard iteration you compute the MSE of the predicted solution and the observed solution and update the filter C to minimize the mse at the end of the picard iteration. To compute the derivatives about the iterative picard solver you use the adjoint method. I am not certain if I understood it correctly or not.\n\n(3) Network Architecture:\nThe paper uses a uncommon optimization scheme for deep networks, i.e., IPopt + Adam. And the experiments show that pure Adam does not work, could the authors please discuss the differences between IpOPT and Adam and discuss why pure Adam does not work?\n\n(4) Experiments:\nThe section provides an overview about simple 1d and 2d PDE problems. The data samples are really small 4 - 6 samples (if I understand correctly) and the obtained performance is outrageously good mse, i.e., 10^-14. This seems a bit fishy to me and proposes that the experiments are way too simple and the provided data is way too perfect. The problem with real-world data is that it is really noisy and one would need to recover the structure despite the noise. Therefore, I would like to see this approach applied to much more challenging problems, preferably with real data.\n\n(5) Related Work:\nAs mentioned before the connections to other works are not really made clear, especially to the application centric graph networks.  \n\nOne more general question:\nIf I understand the motivation correctly, is this approach also applicable to controlled systems as the controller would change the dynamics frequently and then the filter C would need to be relearned in each iteration of the policy optimization?\n\n\n**Post Rebuttal Comments**: The authors improved the paper during the rebuttal but the clarity is not sufficient and the results are still puzzling. I do not fully understand how one can learn the perfect solution with only 3-4 data points.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086407, "tmdate": 1606915766906, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2897/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review"}}}, {"id": "tYxwypDbtUc", "original": null, "number": 2, "cdate": 1603838593380, "ddate": null, "tcdate": 1603838593380, "tmdate": 1606990121250, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review", "content": {"title": "Seems similar to previous works. More experiments maybe needed.", "review": "Summary: the work proposes to use neural networks to learn a kernel C(x,p) for PDEs. It embeds the neural network into iterative solvers and trains it with the Adjoint method.\n\nThe writing is clear in general but notation-heavy. For example, it could be better to define notations such as $A$ before using it. \n\nStrong points: \n- Clear, \n- Well-motivated,\n- Theoretically solid, \n- The example given on page 3 is quite helpful.\n\nConcerns:\n- The idea seems similar to the previous works.\n- Lack of comparison and benchmarking.\n\nI am not very certain about evaluating the novelty of this work. The idea to approximate the kernel with the neural networks has been proposed and studied in (https://arxiv.org/abs/2003.03485, https://arxiv.org/abs/2010.08895). The major difference seems to be that they directly learn solution operators, while in this work we embed it into iterative solvers. \nIt will be great if the authors can help me understand the contribution beyond the previous works.\n\nAnother concern is about the experiments. The test equations (Poisson, Helmholtz, Wave Equation) presented in the paper seems fairly simple. I wonder how other methods such as PINN, or the numerical solver perform on these equations. It will be great to have some benchmarks and comparisons with existing works. It can help me better evaluate the performance of the method.\n\nAlso, I found the scale for the neural network is very small. The authors claim larger networks are not needed, but it's better to have some justification or experiments.\n\nQuestions:\n1. how does it differ from the existing work?\n2. how does it empirically compare with numerical solvers or other deep learning-based methods?\n3. is it possible to try large networks in the experiments?\n\nRecommendation:\nIn general, I found this work interesting and concrete, but I am not certain about the novelty. Therefore, I would like to put this paper on margin.\n\n---\n_Updated review:_\n\n>The updated manuscript has some substantial improvements.\n>\n> I feel the biggest problem is that the authors didn't clearly state the problem settings. If I understand correctly, in their framework the equation is fixed but unknown. The training data are several points in the domain (with parameters input) and testing data are other points. So basically, we doing interpolations. But even the PDE is unknown, they do assume some structure of the PDE, I think.\n>\n> Other PDEs frameworks are either 1. solver-type: the equation is known and fixed, they directly solve for the solutions. 2. operator-type, the equations are unknown and changing. Train on inputs-outputs for several equations, and test on others. Their setting is quite different. I guess it's the reason their performance is much better in the updated comparison. On the other hand, it's also hard to evaluate their performance since there are no fair benchmarks.\n\n> In general, I feel this paper is novel and concrete, while it's not very complete and well-presented. I agree with other reviewers that this paper is not ready to publish.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086407, "tmdate": 1606915766906, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2897/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review"}}}, {"id": "dYuAViybQ5t", "original": null, "number": 4, "cdate": 1604296644334, "ddate": null, "tcdate": 1604296644334, "tmdate": 1606804043444, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review", "content": {"title": "Recommend accept", "review": "ICLR Neural PDEs\n\nSummary: \n\nThis paper aims to use neural networks to find hidden structure in PDEs and predict their solution. Their neural network examples are extremely small (up to 325 parameters) and require little data (up to 8 samples). Instead of a classical convolution, which combines the neighbors with fixed weights, they learn a \"functional convolution\" that combines the neighbors with weights that are themselves functions of the neighbors. This takes advantage of \"translational symmetry\" in discretized differential operators. The tiny network is then embedded in a Picard forward iterative procedure. An adjoint backward gradient calculator relies on being able to do auto-differentiation in neural networks. They specifically consider elliptic boundary value problems. It is key that the PDE systems are sparse (dependent on a limited number of neighbors).\n\nStrong points:\n\nI am very impressed by the small networks and limited training data. \nSome of the test MSEs are quite impressive. \nTo my (albeit imperfect) knowledge of the literature on machine learning for PDEs, this is quite creative. It also seems like a big step forward as far as low error with limited data.\nIt's nice to see the variety in the six test cases.\n\nWeak points/Clarification questions:\n\nThis paper needs more details on the training and testing data. \n- What counts as one data sample? \n- The input & output is not always clear.\n- How much variance is there within the training & test data? For example, for the \"constant kernels\" case, I can tell from Figure 8. However, for the \"spatially varying coefficients\" case, I think we never see the data. Also, for some cases, I don't think it's mentioned what is varied between examples.\n- It's sometimes unclear from the captions whether we're seeing training data or test data, or whether it's the true data or the results.\n- I'd like to see both training & test error to see if there is overfitting.\n- I don't see any mention of validation data, which is often what people use to choose hyperparameters. What did you use to choose hyperparameters? If the test data was used for this, we need new test examples, as test examples need to be held out until it's time to report errors. \n\nI may have missed this, but about how long does it take a train your method, and how long does it take to use the trained network for prediction? \n\nThe text in the figures are often too small to see. All parts of Figure 3 (a) & (d) are difficult to see. \nThe Related Work section (Section 5) lists a lot of papers, but doesn't explain what advantages your paper has.\n\n~~~~~\nUpdate:\n\nI think that the revised paper is an improvement,  but it's not ready. I think there is still missing information to make it clear what you did (as the other reviewers have commented as well.) I am particularly concerned that we still don't have a comparison of train & test errors for each network, and that we don't still know which dataset was used for selecting hyperparameters (train vs. val vs. test). These are crucial questions for deep learning results, and I always check this when I peer-review. I reduced my rating based on these concerns. \n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086407, "tmdate": 1606915766906, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2897/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review"}}}, {"id": "8YhAYkXu_s2", "original": null, "number": 1, "cdate": 1603392643950, "ddate": null, "tcdate": 1603392643950, "tmdate": 1606736752799, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review", "content": {"title": "review", "review": "Post-discussion update: The authors only partially adressed my concerns in their rebuttal. The paper suffers from lack of comparisons: only 2 baselines are compared, and only on few systems. Crucially the new Navier-Stokes experiment lacks comparisons. The authors also couldn't respond to my questions about research context or scope: it's difficult to assess what this work actually claims in relation to competing methods. For a machine learning paper this is not enough.\n\n-----\n\nThe paper proposes input-dependent convolutions for PDE learning under Picard solvers. The main contribution seems to be the spatially non-stationary filters, which is a useful and somewhat known CNN technique. It is easy to see how this makes the neural PDEs more powerful, but it\u2019s unclear in which setting the spatially-evolving differentials are warranted in PDEs. The paper should discuss the motivation and justification of this choice more. \n\nIt\u2019s unclear if the adjoints or picard solver are novel or adaptations from earlier works.\n\nI wonder why the spatial convolutions do not seem to overfit. They can learn in principle arbitrarily complex mappings, and with scarce data should overfit badly. How was this handled?\n\nThe paper is written in a clear manner, but almost all math suffers from undefined symbols and variables, which makes the math frustrating to read. Also the problem domain and problem definitions are undefined.\n\nThe experiments show that the method perfectly learns example systems with practically 0 test error. While this sounds great, this also raises a lot of questions. Perhaps the problems were trivial to begin with (there are no comparison to other methods), or maybe they do not generalize (there are no extrapolation experiments to regions where no data has been observed). The problems were also very small (1D or 2D), so are they practically relevant or do they actually require neural machinery of this kind? The author\u2019s should present learning curves and ablation studies to show when how far the method can extend before breaking (fewer data, more noise, more dimensions, more complex systems, etc), and include uncertainty analyses. Extensive comparisons to other methods (both neural and non-neural) are necessary to demonstrate useful contribution. Finally, it\u2019s unclear if these results indicate generalisation (in some sense). \n\nAll experiments seem almost textbook examples of simple, regular and clean cases. Neural networks excel when applied to messy and complex problems. Why are these a good application of neural PDEs?\n\nThe paper presents an incremental convolution extension to learning of PDE convolutions in Picard setting. The results show fantastic performance, which however is undermined by lack of any comparisons and the almost toy-likely simple systems that were studied. \n\n\n\nTechnical comments\no The functional C is undefined and unexplained, what does it mean? It seems to be a \u201cform\u201d instead of differential. How does C relate to PDEs?\no Also \u201cA\u201d and \u201cb\" are explained, how do they relate to PDEs?\no The notation throughout needs to be better defined, eg. domains of all variables\no The paper is lacking the basic PDE definitions completely. Please define the problem this paper is tackling\no Clarify what is \u201cPDE unknown x\u201d. I assume it's the solution.\no What is \u201ccurrent state of x\u201d? Isn\u2019t \u201cx\u201d the state itself?\no eq1 is not a function of \u201cp\u201d but it still uses \u201cp\u201d, please fix\no eq1 seems to redefine C(x,p) to C({x},{p}). How do we handle neighborhoods? How are the neighborhoods defined (sets/subgraphs/tensors?) What does the [..](pj) notation mean? How come we have x^t here, despite A(..) not being a function of time. What is the domain/codomain of C? Please clarify\no Please define x_0,x (is it a matrix?) \no what is \u201cfeature map\u201d in eq 2?\no define \u201cw\u201d in eq2\no I wonder if w(x_mn) would have been more intuitive notation for input-dependent w-functions\no N = {} is either a set or flattened vector, not both\no It would be more sensible to define N and C as matrices than vectors since they are tensors with two indices\no what is x_n (is \u201cn\u201d a time or something else?)\no Is the \u201cA\u201d in eq5 the \u201cA\u201d in eq 2 or 3? Where is the \u201cb\u201d coming from, it\u2019s not defined\no Is the adjoint derivation 2.3. novel? \no What is the \u201cMSE\u201d in experiments? Please report training and testing errors separately\no I don\u2019t understand why a time-dependent wave equation was tested, since the model is not time-dependent but spatially dependent. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086407, "tmdate": 1606915766906, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2897/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Review"}}}, {"id": "Pg4YKtydUma", "original": null, "number": 7, "cdate": 1606274897433, "ddate": null, "tcdate": 1606274897433, "tmdate": 1606274897433, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Comment", "content": {"title": "Rebuttal and Revised Submission (part 2)", "comment": "Other minor comments:\n\n### Reviewer #1:\n\n**What counts as one data sample? How much variance is there within the training & test data?**\n\nA data sample is a mesh grid. In our experiments, a data sample can be a $32\\times1$, $128\\times1$ grid (1D examples) or $32\\times32$, $49\\times49$ grid (2D examples). Different training data samples are sampled on different target solutions. For example, in the spatially varying case, the 4-size training data is sampled on solutions of 4 randomly generated boundary conditions (for each data sample, $x_0=random(0,1)$ and $x_{31}=random(0,1)$, where $x_0$ and $x_{31}$ are the two boundary points), and the 16 testing case is tested on the target solution with also random generated boundary conditions.  \n\n**I would like to see both training data and testing data to see if there is overfitting.**\n\nFor your reference, the MSE of training data from IpOpt + Adam optimizer in Figure 6 is 8.5e-25, and the MSE of 16 testing data is 5.1e-25. \n\n**How to choose hyperparameters for NN?**\n\nWe use a grid search method within a scale of layers and number of neurons in each layer.\n\n**How long does it take a train in your method, and how long does it take to use the trained network for prediction?**\n\nFor 1D cases, a train can be finished within several seconds. For 2D cases, a train takes less than half an hour to finish for the examples with $32\\times32$ grid and 2 hours for the examples with $49\\times49$ grid. A prediction always takes several seconds to finish. \n\n### Reviewer #2:\n\n**Questions regarding understanding**\n\nYou do understand our method in the correct way. As you suggested, we clearly clarify the target problem Section 1 and add motivation in Section 2, together with two inset illustration figures and Figure 1 to better state the problem and our method. \n\n**IpOpt and Adam?**\n\nWe use IpOpt to accelerate the exploration of the accurate local minimum and Adam to enhance its global search ability. IpOpt is good at fast converging but is vulnerable to bad initiation and easily falls into local minimum. Adam is a stable optimizer but takes relatively long time to converge. Furthermore, Adam dangles around a suboptimum in some of our cases, as mentioned in this paper [https://arxiv.org/abs/1904.09237v1] \n\n\n**Is our model applicable to controlled systems?**\n\nFor PDE systems with a temporally constant C (such as a soft body system with elastic material model), our method is applicable. We cannot handle temporally varying C with the current scheme.\n\n\n### Reviewer #3:\n\n**Is it possible to try larger neural networks?**\n\nSure.With larger neural networks, the loss can be further reduced. Take the 2D Poisson case as an example, while the $3\\times10\\times10\\times10\\times10\\times10\\times10\\times3$ NN obtains a testing MSE of 3.7e-3, the $3\\times12\\times12\\times12\\times12\\times12\\times12\\times3$ NN can obtain a testing MSE of 3.2e-6. In most cases, however, the small NN has already reached high accuracy and larger NN can only lead to marginally reduced loss.\n\n### Reviewer #4:\n\n**Unclear definition of symbols**\n\nAs you suggested, we have modified the manuscript by providing clear definition of target problem, symbols and equations, such as section 1, 2 and Figure 1.\n\n**Ablation tests**\n\nWe compare our method with results from other optimization methods which serves as an ablation test. For your concern, we also add new experiments to demonstrate our model\u2019s performance with more noise (Section 5.1) and in complex system (Navier stokes equation in 5.2) \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D4A-v0kltaX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2897/Authors|ICLR.cc/2021/Conference/Paper2897/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843336, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Comment"}}}, {"id": "rHen4nc4x6", "original": null, "number": 6, "cdate": 1606273732623, "ddate": null, "tcdate": 1606273732623, "tmdate": 1606273963289, "tddate": null, "forum": "D4A-v0kltaX", "replyto": "D4A-v0kltaX", "invitation": "ICLR.cc/2021/Conference/Paper2897/-/Official_Comment", "content": {"title": "Rebuttal and Revised Submission (part 1)", "comment": "Dear Reviewers,\n\nThank you for  the comments and the valuable feedback. We are very pleased to address your concerns and incorporate your suggestions by making several main updates (see the bullets below) in the revised manuscript. All the changes are colored as blue in the new version. \n\n### Main changes\n- We added a motivating example section to better illustrate the background of numerical PDE (including the linear Poisson and nonlinear Picard). (Section 2, Figure 1)\n- We made all the suggested expositional changes regarding the equations, symbols, and descriptions. (Section 1 with two inset figures and Section 2 naming convention)\n- More comparison with state-of-art PDE solvers (PINN in Section 5.3, B2 and Figure 8)\n- Stability test with noise (Section 5.1, Section C2, Figure 10)\n- An additional example showing complex systems (Navier-Stokes equation, see Section 5.2, C4 and Figure 12)\n- High-resolution figures (we updated the quality of all the figures)\n\n### Compared with the SOTA (R1,R2,R3,R4)\nWe want to highlight that our method targets at solving an inverse problem by uncovering an effective and compact representation (the structure of C) of an unknown PDE system from scarce observation data. This deviates from the vast majority of the existing neural PDE solvers or the numerical PDE solvers, which all assume a known PDE description in hand. Therefore, our approach does not aim to improve an existing numerical paradigm to find more accurate or efficient numerical solutions (for example, PINN, PDE-Net or other PDE solvers). Instead, we strive to solve a PDE from sparse observation without knowing its expression. Mathematically, our model aims to reconstruct the entire solution manifold of a series of PDE problems that have the same hidden intrinsic structure (the same pattern of C).\n\n### Picard solver (R2,R3,R4)\nIncorporating mathematical or numerical inductive priors into a network architecture has been emerging as an effective way to tackle the learning problems\u2019 nonlinearity. A famous example in the community is the Koopman operator [https://doi.org/10.1073/pnas.1517384113], which embeds the prior of iterative linearization of a nonlinear dynamic system by constructing a data-driven paradigm that can linearize its evolution. Our approach of employing the Picard interaction follows the same philosophy. We introduced a combo of classical numerical tools (adjoint solver+Picard iteration) and modified them to accommodate data-driven system identification, which turns out to be extremely effective in uncovering nonlinear PDE structures. The Picard iteration method is a well-established scheme in numerical mathematics, and as far as we know, our method is the first to introduce it to facilitate data-driven applications.\n\n### Simple PDE forms (R2,R3,R4)\nWe want to emphasize that the complexity of a PDE system (in particular, its solution manifold) is hidden behind its mathematical form. In many cases, the simple form of an equation does NOT indicate a trivial solution. A typical example is the difference between the Poisson and the Helmholtz equations: they share very similar expressions (with only a different +/- symbol) yet their solutions\u2019 distribution, frequency, and numerical difficulties on convergence are completely different. We believe the numerical tests we conducted well covered the broad spectrum of the common PDEs, in both 1D and 2D, with a particular focus on elliptical, nonlinear systems (which is a rarely explored domain in the scientific ML community). On another hand, we want to emphasize that the mathematical form of the target PDEs is completely hidden to our learning algorithm. Our algorithm did not leverage any prior information from the equation itself to reason its solution structure, which separates the network and the PDE clearly. Last, to further demonstrate the complexity of our target systems, we added an experiment for solving the dynamic Navier-Stokes equations (see Section 5.2) in our updated manuscript.  \n\n### Robustness with messy data (R2,R4)\nWe conducted tests with noise in Section 5.1 in the previous manuscript. To strengthen this part, we added another experiment to test our model\u2019s robustness in more drastically noisy settings (see Section 5.1). The test shows that our framework is able to denoise without even knowing the clean solution throughout the training. The reason is that by fitting the parameters in C to minimize the loss, despite that the loss is affected by noise, our model is still able to uncover the hidden structure of matrix A\u2019s kernel.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Partial Differential Equations with Functional Convolution", "authorids": ["~Ziqian_Wu1", "~Xingzhe_He1", "~Michael_Zhang6", "~Yijun_Li4", "~Cheng_Yang4", "~Rui_Liu7", "~Shiying_Xiong1", "~Bo_Zhu2"], "authors": ["Ziqian Wu", "Xingzhe He", "Michael Zhang", "Yijun Li", "Cheng Yang", "Rui Liu", "Shiying Xiong", "Bo Zhu"], "keywords": ["neural PDE", "functional convolution", "adjoint method"], "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|neural_partial_differential_equations_with_functional_convolution", "one-sentence_summary": "We developed a new family of neural networks to efficiently extract hidden structures of a nonlinear PDE based on sparse observation. ", "supplementary_material": "/attachment/b29388297304b42d5f92b231bdf14c0016e73ad6.zip", "pdf": "/pdf/834edc4e8ae2018eb957d6e06943e2ff143d7c3e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RiLQ4u8wev", "_bibtex": "@misc{\nwu2021neural,\ntitle={Neural Partial Differential Equations with Functional Convolution},\nauthor={Ziqian Wu and Xingzhe He and Michael Zhang and Yijun Li and Cheng Yang and Rui Liu and Shiying Xiong and Bo Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=D4A-v0kltaX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D4A-v0kltaX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2897/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2897/Authors|ICLR.cc/2021/Conference/Paper2897/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2897/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843336, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2897/-/Official_Comment"}}}], "count": 8}