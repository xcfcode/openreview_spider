{"notes": [{"id": "3eIy8JGoU8C", "original": null, "number": 8, "cdate": 1588970622001, "ddate": null, "tcdate": 1588970622001, "tmdate": 1588970622001, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "e4DGUT1Ps4", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Datasets", "comment": "I apologize for the delay.\n\nWe updated the repository, you can find the datasets here:\nhttps://github.com/google-research/google-research/tree/master/value_dice"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "e4DGUT1Ps4", "original": null, "number": 1, "cdate": 1588510839234, "ddate": null, "tcdate": 1588510839234, "tmdate": 1588510839234, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Public_Comment", "content": {"title": "Question about Source Code", "comment": "Hi Ilya, thank you for a very interesting paper.\n\nWe tried running your source code to replicate the experiments, but encountered a problem. The directory for expert trajectories is specified as follows:\n\nIn \"run_experiments.sh\":\n\nLine 20: expert_dir=\"./experts/\"\n\nHowever, this directory does not exist. It is stated in the paper that trajectories are taken from GAIL, however neither the data in the official GAIL repo, nor your DAC repo, are of the correct format.\n\nCould you please kindly point us to where we can find the expert trajectories?\n\nMany thanks!"}, "signatures": ["~Bronson_T._C.1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Bronson_T._C.1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202827, "tmdate": 1576860563983, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Public_Comment"}}}, {"id": "Hyg-JC4FDr", "original": "BJlAzSGuDB", "number": 882, "cdate": 1569439192803, "ddate": null, "tcdate": 1569439192803, "tmdate": 1583912035901, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "SkpXWoAZAN", "original": null, "number": 1, "cdate": 1576798708663, "ddate": null, "tcdate": 1576798708663, "tmdate": 1576800927713, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work addresses new insights in the imitation learning setting, and shows how a popular type of approach can be extended in a principled way to the off-policy learning setting. Several requests for clarification were addressed in the rebuttal phase, in particular regarding the empirical evaluation in off-policy settings. The authors improved the empirical validation and overall clarity of the paper. The resulting manuscript provides valuable new insights, in particular in its principled connections, and extension to previous work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721343, "tmdate": 1576800272345, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper882/-/Decision"}}}, {"id": "BklKqZM9sS", "original": null, "number": 5, "cdate": 1573687696982, "ddate": null, "tcdate": 1573687696982, "tmdate": 1573687696982, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "rygmkPmKoS", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Re: Thank you for the response", "comment": "For Figure 2, as we mention in the paper, we used the implementation of BC from the original implementation of GAIL. For Figure 4, in order to plot the rewards w.r.t. different number of updates, we use our own implementation. The use of our own implementation also allowed us to borrow the same settings as used for ValueDICE (same network, same optimizer, same regularizations). The results of BC in the Appendix are after appropriate tuning of our implementation over learning rates and regularization weight. In the next revision we will mention this more explicitly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "rygmkPmKoS", "original": null, "number": 4, "cdate": 1573627611141, "ddate": null, "tcdate": 1573627611141, "tmdate": 1573627611141, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "BklxvZXvoB", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Thank you for the response", "comment": "Thank you for the response and the additional experiment.\n\nDo you use the same settings as Figure 2? It seems that for Hopper, And and Walker2d the results are not consistent in Figure 2 and 4 for Behavior Cloning method.\n\nI am surprised for the implication that \"additional online experience is not necessary for the MuJoCo tasks we used for evaluation\". I think if you can empirically show that ValueDice can effectively learn expert policy in off-policy setting, this will be extremely useful in many real world task where we cannot collect transition data directly for exploration."}, "signatures": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "rylI7N7vjr", "original": null, "number": 3, "cdate": 1573495838493, "ddate": null, "tcdate": 1573495838493, "tmdate": 1573495838493, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "HklnbLThtr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Re: Official Blind Review #1", "comment": "Thanks for the useful feedback! Our responses are below.\n\nRegarding lack of practical advantages, since the submission we have made minor updates to our implementation. The updated experiment results (see the updated PDF) now show that ValueDICE performs better on Hopper and HalfCheetah (and generally exhibits better and more stable performance than the baselines). Our implementation updates were pretty minor and not algorithmic: we noticed that our implementation of gradient penalty was affected by the instability of tf.norm (there is tf issue for this that has been open for years https://github.com/tensorflow/tensorflow/issues/12071), and we switched our implementation to a numerically stable version of tf.norm suggested in the discussion of the issue.\n\n\u201cThis would likely not work well in practice - similar to how behavior cloning often does worse than GAIL when learning from a small number of expert examples. In order to combat this, the authors incorporate replay buffer regularization\u201d\n\nWe have updated the paper to include results of a completely offline version of ValueDICE (see Appendix C).  We show that even without any replay buffer regularization, ValueDICE can outperform BC. We believe this is the first demonstration of an imitation learning algorithm beating BC in the completely offline setting.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "rylbaG7wiS", "original": null, "number": 2, "cdate": 1573495481215, "ddate": null, "tcdate": 1573495481215, "tmdate": 1573495481215, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "SkxD8yNatH", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Re: Official Blind Review #2", "comment": "We thank the reviewer for the close reading of the paper and the helpful feedback!\n\nWe respectfully disagree that the addition of replay buffer regularization is not principled. As we discuss in Section 5.2, the incorporation of replay buffer experience into the loss function does not change the optimal solution (pi* = expert) (equations 19 and 20). Furthermore, we note that the same does not apply to previous incorporations of the replay buffer in imitation learning (e.g., DAC). The key differentiator is that our regularization is applied to both numerator and denominator of the (implicitly estimated) density ratios, as opposed to replacing the denominator with dRB, as done by DAC. \n\nRegarding overfitting on HalfCheetah and Ant: It is not possible to measure KL exactly in MuJoCo and rewards are used only as a proxy for evaluation of performance. Additionally, the fact that imitation learning minimizes KL between \\pi and a small finite sample from \\pi_{expert} means that a better imitation learning algorithm (that achieves lower KL) may not recover \\pi_{expert} exactly.\n\nRegarding optimization instabilities, in our preliminary experiments, we also found direct application of DualDICE to be difficult to tune, and thus developed a number of remedies to the instability (which may be generally useful). First, we introduced the use of log-average-exp into the variational form of KL (as opposed to the squared loss or f-divergence form suggested by DualDICE). Second, we used gradient penalty regularization, borrowed from previous works on adversarial learning algorithms and dual representations of the KL. We found the combination of these two to provide much more stable performance.\n\nAs the reviewer suggested, we are actively working to opensource our implementation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "BklxvZXvoB", "original": null, "number": 1, "cdate": 1573495128018, "ddate": null, "tcdate": 1573495128018, "tmdate": 1573495128018, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hkg8cdP6YB", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3", "comment": "Thanks for the helpful feedback on our paper!\n\nFollowing the reviewers suggestion, we have included results of offline ValueDICE on the MuJoCo tasks (see Appendix C). Anecdotally, we found that the use of additional online experience is not necessary for the MuJoCo tasks we used for evaluation. We originally focused on the online version because we believe it is more generally useful (and indeed, on the Ring MDP the use of additional experience is crucial to generalize to states not observed in the original expert dataset).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hyg-JC4FDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper882/Authors|ICLR.cc/2020/Conference/Paper882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164758, "tmdate": 1576860530242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper882/Authors", "ICLR.cc/2020/Conference/Paper882/Reviewers", "ICLR.cc/2020/Conference/Paper882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Comment"}}}, {"id": "HklnbLThtr", "original": null, "number": 1, "cdate": 1571767812068, "ddate": null, "tcdate": 1571767812068, "tmdate": 1572972540469, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The primary contribution of this paper is a principled algorithm for off-policy imitation learning. Generative Adversarial Imitation Learning (GAIL), proposed by Ho and Ermon in 2016, is an on-policy imitation learning method that (provably) minimizes the divergence between a target state-action distribution and the policy state-action distribution. Followups to this work (Kostrikov et al., 2019) show that the same algorithm can be applied to an off-policy setting (replacing the on-policy samples with samples from the replay buffer) and the method still works, but is no longer theoretically justified. I believe using importance ratios would make this approach justified as well, but Kostrikov et al. found that using importance ratios actually degrades the performance of their method (due the difficulty associated with estimating importance ratios). This paper attempts to bridge this gap: a method that is theoretically justified, and still works. \n\nThe paper takes most of its inspiration from the recently proposed DualDICE paper (Nachum et al, 2019), where the authors introduce a method for estimating discounted stationary distribution ratios (i.e. d^{\\pi}/d^{D}, where \\pi is some (known) policy, and D is a given dataset of experience (for example, a replay buffer)). The authors essentially apply the method proposed in DualDICE to estimate d^{\\pi}/d^{exp} instead, where d^{exp} is a dataset of expert trajectories. While it would be possible to simply use this term as a reward and then run reinforcement learning, the authors note that the specific form of estimating d^{\\pi}/d^{exp} allows them to instead directly a train a value function, which can then be used for updating a policy.\n\nThe authors also argue that their method reduces complexity since it does not require a separate RL optimization routine. However, I think having a separate RL optimization routine has its own advantages - it is relatively easy to implement GAIL on top of any existing RL algorithm, which makes it easy to take advantage of recent advances in RL. For the method proposed in this paper, it would not be straightforward to do so. \n\nThe authors also note that they need a number of practical modifications to their original ValueDICE objective in order to make things work (Section 5: Some practical considerations). Notably, the the original ValueDICE objective only needs access to expert samples and the initial state distribution, and does not need access to the the replay buffer samples (apart from the initial state ones). This would likely not work well in practice - similar to how behavior cloning often does worse than GAIL when learning from a small number of expert examples. In order to combat this, the authors incorporate replay buffer regularization \n\nThe authors provide experiments on  a simple synthetic \"ring MDP\", and on four continuous control tasks from OpenAI gym - HalfCheetah, Hopper, Ant and Walker2d. When compared to prior approaches (i.e. Kostrikov et al 2019) in the low-data regime (where behavior cloning fails), the proposed method does significantly better on one task (Ant), slightly worse on one task (walker), and about the same on two tasks (Hopper and HalfCheetah). I do notice the proposed method as being somewhat unstable though - the reward appears to be going down after reaching the max on two of the tasks - HalfCheetah and Ant. Overall, I don't thing experiments are thorough enough to demonstrate that the method is empirically better than competing approaches, but I believe that is not the main point of the paper. \n\nOverall, my recommendation is a weak accept. It is interesting that the authors were able to get a principled method for off-policy imitation learning working (large following from prior work in Nachum et al 2019), but I don't think the method currently offers any significant practical advantages over competing methods.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575477565929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper882/Reviewers"], "noninvitees": [], "tcdate": 1570237745610, "tmdate": 1575477565940, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Review"}}}, {"id": "SkxD8yNatH", "original": null, "number": 2, "cdate": 1571794766712, "ddate": null, "tcdate": 1571794766712, "tmdate": 1572972540425, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an algorithm for adversarial imitation that uses off-policy data in a principled manner, unlike prior work. The core idea is to express the KL-divergence between the policy's state-action marginal and the expert's state action marginal using the Donsker-Varadhan representation and then applying the change of variable similar to DualDICE to avoid computing the marginal of the current policy, thus getting rid of the on-policy sampling requirement. The paper then shows how the auxiliary variable (critic) added to the optimization is a value function that maximizes the corresponding induced reward in AIL methods, thus unifying the objectives for policy optimization and reward learning. The authors then present practical considerations needed in getting this formulation to work, including sampling from a replay buffer, biased sampling for the exponentiated term and avoid the double-sampling issue. Finally, the paper presents some results, which show that valueDICE is comparable to most of the other imitation learning methods. \n\nI lean towards accepting this paper. The overall idea seems neat, however, Section 5, and the addition of a replay buffer distribution in the KL-divergence objective, though motivated enough seem to be somewhat not so principled. The experimental section is a bit weak, and I encourage the authors to strengthen this section. Also, in the case of HalfCheetah and Ant (Figure 2), valueDICE usually exhibits overfitting-like trends (the performance drops with more training), why does this happen? Can this be corrected? Overall the idea is neat, I would still say that the components very prominently exist in the literature (f-GANs, DualDICE, etc).   \n\nI would encourage the authors to add some more details experimentally and make the implementation available. For example, optimization of Bellman backup functions without target networks or delays can be unstable, solving saddle point problems can be unstable, etc. How should these factors be tuned? And overall, is the optimization of the ValueDICE objective easy?. DualDICE is known to be unstable (the DualDICE Github implementation mentions this), do similar problems arise with ValueDICE?"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575477565929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper882/Reviewers"], "noninvitees": [], "tcdate": 1570237745610, "tmdate": 1575477565940, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Review"}}}, {"id": "Hkg8cdP6YB", "original": null, "number": 3, "cdate": 1571809421645, "ddate": null, "tcdate": 1571809421645, "tmdate": 1572972540384, "tddate": null, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "invitation": "ICLR.cc/2020/Conference/Paper882/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper provides a novel off policy objective to solve imitation learning. It resolves the limitation of the famous GAIL algorithm that we need on-policy samples to interact with the environment. The new algorithm is simple but efficient, and can handle off-policy settings. The derivation of equation (12) is nice and intuitive, provide a potential on creating new imitation learning algorithm. Empirical results show that the new algorithm can perform as good as the state-of-the-art baseline, under on-policy setting.\n\nClarity:\nThe paper is well written an intuitive. It clearly introduces the previous works and their limitation, and naturally derives the new objective by DualDice trick to resolve the limitation. Section 5 discusses the bias introduce by the exponential of expectation, which in practice does not hurt the performance much. Experimental design is good and informative.\n\nMajor concern:\nIn experiment we only saw the result of on-policy setting using the replay buffer regularizer. However, the first half of the paper focuses on deriving an off-policy objective for imitation learning. A natural question is: how good is the performance if we only use off-policy data? In Figure 3 with enough expert trajectories, how does the off-policy ValueDice perform compared to behavior cloning?\n\nIn sum, I think the paper is clearly above the acceptance threshold. But I will leave it 6 point and raise my point if the authors can answer my question above.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper882/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Imitation Learning via Off-Policy Distribution Matching", "authors": ["Ilya Kostrikov", "Ofir Nachum", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "ofirnachum@google.com", "tompson@google.com"], "keywords": ["reinforcement learning", "deep learning", "imitation learning", "adversarial learning"], "abstract": "When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.", "pdf": "/pdf/e760b8c2e400ca1d33a3bbac52125055587fe455.pdf", "paperhash": "kostrikov|imitation_learning_via_offpolicy_distribution_matching", "_bibtex": "@inproceedings{\nKostrikov2020Imitation,\ntitle={Imitation Learning via Off-Policy Distribution Matching},\nauthor={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hyg-JC4FDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bfcf0e6479dc168f8a2782fc3a0a60488769425c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hyg-JC4FDr", "replyto": "Hyg-JC4FDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575477565929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper882/Reviewers"], "noninvitees": [], "tcdate": 1570237745610, "tmdate": 1575477565940, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper882/-/Official_Review"}}}], "count": 12}