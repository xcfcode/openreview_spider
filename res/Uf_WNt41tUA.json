{"notes": [{"id": "Uf_WNt41tUA", "original": "C1QREVd3cWP", "number": 2844, "cdate": 1601308315707, "ddate": null, "tcdate": 1601308315707, "tmdate": 1614985643372, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RmVD7aLDwxf", "original": null, "number": 1, "cdate": 1610040518507, "ddate": null, "tcdate": 1610040518507, "tmdate": 1610474126944, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a method for the interesting task of dialog summarisation which is slowly getting attention from the research community. In particular, they propose a method which first generates a summary draft and then a final draft.\n\nPros:\n1) The paper is well written\n2) Addresses an interesting problem\n3) SOTA results\n\nCons:\n1) Lack of novelty\n2) No quantitative analysis of the summary draft though it is as an important part of the proposed solution\n3) Human evaluations are not adequate (the authors have said they will expand on this but clear details are not provided)\n4) The BART model seems to have some advantage as it is pre-trained on XSUM data whereas some of the other models are not (the authors haven't clarified this sufficiently in the rebuttal)\n\nOverall, the reviewers were not completely happy with the work and there was not clear champion. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040518493, "tmdate": 1610474126928, "id": "ICLR.cc/2021/Conference/Paper2844/-/Decision"}}}, {"id": "s4xlUs4AbPT", "original": null, "number": 7, "cdate": 1605558210155, "ddate": null, "tcdate": 1605558210155, "tmdate": 1605558210155, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "6S5mrLlUSyx", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment", "content": {"title": "Thank you so much for your review", "comment": "Dear reviewer,\n\nPlease let us answer your concerns or questions in the following:\n\n**[largely ad-hoc and engineering intensive]**\nOur work is more like an empirical work and the main contribution and goal is not to propose a new generative model or a new optimization algorithm, instead, we are trying to make a claim that for dialogue summarization task: 1) a noisy augmented draft can actually help final summary quality if an auto-regressive way, and 2) a simple strategy enables us to control summary granularity and it works surprisingly well. We are the first to propose a simple yet efficient training solution that surpasses any existing dialogue summarization models, including contemporaneous works [1][2]. We will include the reference you pointed out in our final version. \n[1] Multi-View Sequence-to-Sequence Models with Conversational Structure\nfor Abstractive Dialogue Summarization (EMNLP 2020)\n[2] Improving Abstractive Dialogue Summarization with Conversational Structure and Factual Knowledge (under review, ICLR 2021)\n\n**[Experimental results on one dataset are rather weak]**\nSAMSum is the largest dialogue summarization dataset with high-quality annotations that we can evaluate on. We will include the AMI evaluation (a very small one) in the final version. Table 1 is a fair comparison because it can prove that our training strategy is useful and generalizable. Our method is not restricted to the BART model and it can be applied to any existing or future pre-trained language models.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uf_WNt41tUA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2844/Authors|ICLR.cc/2021/Conference/Paper2844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843919, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment"}}}, {"id": "VAutVl813CW", "original": null, "number": 6, "cdate": 1605558127816, "ddate": null, "tcdate": 1605558127816, "tmdate": 1605558127816, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "ACZNsV6ul1p", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment", "content": {"title": "Thank you so much for your review", "comment": "Dear reviewer,\n\nPlease let us answer your concerns or questions in the following:\n\n**[Why Controllability of Dialogue Summarization?]**\nControllable generation itself is an interesting yet difficult problem, and in this paper, we focus specifically on dialogue summarization because dialogue applications have received much attention due to the prevalence of smart speakers. We believe there is a vast application potential of dialogue summarization systems, and most importantly, it will be an appealing feature if we can control the granularity of summary based on user preference.\n\n**[Compared to Existing Draft Construction]**\nOur work is closely related to the literature of \u201cretrieval-augmented generation\u201d. The goal of our work is to show that even with a good pre-trained language model, we can still further improve its generation performance by fine-tuning it with weakly-annotated labels. What is the best way to construct a summary draft is not the main investigation of this work (Actually our solution is similar and more advanced to the Bottom-Up [2] approach). We will add one existing extractive-augmented method as a reference in the final version.\n\n**[Longest Common Subsequence (LCS)]**\nWe do LCS after the parsing using the trained constituency parser (Section 2.2), so our LCS results are dependent on the parsing results. In this particular example, `s just one of many boring days at work`     the parsed constituent overlapping with \u2018at work\u2019 in the summary. However, in other examples, not all overlapped words are meaningful (e.g. stop words). We thus filter the LCS results and only keep important key phrases. We\u2019ll explain more details about the process in the Appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uf_WNt41tUA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2844/Authors|ICLR.cc/2021/Conference/Paper2844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843919, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment"}}}, {"id": "GWPoRQOFYyp", "original": null, "number": 5, "cdate": 1605557946531, "ddate": null, "tcdate": 1605557946531, "tmdate": 1605557946531, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "QOnlVerBmC1", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment", "content": {"title": "Thank you so much for your review", "comment": "Dear reviewer,\n\nWe will try our best to make every part of the paper clear in the final version. Please let us answer your concerns or questions in the following:\n\n**[Novelty]**\nWe propose a simple yet efficient solution that surpasses any existing dialogue summarization models, including contemporaneous works [1][2]. The main contribution of our work is not to propose a new generative model (we rely on pre-trained language models, which can be replaced or improved by any SOTA LMs), instead, we are trying to make a claim that 1) a noisy augmented draft can actually help final summary quality in a simple auto-regressive way, and 2) a simple strategy enables us to control summary granularity and it works surprisingly well. \n[1] Multi-View Sequence-to-Sequence Models with Conversational Structure\nfor Abstractive Dialogue Summarization (EMNLP 2020)\n[2] Improving Abstractive Dialogue Summarization with Conversational Structure and Factual Knowledge (under review, ICLR 2021)\n\n**[How are the intents annotated?]**\nIt is a totally automatic process. It is better than \u201ckeywords matching\u201d since we use the Snorkel library to weakly label the data.\n\n**[Finding the last cutting points]**\nYes, we have the assumption that the last chunk must be similar to the last summary sentence. How to further improve the mapping could be one important and interesting future work.\n\n**[How are the outputs generated?]**\nFor CorDial, output summary is generated sentence-by-sentence, with the \u201csame input\u201d but different \u201chighlighted portions\u201d (Section 2.3). For example, <hl> X1, X2 <hl/> X3, X4, X5 \u2192 Y1 and X1, X2 <hl> X3, X4, X5 <hl/>  \u2192 Y2.\n\n**[XSUM for initialization]**\nThe \u201cBART\u201d results shown in the paper are all \u201cBART that is fine-tuned with XSUM data\u201d. PEGASUS model itself is not pre-trained on XSUM but it is pre-trained by important sentence masking.\n\n**[Model Size]**\nFor BART and CorDial, we are using a BART-large model with 400M parameters. PEGASUS has 568M parameters. We will include the model size information in the Appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uf_WNt41tUA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2844/Authors|ICLR.cc/2021/Conference/Paper2844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843919, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment"}}}, {"id": "uqdUiTF_cVz", "original": null, "number": 4, "cdate": 1605557770717, "ddate": null, "tcdate": 1605557770717, "tmdate": 1605557770717, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "lvs6FUDF8lj", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment", "content": {"title": "Thank you so much for your review", "comment": "\nDear reviewer,\n\nPlease let us answer your concerns or questions in the following and we will incorporate all your feedback in the final version.\n\n **[summary draft quality]**\nGood suggestion and we will include some generated drafts in the Appendix. So far, we treat the draft in our approach as an intermediate text, and what we care about the most is the quality of the final summary. Thus, it may not be that insightful to look at those \u201cdrafts\u201d as they are constructed for weak supervision purposes (Section 2.2). \n\n **[Human Evaluation Scale]**\nWe use roughly 6% of the test set data for human evaluation and we do some filtering based on the annotation of the \u201cgold summary\u201d. Specifically, we filter those annotations if a \u201cgold summary\u201d has been annotated as \u201c-1\u201d (the meaning of each score is shown below), implying that the annotators may not pay attention to the scoring. The final results reported in Table 3 is the mean from three different annotators after filtering (4% data). \n\nThe \u201cgold summary\u201d is actually not perfect and it might contain some noisy annotation, this is the reason why some workers may give 0 even if it is a \u201cgold summary\u201d. Please check the information below for the scoring instruction we sent to our workers. We will conduct one more round of human evaluation and include all of them in the final version:\n\n* Factual Consistency (Precision): The rating measures whether the information provided in a summary is correct.\nScore -1 if a summary contains a serious factual error.\nScore 0 if a summary has some minor factual errors. \nScore 1 if everything in a summary is factually correct. \n\n* Informative (Recall): The rating measures whether all the important information in a dialogue is included in a summary. \nScore -1 if a summary misses serious key points. \nScore 0 if a summary misses a few key points.\nScore 1 if a summary covers all key points."}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uf_WNt41tUA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2844/Authors|ICLR.cc/2021/Conference/Paper2844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843919, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Comment"}}}, {"id": "6S5mrLlUSyx", "original": null, "number": 1, "cdate": 1603786012150, "ddate": null, "tcdate": 1603786012150, "tmdate": 1605024120178, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review", "content": {"title": "Review #1", "review": "\n<Summary>\n\nThis paper addresses the problem of abstractive dialogue summarization. Its key idea is to label an interrogative pronoun category and extract key phrases from each dialogue turn as weak guide for dialogue summarization. It also proposes a length-controllable generation method for final summary. The proposed approach is evaluated on the SAMSum as one of the largest abstractive dialogue summarization benchmarks, on which it shows competitive performance over recent models. \n\n<Strengths> \n\n1. It proposes a two-step coarse-to-fine approach for abstractive dialogue summarization; it first extracts category labels and key phrases from each dialogue turn, and then generates final summaries by controlling granularity. This idea itself could be novel.\n\n2. It shows strong performance over other recent methods on the recently released SAMSum dataset. \n\n3. It tests the proposed approach with four recent pre-trained language models including DialoGPT, UniLM, PEGASUS and BART.\n\n<Weakness>\n\n1. This paper proposes a novel coarse-to-fine approach for abstractive dialogue summarization but its implementation is largely ad-hoc and engineering intensive and thus bears little technical novelty.\n\n(1) The \u201ccoarse\u201d part aims at generating drafts using interrogative pronoun category prediction and key phrase  extraction. \nThese two are largely based on existing techniques (e.g. Ratner et al 2019 and Kitaev & Klien 2018) and some heuristics (e.g. thresholding for key phrases detection). \n\n(2) The \u201cfine\u201d part aims at generating target summary with controllability of granularity. \nIts implementation is also based on a series of engineering heuristics (e.g. dialogue splitting by ROUGE score, binary classification for cutpoint detection). \n\n(3) In summary, it is hard to find methodological novelty in the proposed method.  Given that ICLR is a top premier ML venue, it could be a significant weakness to be a publishable work. \n\n2. Experimental results are rather weak. \n\n(1) Although SAMSum dataset may be one of the best benchmarks for the target task, experiments on only a single dataset is limited to show the generality and effectiveness of the proposed method.\nGiven that the proposed method is ad-hoc, I suspect much additional endeavor may be required to apply to another dataset. \n\n(2) I am not sure whether the comparison in Table 1 is fair enough. Since the proposed approach relies on the additional components for draft construction, it could require more other types of training data or learned modules that other method may not need. This should be clarified in the draft.\n\n<Conclusion>\n\nMy initial decision is \u2018reject\u2019 mainly due to lack of technical novelty. Limited experiments could be another issue to be improved. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087547, "tmdate": 1606915805320, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review"}}}, {"id": "ACZNsV6ul1p", "original": null, "number": 2, "cdate": 1603911677397, "ddate": null, "tcdate": 1603911677397, "tmdate": 1605024120117, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review", "content": {"title": "Correlation between the given problem and the proposed solution?", "review": "The paper proposes CorDial for abstractive dialogue summarization. CorDial extends BART by generating an intermediate \"summary draft\" which provides weakly-supervised signals and controling the length of the final summary. Results show significant improvements over competitive summarization models such as PEGASUS and BART in multiple different metrics.\n\nSome comments:\n\n1. The paper emphasizes that dialogue summarization is challenging due to its multi-speaker standpoints, casual language, and limited data. Although the use of the proposed summary draft would help solve the first challenge, it is hard to see any correlation between the other problems mentioned and the proposed solutions in the paper. This is especially the case for the controlling of the summary length. Why is this useful specifically for dialogue summarization?\n\n2. The \"summary draft\" is one kind of a content plan, which is widely used in text generation, including text summarization [1]. The technique of extracting key phrase is similar to how content selection is done in [2]. Please compare the proposed solution to other kinds of content planning.\n\n3. To extract key phrases, the method identifies the longest common sub-sequence (LCS) parameterized by a threshold, however how this threshold is set and used is not discussed in the paper. This is important information in order to understand how these key phrases would look like. For example, in Figure 1, how is \"s just one of many boring days at work\" extracted when the LCS is only \"at work\" for turn 2?\n\n[1] https://www.aclweb.org/anthology/C18-1101.pdf\n\n[2] https://arxiv.org/pdf/1808.10792.pdf", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087547, "tmdate": 1606915805320, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review"}}}, {"id": "QOnlVerBmC1", "original": null, "number": 3, "cdate": 1603939029344, "ddate": null, "tcdate": 1603939029344, "tmdate": 1605024120053, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review", "content": {"title": "Review for CorDial", "review": "This paper proposes CorDial, a new method for dialog summarization. CorDial firstly constructs a coarse draft by generating intent and key phrases for every dialog turn, and splits the dialog into chunks by inserting special boundary tokens; then the segmented original dialog and the constructed draft are feed as input to generate the final summary. CorDial employs BART-xsum as its backbone model, which is a pre-trained language model finetuned on XSUM summary dataset. Experiment result on SAMsum dataset shows CorDial achieves SotA performance under both automatic evaluation metric and human evaluations.\n\nOverall, the paper presents an interesting, practical recipe for dialog summarization. It requires few additional annotation besides the summary, and the human evaluation results looks promising. However, the method proposed here somewhat lacks in novelty, and some part of the paper is not clearly written. Thus I give this paper a weak reject rating.\n\nComments:\n1. In 2.2, how are the intents annotated? Is it a purely automatic process based on keywords matching? Or the keywords are merely cues for human annotators?\n2. In 2.3, the algorithm for finding the cutting points is an incremental one. It can't account for the similarity between last chunk and the last sentence in the summary, since the cutting point of second to last chunk already determines the boundary of the last chunk.\n3. How are the output generated exactly? The last sentence in 2.4 states each sentence is generated separately. Does it mean each output sentence have different input? How is it different from standard auto-regressive token-by-token generation?\n4. 2.4 should also explicitly refer to Figure. 1 for clarity.\n5. CorDial uses the BART-xsum as initialization, which is trained on XSUM dataset. Are other baselines also gone through the same XSUM training?\n6. What is the model size of all the models in the experiments? It would be better to have some descriptions on model architectures in the experiment section.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087547, "tmdate": 1606915805320, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review"}}}, {"id": "lvs6FUDF8lj", "original": null, "number": 4, "cdate": 1604016265601, "ddate": null, "tcdate": 1604016265601, "tmdate": 1605024119984, "tddate": null, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "invitation": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review", "content": {"title": "This paper proposes CORDIAL to improve the abstractive dialogue summarization quality and controllability. It has a coarse-to- fine generation strategy that generates a summary draft followed by a final summary and a simple strategy to control the granularity of the final summary.", "review": "This paper is well written and investigates dialogue summarization that has not received much attention. It proposes a new model called CORDIAL which can generate a summary draft followed by a final summary.  It achieves comparable or better results in term of both automatic evaluation metrics, e.g. compress ratio, rouge score, and human evaluations (Consistent and Informative) about the quality of generated summaries in different settings. \n\nHowever, there are still several disadvantages of this paper:\n(1) It can generate a summary draft but its quality is almost not presented in the paper except the ablation study of table 1. Even the results within table 1 are still about the quality of the final summary. The paper slightly overclaims its usefulness in the draft summary generation. More results or analysis about draft summary should be presented.\n(2) The human evaluation has only 30 examples and the scale is too small. Also, does the score is -1, 0, 1, or other scale? Do you use majority vote or mean plus standard deviation to get results in the table? Why gold in table 3 is so low in Consistent?\n(3) The method looks applicable to the generable summarization task. More results of this are also interesting. \n\nAlthough these drawbacks, its quality is good overall.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2844/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2844/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity", "authorids": ["~Chien-Sheng_Wu1", "likicode@gmail.com", "wenhao.liu@salesforce.com", "p.stenetorp@cs.ucl.ac.uk", "~Caiming_Xiong1"], "authors": ["Chien-Sheng Wu", "Linqing Liu", "Wenhao Liu", "Pontus Stenetorp", "Caiming Xiong"], "keywords": ["dialogue", "summarization", "controllable generation", "natural language processing"], "abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|cordial_coarsetofine_abstractive_dialogue_summarization_with_controllable_granularity", "one-sentence_summary": "We propose CorDial, a state-of-the-art dialogue summarization model with coarse-to-fine generation and granularity controllability.", "supplementary_material": "/attachment/0254902c4458e82980c72dece70f65e11e615895.zip", "pdf": "/pdf/6bbd5b826b91bcdb7de6062c892f1a9119c2e7e5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=bwA3PAgs-e", "_bibtex": "@misc{\nwu2021cordial,\ntitle={CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity},\nauthor={Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=Uf_WNt41tUA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uf_WNt41tUA", "replyto": "Uf_WNt41tUA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087547, "tmdate": 1606915805320, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2844/-/Official_Review"}}}], "count": 10}