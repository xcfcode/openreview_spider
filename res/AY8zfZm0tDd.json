{"notes": [{"id": "AY8zfZm0tDd", "original": "o9PMaw7GXSS", "number": 1662, "cdate": 1601308183997, "ddate": null, "tcdate": 1601308183997, "tmdate": 1615968271630, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jHHqXpdU8I", "original": null, "number": 1, "cdate": 1610040364804, "ddate": null, "tcdate": 1610040364804, "tmdate": 1610473955257, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper develops an effective model-free algorithm that achieves high sample efficiency. The empirical performance is appealing, which is comparable to model-based policy optimization and significantly outperforms SAC. The paper is well-written, and contains rigorous ablation studies. Weakness: the theoretical analysis is Section 3.1 is not thorough\u00a0yet,\u00a0 and it would be helpful to include more numerical comparisons with the Maxmin approach by Lan et al.\u00a0"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040364787, "tmdate": 1610473955238, "id": "ICLR.cc/2021/Conference/Paper1662/-/Decision"}}}, {"id": "5lT-ZXnIXT", "original": null, "number": 4, "cdate": 1603959125450, "ddate": null, "tcdate": 1603959125450, "tmdate": 1606806878815, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review", "content": {"title": "Review", "review": "[Summary]\n\nThis paper proposes Randomized Ensembled Double Q-Learning (REDQ), a new model-free RL algorithm that aims to improve the sample efficiency over existing model-free methods. Experiments on Mujoco show that REDQ achieves better sample efficiency than popular model-free methods such as SAC and is comparable with model-based methods such as MBPO. The paper further provides extensive ablation studies that justify the necessity of the algorithmic components in REDQ and show that improved Q estimation bias may have been the key reason for the performance gain. The paper also provides some theoretical analysis of the Q estimation bias.\n\n[Pros]\n\n--- The empirical performance of REDQ seems rather strong: significantly better than SAC and can match or exceed MBPO depending on the environment.\n\n--- Section 3 provides a rather convincing explanation of the performance gain through the perspective of the Q estimation bias: REDQ manages to achieve a low bias in terms of both average and std across (s,a) pairs. In comparison SAC fails on both grounds, while AVG (naive ensemble without the in-target minimization) achieves a low std but still a rather high average. The theoretical analysis (Theorem 1) also helped improve my understanding on this front by illustrating how the factors (M, N) could affect the bias. \n\n--- The ablation study in Section 4 is very detailed and answers a lot of questions I had (e.g. importance of M, N) and I liked it a lot. \n\n--- Overall the paper is quite clearly written and conveys the message quite clearly.\n\n[Cons, and comments]\n\n--- My main concern is that the most similar approach Maxmin (Lan et al. 2020) which the authors cited multiple times was not comprehensively tested in the experiments. More concretely, Maxmin was not presented in the main plots (Figure 1 and 2) and only showed up (and in a modified fashion) as an ablation point in Figure 3 where it seemed like its performance was pretty bad. From the original Maxmin paper it seemed like they did not try Mujoco; is it the case that Maxmin did not really scale up to Mujoco? \n\nSpecifically, it is a bit disturbing that Maxmin did not appear in Figure 2 which studied the Q estimation bias. Compared with the baselines in that figure (SAC20, AVG), Maxmin sounds much better in terms of reducing the Q estimation error (it also has both ensemble and in-target minimization). I am quite curious how Maxmin does on the bias, and specifically if it does well on the bias but performs worse than REDQ, what is really going on.\n\n\n--- The key algorithmic novelty of REDQ seems to be the combination of two existing ideas: an ensemble of N=10 networks, as well as in-target minimization over only M=2 randomly sampled networks from the ensemble. I am not entirely sure whether this could be considered as of enough novelty in this area (I am not super familiar with the literature here, so authors / other reviewers please feel free to correct me if I am wrong.) Also, given that the ablation studies showed that having both parts (large N, small M) is indeed important, at this moment I am only thinking of this as a weak concern.\n\n------\nAfter rebuttal: Thank the authors for their efforts in the rebuttal and revision. The authors' response to the Maxmin question (along with the revised discussions) sound convincing to me. I would like to keep my original evaluation and would lean towards acceptance for this paper.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113600, "tmdate": 1606915762577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review"}}}, {"id": "jQ5RrAdDQl7", "original": null, "number": 2, "cdate": 1603674568825, "ddate": null, "tcdate": 1603674568825, "tmdate": 1606754422609, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review", "content": {"title": "Official Review 1", "review": "Summary: This paper proposes a new model-free algorithm called Randomized Ensemble Double Q Learning (REDQ) for the optimal control problem. Also, this paper empirically shows that the performance of REDQ is not worse than the model-based algorithm, MBPO. \n\nComments: \nPro. \n1. Empirical studies significantly show that REDQ takes less environment interactions but achieves much higher average return compared to SAC algorithm. Also, the performance of REDQ is even better than MBPO in the Hopper problem and Humanoid problem.\n2. The theoretical analysis introduces the relation among two hyper-parameters (M, N) and the expected random approximation error. \n\nCon. \n1. In Section 3.1 Theoretical Analysis, the theoretical result is not complete enough. The Maxmin Q-Learning paper, Lan et al. (2020), also proves that Maxmin Q-learning algorithm has a vanishing approximation variance (with N tends to infinity). This result is vaguely mentioned below Theorem 1 but there is no rigorously proof provided. \n2. The authors claim that the REDQ algorithm is at least no worse than MBPO. This statement is based on the empirical studies or intuitive explanations rather than theoretical analysis. It will be better to provide more comprehensive analysis of the algorithm. \n\n%--------------------------------%\nI thank the authors for clarifying my questions and concerns. The authors have included further theoretical developments in the revision, and they look satisfactory to me. Overall, I tend to accept this paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113600, "tmdate": 1606915762577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review"}}}, {"id": "t0WC-04HtwF", "original": null, "number": 7, "cdate": 1606294907301, "ddate": null, "tcdate": 1606294907301, "tmdate": 1606294907301, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Revision has been uploaded", "comment": "We would like to thank the reviewers again for your insightful comments. We have just uploaded our revision. Here is a summary of changes:\n\nIn response to Reviewer2, We now have a more extensive discussion on why Maxmin does not work well on the continuous action space MuJoCo environments with large ensemble sizes. In section 3 (top of page 5) we mentioned the explanation is given in section 4. In section 4 (variants and ablations, page 7-8), we now have a more clear and detailed discussion. We also cited recent works in offline (batch) DRL that might provide more insight for the readers. \n\nFollowing the suggestions of Reviewer 1 and Reviewer 4, we added two additional theoretical results to section 3 of the main paper (page 6-7), and section A of the appendix. We derive a bound for the variance of the target for the weighted version of REDQ, and provide a proof of convergence for REDQ in the tabular case. \n\nFollowing the suggestions of Reviewer3, we added comparison of REDQ and the Weighted variant on all four environments to appendix H. Results show that overall REDQ is more robust, though their performance is similar in environments other than Ant. We cited additional works on exploration to help explain the difference in performance. \n\nWe have also fixed some minor issues and typos. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "y0X57wGF-sw", "original": null, "number": 6, "cdate": 1605708046275, "ddate": null, "tcdate": 1605708046275, "tmdate": 1605708046275, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "7cI0qz49cYu", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "Thank you for your positive feedback and detailed review. \n\nIndeed, the algorithmic idea of REDQ is simple and straightforward. However, we argue that applying REDQ to the high UTD ratio setting is very novel and significant. For model-free methods, a UTD of 1 is the most common value for recent algorithms. Our work answers the very important question of \u201cwhy can\u2019t we exploit the data further with more updates\u201d. Our experiments show that naively increasing the UTD leads to fast accumulation of bias, but when the bias is properly controlled, we can get huge improvements in sample efficiency. \n\nThroughout the paper, we have deliberately made an effort to keep our algorithm simple and refrained from using complicated hacks to boost performance. As discussed in the paper, a number of decisions are made to ensure our comparisons are meaningful and fair. This simplicity is crucial for making clean analysis and ablations. \n\nThanks for pointing out that our work can benefit from more theoretical results. The revised submission will include two additional theoretical results: (1) We will provide an outline of the proof of convergence of the tabular form of REDQ when the Robbins-Monroe conditions for the step sizes are satisfied. An outline is sufficient, since the proof is very similar to other such convergence proofs in the literature (for example, in the Maxmin paper). (2) We will derive a bound for the variance of the target for the weighted version of REDQ (i.e., when the target is the expectation of across all samples of size M from the N Q functions). We also show that this bound goes to zero as N goes to infinity. \n\nThank you for pointing out the latex issues, we will fix them in our revision!"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "SEOYnLv0uc", "original": null, "number": 5, "cdate": 1605707969480, "ddate": null, "tcdate": 1605707969480, "tmdate": 1605707969480, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "jQ5RrAdDQl7", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "Thank you for your mostly positive feedback!\n\nThanks for pointing out that our work can benefit from more theoretical results. The revised submission will include two additional theoretical results: (1) We will provide an outline of the proof of convergence of the tabular form of REDQ when the Robbins-Monro conditions for the step sizes are satisfied. An outline is sufficient, since the proof is very similar to other such convergence proofs in the literature (for example, in the Maxmin paper). (2) We will derive a bound for the variance of the target for the weighted version of REDQ (i.e., when the target is the expectation across all samples of size M from the N Q functions). We also show that this bound goes to zero as N goes to infinity. This result parallels the variance result in the Maxmin paper, although it is different since the algorithm is different. We also use different tools since we are making much weaker assumptions about the noise term. \n\nFor DRL algorithms with non-linear neural networks, it can be challenging to prove mathematically that one method is better than another. But it is certainly an interesting direction for future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "kGOrLkCyG-K", "original": null, "number": 4, "cdate": 1605707830256, "ddate": null, "tcdate": 1605707830256, "tmdate": 1605707830256, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "_uIvVmQLj-K", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "Thank you for your detailed review and positive feedback. \n\nFor the \u201cWeighted\u201d variant, we agree that exploration can be an issue. Indeed, the \u201cWeighted\u201d variant tends to give more conservative Q estimates, which might discourage the policy from taking exploratory actions. We will make this more clear in the revision. \n\nThe study on more policy updates is in fact already given in the appendix, section F. In summary, the results show that for SAC with high UTD, more policy updates often leads to faster bias accumulation and worse performance. For REDQ, more policy updates leads to slightly better performance in some environments, and slightly worse in others. In general, taking fewer policy updates can stabilize Q values and sometimes boost performance, but when the bias is adequately controlled, training is less affected by additional policy updates. \n\nIn terms of the number of training epochs, as discussed in section 2.1, page 3, we are mainly comparing with the MBPO results, many of our hyperparameters are set to be consistent with MBPO for a fair comparison, and all our experiments are run using the same number of epochs, with the same evaluation protocol, as done in the MBPO paper. \n\nOur work is currently focused on the MuJoCo benchmark; however, trying this idea in stochastic environments is certainly an exciting direction for future work. \n\nThank you for the suggestion on performing an analysis of the gradient of Q against actions. However, we are not quite clear what you are suggesting here. Would you be able to provide more explicit details about what you have in mind?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "AeMM_Y_ElnO", "original": null, "number": 3, "cdate": 1605707739486, "ddate": null, "tcdate": 1605707739486, "tmdate": 1605707739486, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "5lT-ZXnIXT", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "Thank you for your very detailed review and your positive feedback. \n\nWe would like to first respond to your main concern about Maxmin. We have actually tested multiple Maxmin variants with different ensemble sizes for the MuJoCo environment. The results are very similar to when we change the in-target minimization parameter M in the ablation section: when we increase the ensemble size to be larger than 3, Maxmin starts to reduce the bias so much that we get a highly negative Q bias, and this negative bias accumulates so fast that the Q network becomes unstable, resulting in poor performance. A short discussion on this has been given on page 7. In our revision, we will modify this section to make it more clear.\n\nIn the Maxmin paper, the Maximin algorithm was mainly tested on Atari environments with a small finite action space, in which case it provides good performance. Our results show that when using environments with high-dimensional continuous action spaces, such as MuJoCo, the rapid accumulation of (negative) bias becomes a problem. This result parallels some recent research in offline (i.e., batch) DRL. In the paper An Optimistic Perspective on Offline Reinforcement Learning (Agarwal et al., 2019), it is shown that with small finite action spaces, naive offline training with DQN only slightly reduces performance. However, continuous action Q-learning based methods (such as DDPG, SAC) suffer much more from Q bias accumulation compared to discrete action methods. For example, see Off-Policy Deep Reinforcement Learning without Exploration (Fujimoto et al., 2018) and Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction (Kumar et al., 2019) for the continuous action case, where naive offline training can entirely fail. \n\nFor your concern on the novelty of the paper, a major novelty of our work is a focused study on the high update-to-data (UTD) ratio setting. We show that naively increasing this ratio will lead to high bias and poor performance, but by properly controlling the Q bias, model-free methods can benefit hugely from high UTD. (Additional experiments on different UTDs are presented in appendix section G.) This issue is very important yet rarely studied in the model-free DRL literature. Although our REDQ algorithm seems simple, simplicity is a major strong point. As discussed in the paper, we have deliberately made an effort to keep our algorithm simple, and refrained from using complicated hacks to boost performance, allowing for a fair comparison with other algorithms. Having a simple structure also makes our ablations cleaner, and makes it easier to apply REDQ to other existing algorithms. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "BUWaGGP1sR", "original": null, "number": 2, "cdate": 1605707633223, "ddate": null, "tcdate": 1605707633223, "tmdate": 1605707633223, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment", "content": {"title": "Response to All Reviewers", "comment": "We would like to thank all the reviewers for their detailed reviews and positive feedback. In response to your insightful comments, we are preparing a minor revision of our original submission. We will upload the revision before the end of the rebuttal period. \n\nIn our paper, we introduce a novel model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, the state-of-the-art model-based algorithm for the MuJoCo benchmark. This result indicates that, at least for the MuJoCo benchmark, models may not be necessary for achieving high sample efficiency. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ also provides a huge improvement in sample efficiency over SAC.\n\nWe defined the Update-To-Data (UTD) ratio as the number of updates taken by the agent compared to the number of actual interactions with the environment. Most of the well-known model-free algorithms (SAC, DDPG and so on) use a UTD ratio equal to one; when higher UTD ratios are used, performance declines dramatically. REDQ, on the other hand, is able to use a UTD ratio >> 1 and achieve high sample efficiency and high performance. This is achieved by carefully integrating double Q-learning with an ensemble framework. To our knowledge, for continuous action spaces, REDQ is the first successful model-free DRL algorithm using a UTD ratio >> 1. Our paper provides a comprehensive analysis through mathematical arguments as well as extensive experimental and ablation studies. \n\nWe believe that the paper is important for the community. In order to improve the paper\u2019s visibility, if you are satisfied with our response and revision, we kindly ask you to consider increasing your score and recommending acceptance as a spotlight paper. Thank you. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AY8zfZm0tDd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1662/Authors|ICLR.cc/2021/Conference/Paper1662/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857169, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Comment"}}}, {"id": "_uIvVmQLj-K", "original": null, "number": 3, "cdate": 1603739728087, "ddate": null, "tcdate": 1603739728087, "tmdate": 1605024389057, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review", "content": {"title": "Good idea and well written", "review": "Summary\nThe paper proposes three techniques that altogether greatly improves the performance of soft actor-critic (SAC), resulting in a new algorithm called REDQ. (1) A higher update-to-data ratio, which speeds up the critic update. (2) Using the average ensemble Q for the policy gradient, therefore reducing its variance. (3) Taking the min of a small subset of the ensemble Qs to compute the target Q, therefore reducing the Q bias. The paper also performs extensive ablation studies to prove the importance of each technique.\n\nRecommendation\nOverall I think the paper deserves an acceptance. The proposed solution is simple, effective, and justified.\n\nStrengths\n1. REDQ is simple and effective. Implementation requires little change to the backbone of SAC. The authors also provide example code. Fig 1 shows clear advantage over SAC.\n2. Ablation studies are rigorous: e.g. Fig 3 even studies fractional M = 2.5, when M = 3 is clearly under-performing.\n3. The paper is very well written. In particular, the most important algorithm block and experimental results are shown early, leading to a smooth reading experience. \n\nWeaknesses\n1. The paper analyzes the normalized standard deviation of Q bias, but in fact the gradient of Q against actions is more important (line 12, Alg 1). It would be great to see analysis on the gradient as well.\n\n2. Some (minor) missing studies. \n(a). I am surprised that \u201cWeighted\u201d performs much worse than REDQ  on Ant (Fig 3g). The authors suggest overfitting as a potential cause. But I am curious whether exploration is the real issue, and whether the same happens on other envs.\n(b). Algorithm 1 suggests training Q G times before training the policy once. Does training the policy more frequently help? It makes sense since the avg Q has lower variance.\n(c). Why not train up to 1, 3, 10 million steps as in the SAC paper (Fig 1)? Especially for Humanoid, The performance isn\u2019t near 6,000 as reached in the SAC paper.\n\nOther feedbacks\nMujoco envs are rather deterministic and the noise comes from the policy itself. Have you considered other more noisy envs? The ensemble could have a larger impact there.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113600, "tmdate": 1606915762577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review"}}}, {"id": "7cI0qz49cYu", "original": null, "number": 1, "cdate": 1603674440248, "ddate": null, "tcdate": 1603674440248, "tmdate": 1605024388894, "tddate": null, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "invitation": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review", "content": {"title": "A variant of double Q-learning, with randomized ensembling", "review": "This work proposes a modification for double Q-learning, termed as randomized ensembled double Q-learning (REDQ). REDQ maintains $N$ different Q functions, and for each update, the target value is a minimization over $M$ randomly chosen Q functions, where $1 \\le M \\le N$. In addition, REDQ adopts a high update-to-data ratio to improve the sample efficiency. Empirical results show that the proposed method outperforms state-of-the-art model-based algorithms in certain tasks with continuous action space.\n\nOverall, I think this is a well written paper. \n\nPros:\n\n- The algorithmic idea of REDQ is quite intuitive and reasonable. In particular, setting $M$ and $N$ separately allows for more flexibility (compared with double Q and other variants): by increasing $M$, one can achieve a smooth transition from over-estimation of value functions to under-estimation (as validated by a simple theoretical analysis). Moreover, this idea is quite general and can be easily plugged into many existing off-policy model-free algorithms.\n\n- The numerical experiments are quite extensive. The results convincingly show that the proposed REDQ algorithm achieves a minor underestimation bias with low standard deviation, leading to better overall performance. In addition, REDQ is also computationally efficient.\n\nCons:\n\n- The novelty in the algorithmic idea of REDQ is a bit simple and not very significant. \n\n- The theoretical analysis of this work is quite limited, e.g. lacking the (most basic) asymptotic convergence analysis in the tabular case.\n\nA minor comment: it might be better to use \"\\gg\" and \"\\ll\" in latex, instead of \">>\" and \"<<\". ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1662/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1662/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "authorids": ["~Xinyue_Chen1", "~Che_Wang1", "~Zijian_Zhou1", "~Keith_W._Ross1"], "authors": ["Xinyue Chen", "Che Wang", "Zijian Zhou", "Keith W. Ross"], "keywords": ["Artificial Integlligence", "Machine Learning", "Deep Reinforcement Learning"], "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio $\\gg 1$; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio $\\gg 1$. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|randomized_ensembled_double_qlearning_learning_fast_without_a_model", "one-sentence_summary": "We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.", "supplementary_material": "/attachment/60d908843f7fdbb8868156ff3c93985657859fb9.zip", "pdf": "/pdf/c4e6642e1eb45768ab5d7a3291c2a0088665e5c4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021randomized,\ntitle={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},\nauthor={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AY8zfZm0tDd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AY8zfZm0tDd", "replyto": "AY8zfZm0tDd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113600, "tmdate": 1606915762577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1662/-/Official_Review"}}}], "count": 12}