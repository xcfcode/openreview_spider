{"notes": [{"id": "lcOwh022Vta", "original": null, "number": 18, "cdate": 1614992382166, "ddate": null, "tcdate": 1614992382166, "tmdate": 1614992382166, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "5GgOgMdJWyc", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Re: Question...", "comment": "Hi, glad you're finding it useful! For small datasets, any augmentation of pitch and loudness you can do should be helpful as long as it doesn't also dramatically change the timbre of the sound. \n\nSince the model is deterministic, complex timbres are difficult if they require a one to many mapping (one set of pitch/loudness resulting in many timbres). You could get around this by using a generative model such as autoregression or a GAN, but that is not currently in the code base.\n\nThank you for bringing attention to the bug. There is currently an issue open on the GitHub and we'll hopefully resolve it soon."}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference", "ICLR.cc/2020/Conference/Paper435/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "5GgOgMdJWyc", "original": null, "number": 5, "cdate": 1612882631744, "ddate": null, "tcdate": 1612882631744, "tmdate": 1612882631744, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment", "content": {"title": "Question on how to model the dataset", "comment": "Hello, thank you for sharing your work! \nMe and my colleagues are trying to use it in order to model timbres of different sounds found in nature (like animals) and we have a few doubts about how to best design the dataset.\n\nSince it\u2019s very hard to find those sounds with a high variability we have to rely on data augmentation performing pitch scaling and time stretching. \nDo you think it would be necessary/helpful to have different samples in terms of loudness and other parameters too?\n\nIf the timbre of a sound is very stable and its only occurrences have a fixed pitch and very little dynamics (e.g. car horn) would it be acceptable to have a smaller dataset with respect to those more complex and dynamic timbres for network to learn its characteristics?\n\nLastly we were wondering if the model would be able to learn complex timbres with a very peculiar envelope or transient like a bird sound.\n\nIn addition to that we would like to report that the Colab \u201ctimbre_transfer\u201d gives an error when trying to upload a file instead of recording it. We fixed it by changing \u201caudios[0]\u201d with \u201caudios[0][-1]\u201d  in line 16 of the cell Record or Upload Audio. \n\nThank you in advance!"}, "signatures": ["~Luigi_Attorresi1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference", "~Luigi_Attorresi1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209149, "tmdate": 1576860578939, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment"}}}, {"id": "XsuQcqUdlG8", "original": null, "number": 4, "cdate": 1598501658800, "ddate": null, "tcdate": 1598501658800, "tmdate": 1598501658800, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "4Qxgcey47uO", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment", "content": {"title": "section 3.5", "comment": "Yes, I meant section 3.5, thanks."}, "signatures": ["~Yuchao_Song1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yuchao_Song1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209149, "tmdate": 1576860578939, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment"}}}, {"id": "4Qxgcey47uO", "original": null, "number": 17, "cdate": 1598472844496, "ddate": null, "tcdate": 1598472844496, "tmdate": 1598472844496, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "ROzs6NlUwaV", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "good catch!", "comment": "I think you mean section 3.5 right? And it's a good catch, it was a typo and we meant DFT, because we are converting the noise into the frequency domain. We've revised the paper, thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "B1x1ma4tDr", "original": "B1x_Xca8Pr", "number": 435, "cdate": 1569438999422, "ddate": null, "tcdate": 1569438999422, "tmdate": 1598472744098, "tddate": null, "forum": "B1x1ma4tDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "ROzs6NlUwaV", "original": null, "number": 3, "cdate": 1598472252432, "ddate": null, "tcdate": 1598472252432, "tmdate": 1598472252432, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment", "content": {"title": "question regarding the noise synthesizer section", "comment": "Great work!\n\nOne thing to confirm, in section 3.4, the last sentence, is the $N_l$ IDFT of the noise or DFT? Thanks."}, "signatures": ["~Yuchao_Song1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yuchao_Song1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209149, "tmdate": 1576860578939, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment"}}}, {"id": "BEWx--a56pB", "original": null, "number": 16, "cdate": 1589572414115, "ddate": null, "tcdate": 1589572414115, "tmdate": 1589572414115, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "9CpgB7vNlmq", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Re: Tiny violin", "comment": "Hi! The gin config for the tiny violin model can now be found in the github repo: https://github.com/magenta/ddsp/blob/master/ddsp/training/gin/papers/iclr2020/tiny_instrument.gin\n\nAnd the exact pieces in the dataset can be found in the latest draft of the paper (pg.7, footnote 4)."}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "9CpgB7vNlmq", "original": null, "number": 2, "cdate": 1589134924018, "ddate": null, "tcdate": 1589134924018, "tmdate": 1589134924018, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment", "content": {"title": "Tiny violin model", "comment": "Hello, thanks for the library, it's really promising. Could you please share exactly what parameters and corpus were used to train the tiny violin model in this example?\nhttps://storage.googleapis.com/ddsp/index.html#tiny"}, "signatures": ["~Andrey_Kramer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Andrey_Kramer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209149, "tmdate": 1576860578939, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment"}}}, {"id": "UiyfRr2qEe", "original": null, "number": 15, "cdate": 1580514017290, "ddate": null, "tcdate": 1580514017290, "tmdate": 1580514017290, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "lqR3ddKulK", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Code now available", "comment": "Hi Keunwoo,\n\nThanks for the helpful questions. We've updated the paper with relevant details, and all further specifics can be found at the GitHub repo: https://github.com/magenta/ddsp\n\nSome real quick responses for posterity.\n\nEncoder\n- How many MFCCs in the encoder?\n>>> 30\n\n- So is GRU in the encoder many-to-many?\n>>> Yes\n\n- Is Dense in the encoder time-distributed dense?\n>>> Yes\n\n- If the Dense in the encoder a 512-unit linear, how the dim of `z` becomes 16 dim?\n>>> It uses a linear layer to compress to 16 dims per a timestep.\n\n- So `z` has 250 time steps, meaning (assuming window size == fft size of 1024), because the hop length is 256, the input length of the target audio (1024 + 256 * 249 = 64798 samples, which is then 4.048 second?)\n>>> Yes, but we trim the end to match.\n\n- (Hantrakul et al., 2019) does not have any Appendix pages. What is the exact formula for the l-encoder?\n>>> https://github.com/magenta/ddsp/blob/master/ddsp/spectral_ops.py#L171\n\nDecoder\n- What's the length of f(t), l(t), z(t)? 250 time steps?\n>>> Yes.\n\n- So are the Dense layers in MLP all time-distributed dense layers?\n>>> Yes.\n\n- \"The outputs of MLPs are concatenated\" so, each output is (batch, 512, time=250), and the time-step is preserved, making their concat (batch, 1536, 250)?\n>>> Yes.\n\n- \"concatenate the GRU outputs\" \u2192 Does it mean (batch, 512, time=250) is concatenated over time axis, making (batch, 512 * 250=128000)?\n>>> It means they for (batch, 1536, 250) as in the previous question. https://github.com/magenta/ddsp/blob/master/ddsp/training/decoders.py#L60\n\n\n- What is the number of hidden units in the final Dense layers? \n>>> Same, 512.\n\n3.2\n- The amplitudes and harmonic distribution -- in other words, are they sequentially i) bilinear-upsampled and then ii) smoothed? \n>>> Just bilinearly upsampled. https://github.com/magenta/ddsp/blob/master/ddsp/core.py#L407\n\n3.3 Filter design\n- What is the FFT size, hop length, etc in this module?\n>>> Hop length is given by the frame size of the filter coefficients (64000 / 250). FFT Size is 512 I believe.\n\n3.5 Reverb\n- What is the hyperparams of this module? E.g., the reverberation length. \n>>> 64000 samples long, but that's overkill, shorter is fine for most samples.\n\nTraining\n- What's the length/size of the input audio files? 4 second?\n>>> For training, yes.\n\n---\nEDIT: some more questions.\n- What happened when there's Z module as well during training on the solo violin dataset? Did it hinder Reverb module to learn the reverberation? \n>>> We didn't use the Z module during training, to encourage the model to take care of timbre / expression so the user wouldn't have to."}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "lqR3ddKulK", "original": null, "number": 1, "cdate": 1577649550691, "ddate": null, "tcdate": 1577649550691, "tmdate": 1577730097062, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment", "content": {"title": "Questions for reproducing the result", "comment": "Hi, thanks for the great work. I have some questions that arose while implementing it. I heard from Hanoi that it's going to be open-sourced very soon, but I thought it'd be helpful anyway to do this Q&A public. I hope it's not overwhelming :) Some of the questions are to be extra sure, but most of them are necessary details that I couldn't get from the paper. \n\n---\nEncoder\n- How many MFCCs in the encoder?\n- So is GRU in the encoder many-to-many?\n- Is Dense in the encoder time-distributed dense?\n- If the Dense in the encoder a 512-unit linear, how the dim of `z` becomes 16 dim?\n- So `z` has 250 time steps, meaning (assuming window size == fft size of 1024), because the hop length is 256, the input length of the target audio (1024 + 256 * 249 = 64798 samples, which is then 4.048 second?)\n- (Hantrakul et al., 2019) does not have any Appendix pages. What is the exact formula for the l-encoder?\n\nDecoder\n- What's the length of f(t), l(t), z(t)? 250 time steps?\n- So are the Dense layers in MLP all time-distributed dense layers?\n- \"The outputs of MLPs are concatenated\" so, each output is (batch, 512, time=250), and the time-step is preserved, making their concat (batch, 1536, 250)?\n- \"concatenate the GRU outputs\" \u2192 Does it mean (batch, 512, time=250) is concatenated over time axis, making (batch, 512 * 250=128000)?\n- What is the number of hidden units in the final Dense layers? \n\n3.2\n- The amplitudes and harmonic distribution -- in other words, are they sequentially i) bilinear-upsampled and then ii) smoothed? \n\n3.3 Filter design\n- What is the FFT size, hop length, etc in this module?\n\n3.5 Reverb\n- What is the hyperparams of this module? E.g., the reverberation length. \n\nTraining\n- What's the length/size of the input audio files? 4 second?\n\n---\nEDIT: some more questions.\n- What happened when there's Z module as well during training on the solo violin dataset? Did it hinder Reverb module to learn the reverberation? \n"}, "signatures": ["~Keunwoo_Choi1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Keunwoo_Choi1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209149, "tmdate": 1576860578939, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Public_Comment"}}}, {"id": "Lk4m9JvHck", "original": null, "number": 1, "cdate": 1576798696332, "ddate": null, "tcdate": 1576798696332, "tmdate": 1576800939315, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper proposes a novel differentiable digital signal processing in audio synthesis. The application is novel and interesting. All the reivewers agree to accept it. The authors are encouraged to consider the reviewer's suggestions to revise the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717543, "tmdate": 1576800267873, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper435/-/Decision"}}}, {"id": "rkgTv1y5iB", "original": null, "number": 13, "cdate": 1573674853096, "ddate": null, "tcdate": 1573674853096, "tmdate": 1573674853096, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "rJemdEkV9H", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "Thank you for your review and helpful comments. We have replied to your main question below.\n\n> \u201chow susceptible do you think the system is robust with respect to f0 and loudness encoders? Have you experimented with situations where the f0 and the loudness encoders might fail (such as more non-periodic and noisy signals)?\u201d\n\nFor the specific harmonic+noise model we consider in this paper, accurate f0 estimation is very important. It is a strong constraint of the model by construction. We can see this when we train a DDSP autoencoder with a learnable f0 encoder. The model first reduces loss by covering the spectrogram with filtered noise, and only later replaces that noise with harmonic content when it can more accurately follow the f0 contours. The loudness conditioning signal is extracted directly from the audio, so is not a source of such variability. \n\nDue to its construction, the harmonic constraints are not appropriate for modeling non-periodic signals, and we haven\u2019t tested on that type of data. However, there are many similar variants (such as an unconstrained sinusoidal+residual model) that have been shown to work well on more general-purpose sounds, and this is definitely an area we\u2019d like to explore in future research.\n\nBeyond sinusoidal+noise modeling, we found in our experiments that waveforms can be added linearly. This means using DDSP components does not preclude using raw waveform generation. We demonstrate this in the paper by adding the output waveforms of the additive and noise synthesizers, but future work can extend this waveforms generated directly by neural networks, which may be an efficient manner of representing transients.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "S1e0NJJcsH", "original": null, "number": 12, "cdate": 1573674806209, "ddate": null, "tcdate": 1573674806209, "tmdate": 1573674806209, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "SygxGy1cor", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3 (continued 2/2)", "comment": "> \u201c ...the authors claim that one of the strengths of the proposed method is that the models are \u201crelatively small\u201d... ...Section 4.2 should provide some description of the parameters counts for each of the components considered and how this compares with existing auto-regressive generation algorithms.\u201d\n\nThank you for highlighting that we should compare model sizes more explicitly, as we agree that it would help support the paper. Accordingly, we have added a new table in the supplemental with parameter counts from all comparable models. We have also conducted some promising initial experiments in reducing model size that we have added to the supplement. We have correspondingly added the following paragraph to Section 4.1 (which has been swapped with section 4.2 for better flow):\n\n\u201c\u201d\u201d\nModel Size:\nTable B.6, compares parameter counts for the DDSP models and comparable models including GANSynth (Engel et al., 2019), WaveRNN (Hantrakul et al., 2019), and a WaveNet Autoencoder (Engel et al., 2017). The DDSP models have the fewest parameters (up to 10 times less), despite no effort to minimize the model size in these experiments. Initial experiments with very small models (240k parameters, 300x smaller than a WaveNet Autoencoder) have less realistic outputs than the full models, but still have fairly high quality and are promising for low-latency applications, even on CPU or embedded devices. Audio samples are available in the online supplement.\n\u201c\u201d\u201d\n\nReference to the size comparisons has also been added in the introduction under the part for autoregressive models being very large.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "SygxGy1cor", "original": null, "number": 11, "cdate": 1573674760168, "ddate": null, "tcdate": 1573674760168, "tmdate": 1573674760168, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "rJlmneOVqr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3 (part 1/2)", "comment": "Thank you for your time and expertise in your review. We\u2019ve done our best to address your key points with paper revisions and the comments below.\n\n> \u201cI think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar, since the description and experiments very specifically deal with audio synthesis with vocoders, even though the components might be general DSP components.\u201d\n\nThis is a very valid point and something we\u2019ve done a lot of back and forth about. A more limited title would be more descriptive of the specific experiments performed in the paper. However, a main partner of the paper is the release of the corresponding Tensorflow library, that we hope will see use in a much broader range of applications. As the library will likely not receive its own paper in another venue, we would like this paper to serve as the initial and primary reference, which the current title accomplishes. It\u2019s a tough call, but I think we\u2019d like to stick with the title as it is.\n\n> \u201c1. The reference provided for RNNs, Sutskever et. al. 2014, should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time.\u201d\n \nAgreed. We\u2019ve added citations to a popular paper by Werber and book by Williams and Zipser from 1990. \n\n> \u201c2. \u201cThe bias of the natural world is to vibrate.\u201d I am not sure what exactly is meant here. Is it really a \u201cbias\u201d if every object in the universe vibrates? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors. Bias as used in the paper, seems to have several means. It would help the description if the authors cleared up this confusing use of the term.\u201d\n\nThank you for pointing out that we\u2019ve overloaded the term \u201cbias\u201d, which we indeed use interchangeably with structural/inductive priors. We\u2019ve amended the paper in several places to be more explicit. For instance we\u2019ve rewritten the paragraph in the introduction to try and be more straightforward:\n\n\u201c\u201d\u201d\nObjects have a natural tendency to periodically vibrate. Small shape displacements are usually restored with elastic forces that conserve energy (similar to a canonical mass on a spring), leading to harmonic oscillation between kinetic and potential energy (Smith, 2010).... ...However, neural synthesis models often do not exploit this periodic structure for generation and perception.\n\u201c\u201d\u201d\n\n> \u201c4. In Section 3, some additional high-level description for the Harmonic plus Noise model (Serra & Smith, 1990) should be provided to motivate the discussion and experiments in the rest of the paper.\u201d\n\nWe agree this would strengthen the paper, as the model expressivity seems to have been a point of confusion for some readers. We have added a Section 3.1 with citations to better motivate the use and expressivity of the model:\n\n\u201c\u201d\u201d3.1\nSPECTRAL MODELING SYNTHESIS\nHere, as an example DDSP model, we implement a differentiable version of Spectral Modeling Synthesis (SMS) Serra & Smith (1990). This model generates sound by combining an additive synthesizer (adding together many sinusoids) with a subtractive synthesizer (filtering white noise). We choose SMS because, despite being parametric, it is a highly expressive model of sound, and has found widespread adoption in tasks as diverse as spectral morphing, time stretching, pitch shifting, source separation, transcription, and even as a general purpose audio codec in MPEG-4 (Tellman et al., 1995; Sanjaume, 2002; Klapuri et al., 2000; Purnhagen & Meine, 2000).\n\nAs we only consider monophonic sources in these experiments, we use the Harmonic plus Noise model, that further constrains sinusoids to be integer multiples of a fundamental frequency (Beauchamp, 2007). One of the reasons that SMS is more expressive than many other parametric models because it has so many more parameters. For example, in the 4 seconds of 16kHz audio in the datasets considered here, the synthesizer coefficients actually have \u223c2.5 times more dimensions than the audio waveform itself ((1 amplitude + 100 harmonics + 65 noise band magnitudes) * 1000 timesteps = 165,000 dimensions, vs. 64,000 audio samples). This makes them amenable to control by a neural network, as it would be difficult to realistically specify all these parameters by hand.\n\u201c\u201d\u201d\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "SkxG6ARYiH", "original": null, "number": 10, "cdate": 1573674682480, "ddate": null, "tcdate": 1573674682480, "tmdate": 1573674682480, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "SyeL1QN2qS", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2", "comment": "Thank you for the review and the encouraging comments. We\u2019ve done our best to address your questions with paper revisions and the comments below.\n\n> \u201cThe authors present a perceptual loss that addresses the point (Eq. 4 - clarify the difference w.r.t. Wang et al.), . A natural question thus is whether applying this loss on e.g. Wavenet, Sample RNN or Wave RNN would solve the problem.\u201d\n\nThe loss in Equation 4 is a multi-scale spectrogram loss in both magnitude and log-magnitude. Wang et al. (2019) use a similar loss. Their loss differs in that they use different window/hop sizes than this paper, 3 scales instead of 6, and a phase loss instead of a linear magnitude loss. As described in Section B.4, we also use a \u201cperceptual\u201d loss for the unsupervised DDSP autoencoder (that must jointly learn to infer f(t)), using the activations of a pretrained CREPE model.\n\nWe agree that it would be great to be able to use the multi-scale spectrogram losses and pretrained model losses for autoregressive waveform models such as WaveNet, SampleRNN, or WaveRNN. Unfortunately, one of the drawbacks of these models (as described in Section 1) is that they are trained with teacher forcing and do not generate samples during training (which would be computationally infeasible). This prevents using any losses that actually compare target audio and generated audio, and motivates the approach taken in this paper (and others e.g. GANSynth, NSF, etc.).\n\n\n> \u201cI understand that the auto-encoder is enriched with a FIR filter at its core: the input is mapped into the time-frequency domain; convolved with the output of the neural net H_l, and the result is recovered from the time-frequency domain. Explain \"frames x_l to match the impulse responses h_l\".\u201d\n\nFor memory/compute efficiency, the outputs of the neural network are not at audio rate. In the paper experiments the FIR coefficients are provided every 4ms i.e. for audio at 16kHz, the FIR coefficients are provided every 64 samples. \n\nTo make this concrete, the size of the coefficients tensor is then (n_batch, n_frames=1000, n_coefficients=65), while the size of audio is (n_batch, n_samples=64000). The audio is then divided into the same number of frames, (n_batch, n_frames=1000, n_samples_per_frame=64), using non-overlapping box windows. We then take the DFT of each frame, multiply them with IDFT(Window(DFT(coefficients)) and take the IDFT to get the filtered audio frames (n_batch, n_frames=1000, n_samples_per_frame=64). Finally we flatten the frames (box window with no overlap) to get the filtered audio (n_batch, n_samples=64000). \n\n> \u201cOverall, the approach works in two modes: one where the fundamental frequency is extracted, one where it is learned. I miss this comparison in the audio material, could you tell where to look / hear ?\u201d\n\nGood point. We have added comparisons of the reconstructions to the online supplement.\n\n> \u201cNN operate at a slower frame rate (sect. 3.2): how much slower ? How sensitive to this parameter ?\u201d\n\nAs we mentioned above, the network in these experiments operates at 1 frame every 4ms (64 samples). We\u2019ve also run experiments with 1 frame every 16ms (256 samples), which are actually faster to optimize, but are limited in the temporal response of quick attacks.\n\n> \u201cwere, end p. 5. A word missing? are useful --> is useful? could produced, p.6\u201d\n\nThanks for the help catching the typos! We\u2019ve fixed them in the revised version of the manuscript.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "SkeFR6AFor", "original": null, "number": 7, "cdate": 1573674448839, "ddate": null, "tcdate": 1573674448839, "tmdate": 1573674603278, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "Hyx1NHgeir", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to Area Chair1 (part 1/3)", "comment": "We thank the area chair for their helpful comments and for calling attention to some aspects of our paper that could benefit from clarification. We have done our best to address each comment independently below and amend the paper where appropriate.\n\nFrom a high-level, most of the confusion seems to stem from the perspective in Comment (1) that \u201cDDSP is based on an autoencoder architecture\u201d. As we discuss below, DDSP is the library of differentiable Digital Signal Processing (DSP) components, agnostic to the given network architecture. It is important to consider DDSP components as analogous to network components (such as convolutional layers or recurrent layers) rather than a specific generative model itself.\n\nTo further clarify, we have added the following paragraph to Section 4.1 where we introduce the autoencoder architecture:\n\u201c\u201d\u201d\u201d\nDDSP components do not put constraints on the choice of generative model (GAN, VAE, Flow, etc.), but we focus here on a deterministic autoencoder to investigate the strength of DDSP components independent of any particular approach to adversarial training, variational inference, or Jacobian design. Just as autoencoders utilizing convolutional layers outperform fully-connected autoencoders on images, we find DDSP components are able to dramatically improve autoencoder performance in the audio domain. Introducing stochastic latents (such as in GAN, VAE, and Flow models) will likely further improve performance, but we leave that to future work as it is orthogonal to the core question of DDSP component performance that we investigate in this paper.\n\u201c\u201d\u201d\n\n\n>\u201d (1) The authors replaced the decoder with some deterministic synthesizer module, which would constrain the model expressive power due to the low complexity of the predefined module.\u201d\n\nThis is an important point, thank you for highlighting that this could use further clarification. Many parametric synthesis models have constrained expressive power because they seek to reduce the signal to only a few parameters. However, the Sinusoidal plus Noise model considered in this paper is very expressive because it actually has many parameters, and has been used in diverse applications, including even general-purpose audio compression in the MPEG-4 standard[1].\n\nTo make this clearer to the reader we have added the following section 3.1 to the text:\n\n\u201c\u201d\u201d3.1\nSPECTRAL MODELING SYNTHESIS\nHere, as an example DDSP model, we implement a differentiable version of Spectral Modeling Synthesis (SMS) Serra & Smith (1990). This model generates sound by combining an additive synthesizer (adding together many sinusoids) with a subtractive synthesizer (filtering white noise). We choose SMS because, despite being parametric, it is a highly expressive model of sound, and has found widespread adoption in tasks as diverse as spectral morphing, time stretching, pitch shifting, source separation, transcription, and even as a general purpose audio codec in MPEG-4 (Tellman et al., 1995; Sanjaume, 2002; Klapuri et al., 2000; Purnhagen & Meine, 2000).\n\nAs we only consider monophonic sources in these experiments, we use the Harmonic plus Noise model, that further constrains sinusoids to be integer multiples of a fundamental frequency (Beauchamp, 2007). One of the reasons that SMS is more expressive than many other parametric models because it has so many more parameters. For example, in the 4 seconds of 16kHz audio in the datasets considered here, the synthesizer coefficients actually have \u223c2.5 times more dimensions than the audio waveform itself ((1 amplitude + 100 harmonics + 65 noise band magnitudes) * 1000 timesteps = 165,000 dimensions, vs. 64,000 audio samples). This makes them amenable to control by a neural network, as it would be difficult to realistically specify all these parameters by hand.\n\u201c\u201d\u201d\n\nAs seen in Figure 2, the autoencoder in the paper still uses a neural decoder to generate controls for the DSP components. It is true that it cannot generate arbitrary waveforms, unlike generating directly in the time or frequency domain, but also does not suffer the challenges of phase alignment and spectral leakage detailed in Section 1.\n\nLastly, as noted in the paper: waveforms can be added linearly. This means using DDSP components does not preclude using raw waveform generation. We demonstrate this in the paper by adding the output waveforms of the additive and noise synthesizers, but future work can extend this waveforms generated directly by neural networks, which may be an efficient manner of representing transients.\n\n[1] https://ieeexplore.ieee.org/document/856031, (download: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.1019&rep=rep1&type=pdf)\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "H1gnICCFjr", "original": null, "number": 9, "cdate": 1573674580024, "ddate": null, "tcdate": 1573674580024, "tmdate": 1573674580024, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "HJxrEA0KiS", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to Area Chair1 (continued 3/3)", "comment": "> \u201c(6) In the supplemental, it is claimed that the f(t) is replaced with a fixed network during the training, which is different from the claim in the main body that f(t) is the output of the encoder. Does it mean the direct neural encoder f(t) is not a good option?\u201d\n\nWe apologize for the confusion.  As pointed out, there is a misstatement in the supplementary that claims that we wait for future work to jointly learn the f(t) encoder, when if fact it is done in this work. Thank you for pointing this out and we have significantly reworked Section 4.1 and the supplemental to make this clearer. We now consistently refer to the Supervised DDSP Autoencoder as using a pretrained CREPE model (with fixed weights) for f(t) estimation, while the Unsupervised DDSP Autoencoder uses a Resnet on mel-spectrograms (jointly trained with the rest of the model) to estimate f(t). We have added complete details of the Resnet architecture to the supplemental. Both versions employ neural networks to estimate f(t). Non-neural methods can also be used, but are not currently state-of-the-art at the task.\n\n\n> \u201c(7) As claimed in the paper, f(t) is a fixed network, while z is not used in the model trained on solo violin. It means all the encoder parts is no longer exist. Does it mean the decoder is the main component of DDSP?\u201d\n\nThe main components of DDSP are the differentiable signal processing components controlled by decoder outputs, used in all models (the yellow components in Figure 2). As shown in Figure 2, we denote the decoder as the neural network that controls these components. We draw this distinction to highlight that the DDSP components are agnostic to model architectures and loss function (spectral, adversarial, waveform), as long as they provide the appropriate control signals. \n\nFor clarity, we note that prior to this work, such digital signal processing components have not been implemented in a differentiable form, and could not be trained end-to-end in the manner described here.\n\nAs noted, for the solo violin experiments, we did not include a z encoder. Firstly, this allows us to demonstrate that z(t) is not required for high-quality synthesis, getting good reconstructions from only f(t) and l(t). Unlike the NSynth dataset, the solo violin dataset is a single instrument so z(t) is not needed to account for different instruments. Excluding z(t) also allows us to generalize between domains as in the timbre transfer example. Since we do not explicitly promote cross-domain generalization in z(t) (as in a CycleGAN setup), z(t) wouldn\u2019t be useful for timbre transfer. However, f(t) and l(t) are interpretable by design, allowing extracted features to be applied sensibly in a new domain (the synthesized audio follows f(t) for pitch and l(t) for loudness) as shown in Figure 4.\n\n\n> \u201c(8) Regarding the multi-scale spectral loss in Eq.(4), a trade off parameter is needed to balance the two L1 loss.\u201d\n\nWhile hyperparameter tuning of the losses could likely improve results further, in all experiments of this paper, the coefficients each L1 loss is 1.0, so the text of Eq. 4 is technically correct. It is a good point that this is a special case, and we have updated the Eq.4 to have a weighting term (\\alpha), and included the sentence: \u201c...where $\\alpha$ is a weighting term set to 1.0 in our experiments.\u201d \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "HJxrEA0KiS", "original": null, "number": 8, "cdate": 1573674540683, "ddate": null, "tcdate": 1573674540683, "tmdate": 1573674540683, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "SkeFR6AFor", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Responses to Area Chair1 (continued 2/3)", "comment": "> \u201c(2) An MLP decoder is used along with simple concatenation. The output is then used for the harmonic synthesizer and the filtered noise synthesizer. It would fail to get any meaningful interpretability for the latent code since all latent encoder are highly entangled.\u201d\n\nIt is true that, like other deterministic autoencoders, there is no explicit regularization applied to the latent z(t) in this work. However, there is an explicit factorization of z(t) from the other latents (f(t) and l(t)). In practice this results in strong factorization of the effects of each component. As shown in Figure 3 and Table 2, we see quantitative and qualitative evidence that varying f(t) has little effect on the generated loudness, varying l(t) has little effect on the generated frequency, and varying z(t) has little effect on either the generated loudness or frequency. That said, like autoencoders used in image generation, z(t) still captures other aspects of variation even without explicit regularization. We demonstrate this in the audio domain in the right cell of Figure 3, where interpolating z(t) causes the spectral centroid of generated audio to vary smoothly between that of two test samples, while the frequency and loudness contours remain constant. \n\n\n> \u201c(3) It is highly claimed in the paper that the proposed DDSP shows potential in many interesting tasks, while insufficient experiment results are shown. The corresponding experiment results are needed to support the claims in the paper.\u201d\n\nThe claim of broad applications is based upon the diverse use of traditional Digital Signal Processing components in different applications such as signal processing, communications, image processing, video coding, radar, ultrasound etc. [1]. The DDSP library contains differentiable DSP components (time-varying filters, resampling, oscillators, and others) that mirror their widely used non-differentiable counterparts. To demonstrate the core principle of integrating interpretable DSP with expressive neural networks, we needed to focus on a specific application. We chose audio synthesis, as it is an area that end-to-end learning still struggles for the reasons outlined in Section 1 of the paper. \n\n[1] https://en.wikipedia.org/wiki/Digital_signal_processing#Applications\n\n\n> \u201c(4) It is claimed in Section 1.2 that \"small errors in parameters can lead to large errors in the audio that cannot propagate back to the network\". I am very curious about how this issue is addressed or elevate in this paper since a similar deterministic synthesizer module in DDSP.\u201d\n\nThe determinism of the DDSP components is orthogonal to this claim about differentiability. Through the experiments in the paper, we demonstrate that neural networks that control DDSP components can be trained in an end-to-end fashion to minimize losses on the generated audio in an unsupervised manner. This contrasts with previous work that directly models synthesizer coefficients (inferred using hand-designed vocoder algorithms) with an autoregressive model or GAN [1].\n\nThe DDSP approach is not additionally constrained by limitations of hand-designed analysis algorithms to infer the appropriate coefficients for resynthesis, the are learned jointly with audio loss functions and deep networks. Further, evaluating loss functions on generated audio is more aligned to perception than loss functions on synthesizer coefficients. \n\n[1] Merlijn Blaauw and Jordi Bonada. A neural parametric singing synthesizer modeling timbre and expression from natural songs. Applied Sciences, 7(12):1313, 2017. (https://arxiv.org/abs/1704.03809)\n\n\n> \u201c(5) Too many contents are left in the supplemental. Section 3 only details the formula of the yellow component of Figure 2, while other components are omitted. It fails to give a clear description of the whole DDSP work.\u201d\n\nWe agree that it would be preferable to be able to describe the network architecture in more depth in the main text of the paper. However, given space requirements, we focus the main text on the DDSP components (yellow in Figure 2) that make up the library and are the novel contributions of the paper. The network architecture is specifically chosen to be generic (fully connected, deterministic autoencoder) to demonstrate that it is the DDSP components, and not other modeling decisions, that enables the quality of the work. \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "SkxtJ6CKiH", "original": null, "number": 6, "cdate": 1573674208671, "ddate": null, "tcdate": 1573674208671, "tmdate": 1573674208671, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "Paper Updates", "comment": "We would like to thank all the reviewers and the area chair for their thoughtful and helpful comments. We have done our best to thoroughly address each one and make appropriate changes to the manuscript. In particular, we would like to highlight several changes in the updated draft\n\n* New table provided in the supplemental with parameter counts for all relevant models.\n* Initial results from new \u201ctiny\u201d model (240k parameters) added to the online audio supplement.\n* Reworked section 4.1 and 4.2 (DDSP Autoencoder) to more explicitly define the difference between supervised and unsupervised variants.\n* Corrections to the f(t) encoder description for unsupervised experiments, including a new table provided in the supplemental with complete architecture details.\n* NSynth audio reconstruction comparisons between unsupervised and supervised variants added to the online audio supplement.\n* Further motivation provided for the Harmonic plus Noise model in Section 3.1\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "Hyx1NHgeir", "original": null, "number": 5, "cdate": 1573025063316, "ddate": null, "tcdate": 1573025063316, "tmdate": 1573025063316, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment", "content": {"title": "some concerns about the proposed autoencoder architecture", "comment": "A signal processing framework, called DDSP, is proposed. DDSP is based on an autoencoder architecture. Three different encoders are adopted to represent the latent coders, while a deterministic synthesizer is used to fusion the latent coder to do reconstruction. Replacing the encoder/decoder with some deterministic module is a popular approach to pursue interpretability. And the results along with the online audio are very impressive.  \n\nHowever, I have some concerns about the proposed autoencoder architecture:\n\n(1) The authors replaced the decoder with some deterministic synthesizer module, which would constrain the model expressive power due to the low complexity of the predefined module.\n\n(2) An MLP decoder is used along with simple concatenation. The output is then used for the harmonic synthesizer and the filtered noise synthesizer. It would fail to get any meaningful interpretability for the latent code since all latent encoder are highly entangled. \n\n(3) It is highly claimed in the paper that the proposed DDSP shows potential in many interesting tasks, while insufficient experiment results are shown. The corresponding experiment results are needed to support the claims in the paper.\n\n(4) It is claimed in Section 1.2 that \"small errors in parameters can lead to large errors in the audio that cannot propagate back to the network\". I am very curious about how this issue is addressed or elevate in this paper since a similar deterministic synthesizer module in DDSP. \n\n(5) Too many contents are left in the supplemental. Section 3 only details the formula of the yellow component of Figure 2, while other components are omitted. It fails to give a clear description of the whole DDSP work.\n\n(6) In the supplemental, it is claimed that the f(t) is replaced with a fixed network during the training, which is different from the claim in the main body that f(t) is the output of the encoder. Does it mean the direct neural encoder f(t) is not a good option? \n\n(7) As claimed in the paper, f(t) is a fixed network, while z is not used in the model trained on solo violin. It means all the encoder parts is no longer exist. Does it mean the decoder is the main component of DDSP?\n\n(8) Regarding the multi-scale spectral loss in Eq.(4), a trade off parameter is needed to balance the two L1 loss."}, "signatures": ["ICLR.cc/2020/Conference/Paper435/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x1ma4tDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper435/Authors|ICLR.cc/2020/Conference/Paper435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171533, "tmdate": 1576860545576, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper435/Authors", "ICLR.cc/2020/Conference/Paper435/Reviewers", "ICLR.cc/2020/Conference/Paper435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Comment"}}}, {"id": "rJlmneOVqr", "original": null, "number": 2, "cdate": 1572270250593, "ddate": null, "tcdate": 1572270250593, "tmdate": 1572972595615, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a model for audio generation/synthesis  where the model is trained to output time-varying parameters of a vocoder/synthesiser rather than directly outputting audio samples/spectrogram frames. The model is trained by mining an L1 loss between the synthesised audio and the real training audio. Overall, I found the paper to be well written, I found the online supplementary material helpful in getting an intuition for the quality of audio samples generated and to understand the various parts of the proposed architecture. I also found the figures to be extremely  informative, especially figure 2. Although, I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar, since the description and experiments very specifically deal with audio synthesis with vocoders, even though the components might be general DSP components. I think the paper presents a reasonable alternative to current autoregressive audio generation methods and should be accepted for publication. \n\nMinor Comments\n\n1. The reference provided for RNNs, Sutskever et. al. 2014, should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. \n2. \u201cThe bias of the natural world is to vibrate.\u201d I am not sure what exactly is meant here. Is it really a \u201cbias\u201d if every object in the universe vibrates? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors. Bias as used in the paper, seems to have several means. It would help the description if the authors cleared up this confusing use of the term. \n3. In Section 1.3, the authors claim that one of the strengths of the proposed method is that the models are \u201crelatively small\u201d, however all discussion of the sizes is relegated to the appendix. It would be helpful to the reader if some model complexity comparisons are presented in the results section and a high level summary is presented at the end of Section 1. \n4. In Section 3, some additional high-level description for the Harmonic plus Noise model (Serra & Smith, 1990) should be provided to motivate the discussion and experiments in the rest of the paper. \n5. Section 4.2 should provide some description of the parameters counts for each of the components considered and how this compares with existing auto-regressive generation algorithms. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574944321795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper435/Reviewers"], "noninvitees": [], "tcdate": 1570237752176, "tmdate": 1574944321814, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Review"}}}, {"id": "rJemdEkV9H", "original": null, "number": 1, "cdate": 1572234346803, "ddate": null, "tcdate": 1572234346803, "tmdate": 1572972595573, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops a framework for for audio generation using oscillators with differentiable neural network type learning. They showcase the usefulness and effectiveness of the approach with several examples such as timbre transfer, dereverberation, changing the room impulse response, pitch extrapolation and so on. I can imagine the proposed learnable oscillator based autoencoders in a variety of applications. \n\nI think that this suggested software library can be useful for a wide range of audio researchers, and I commend the authors for this contribution. It is very nice to see an example of research where we make use of our physical understanding of the sound medium rather than blindly throwing a neural network at the problem. \n\nI have one important question though: how susceptible do you think the system is robust with respect to f0 and loudness encoders? Have you experimented with situations where the f0 and the loudness encoders might fail (such as more non-periodic and noisy signals)? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574944321795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper435/Reviewers"], "noninvitees": [], "tcdate": 1570237752176, "tmdate": 1574944321814, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Review"}}}, {"id": "SyeL1QN2qS", "original": null, "number": 3, "cdate": 1572778717572, "ddate": null, "tcdate": 1572778717572, "tmdate": 1572972595526, "tddate": null, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "invitation": "ICLR.cc/2020/Conference/Paper435/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This very nice paper tackles the integration of of domain knowledge in signal processing within neural nets; the approach is illustrated in the domain of music. \n\nThe proof of concept (audio supplementary material) is very convincing. \n\nThe argument is that a \"natural\" latent space for audio is the spectro-temporal domain. Approaches working purely in the waveform, or in the frequency domains must handle the phase issues. Approaches can learn to handle these issues, at the expense of more data. \nA key difficulty is that the L_2 loss does not match the perception. The authors present a perceptual loss that addresses the point (Eq. 4 - clarify the difference w.r.t. Wang et al.), . A natural question thus is whether applying this loss on e.g. Wavenet, Sample RNN or Wave RNN would solve the problem.\n\nIn short, the contribution is in designing a latent space that accounts for independent components of the music signal (pitch; loudness; reverberation and/or noise), using existing components (oscillators, envelopes and filters), and making them amenable to end-to-end optimization (noting that loudness can be extracted deterministically).\n\nI understand that the auto-encoder is enriched with a FIR filter at its core: the input is mapped into the time-frequency domain; convolved with the output of the neural net H_l, and the result is recovered from the time-frequency domain. \nExplain \"frames x_l to match the impulse responses h_l\".\n\nCare (domain knowledge and trials and errors, I suppose) is exercized in the conversion and recovery (shape and size of the window) to remove undesired effects. \n\nOverall, the approach works in two modes: one where the fundamental frequency is extracted, one where it is learned. I miss this comparison in the audio material, could you tell where to look / hear ? \n\n\nQuestions:\n* NN operate at a slower frame rate (sect. 3.2): how much slower ? How sensitive to this parameter ?\n\nDetail\n* were, end p. 5. A word missing ?  \n* are useful --> is useful ? \n* could produced, p.6 "}, "signatures": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper435/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jesseengel@google.com", "hanoih@google.com", "gcj@google.com", "adarob@google.com"], "title": "DDSP: Differentiable Digital Signal Processing", "authors": ["Jesse Engel", "Lamtharn (Hanoi) Hantrakul", "Chenjie Gu", "Adam Roberts"], "pdf": "/pdf/bd8d353bca498f66f2bf5db02c5fda8135120349.pdf", "TL;DR": "Better audio synthesis by combining interpretable DSP with end-to-end learning.", "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.\n", "keywords": ["dsp", "audio", "music", "nsynth", "wavenet", "wavernn", "vocoder", "synthesizer", "sound", "signal", "processing", "tensorflow", "autoencoder", "disentanglement"], "paperhash": "engel|ddsp_differentiable_digital_signal_processing", "code": "https://github.com/magenta/ddsp", "_bibtex": "@inproceedings{\nEngel2020DDSP:,\ntitle={DDSP: Differentiable Digital Signal Processing},\nauthor={Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x1ma4tDr}\n}", "original_pdf": "/attachment/6f0a406776dca51c4f1770b8bcc6c64eb75235b8.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x1ma4tDr", "replyto": "B1x1ma4tDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574944321795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper435/Reviewers"], "noninvitees": [], "tcdate": 1570237752176, "tmdate": 1574944321814, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper435/-/Official_Review"}}}], "count": 23}