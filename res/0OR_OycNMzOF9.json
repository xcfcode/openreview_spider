{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362655740000, "tcdate": 1362655740000, "number": 1, "id": "A2auXgoqFvTyV", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "0OR_OycNMzOF9", "replyto": "7N2E7oCO6yPiH", "signatures": ["Sainbayar Sukhbaatar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for the detailed review. Those are good points, and we will consider them in our next revision. We also want to give some explanations.\r\n\r\nAbout fixed cluster size:\r\n- Yes. In topographic maps, clusters are not required to have the same size. We will fix this sentence in the next revision. We meant to say that those methods will require an additional mechanism to have adaptive (depends on the nature of its features) cluster sizes.\r\n\r\nAbout topographic maps:\r\n- Topographic maps may have their advantages, but I still think it puts artificial restrictions on clustering. For example, edge detectors have at least four dimensions: orientation, location and length. Therefore, an ideal clustering can be achieved by placing edge detectors in a four-dimensional map, and grouping nearby edge detectors. It will be difficult to map such a four-dimensional clustering into a two-dimensional plane.\r\n- Our pooling method, on the other hand, allows any soft clustering. In addition, the clear advantage of our method over topographic maps is its modularity. The proposed method can be used with any feature learning algorithm, while topographic maps need to alter the feature learning process.\r\n\r\nPerformance comparison to topographic maps:\r\n- Unfortunately, we could not find any reported result by those approaches on CIFAR10 (please refer to any papers that we may have missed), which is a widely used benchmark for image classification. In the future versions, we will try to implement those algorithms and apply them to CIFAR10.\r\n\r\nAbout Mobahi et al.\u2019s method:\r\n- We didn\u2019t compare our method to Mobahi et al.\u2019s methods, because their method is not completely unsupervised. They combined supervised classification learning with unsupervised video coherence learning. I think those two cannot be separated, so their method cannot learn invariant features without labeled data. Our method, on the other hand, can be trained in a completely unsupervised way. The classification with labeled data is only used to show the effectiveness of pooling.\r\n\r\nComparison to spatial pooling:\r\n- We compared our method to spatial pooling because it is the most widely used pooling method. Although spatial pooling is simple, it has an advantage of utilizing the spatial information. Since the most dominant variance in the lowest level is spatial shifts, we think that beating spatial pooling without using any spatial information is a notable result.\r\n- It is true that our model has more parameters than spatial pooling, and it can be considered as an additional coding layer. Therefore, we may have to compare it to deeper networks. In future works, we will apply our pooling method to deep networks and compare it to other deep networks.\r\n\r\nAbout the performance:\r\n- The performance on CIFAR10 depends on three factors: feature learning, pooling and classification. With the same feature learning and classification setting as our experiments (autoencoder white 100 features + linear SVM), Adam Coates et al. reported 62% test accuracy in 'An Analysis of Single-Layer Networks in Unsupervised Feature Learning', which we improved to 69% by only changing the pooling step. Coates et al. showed the performance can be greatly improved by increasing the number of features. However, restricted by the computation time, we used only 100 features instead of 1600, which we think was the main reason of the poor performance. In future, we think we can greatly shorten the computation time of our method by combining auto-pooling with spatial pooling.\r\n\r\nNovelty of our paper:\r\n- Using similarity in neighboring frames is indeed very old idea. However, the main contribution of our paper is combination of the slow-change constraint with the low-information-loss constraint. With a very simple implementation, we showed that a pooling based on slowness can improve traditional spatial pooling, without changing the feature learning process."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362650580000, "tcdate": 1362650580000, "number": 1, "id": "IFLJkDHcu-Ice", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "0OR_OycNMzOF9", "replyto": "lvwFsD4fResyH", "signatures": ["Sainbayar Sukhbaatar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "First of all, thank you for reviewing our paper. It was a valuable feedback. We will try to include mentioned papers in the next revision.\r\n\r\nAbout the video dataset:\r\n- We will include a detailed explanation in the next revision. In short, 40 short (2-5 minutes in length) videos are used in our experiments. We tried to collect videos containing the same objects as CIFAR10. However, images extracted from the videos were very different from CIFAR10 images. Many of them didn't include any object, and some only showed a small part of an object.\r\n\r\nComparison to Jia and Huang's method:\r\n- We didn't compare our method to Jia and Huang's method, because they are fundamentally different methods. While Jia and Huang's method learns pooling regions in a supervised way, our method tries to learn pooling regions in an unsupervised way, which has many advantages. \r\n- Although our method uses additional data, the data used for learning pooling regions was not labeled. On the other hand, Jia and Huang's method has an advantage of using labeled data, which produces pooling regions specialized for the classification task.\r\n\r\nComparison to state-of-art methods:\r\n- It is true that our result on CIFAR10 is below the state-of-art. However, as shown by Adam Coates (ICML, 2011), classification results are largely influenced by the configuration of feature learning, especially by the number of features. Since the feature learning was not our research focus, we did little tweaking and optimization in the feature learning step. Also, restricted by the computation time, we didn't use large number of features (100 instead of 1600), which is likely the main reason of the low test accuracies.\r\n\r\n- In the end, let us restate the main contribution of our paper. Our pooling method is novel because it learned pooling regions in an unsupervised way. In addition, it does not use explicit spatial information and it can be used with any pre-learned features. To the best our knowledge, there is no other pooling method that suffices those conditions."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362276780000, "tcdate": 1362276780000, "number": 1, "id": "agstF_wXReF7S", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0OR_OycNMzOF9", "replyto": "0OR_OycNMzOF9", "signatures": ["Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Interesting paper. \r\nYou might be interested in this paper by Karol Gregor and myself: http://arxiv.org/abs/1006.0448\r\nThe second part of the paper also describes a kind of pooling based on temporal constancy."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362203160000, "tcdate": 1362203160000, "number": 2, "id": "7N2E7oCO6yPiH", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0OR_OycNMzOF9", "replyto": "0OR_OycNMzOF9", "signatures": ["anonymous reviewer 2c2a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "review": "Many vision algorithms comprise a pooling step, which combines the outputs of a feature extraction layer to create invariance or reduce dimensionality, often by taking their average. This paper proposes to refine this pooling step by 1) not restricting pooling to merely spatial dimensions (so that several different features can be combined), and 2) learning it instead of deciding the structure of the pools beforehand.\r\nThis is achieved by replacing the pooling step by a linear transformation of the outputs of the feature extractor (here, an autoencoder), with the constraint that all weights be nonnegative. The main intuition for training is that an invariant representation should not change much between two neighboring frames in a video. Thus, training is conducted by minimizing a cost function that combines a reconstruction error cost and a frame-to-frame dissimilarity cost: the reconstruction error cost ensures that the representation before pooling can be reconstructed from the pooled output without too much discrepancy, and the dissimilarity cost encourages two neighboring frames in a video to have similar pooled representation.\r\nTwo experiments are provided: the first one shows that training on patches from natural videos yields pool that combine similar features; the second one tests the algorithm on CIFAR10 and shows that the scheme proposed here performs better than spatial pooling.\r\n\r\nLearning pooling weights instead of pre-selecting them is appealing, however this work does not demonstrate the value of the advocated approach.\r\nFirst, the context given is insufficient; much previous work has explored how to combine different feature maps across feature types rather than only across space, with good results; some of this work is cited here (e.g., ref. 5, Hyv\u00e4rinen, Hoyer, Inki 2001, ref. 7 Kavukcuoglu et al. 2009), but only briefly mentioned and dismissed because (1) the clusters are required to be fixed manually, (2) clusters are required to have the same size (I am not sure why this paper mentions that, this is not true -- the clusters do have the same size in these papers but it is not a requirement), and (3) there is no 'guarantee that the optimal feature clustering can be mapped into two-dimensional space'. This is true, but the two-dimensional mapping into a topographical map is a bonus, and the same cost functions could be applied with no overlap between the pools, as in the approach advocated here, and still obtain pools that group similar features.\r\nIn any case, it is not sufficient to merely state the shortcomings of these previous approaches, without showing that the method here outperforms them and that these supposed shortcomings truly hurt performance.\r\nAnother line of work that should definitely be introduced (and isn't), is work enforcing similarity of representations for similar images to train coding. There has been much work on this, even also using video,\r\ne.g. Mobahi, Collobert, Weston, Deep Learning from Temporal Coherence in Video, ICML 2009 -- or before that with collections of still images with continuously varying parameters, Hadsell, Chopra and LeCun, Dimensionality Reduction by Learning an Invariant Mapping (CVPR 2006), and much other work. Those older works use similarity-based losses to train encoding features rather than pooling, but this is not a real difference, which is my second point:\r\n\r\nSecond, comparing the pooling step here to a simple spatial pooling step is somewhat misleading; the 'auto-pooling step' in this paper is a full-fledged linear mapping, with the added restriction that the weights have to be nonnegative. Thus the system is more akin to a two-layer encoding network than a single-layer network. The distinction between 'coding' and 'pooling' is an artificial one anyways; given that auto-pooling has as many parameters are a standard coding step, it should not only be compared to the much simpler spatial pooling.\r\nIn terms of performance, the performance on Cifar 10 is much below what can be obtained with a single layer of features (e.g. compare the 69.7% here to results between 68.6% and  79.6% in Coates et al.'s 'An Analysis of Single-Layer Networks in Unsupervised Feature Learning', and better performance in subsequent papers by Coates et al.), so this is indeed not very convincing.\r\n\r\nThe ideas combined here (learning a pooling map, using similarity in neighboring frames,\r\n\r\nPros/cons:\r\n- pros: ideas for generalizing pooling are intuitive and appealing\r\n- cons: many of these ideas have been explored elsewhere before, and this paper does not do a suitable job of delineating what the specific contribution is. In fact, it seems that the proposed approach does not have much novelty and most ideas here are already part of existing algorithms; experimental results fail to demonstrate the superiority of the proposed scheme."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362197040000, "tcdate": 1362197040000, "number": 5, "id": "Deofes8a4Heux", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0OR_OycNMzOF9", "replyto": "0OR_OycNMzOF9", "signatures": ["anonymous reviewer 8b0d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "review": "The paper presents a method to learn invariant features by using temporal coherence.  A set of linear pooling units are trained on top of a set of (pre-trained) features using what is effectively a linear auto-encoder with a penalty for changes over time (a 'slowness' penalty).  Visualizations show that the learned weights of the pooling units tend to combine features for translated or slightly rotated edges (as expected for complex cells), and benchmark results show some improvement over hand-coded pooling units.\r\n\r\nThis is a fairly straight-forward idea that gives pleasing results nonetheless.  The main attraction to the method proposed here is its simplicity and modularity:  a linear auto-encoder and slowness penalty is very easy to implement and could be used in almost any pipeline.  This is simultaneously my main concern about the method:  it is significantly subsumed by prior work (though the very simple instance here might differ).  For example, see the work of Zou et al. (NIPS 2012) which uses essentially the same training method with nonlinear pooling units, Mobahi et al. (ICML 2009), and work with 'slowness' criteria more generally.  That said, considering the many algorithms that have been proposed to learn pooling regions and invariant features without video, the fact that an extremely simple instance like the one here can give reasonable results is worth emphasizing.\r\n\r\nPros:\r\n(1) A very simple approach that appears to yield plausible invariant features and a modest bump over hand-built pooling in the unsupervised setting.\r\n\r\nCons:\r\n(1) Only linear pooling units are considered.  As a result they do not add much power beyond slight regularization of the linear SVM.\r\n(2) Only single-layer networks are considered;  results with deep layers might be very interesting.\r\n(3) There is quite a lot of prior work with very similar ideas and implementations;  hopefully these can be cited and discussed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361924340000, "tcdate": 1361924340000, "number": 3, "id": "2U4l21HEl7SVL", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0OR_OycNMzOF9", "replyto": "0OR_OycNMzOF9", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "First off, let me say thank you for citing my + my co-authors' paper on measuring invariances.\r\n\r\nI have a few thoughts about invariance and temporal coherence that I hope you might find helpful.\r\n\r\nRegarding invariance, I think that invariance is not such a great property on its own. What you really want is to disentangle the different factors of variation in the dataset. Invariance plays a role in this process, because if you want one feature to correspond only to one factor of variation, it must be invariant to all of the others. But it's really a very small part of the picture. For the purposes of our paper on measuring invariances, invariance was a good enough proxy for disentangling that we could use it test the hypothesis that deep learning systems become more invariant with depth. But I don't think invariance is a good enough property to serve as the main part of your objective function.\r\n\r\nRegarding temporal coherence, I think a common mistake people make is to jump from the idea that features should be 'coherent' to the idea that features should be 'slow.' I think that useful features are spread over a wide spectrum of timescales. It's true that the fastest varying features are probably just noise. But the slowest varying features are probably not especially useful either. For example, if you put a camera on a streetcorner, the amount of sunlight in the scene would usually change slower than the identities of the people in the scene. I think probably the way to make progress with applications of temporal coherence is to study new ways of encouraging features to be coherent rather than just slow.\r\n\r\nSome general suggestions on how to improve your results: You should read Adam Coates' ICML 2011 paper, which is about finding the best training algorithm and feature encoding method for single-layer architectures. I think if you use larger dictionaries (1600 instead of 100), train using OMP-1 or sparse coding instead of sparse autoencoders, and extract using T-encoding you will do much better and have a shot at beating state of the art, or at least beating Jia and Huang. Adam Coates' work shows that sparse autoencoders don't make very good feature extractors, and also that small dictionaries don't perform very well, so you're really hurting your numbers by using that setup as your feature extractor.\r\n\r\nFinally, I think you're missing a few references. In particular, your approach is very closely related to Slow Feature Analysis, so you should cite Laurenz Wiskott and comment on the similarities."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361921040000, "tcdate": 1361921040000, "number": 4, "id": "lvwFsD4fResyH", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0OR_OycNMzOF9", "replyto": "0OR_OycNMzOF9", "signatures": ["anonymous reviewer 1dcf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "review": "Summary:\r\nThis paper proposes learning a pooling layer (not necessarily of a convolutional network) by using temporal coherence to learn the pools. Training is accomplished by minimizing a criterion that encourages the features to change slowly but have high entropy over all.\r\nDetailed comments:\r\n-The method demonstrates improvement over a spatial pooling baseline\r\n-The experiments here don't allow comparison to prior work on learning pools, such as the paper by Jia and Huang.\r\n- The method is not competitive with the state of the art\r\n\r\nSuggestions to authors:\r\n\r\nIn future revisions of this paper, please be more specific about what your source of natural videos was. Just saying vimeo.com is not very specific. vimeo.com has a lot of videos. How many did you use? Do they include the same kinds of objects as you need to classify on CIFAR-10?\r\nComparing to Jia and Huang is very important, since they also study learning pooling structure. Note that there are also new papers at ICLR on learning pooling structure you should consider in the future. I think Y-Lan Boureau also wrote a paper on learning pools that might be relevant.\r\nPros:\r\n-The method demonstrates some improvement over baseline pooling systems applied to the same task.\r\n\r\nCons:\r\n-Doesn't compare to prior work on learning pools\r\n-The method isn't competitive with the state of the art, despite having access to extra training data."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358316900000, "tcdate": 1358316900000, "number": 41, "id": "0OR_OycNMzOF9", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "0OR_OycNMzOF9", "signatures": ["tesatory@gmail.com"], "readers": ["everyone"], "content": {"title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "pdf": "https://arxiv.org/abs/1301.3323", "paperhash": "sukhbaatar|autopooling_learning_to_improve_invariance_of_image_features_from_image_sequences", "keywords": [], "conflicts": [], "authors": ["Sainbayar Sukhbaatar", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["tesatory@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 8}