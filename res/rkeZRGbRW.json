{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730158502, "tcdate": 1509137964192, "number": 1038, "cdate": 1518730158491, "id": "rkeZRGbRW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkeZRGbRW", "original": "ryhsaMWAb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Variance Regularizing Adversarial Learning", "abstract": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "pdf": "/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf", "TL;DR": "We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution.", "paperhash": "grewal|variance_regularizing_adversarial_learning", "_bibtex": "@misc{\ngrewal2018variance,\ntitle={Variance Regularizing Adversarial Learning},\nauthor={Karan Grewal and R Devon Hjelm and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=rkeZRGbRW},\n}", "keywords": ["Generative Adversarial Network", "Integral Probability Metric", "Meta-Adversarial Learning"], "authors": ["Karan Grewal", "R Devon Hjelm", "Yoshua Bengio"], "authorids": ["karanraj.grewal@mail.utoronto.ca", "erroneus@gmail.com", "yoshua.umontreal@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260076896, "tcdate": 1517250188226, "number": 841, "cdate": 1517250188202, "id": "HJ4981TBG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers found a number of short-comings in this work that would prevent it from being accepted at ICLR in its current form, both in terms of writing (not specifying the loss function),  experiments that are too limited, and inconclusive comparisons with existing regularization techniques. I recommend the authors take into account the feedback from reviewers in any follow-up submissions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Regularizing Adversarial Learning", "abstract": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "pdf": "/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf", "TL;DR": "We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution.", "paperhash": "grewal|variance_regularizing_adversarial_learning", "_bibtex": "@misc{\ngrewal2018variance,\ntitle={Variance Regularizing Adversarial Learning},\nauthor={Karan Grewal and R Devon Hjelm and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=rkeZRGbRW},\n}", "keywords": ["Generative Adversarial Network", "Integral Probability Metric", "Meta-Adversarial Learning"], "authors": ["Karan Grewal", "R Devon Hjelm", "Yoshua Bengio"], "authorids": ["karanraj.grewal@mail.utoronto.ca", "erroneus@gmail.com", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642379062, "tcdate": 1511609401194, "number": 1, "cdate": 1511609401194, "id": "HyZBE0IlM", "invitation": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "signatures": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting paper that needs more investigation", "rating": "5: Marginally below acceptance threshold", "review": "This paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution.\n\nThe approach suggested in this paper models the output of the discriminator using a mixture of two Gaussians (one for \u201cfake\u201d and the other for \u201cnot fake\u201d). This seems like a rather crude approximation as the distribution of each \u201cclass\u201d is likely to be multimodal. Can the authors comment on this? Could they extend their approach to use a mixture of multimodal distributions?\n\nThe paper mentions that fixing the means of the distribution can be \u201cproblematic during optimization as the discriminator\u2019s goal is to maximize the difference between these two means.\u201c. This relates to my previous comment where the distribution might not be unimodal. In this case, shifting the mean doesn\u2019t seem to be a good solution and might just yield to oscillations between different modes. Can you please comment on this?\n\nMode collapse: Can you comment on the behavior of your approach w.r.t. to mode collapse?\n\nImplementation details: How is the mean of the two Gaussians initialized? \n\nRelation to instance noise and regularization techniques: Instance noise is a common trick being used to train GANs, see e.g. http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\nThis also relates to some regularization techniques, e.g. Roth et al., 2017 that provides a regularizer that amounts to convolving the densities with white Gaussian noise. Can you please elaborate on the potential advantages of the proposed solution over these existing techniques?\n\nComparison to existing baselines: Given that the paper addresses the stability problem, I would expect some empirical comparison to at least one or two of the stability methods cited in the introduction, e.g. Gulrajani et al., 2017 or Roth et al., 2017.\n\nRelation to Kernel MMD: Can the authors elaborate on how their method relates to approaches that replace the discriminator with MMD nets. e.g.\n- Training generative neural networks via Maximum Mean Discrepancy optimization, Dziugaite et al\n- Generative models and model criticism via optimized maximum mean discrepancy, Sutherland et al\nMore explicitly, the variance in these methods can be controlled via the bandwidth of the kernel and I therefore wonder what would one use a simple mixture of Gaussians instead?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Regularizing Adversarial Learning", "abstract": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "pdf": "/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf", "TL;DR": "We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution.", "paperhash": "grewal|variance_regularizing_adversarial_learning", "_bibtex": "@misc{\ngrewal2018variance,\ntitle={Variance Regularizing Adversarial Learning},\nauthor={Karan Grewal and R Devon Hjelm and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=rkeZRGbRW},\n}", "keywords": ["Generative Adversarial Network", "Integral Probability Metric", "Meta-Adversarial Learning"], "authors": ["Karan Grewal", "R Devon Hjelm", "Yoshua Bengio"], "authorids": ["karanraj.grewal@mail.utoronto.ca", "erroneus@gmail.com", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378963, "id": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1038/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer3"], "reply": {"forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378963}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642379022, "tcdate": 1511833043264, "number": 2, "cdate": 1511833043264, "id": "ByjCp4qgM", "invitation": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "signatures": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting idea, but needs more work", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes variance regularizing adversarial learning (VRAL), a new method for training GANs.\n\nThe motivation is to ensure that the gradient for the generator does not vanish. The authors propose to use a discriminator whose output targets a mixture of two Gaussians (one component each for real and fake data).  The means and variances are fixed so that the discriminator does not overfit, which ensures that the generator learning is not hindered. \n\nThe discriminator itself is trained through two additional meta-discriminators (!) Are the meta-discriminators really necessary? Have you tried matching moments or using other methods for comparing the distributions?\n\nIt would be useful to write down the actual loss function so that it's easier to compare with other GAN variants. In particular, I'm curious to understand the difference between VRAL and Fisher-GAN. The authors discuss this in the end of Section 3, but a more careful comparison is needed.\n\nThe experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.\n\nOverall, I think that the idea is interesting, but the paper needs more work and does not meet the ICLR acceptance bar.\n\nFYI, another concurrent submission showed that gradient penalties stabilize training of GANs:\nMANY PATHS TO EQUILIBRIUM: GANS DO NOT NEED TO DECREASE A DIVERGENCE AT EVERY STEP\nhttps://openreview.net/pdf?id=ByQpn1ZA-", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Regularizing Adversarial Learning", "abstract": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "pdf": "/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf", "TL;DR": "We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution.", "paperhash": "grewal|variance_regularizing_adversarial_learning", "_bibtex": "@misc{\ngrewal2018variance,\ntitle={Variance Regularizing Adversarial Learning},\nauthor={Karan Grewal and R Devon Hjelm and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=rkeZRGbRW},\n}", "keywords": ["Generative Adversarial Network", "Integral Probability Metric", "Meta-Adversarial Learning"], "authors": ["Karan Grewal", "R Devon Hjelm", "Yoshua Bengio"], "authorids": ["karanraj.grewal@mail.utoronto.ca", "erroneus@gmail.com", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378963, "id": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1038/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer3"], "reply": {"forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378963}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642378980, "tcdate": 1511837783771, "number": 3, "cdate": 1511837783771, "id": "HkxvlUclG", "invitation": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "signatures": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "The authors provided empirical analysis of different variants of GANs and proposed a regularization scheme to combat the vanishing gradient when the discriminator is well trained. \n\nMore specifically, the authors demonstrated the importance of intra-class variance in the discriminator\u2019s output. Methods whose discriminators tend to map inputs of a class to single real values are unable to provide a reliable learning signal for the generator, such as the standard GAN and Least Squares GAN. Variance in the discriminator\u2019s output is essential to allow the generator to learn in the presence of a well-trained discriminator. To ensure the discriminator\u2019s output follows the mixture of two univariate Gaussians, the authors proposed to add two additional discriminators which are trained in a similar was as the original GAN formulation. The technique is related to Linear Discriminant Analysis. From a broader perspective, the new meta-adversarial learning can be applied to ensure various desirable properties in GANs.\n\nThe performance of variance regularization scheme was evaluated on the CIFAR-10 and CelebA data.\n\nSummary:\n\u2014\u2014\nI think the paper discusses a very interesting topic and presents an interesting direction for training the GANs. A few points are missing which would provide significantly more value to readers. See comments below for details and other points.\n\nComments:\n\u2014\u2014\n1.\tWhy would a bi-modal distribution be meaningful? Deep nets implicitly transform the data which is probably much more effective than using complex bi-modal Gaussian distribution; the bi-modal concept can likely be captured using classical techniques.\n\n2.\tOn page 4, in Eq. (8) and (9), it remains unclear what $\\mathcal{R}$ and $\\mathcal{F}$ really are beyond two-layer MLPs; are the results of those two-layer MLPs used as the mean of a Gaussian distribution, i.e., $\\mu_r$ and $\\mu_f$?\n\n3.\tRegarding the description above Eq. (12), what is really used in practice, i.e., in the experiments? The paper omits many details that seem important for understanding. Could the authors provide more details on choosing the generator loss function and why Eq. (12) provides satisfying results in practice?    \n\nMinor Comments:\n\u2014\u2014\n1.\tIn Sec 2.1, the sentence needs to be corrected: \u201cAs shown in Arjovsky & Bottou (2017), the JS divergence will be flat everywhere important if P and Q both lie on low-dimensional manifolds (as is likely the case with real data) and do not prefectly align.\u201d\n\n2.\tLast sentence in Conclusion: \u201cwhich can be applied to ensure enforce various desirable properties in GANs.\u201d Please remove either \u201censure\u201d or \u201cenforce.\u201d\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Regularizing Adversarial Learning", "abstract": "We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator. As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables. In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training. We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.", "pdf": "/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf", "TL;DR": "We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution.", "paperhash": "grewal|variance_regularizing_adversarial_learning", "_bibtex": "@misc{\ngrewal2018variance,\ntitle={Variance Regularizing Adversarial Learning},\nauthor={Karan Grewal and R Devon Hjelm and Yoshua Bengio},\nyear={2018},\nurl={https://openreview.net/forum?id=rkeZRGbRW},\n}", "keywords": ["Generative Adversarial Network", "Integral Probability Metric", "Meta-Adversarial Learning"], "authors": ["Karan Grewal", "R Devon Hjelm", "Yoshua Bengio"], "authorids": ["karanraj.grewal@mail.utoronto.ca", "erroneus@gmail.com", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378963, "id": "ICLR.cc/2018/Conference/-/Paper1038/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1038/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1038/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1038/AnonReviewer3"], "reply": {"forum": "rkeZRGbRW", "replyto": "rkeZRGbRW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378963}}}], "count": 5}