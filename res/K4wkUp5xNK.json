{"notes": [{"id": "K4wkUp5xNK", "original": "90lxzSewIhJ", "number": 3443, "cdate": 1601308382242, "ddate": null, "tcdate": 1601308382242, "tmdate": 1614985731613, "tddate": null, "forum": "K4wkUp5xNK", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Hzxo4gh5goJ", "original": null, "number": 1, "cdate": 1610040407778, "ddate": null, "tcdate": 1610040407778, "tmdate": 1610474004702, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper aims to provide a framework for learning non-linear feature mappings such that are invariant to environments. The critical concern raised by the reviewers is their assumption: that causal features of the label are conditionally independent given the label. But in any DAG, conditioning on a common child (here, the label) renders the parents dependent. Their assumption thus is not going to hold other than on a measure zero set of parameters."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040407765, "tmdate": 1610474004686, "id": "ICLR.cc/2021/Conference/Paper3443/-/Decision"}}}, {"id": "GHvZzalZhw", "original": null, "number": 4, "cdate": 1606222670794, "ddate": null, "tcdate": 1606222670794, "tmdate": 1606222670794, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "AWqPXUSwB9Y", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for your comments.\n\n**Question 1.**\n\n\"*The clarity and organization of the paper could be improved. The algorithm should be moved from the appendix to the main text and the procedure should be described more holistically to give the reader an outline before diving into the details of each component section. The experiments section is also very unclear.*\"\n\n**Authors' Response**:\n\nThank you for the suggestion. We have updated all in the revision.\n\n**Question 2.**\n\n\"*The main issue that remains unclear to me is how the environment variable E is being used explicitly. It doesn\u2019t seem clear to me that you would generally have access to E, but it is required in the rules necessary to determine the direct causes of Y. What are the E variables being used in each of the experiments?*\"\n\n**Authors' Response**:\n\nIn this paper, the environment variable $E$ is only an environment or domain index. For example, if we have $N$ training environments, then the environment variable $E$ takes value in {1, $\\ldots$, N}. We have clarified it in the last paragraph of Section 3.1.\n\n**Question 3.**\n\n\"*The novelty seems somewhat limited. It seems the theoretical results can be divided into (a) results about identifiability of the latent variable model and (b) the method for identifying the direct causes. Some of (a) follows directly from Khemakhem et al. (2020) - it is difficult to determine whether there is sufficient novelty here. (b) follows directly from well known constraint-based and bivariate causal discovery approaches.*\"\n\n**Authors' Response**:\n\nIt is worth emphasizing that our contribution in this paper is to propose **a novel learning paradigm** that enables OOD generalization **in the nonlinear setting**. This challenging problem, which was not solved before, is decently addressed in our paper by creatively integrating some existing methods in a comprehensive manner. Empirical results also demonstrate that our approach significantly outperforms IRM and IRMG in the nonlinear setting. Hence, our work would be a complement to the community of OOD generalization. \n\n**Question 4.**\n\n\"*The experiments are unconvincing. The proposed method outperforms existing approaches in a high noise synthetic data example and a kind of adversarial example where grayscale MNIST is colored in a way that is strongly correlated with the class label (the experimental setup and evaluation metric is very confusing here). It would be more convincing to the proposed method evaluated in a more realistic setting.*\"\n\n**Authors' Response**:\n\nIn this paper, we followed the same experiment settings of the pioneering works on the OOD generalization (i.e., IRM and IRMG) to conduct all the experiments for the fair comparison. The main goal of our experiments in the paper is to **CONCEPTUALLY** demonstrate that the proposed method can enable the OOD generalization in the nonlinear setting.\n\n**Question 5.**\n\n\"*Further, there are no (synthetic) experiments which confirm that the proposed method does in fact learn the causally relevant latent variables and its robustness in doing so. Since identifying the correct causal latent variables requires multiple conditional independence tests and bivariate causal discovery methods (on latent variables which may be estimated incorrectly), there is an obvious concern about how robust this procedure is in practice. It would be more convincing to see (e.g.) precision and recall with regard to selecting the correct causal latent variables when the ground truth is known.*\"\n\n**Authors' Response**:\n\nConsidering that the ground truth of the causal latent variables in the image experiments is unknown, we also conducted the experiments on the fully synthetic data in Section 5.1. In Appendix G, we provide an in-depth analysis on our approach, including the analysis on the importance of Assumption 1 and on the necessity of iVAE in Phase 1, how accurately and robustly the direct causes can be recovered in Phase 2, and how well the two optimization problems can be addressed in Phase 3.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K4wkUp5xNK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3443/Authors|ICLR.cc/2021/Conference/Paper3443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837489, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment"}}}, {"id": "aC2WZV-vLo", "original": null, "number": 3, "cdate": 1606222116089, "ddate": null, "tcdate": 1606222116089, "tmdate": 1606222350159, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "prtT9EC0R9L", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your feedback.\n\n**Question 1.**\n\n\"*How to verify assumption 1 in a real case? Although the authors argued this assumption is not very restrictive and similar to the assumption in the iVAE paper, I think there is a difference between these two papers. In the iVAE paper, they assumed the latent variables to be conditionally factorial, while here the authors assume the potential causals (unobserved variables) are independent.*\"\n\n**Authors' Response**:\n\nIn fact, following the iVAE paper, in this paper we also assume the latent variables to be conditionally factorial, which is formally stated in Assumption 1. \n\nNote that, like many other areas (e.g., healthcare, epidemiology, medicine, etc.) in causality, the only way to verify the assumed causal diagram is through experiment. Empirical results demonstrate that this assumption works quite well. \n\n**Question 2.**\n\n\"*After discovering direct causes, they still need the IRM phase to learn an invariant predictor. IRM itself can identify spurious causes and learn an invariant predictor, so what is the gain of learning the first two phases? What if some spurious causations are wrongly detected by the second phase, will it affect the predictor?*\"\n\n**Authors' Response**:\n\nCompared to IRM, our method has at least two advantages. \n\nFirst, the challenging bi-leveled optimization problem in IRM can be reduced to two simpler independent optimization problems: (i) learning the invariant data representation $\\Phi$ from $O$ to $\\text{Pa}(Y)$, and (ii) learning the invariant classifier $w$ from $\\text{Pa}(Y)$ to $Y$. Both (i) and (ii) can be separately performed in a more efficient and effective manner. \n\nSecond, our method has generalization guarantees in the nonlinear setting, whist IRM only works in the linear setting. This guarantee come from the basic idea that for both (i) and (ii), since there exist no spurious correlations between ${O}$ and $\\text{Pa}({Y})$ and between $\\text{Pa}({Y})$ and ${Y}$, learning theory guarantees that in the limit of infinite data, we will converge to the true invariant data representation $\\Phi$ and the true invariant classifier $w$.\n\nIf some spurious causations are wrongly detected by the second phase, it will affect the predictor for sure. \n\n**Question 3.**\n\n\"*The synthetic data experiment is not convincing at all. ICRL outperforms ERM and IRM in a very extreme case, where all the algorithms perform terribly, I don't think I can conclude ICRL is a better algorithm among others from this test case. If the authors can visually show the invariant representation of ICRL is more robust, that would be a good illustration.*\"\n\n**Authors' Response**:\n\nConsidering that the ground truth of the causal latent variables in the image experiments is unknown, we also conducted the experiments on the fully synthetic data in Section 5.1. In Appendix G, we provide an in-depth analysis on our approach, including the analysis on the importance of Assumption 1 and on the necessity of iVAE in Phase 1, how accurately and robustly the direct causes can be recovered in Phase 2, and how well the two optimization problems can be addressed in Phase 3.\n\n**Question 4.**\n\n\"*In the colored MNIST experiment, I assume the setting is the same as the IRM paper. While they said their IRM can reach 66.9+-2.5 acc, which is 7 percent higher than this paper and even higher than ICRL. So I wonder what causes this gap.*\"\n\n**Authors' Response**:\n\nSince the IRMG paper includes more baselines, for a fair comparison we followed their experimental setting and directly used their dataset. The baseline results in our paper directly come from the IRMG paper. The gap might be caused by the preprocessing methods used in the IRMG paper while creating the datasets."}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K4wkUp5xNK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3443/Authors|ICLR.cc/2021/Conference/Paper3443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837489, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment"}}}, {"id": "j0nKuTJIL7q", "original": null, "number": 2, "cdate": 1606221799998, "ddate": null, "tcdate": 1606221799998, "tmdate": 1606222312484, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "ePlVFLUOPjG", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your comments. \n\n**Question 1**: \n\n\"*I don't see how Assumption 1 can be justified as a plausible assumption in the general case where more than one latent variable is a direct cause of the outcome.*\"\n\n**Authors' Response**:\n\nIn fact, Assumption 1 applies to the general case where more than one latent variable is a direct cause of the outcome. Let us explain this point more clearly. \n\nConsider the example you mentioned that \u201cIf both $X_1$ and $X_2$ are direct causes of $Y$, generically they will not be independent conditional on $Y$ and $E$\u201d. This is absolutely true. In this case, $X_1$ and $X_2$ will be coupled together and treated as one variable. Without loss of generality, let us assume that $X_2$ is absorbed into $X_1$. Similarly, if $Y$ has more than two direct causes, all the other causes will be absorbed into $X_1$. Now, the question goes to how to represent the variable $X_1$ so that it is flexible enough to contain the multiple direct causes of $Y$. \n\nIf the data is simple, it is enough that $X_1$ is a one-dimensional continuous variable.\n\nIf the data is complex, we can let $X_1$ be a multi-dimensional continuous variable, say m-dim. Further, for simplicity we can assume that all $X_i$ is a m-dimensional variable. In this case, our approach will not change except replacing one-dimensional $X_i$ with m-dimensional $X_i$.\n\nWe have updated Section 3.2 to make it clearer.\n\n\n\n**Question 2.**\n\n\"*Theorem 4 is less than fully rigorous and is misleading. For example, the proof of Theorem 4 invokes a method for inferring causal directions in Zhang et al. (2017), but as far as I know, that method does not yet have a rigorous theoretical justification.*\" \n\n**Authors' Response**:\n\nThank you for the comment. In order to avoid such a kind of confusion, we have clarified in the revision that the method for inferring causal directions in Zhang et al. (2017) is a heuristic one without a rigorous theoretical justification yet.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "K4wkUp5xNK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3443/Authors|ICLR.cc/2021/Conference/Paper3443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837489, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Comment"}}}, {"id": "AWqPXUSwB9Y", "original": null, "number": 1, "cdate": 1603647136668, "ddate": null, "tcdate": 1603647136668, "tmdate": 1605023999896, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review", "content": {"title": "Proposes some interesting ideas, but the novelty is limited and experimental results are unconvincing ", "review": "The paper proposes Invariant Causal Representation Learning, which seeks to learn representations for downstream tasks that are based on only causally invariant latent variables so the representation is robust to shifts in the test environment.  \n\nA model is assumed where an environment variable is a cause of all the latent variables and a target. The iVAE algorithm is used to learn the latent variable model. Then, a series of conditional independence tests and bivariate causal discovery methods are used to distinguish which latent variables correspond to causes (effects) of the target. Finally representations are learned from the observed variables to the causal latent variables of the target and then from these variables to the target.\n\nThe approach is evaluated using synthetic data and semi-synthetic data based on MNIST.\n\nThe clarity and organization of the paper could be improved. The algorithm should be moved from the appendix to the main text and the procedure should be described more holistically to give the reader an outline before diving into the details of each component section. The experiments section is also very unclear.\n\nThe main issue that remains unclear to me is how the environment variable E is being used explicitly. It doesn\u2019t seem clear to me that you would generally have access to E, but it is required in the rules necessary to determine the direct causes of Y. What are the E variables being used in each of the experiments?\n\nThe novelty seems somewhat limited. It seems the theoretical results can be divided into (a) results about identifiability of the latent variable model and (b) the method for identifying the direct causes. Some of (a) follows directly from Khemakhem et al. (2020) - it is difficult to determine whether there is sufficient novelty here. (b) follows directly from well known constraint-based and bivariate causal discovery approaches. \n\nThe experiments are unconvincing. The proposed method outperforms existing approaches in a high noise synthetic data example and a kind of adversarial example where grayscale MNIST is colored in a way that is strongly correlated with the class label (the experimental setup and evaluation metric is very confusing here). It would be more convincing to the proposed method evaluated in a more realistic setting.\n\nFurther, there are no (synthetic) experiments which confirm that the proposed method does in fact learn the causally relevant latent variables and its robustness in doing so. Since identifying the correct causal latent variables requires multiple conditional independence tests and bivariate causal discovery methods (on latent variables which may be estimated incorrectly), there is an obvious concern about how robust this procedure is in practice. It would be more convincing to see (e.g.) precision and recall with regard to selecting the correct causal latent variables when the ground truth is known.\n\nIn summary, the paper introducing some interesting ideas, but the clarity could be improved, the novelty may be somewhat limited and the experimental results could be improved.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075740, "tmdate": 1606915774797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review"}}}, {"id": "prtT9EC0R9L", "original": null, "number": 2, "cdate": 1603736145202, "ddate": null, "tcdate": 1603736145202, "tmdate": 1605023999833, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review", "content": {"title": "Interesting topic but the assumption and the experiments are not convincing.", "review": "This paper proposes an invariant causal representation learning paradigm in the nonlinear setting. Based on a conditional factorial assumption, they proved identifiability up to a linear transform. The ICRL objective, in this case, is able to discover all the direct causes of the outcome, and thus enables OOD generalization.\n\nThe novelty of the paper seems to be in the generalization of the IRM framework to the nonlinear case which is interesting to me. \nThe authors combined iVAE and IRM to solve this problem. Overall the paper is clearly written and easy to follow, but some conceptual issues remain.\n\nHere are my issues with the paper:\n- How to verify assumption 1 in a real case? Although the authors argued this assumption is not very restrictive and similar to the assumption in the iVAE paper, I think there is a difference between these two papers. In the iVAE paper, they assumed the latent variables to be conditionally factorial, while here the authors assume the potential causals (unobserved variables) are independent. \n\n- After discovering direct causes, they still need the IRM phase to learn an invariant predictor. IRM itself can identify spurious causes and learn an invariant predictor, so what is the gain of learning the first two phases? What if some spurious causations are wrongly detected by the second phase, will it affect the predictor?\n\n-The synthetic data experiment is not convincing at all. ICRL outperforms ERM and IRM in a very extreme case, where all the algorithms perform terribly, I don't think I can conclude ICRL is a better algorithm among others from this test case. If the authors can visually show the invariant representation of ICRL is more robust, that would be a good illustration. \n\n- In the colored MNIST experiment, I assume the setting is the same as the IRM paper. While they said their IRM can reach 66.9+-2.5 acc, which is 7 percent higher than this paper and even higher than ICRL. So I wonder what causes this gap.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075740, "tmdate": 1606915774797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review"}}}, {"id": "ePlVFLUOPjG", "original": null, "number": 3, "cdate": 1603896893020, "ddate": null, "tcdate": 1603896893020, "tmdate": 1605023999769, "tddate": null, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "invitation": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review", "content": {"title": "Interesting approach to an important problem, but with a substantial flaw", "review": "This paper proposes a method for learning invariant (nonlinear) data representations and classifiers, using data from multiple domains. A key step in the method is to discover the direct causes of the outcome of interest from a set of latent variables that are recovered from observed variables via identifiable VAE. The problem being tackled is significant, and the general idea is interesting and sensible. The empirical results also look encouraging. However, there appears to be a major flaw in the theoretical setup. In order to apply identifiable VAE, the method needs to assume that any two latent variables are independent conditional on the outcome variable and the domain index (Assumption 1). But what about latent variables that are direct causes of the outcome variable? If both X_1 and X_2 are direct causes of Y, generically they will not be independent conditional on Y and E, will they? In the motivating example, only one latent variable is a direct cause of the outcome, so this issue does not arise, but the ambition, as I understand it, is to handle any number of direct causes. I don't see how Assumption 1 can be justified as a plausible assumption in the general case where more than one latent variable is a direct cause of the outcome.\n\nMoreover, Theorem 4 is less than fully rigorous and is misleading. For example, the proof of Theorem 4 invokes a method for inferring causal directions in Zhang et al. (2017), but as far as I know, that method does not yet have a rigorous theoretical justification. As it is formulated, Theorem 4 sounds like a theoretical identifiability result, and as such is not rigorously established by the proof given in the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3443/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3443/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant Causal Representation Learning", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|invariant_causal_representation_learning", "one-sentence_summary": "We propose Invariant Causal Representation Learning (ICRL), a novel learning paradigm that enables out-of-distribution generalization in the nonlinear setting.", "pdf": "/pdf/8d88bacf2003373864c0da490d54b99f9e7a81e3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=qPX6HRNDD", "_bibtex": "@misc{\nlu2021invariant,\ntitle={Invariant Causal Representation Learning},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nyear={2021},\nurl={https://openreview.net/forum?id=K4wkUp5xNK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "K4wkUp5xNK", "replyto": "K4wkUp5xNK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075740, "tmdate": 1606915774797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3443/-/Official_Review"}}}], "count": 8}