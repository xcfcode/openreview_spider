{"notes": [{"id": "r1luCsCqFm", "original": "SklJW9q9Fm", "number": 908, "cdate": 1538087887883, "ddate": null, "tcdate": 1538087887883, "tmdate": 1545355401066, "tddate": null, "forum": "r1luCsCqFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating", "abstract": "Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse \"internal covariate shift\". Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on \"good\" samples to reduce large shifting, and encourage \"bad\" samples to learn from \"good\" sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.", "keywords": ["Curriculum Learning", "Internal Covariate Shift"], "authorids": ["sunbenyuan@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Benyuan Sun", "Yizhou Wang"], "pdf": "/pdf/4ae687244ce452107e640d4936afefc3321e8a50.pdf", "paperhash": "sun|learn_from_neighbour_a_curriculum_that_train_low_weighted_samples_by_imitating", "_bibtex": "@misc{\nsun2019learn,\ntitle={Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating},\nauthor={Benyuan Sun and Yizhou Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=r1luCsCqFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkg1p7Ngx4", "original": null, "number": 1, "cdate": 1544729527465, "ddate": null, "tcdate": 1544729527465, "tmdate": 1545354511617, "tddate": null, "forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper908/Meta_Review", "content": {"metareview": "This paper attempts to address a problem they dub \"inverse\" covariate shift where an improperly trained output layer can hamper learning. The idea is to use a form of curriculum learning. The reviewers found that the notion of inverse covariate shift was not formally or empirically well defined. Furthermore the baselines used were too weak: the authors should consider comparing against state-of-the-art curriculum learning methods.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper908/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper908/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating", "abstract": "Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse \"internal covariate shift\". Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on \"good\" samples to reduce large shifting, and encourage \"bad\" samples to learn from \"good\" sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.", "keywords": ["Curriculum Learning", "Internal Covariate Shift"], "authorids": ["sunbenyuan@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Benyuan Sun", "Yizhou Wang"], "pdf": "/pdf/4ae687244ce452107e640d4936afefc3321e8a50.pdf", "paperhash": "sun|learn_from_neighbour_a_curriculum_that_train_low_weighted_samples_by_imitating", "_bibtex": "@misc{\nsun2019learn,\ntitle={Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating},\nauthor={Benyuan Sun and Yizhou Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=r1luCsCqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper908/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353041318, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper908/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper908/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper908/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353041318}}}, {"id": "B1eXpM0L6m", "original": null, "number": 3, "cdate": 1542017722682, "ddate": null, "tcdate": 1542017722682, "tmdate": 1542017722682, "tddate": null, "forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "content": {"title": "Some basic intuition, but very handwavy, unclear paper, with dubious experimental significance.", "review": "This paper suggests a source of slowness when training a two-layer neural networks: improperly trained output layer (classifier) may hamper learning of the hidden layer (feature). The authors call this \u201cinverse\u201d internal covariate shift (as opposed to the usual one where the feature distribution shifts and trips the classifier). They identify \u201chard\u201d samples, those with large loss, as being the impediment. They then propose a curriculum, where such hard samples are identified at early epochs, their loss attenuated and replaced with a requirement that their features be close to neighboring (in feature space) samples that are similarly classified, but with a more comfortable margin (thus \u201ceasy\u201d.) The authors claim that this allows those samples to contribute through their features at first, without slowing the training down, then in later epochs fully contribute. Some experiments are offered as evidence that this indeed helps speedup.\n\nThe paper is extremely unclear and was hard to read. The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said. Based on this only, I do not deem this work ready for sharing. Furthermore, there are some general issues with the concepts. Here are some specific remarks.\n\n-\tThe intuition of the inverse internal covariate shift is perhaps the main merit of the paper, but I\u2019m not sure if this was not mostly appreciated already.\n\n-\tThe paper offers some experimental poking and probing to find the source of the issue. But that part of the paper (section 3) is disconnected from what follows, mainly because hardness there is not a single point\u2019s notion, but rather that of regions of space with a heterogeneous presence of classes. This is quite intuitive in fact. Later, in section 4, hard simply means high loss. This isn\u2019t quite the same, since the former notion means rather being near the decision boundary, which is not captured by just having high loss. (Also, the loss is not specified.)\n\n-\tSome issues with Section 3: the notions of \u201ctask\u201d needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. it\u2019s all too vague. The term \u201cnon-computable\u201d has very specific meaning, best to avoid. Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own). The whole paragraph preceding Figure 3 is hard to follow. I sort of can make up what is going, especially with the hindsight of Section 4, since it\u2019s basically a variant of the proposed schedule (easy to hard making sure all clusters, as proxy to classes, are represented) without the feature loss, but it needs a rewriting.\n\n-\tIt is important to emphasize that the notion of \u201ceasy\u201d and \u201chard\u201d can change along the training, because they are relative to what the weights are at the hidden layer. Features of some samples may be not very separable at some stage, but they may become very separable later. The suggested algorithm does this reevaluation, but this is not made clear early on.\n\n-\tIn Section 4, the sentence where S_t(x) is mentioned is unclear. I assume \u201csurpass\u201d means achieving a better loss. Also later M_t (a margin) is used, when I think what is meant is S_t (a set). The whole notation (e.g. \u201ctopk\u201d, indexing that is not subscripted, non-math mode math) is bad.\n\n-\tIf L_t is indeed a loss (and not a \u201cperformance\u201d like it\u2019s sometimes referred to, as in minus loss), then I assume larger losses means that the weight on the feature loss in equation (3) should be larger. So I think a minus sign is missing in the exponent of equation (2), and also in the algorithm.\n\n-\tI\u2019m not sure if the experiments actually show a speedup, in the sense of what the authors started out motivating. A speedup, for me, would look like the training progress curves are basically compressed: everything happens sooner, in terms of epochs. Instead, what we have is basically the same shape curve but with a slight boost in performance (Figure 4.) It\u2019s totally disingenuous to say \u201cthis is a great boost in speed\u201d (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically.\n\n-\tSo the real conclusion here is that this curriculum may not have sped up the training in the way we expect it at all. However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating", "abstract": "Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse \"internal covariate shift\". Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on \"good\" samples to reduce large shifting, and encourage \"bad\" samples to learn from \"good\" sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.", "keywords": ["Curriculum Learning", "Internal Covariate Shift"], "authorids": ["sunbenyuan@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Benyuan Sun", "Yizhou Wang"], "pdf": "/pdf/4ae687244ce452107e640d4936afefc3321e8a50.pdf", "paperhash": "sun|learn_from_neighbour_a_curriculum_that_train_low_weighted_samples_by_imitating", "_bibtex": "@misc{\nsun2019learn,\ntitle={Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating},\nauthor={Benyuan Sun and Yizhou Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=r1luCsCqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "cdate": 1542234349326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper908/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335830549, "tmdate": 1552335830549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper908/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxpzqEPhm", "original": null, "number": 2, "cdate": 1540995604645, "ddate": null, "tcdate": 1540995604645, "tmdate": 1541533588815, "tddate": null, "forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "content": {"title": "Interesting idea, poor exposition", "review": "This paper describes an approach for automated curriculum learning in a deep learning classification setup. The main idea is to weigh data points according to the current value of the loss on these data points. A naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples). This last part is implemented by caching hidden representations and classification loss values during training and fetching nearest neighbours in the feature space whenever a hard data point is encountered. The final loss takes the form of a linear combination of the classification loss and the representation loss.\n\nThe idea is interesting in the sense that it tries to use information about how difficult classification of a given data point is to improve learning. The proposed representation loss can lead to forming tight cluster of similar data point in the feature space and can make classification easier. It is related to student-teacher networks, where a student is trained to imitate the teacher in generated similar feature representations.\n\nThe authors justify the method by introducing the notion of \u201cinverse internal covariate shift\u201d. However, it is not defined formally, nor is it supported empirically, and is based on the (often criticized [1]) notion of \u201cinternal covariate shift\u201d. For this reason, it is hard to accept the presented argumentation in its current state.\n\nMoreover, there seems to be a mistake in equation (2) in \u00a74.2. The equation defines the method of computing loss weighting for a given datapoint. The authors note that it converges to the value of one with increasing training iterations, but for correctness it should be \\in [0, 1]. If it is > 1, one of the losses in equation (3) is negated and is therefore maximised (instead of being minimised), which can lead to unexpected behaviour. Current parameterization allows it to be \\in [0, + infinity].\n\nExperimental evaluation consists of quantitative evaluation of random sampling (usual SGD) and the proposed approach in training a classification model on MNSIT, CIFAR-10 and CIFAR-100. The proposed approach outperforms random sampling. This is encouraging, but the method should be compared to state of the art in curriculum learning in order to gauge how useful this approach is.\n\nThe paper is poorly written, with many grammatical (lack of \u201cs\u201d at the end of verbs used in singular 3rd person, many places in the paper) and spelling mistakes (e.g. \u00a73.2\u00b66 \u201ctough\u201d instead of \u201cthrough\u201d, I think). Some descriptions are unclear (e.g. \u00a74.2\u00b62), while some parts of the paper seem to be irrelevant to the problem at hand (\u00a73.1 describes training on a single minibatch for multiple iterations as if it were a separate task and motivates random sampling, which is just SGD).\n\nTo summarize, the paper presents a very interesting idea. In its current state it is hard to read, however. It also contains a number of unsupported claims and can be misleading. It could also benefit from a more extensive evaluation. With this in mind, I suggest rejecting this paper.\n\n[1] Rahimi, A (2017). Test of Time Award Talk, NIPS.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper908/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating", "abstract": "Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse \"internal covariate shift\". Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on \"good\" samples to reduce large shifting, and encourage \"bad\" samples to learn from \"good\" sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.", "keywords": ["Curriculum Learning", "Internal Covariate Shift"], "authorids": ["sunbenyuan@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Benyuan Sun", "Yizhou Wang"], "pdf": "/pdf/4ae687244ce452107e640d4936afefc3321e8a50.pdf", "paperhash": "sun|learn_from_neighbour_a_curriculum_that_train_low_weighted_samples_by_imitating", "_bibtex": "@misc{\nsun2019learn,\ntitle={Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating},\nauthor={Benyuan Sun and Yizhou Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=r1luCsCqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "cdate": 1542234349326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper908/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335830549, "tmdate": 1552335830549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper908/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryle7pFQ3X", "original": null, "number": 1, "cdate": 1540754711917, "ddate": null, "tcdate": 1540754711917, "tmdate": 1541533588572, "tddate": null, "forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "content": {"title": "Interesting idea but limited experiments and analysis", "review": "This paper proposes a curriculum that encourages training on easy examples first and postpones training on hard examples. However, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example. The proposed objective is hence biased at the beginning but they dampen it over time to converge to the true objective at the end.\n\nPositives:\n- There is not much work considering each example as an individual subtask.\n- The observation that an under-fitted classifier can destroy a good feature extractor is good.\n\nNegatives:\n- In the intro it says \u201c[update rule of gradient descent] assumes the top layer, F2, to be the right classifier.\u201d. This seems like a fundamental misunderstanding of gradient descent and the chain rule. The term d output/d F1 takes into account the error in F2.\n- The caption of figure 2 says the \u201c... they cannot separate from its neighbors\u2026\u201d. If the loss of all examples in a cluster is high, all are being misclassified. A classifier then might have an easy job fixing them if all their labels are the same or have a difficult job if their labels are random. The second scenario is unlikely if based on the claim of this figure, the entropy has decreased during training. In short, the conclusion made in fig 2 does not necessarily hold given that figure.\n- This method is supposed to speed up training, not necessarily improve the final generalization performance of the model. The figures show the opposite outcomes. It\u2019s not clear why. The improvement might be due to not tuning the hyperparameters of the baselines.\n- Figure 3 does not necessarily support the conclusion. The fluctuations might be caused by any curriculum that forces a fixed ordering across training epochs. Often on MNIST, the ordering of data according to the loss does not change significantly throughout training.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper908/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating", "abstract": "Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse \"internal covariate shift\". Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on \"good\" samples to reduce large shifting, and encourage \"bad\" samples to learn from \"good\" sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.", "keywords": ["Curriculum Learning", "Internal Covariate Shift"], "authorids": ["sunbenyuan@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Benyuan Sun", "Yizhou Wang"], "pdf": "/pdf/4ae687244ce452107e640d4936afefc3321e8a50.pdf", "paperhash": "sun|learn_from_neighbour_a_curriculum_that_train_low_weighted_samples_by_imitating", "_bibtex": "@misc{\nsun2019learn,\ntitle={Learn From Neighbour: A Curriculum That Train Low Weighted Samples By Imitating},\nauthor={Benyuan Sun and Yizhou Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=r1luCsCqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper908/Official_Review", "cdate": 1542234349326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1luCsCqFm", "replyto": "r1luCsCqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper908/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335830549, "tmdate": 1552335830549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper908/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}