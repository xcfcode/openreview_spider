{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730182956, "tcdate": 1509089226659, "number": 285, "cdate": 1518730182947, "id": "B1mAkPxCZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "B1mAkPxCZ", "original": "HyG0yPxAW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING", "abstract": "A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ", "pdf": "/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf", "paperhash": "ma|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning", "_bibtex": "@misc{\nma2018vocabularyinformed,\ntitle={{VOCABULARY}-{INFORMED} {VISUAL} {FEATURE} {AUGMENTATION} {FOR} {ONE}-{SHOT} {LEARNING}},\nauthor={jianqi ma and hangyu lin and yinda zhang and yanwei fu and xiangyang xue},\nyear={2018},\nurl={https://openreview.net/forum?id=B1mAkPxCZ},\n}", "keywords": ["vocabulary-informed learning", "data augmentation"], "authors": ["jianqi ma", "hangyu lin", "yinda zhang", "yanwei fu", "xiangyang xue"], "authorids": ["14302010017@fudan.edu.cn", "16210240036@fudan.edu.cn", "yindaz@cs.princeton.edu", "y.fu@qmul.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260083338, "tcdate": 1517249955648, "number": 646, "cdate": 1517249955634, "id": "H13oH16rf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "Two reviewers recommended rejection, and one was on the edge. There was no rebuttal to address the concerns and questions posed by the reviewers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING", "abstract": "A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ", "pdf": "/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf", "paperhash": "ma|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning", "_bibtex": "@misc{\nma2018vocabularyinformed,\ntitle={{VOCABULARY}-{INFORMED} {VISUAL} {FEATURE} {AUGMENTATION} {FOR} {ONE}-{SHOT} {LEARNING}},\nauthor={jianqi ma and hangyu lin and yinda zhang and yanwei fu and xiangyang xue},\nyear={2018},\nurl={https://openreview.net/forum?id=B1mAkPxCZ},\n}", "keywords": ["vocabulary-informed learning", "data augmentation"], "authors": ["jianqi ma", "hangyu lin", "yinda zhang", "yanwei fu", "xiangyang xue"], "authorids": ["14302010017@fudan.edu.cn", "16210240036@fudan.edu.cn", "yindaz@cs.princeton.edu", "y.fu@qmul.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642425259, "tcdate": 1511831570311, "number": 1, "cdate": 1511831570311, "id": "Sk5zOVceG", "invitation": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "signatures": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Method needs to be clarified; experiments need to be improved", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a feature augmentation method for one-shot learning.  The proposed approach is very interesting. However, the method needs to be further clarified and the experiments need to be improved. \n\nDetails:\n1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read. \n\n2. The approach:\n(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3.  However, under one-shot learning, won\u2019t this  make each class still have only one instance for training? \n\n(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation? \n\n(3) It is not clear how will the vocabulary-information be exploited? In particular, how to ensure the semantic space u to be same as the vocabulary semantic space? How to generate the neighborhood in Neigh(\\hat{u}_i) on page 5? \n\n3.  In the experiments: \n(1) The authors didn\u2019t compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing. \n\n(2) The results are reported for different numbers of augmented instances. Clarification is needed. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING", "abstract": "A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ", "pdf": "/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf", "paperhash": "ma|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning", "_bibtex": "@misc{\nma2018vocabularyinformed,\ntitle={{VOCABULARY}-{INFORMED} {VISUAL} {FEATURE} {AUGMENTATION} {FOR} {ONE}-{SHOT} {LEARNING}},\nauthor={jianqi ma and hangyu lin and yinda zhang and yanwei fu and xiangyang xue},\nyear={2018},\nurl={https://openreview.net/forum?id=B1mAkPxCZ},\n}", "keywords": ["vocabulary-informed learning", "data augmentation"], "authors": ["jianqi ma", "hangyu lin", "yinda zhang", "yanwei fu", "xiangyang xue"], "authorids": ["14302010017@fudan.edu.cn", "16210240036@fudan.edu.cn", "yindaz@cs.princeton.edu", "y.fu@qmul.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642425157, "id": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper285/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer3", "ICLR.cc/2018/Conference/Paper285/AnonReviewer2", "ICLR.cc/2018/Conference/Paper285/AnonReviewer4"], "reply": {"forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642425157}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642425219, "tcdate": 1511864972718, "number": 2, "cdate": 1511864972718, "id": "SyBcch5lM", "invitation": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "signatures": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "This paper proposes a semantic approach for data augmentation, but the experiments are weak/unconvincing at this state", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample . Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space. For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features); b) perturbed decoded image; c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space.   \nThe idea is nice and simple, however the current framework has several weaknesses:\n1. The whole pipeline has three (neural network) components: a) input image features are extracted from VGG net pre-trained on auxiliary data; 2) auto-encoder that is trained on data for one-shot learning; 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers. This three networks need to be clearly described; ideally combined into one end-to-end training pipeline.\n2. The empirical performance is very poor. If you look into literature for zero shot learning, work by Z. Akata in CVPR 2015, CVPR2016, the performance on AwA and on CUB-bird goes way above 50%, where in the current paper it is 30.57% and 8.21% at most (for the most recent survey on zero shot learning papers using attribute embeddings, please, refer to Zero-Shot Learning - The Good, the Bad and the Ugly by Xian et al, CVPR 2017). It is important to understand, why there is such a big drop in performance in one-shot learning comparing to zero-shot learning? One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes). In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?). I am not sure, how can the auto-encoder model not overfit completely to the training data instances. Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting. \nOverall, I like the idea, so I am leaning towards accepting the paper, but the empirical evaluations are not convincing. \n\n \n\n \n\n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING", "abstract": "A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ", "pdf": "/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf", "paperhash": "ma|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning", "_bibtex": "@misc{\nma2018vocabularyinformed,\ntitle={{VOCABULARY}-{INFORMED} {VISUAL} {FEATURE} {AUGMENTATION} {FOR} {ONE}-{SHOT} {LEARNING}},\nauthor={jianqi ma and hangyu lin and yinda zhang and yanwei fu and xiangyang xue},\nyear={2018},\nurl={https://openreview.net/forum?id=B1mAkPxCZ},\n}", "keywords": ["vocabulary-informed learning", "data augmentation"], "authors": ["jianqi ma", "hangyu lin", "yinda zhang", "yanwei fu", "xiangyang xue"], "authorids": ["14302010017@fudan.edu.cn", "16210240036@fudan.edu.cn", "yindaz@cs.princeton.edu", "y.fu@qmul.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642425157, "id": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper285/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer3", "ICLR.cc/2018/Conference/Paper285/AnonReviewer2", "ICLR.cc/2018/Conference/Paper285/AnonReviewer4"], "reply": {"forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642425157}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642425172, "tcdate": 1513014169327, "number": 3, "cdate": 1513014169327, "id": "H1Wo7H2Zf", "invitation": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "signatures": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer4"], "readers": ["everyone"], "content": {"title": "An interesting approach to an important problem, but requires some clarifications / additional experiments in order to be convincing", "rating": "5: Marginally below acceptance threshold", "review": "Summary:\nThis paper proposes a data augmentation method for one-shot learning of image classes. This is the problem where given just one labeled image of a class, the aim is to correctly identify other images as belonging to that class as well. \nThe idea presented in this paper is that instead of performing data augmentation in the image space, it may be useful to perform data augmentation in a latent space whose features are more discriminative for classification. One candidate for this is the image feature space learned by a deep network. However they advocate that a better candidate is what they refer to as \"semantic space\" formed by embedding the (word) labels of the images according to pre-trained language models like word2vec. The reasoning here is that the image feature space may not be semantically organized so that we are not guaranteed that a small perturbation of an image vector will yield image vectors that correspond to semantically similar images (belonging to the same class). On the other hand, in this semantic space, by construction, we are guaranteed that similar concepts lie near by each other. Thus this space may constitute a better candidate for performing data augmentation by small perturbations or by nearest neighbour search around the given vector since 1) the augmented data is more likely to correspond to features of similar images as the original provided image and 2) it is more likely to thoroughly capture the intra-class variability in the augmented data.\nThe authors propose to first embed each image into a feature space, and then feed this learned representation into a auto-encoder that handles the projection to and from the semantic space with its encoder and decoder, respectively. Specifically, they propose to perform the augmentation on the semantic space representation, obtained from the encoder of this autoencoder. This involves producing some additional data points, either by adding noise to the projected semantic vector, or by choosing a number of that vector's nearest neighbours. The decoder then maps these new data points into feature space, obtaining in this way the image feature representations that, along with the feature representation of the original (real) image will form the batch that will be used to train the one-shot classifier.\nThey conduct experiments in 3 datasets where they experiment with augmentation in the image feature space by random noise, as well as the two aforementioned types of augmentation in the semantic space. They claim that these augmentation types provide orthogonal benefits and can be combined to yield superior results.\n\nOverall I think this paper addresses an important problem in an interesting way, but there is a number of ways in which it can be improved, detailed in the comments below. \n\nComments:\n-- Since the authors are using a pre-trained VGG for to embed each image, I'm wondering to what extent they are actually doing one-shot learning here. In other words, the test set of a dataset that is used for evaluation might contain some classes that were also present in the training set that VGG was originally trained on. It would be useful to clarify whether this is happening. Can the VGG be instead trained from scratch in an end-to-end way in this model?\n\n-- A number of things were unclear to me with respect to the details of the training process: the feature extractor (VGG) is pre-trained. Is this finetuned during training?  If so, is this done jointly with the training of the auto-encoder? Further, is the auto-encoder trained separately or jointly with the training of the one-shot learning classifier? \n\n-- While the authors have convinced me that data augmentation indeed significantly improves the performance in the domains considered (based on the results in Table 1 and Figure 5a), I am not convinced that augmentation in the proposed manner leads to a greater improvement than just augmenting in the image feature domain. In particular, in Table 2, where the different types of augmentation are compared against each other, we observe similar results between augmenting only in the image feature space versus augmenting only in the semantic feature space (ie we observe that \"FeatG\" performs similarly as \"SemG\" and as \"SemN\"). When combining multiple types of augmentation the results are better, but I'm wondering if this is because more augmented data is used overall. Specifically, the authors say that for each image they produce 5 additional \"virtual\" data points, but when multiple methods are combined, does this mean 5 from each method? Or 5 overall? If it's the former, the increased performance may merely be attributed to using more data. It is important to clarify this point.\n\n-- Comparison with existing work: There has been a lot of work recently on one-shot and few-shot learning that would be interesting to compare against. In particular, mini-ImageNet is a commonly-used benchmark for this task that this approach can be applied to for comparison with recent methods that do not use data augmentation. Some examples are:\n- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. (Finn et al.)\n- Prototypical Networks for Few-shot Learning (Snell et al.)\n- Matching Networks for One-shot Learning (Vinyals et al.)\n- Few-Shot Learning Through an Information Retrieval Lens (Triantafillou et al.)\n\n-- A suggestion: As future work I would be very interested to see if this method can be incorporated into common few-shot learning models to on-the-fly generate additional training examples from the \"support set\" of each episode that these approaches use for training.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING", "abstract": "A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ", "pdf": "/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf", "paperhash": "ma|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning", "_bibtex": "@misc{\nma2018vocabularyinformed,\ntitle={{VOCABULARY}-{INFORMED} {VISUAL} {FEATURE} {AUGMENTATION} {FOR} {ONE}-{SHOT} {LEARNING}},\nauthor={jianqi ma and hangyu lin and yinda zhang and yanwei fu and xiangyang xue},\nyear={2018},\nurl={https://openreview.net/forum?id=B1mAkPxCZ},\n}", "keywords": ["vocabulary-informed learning", "data augmentation"], "authors": ["jianqi ma", "hangyu lin", "yinda zhang", "yanwei fu", "xiangyang xue"], "authorids": ["14302010017@fudan.edu.cn", "16210240036@fudan.edu.cn", "yindaz@cs.princeton.edu", "y.fu@qmul.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642425157, "id": "ICLR.cc/2018/Conference/-/Paper285/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper285/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper285/AnonReviewer3", "ICLR.cc/2018/Conference/Paper285/AnonReviewer2", "ICLR.cc/2018/Conference/Paper285/AnonReviewer4"], "reply": {"forum": "B1mAkPxCZ", "replyto": "B1mAkPxCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642425157}}}], "count": 5}