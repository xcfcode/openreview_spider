{"notes": [{"id": "SklKcRNYDH", "original": "BklHNau_vS", "number": 1291, "cdate": 1569439377283, "ddate": null, "tcdate": 1569439377283, "tmdate": 1583912038492, "tddate": null, "forum": "SklKcRNYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "SyleXNLiiH", "original": null, "number": 1, "cdate": 1573770264214, "ddate": null, "tcdate": 1573770264214, "tmdate": 1583536218951, "tddate": null, "forum": "SklKcRNYDH", "replyto": "r1lCAy65cH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment", "content": {"title": "Response to R2", "comment": "@Camera-ready update: Done. We replicated the results for SM3, and incorporated the very helpful suggestions for discussing how this fits in with other literature.\n\n===\n\nThanks for the particularly thoughtful review. We have incorporated the minor points into the current revision. Responses to the major points below:\n\n@Tensor approximability: Our original intuition for low-rank tensor preconditioning came from the ubiquitous use of SVD for compression. Unlike SVD, it is difficult to usefully characterize the \u201cbeyond worst-case\u201d gap between the presented tensor approximation and the true preconditioner. Indeed, if there are sparsity structures present inside a particular tensor slice, the part of the O(d) gap attributed to that slice can be reduced to O(sparsity). One could potentially theorize such sparse models of the data and architecture. Instead, we take the route of measuring this \u201cgap\u201d empirically, and see that it is indeed far from worst-case. By now there is strong evidence that such approximations are useful for preconditioners, and it is an interesting future direction to characterize precisely what underlying structures enable this.\n\n@SM3: Indeed, the algorithm proposed by [Anil et al. \u201819] provides another flexible way to do low-memory preconditioning. Since our main emphasis is on an empirical study of the memory vs performance tradeoff and SM3 also enables this, we would be happy to run our tradeoff experiment with SM3 as well. An important point to note is that ET_\\infty, a special case of our algorithm but not SM3, is studied in its own right as \u201cadaptive OGD\u201d (see, e.g. [3]).\n\n@L-BFGS memory comparison: We were not aware of these works on the L-BFGS memory-performance tradeoff, and will add a note in the paper. Since quasi-Newton methods work in a totally different regime than adaptive preconditioning (they tend to behave poorly in typical deep learning settings due to stochasticity), the methods/experiments/conclusions appear to be orthogonal.\n\n@OCO vs non-convex: We are very sympathetic to this point. Connecting insights and theorems from {online, stochastic} convex optimization to their effectiveness (or lack thereof) in the modern deep learning setup remains a major research program in this space (see, e.g. [1,2,3,4]), to which there has not yet been a totally satisfying account. We will add some more comments on some theoretical approaches to bridge the gap. However, we are purposefully not trying to get into the larger debate in this work, instead focusing on algorithms and empirics.\n\n[1] Escaping Saddle Points with Adaptive Gradient Methods. Staib et al., ICML \u201819.\n[2] Optimal Adaptive and Accelerated Stochastic Gradient Descent. Deng et al., arXiv \u201818.\n[3] AdaGrad stepsizes: Sharp convergence over non-convex landscapes, from any initialization. Ward et al., ICML \u201819.\n[4] On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization. Zhou et al., arXiv \u201818."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklKcRNYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1291/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1291/Authors|ICLR.cc/2020/Conference/Paper1291/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158256, "tmdate": 1576860530315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment"}}}, {"id": "CIwDpoJ9ZX", "original": null, "number": 1, "cdate": 1576798719501, "ddate": null, "tcdate": 1576798719501, "tmdate": 1576800917014, "tddate": null, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Post author rebuttal the score of this paper increased.\nDiscussions with reviewers were substantive and the AC recommends acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713350, "tmdate": 1576800262945, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Decision"}}}, {"id": "r1lCAy65cH", "original": null, "number": 3, "cdate": 1572683734496, "ddate": null, "tcdate": 1572683734496, "tmdate": 1574639647448, "tddate": null, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "============= Update after rebuttal\n\nThanks for the clarifications; I have updated by score and recommend acceptance.\n\nPlease do not forget to implement the promise changes in the camera ready version (e.g. about L-BFGS tradeoff work; as well as gap between OCO and non-convex framework).\n\n\n==============\n\nMotivated from NLP applications where models with billions of parameters are used, this paper proposes a *memory efficient* variant of Adagrad by maintaining a rank-one tensor approximation of the \"second-moment\" accumulator normally used in Adagrad. From the theoretical side, a simple regret bound is proved which provides an intuitive quantity quantifying the convergence guarantee loss for memory-efficient version (vs. Adagrad), and which is empirically evaluated to be small on a large-scale language modeling task (Section 5.3). From the empirical side, the memory vs. generalization performance tradeoff is evaluated on this language modeling task, showing that similar performance to Adagrad can be obtained with much a smaller (optimization overhead) memory footprint. A convex toy task also indicate a similar result.\n\nI like this paper, I am leaning towards acceptance. The write-up can be improved in a few places (see detailed comment below); but overall, I find the idea refreshing for optimization and the proof is simple and elegant.\n\nThe main motivation for the paper is that these large models used in NLP are often making us hit the memory limitation of the hardware just to store the model, and so one then has to tradeoff the size of the model with the memory requirement of the *optimization* algorithm (e.g. one step-size accumulator per dimension for Adagrad). Trying to get the gains of an adaptive preconditioned gradient method but with lower memory footprint thus seems valuable (and Section 5.2 which compares doubling the size of the model + using their memory efficient method vs. original model + Adagrad highlights the gains one can get).\n\nOn the other hand, I wish the paper provided a bit more intuition on when the rank-one tensor approximation structure will give good results (e.g. Section 5.3 gives an empirical measure; but what about some simple theoretical examples which give low values, to provide more intuitions?). In particular, the main inequality relating the Adagrad step-size with the \"extreme indexing\" step-size is arising from the first equation in the proof of Lemma 4.3 on p.5 -- basically one replace one entry of the squared gradient to store with the whole sum over all entries of the squared gradient except a slice. This inequality can be very loose (O(d) in the worst-case), so it is surprising that one could get good approximation ratios much better than the O(sqrt(d)) worst-case (a square-root is taken afterwards), and I wonder what structure yields this.\n\nI also note that Anil et al. (2019) tackles a similar problem with a different approach, and so ideally a more detailed comparison (theoretically and empirically) would be provided (rather than just one sentence as in the current submission), especially since this appeared on arXiv 9 months before the ICLR deadline. But the authors presented it as concurrent work (and it does not appear published anywhere yet) [and a Google search shows the authors had a first version on arXiv only a few weeks after this one, so this seems righs], and so I decided to not hold it against this submission. Disclaimer: I made my general opinion about the paper before doing these Google searches and finding the paper on arXiv.\n\nI am not very familiar with the modern transformer architecture experiments, so cannot evaluate their quality, but they seem sensible from an outsider's perspective.\n\n== Detailed comments ==\n\n- p.1 \"the first empirical study of the tradeoff between training convergence and memory in the optimizer.\" This should be properly qualified by \"in the Adagrad setup\" or something like that; I am pretty sure it is false *for general optimizers*. For example, I guess there are several papers which empirically studied the memory - performance tradeoff for L-BFGS optimizers...\n\n- Important: I think the paper should be more transparent about the *big gap* between the online *convex* optimization framework and stochastic optimization for a *non-convex* loss. There is only one sentence at the end of Section 2.1 mentioning vaguely that Agarwal et al. (2018) provide a reduction from stochastic non-convex setting to the online convex setting, but this is buried in the appendix of the said paper, with several caveats, and for a modified algorithm (i.e. as far as I understand, no convergence guarantee would be given for Algorithm 1 to find a stationary point on a non-convex loss; but one could apply Algorithm 1 on a series of convex problems to obtain overall guarantees on the non-convex loss [but this is a different algorithm!]). I suggest that either the argument of Agarwal et al. (2018) is summarized in a few sentences at the end of Section 2.1 to clarify the real link; or give more caveats [also just before 4.1] (e.g., my current guess is that we use the convex analysis setup to gain insights on the behavior in a controlled setup; and then just hope that some of it applies in the non-convex setup, even though no guarantees is provided whatsoever). \n\n- Some undefined notation: line 8 of Algorithm 1, it seems they use the dot for the Hadamard product between two vectors. The big dot in Lemma 4.5 is most likely the dot product between matrices (but please mention explicitly for clarity). \n\n- Typos in Section 5.1 & 5.2: several references seem wrong. E.g. Figure 5 is probably Figure 2; Table 4 is Table 1, etc... Please correct!\n\n- Clarity: I suggest to put much more description in the captions of Figure 2, Table 2, etc. (for example it was much clearer in their arXiv version). I was very confused by the extra light blue dot in Figure 2 which is only explained in 5.2; I suggest that it is already mentioned (with forward pointer) in the caption. Use more of p.9 for the clarity sake...\n\n- Appendix B.1: please provide hardware information when you give wall-clock comparison.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575218008089, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Reviewers"], "noninvitees": [], "tcdate": 1570237739513, "tmdate": 1575218008102, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review"}}}, {"id": "HJx3ljfTFB", "original": null, "number": 1, "cdate": 1571789556060, "ddate": null, "tcdate": 1571789556060, "tmdate": 1574450649770, "tddate": null, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "\n============================== Update after rebuttal =======================================================\n\nI did not have any major concerns about the paper in my initial review, only some suggestions for improving the presentation. The authors have addressed most of these issues in their revision. I would like to keep my score as it is. The work seems simple and sound, but somewhat incremental. To have a more meaningful impact, I strongly encourage the authors to make an optimized implementation publicly available as an easy-to-use, plug-and-play type optimizer.\n\n========================================================================================================\n\nThis paper proposes a new memory-efficient pre-conditioning scheme for stochastic optimizers. The basic idea is to store a coarse-grained pre-conditioner, expressed as a rank-one tensor product, instead of the full-dimensional pre-conditioner typically used in algorithms like AdaGrad. I am not very familiar with prior work in this literature, but the proposed approach seems simple and sound. \n\nA regret bound is provided for the proposed method, however the analysis here seems to be a straightforward application of the results and techniques from a few prior works. So, I was a bit surprised to see so much space devoted to the proofs. These can be safely moved to the appendix in my opinion. Moreover, the bound does not seem to be very useful in practice. For example, in simulations in Figure 3, the proposed ET1 performs better than AdaGrad, but the bound is not able to capture this at all. \n\nThe presentation of the experimental results in section 5 can be improved in my opinion. The authors keep referring to \u201cFigure 5\u201d in this section, but I think this is a typo and these should be \u201cFigure 2\u201d instead. In Figure 2, please indicate on the figure itself what dark blue and light blue colors correspond to (smaller and larger models, respectively). \n\nIn Appendix B, wall clock results are presented for different algorithms. These show that the proposed approach is slower than standard algorithms like Adam or AdaGrad. Please explicitly mention this result in the main text (last paragraph of section 5.1) and discuss why this is the case (is this because of the extra reshaping operations required in the updates?).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575218008089, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Reviewers"], "noninvitees": [], "tcdate": 1570237739513, "tmdate": 1575218008102, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review"}}}, {"id": "rkgDq6IijS", "original": null, "number": 5, "cdate": 1573772687092, "ddate": null, "tcdate": 1573772687092, "tmdate": 1573772687092, "tddate": null, "forum": "SklKcRNYDH", "replyto": "B1eZlFLisH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment", "content": {"title": "Done", "comment": "Uploaded another revision. Moved Table 3 & Appendix B.1 to the main paper, and a note on the time overhead."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklKcRNYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1291/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1291/Authors|ICLR.cc/2020/Conference/Paper1291/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158256, "tmdate": 1576860530315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment"}}}, {"id": "B1eZlFLisH", "original": null, "number": 4, "cdate": 1573771496790, "ddate": null, "tcdate": 1573771496790, "tmdate": 1573771496790, "tddate": null, "forum": "SklKcRNYDH", "replyto": "SkeEI4Uijr", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment", "content": {"title": "please mention wall clock results in main text regardless", "comment": "Thanks for the response. Could you please mention the wall clock results (and the need for extra reshape/reduce operations) in the main text regardless of whether they can be improved with a more careful implementation?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklKcRNYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1291/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1291/Authors|ICLR.cc/2020/Conference/Paper1291/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158256, "tmdate": 1576860530315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment"}}}, {"id": "SkeEI4Uijr", "original": null, "number": 3, "cdate": 1573770316250, "ddate": null, "tcdate": 1573770316250, "tmdate": 1573770316250, "tddate": null, "forum": "SklKcRNYDH", "replyto": "HJx3ljfTFB", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thanks for the review. We\u2019ve fixed both cosmetic points in the revision.\n\n@Relevance of theory: We believe that the theoretical presentation is still useful to develop the motivation and basic guarantee, a spectral approximation of the uncompressed AdaGrad preconditioner. The numerical regret bounds are to provide some insight on the approximation factor in practice. Nevertheless, we agree that it remains a significant open problem to bridge the gap between convex vs non-convex theory for adaptive optimizers. See response to Reviewer 2. \n\n@Appendix B: Indeed, there is a wall-clock performance gap arising from reshape/reduce operations, which can possibly be improved with a more careful implementation (which may be architecture-specific). We didn\u2019t attempt to optimize the wall-clock time here, beyond providing an implementation that worked on the large-scale NLP setting."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklKcRNYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1291/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1291/Authors|ICLR.cc/2020/Conference/Paper1291/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158256, "tmdate": 1576860530315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment"}}}, {"id": "BJlvN4IsiB", "original": null, "number": 2, "cdate": 1573770286719, "ddate": null, "tcdate": 1573770286719, "tmdate": 1573770286719, "tddate": null, "forum": "SklKcRNYDH", "replyto": "H1grZ3oJ9r", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thanks for the encouraging review. We\u2019ve fixed the figure reference typo in the revision.\n\n@Large-scale experiments outside NLP: We focus exclusively on the large-scale NLP setting because it\u2019s the one where studying the memory-performance tradeoff is uniquely important. Also, carefully-tuned SGD often achieves state-of-the-art on vision benchmarks; in NLP we can measure more clearly the advantages of adaptivity.\n\n@Practitioner-friendly exposition: To this end, we will release sample code."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklKcRNYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1291/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1291/Authors|ICLR.cc/2020/Conference/Paper1291/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158256, "tmdate": 1576860530315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Authors", "ICLR.cc/2020/Conference/Paper1291/Reviewers", "ICLR.cc/2020/Conference/Paper1291/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Comment"}}}, {"id": "H1grZ3oJ9r", "original": null, "number": 2, "cdate": 1571957756898, "ddate": null, "tcdate": 1571957756898, "tmdate": 1572972487913, "tddate": null, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "invitation": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper considers the problem of the need for memory-efficient optimizers given the increase in model complexity. They study memory-efficient adaptively preconditioned gradient methods, and see the trade-offs among expressivity and preconditioner quality. They show results on a large-scale NLP model, and show that the memory overhead can be reduced by 3 orders of magnitude, without sacrificing performance. \n\n+ show tradeoff among training convergence and memory in the optimizer.\n+ introduce extreme tensoring -- a modification that can be applied to *any* 2nd moment-based adaptive optimizer. It uses a compressed preconditioner. \n+ extend diagonal Shampoo to tensor factorization \n+ nicely written Related Work.\n+ applied their idea to widely used optimizers, such as AdaGrad, Adam, etc.\n+ the derived regret bound is only a multiplicative constant from the regret bound of AdaGrad.\n+ interesting experiments showing tradeoff among lack of memory (SGD) and full memory (AdaGrad), applied on a real-world machine learning setting of large-scale natural language modeling with Transformers. They show how their extreme tensoring modification can achieve an intermediate ground among lack of memory and full memory.\n- I am missing a plot similar to Figure 4 but instead of being on the synthetic data, being on the application of NLP.\n\nOverall, although I do not have a lot of experience in this area, it appears to be that the introduced family of algorithms can be promising as a nice interpolation among SGD-type algorithms and AdaGrad ones. I would personally have preferred to see a wider range of experiments, considering other application scenarios besides NLP, just to see the wide success of the proposed approach. Also, I would urge the authors to spend a bit more text on explaining the algorithm, so that it is easy for practitioners to try it out."}, "signatures": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1291/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extreme Tensoring for Low-Memory Preconditioning ", "authors": ["Xinyi Chen", "Naman Agarwal", "Elad Hazan", "Cyril Zhang", "Yi Zhang"], "authorids": ["xinyic@google.com", "namanagarwal@google.com", "ehazan@cs.princeton.edu", "cyril.zhang@cs.princeton.edu", "y.zhang@cs.princeton.edu"], "keywords": ["optimization", "deep learning"], "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.", "pdf": "/pdf/cbc8beb0d5c1eb1fa1422f556e190231cbd11556.pdf", "paperhash": "chen|extreme_tensoring_for_lowmemory_preconditioning", "_bibtex": "@inproceedings{\nChen2020Extreme,\ntitle={Extreme Tensoring for Low-Memory Preconditioning },\nauthor={Xinyi Chen and Naman Agarwal and Elad Hazan and Cyril Zhang and Yi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklKcRNYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cb4811fc71b7d6a4b2c8365f5626d0a09fe60bad.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklKcRNYDH", "replyto": "SklKcRNYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575218008089, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1291/Reviewers"], "noninvitees": [], "tcdate": 1570237739513, "tmdate": 1575218008102, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1291/-/Official_Review"}}}], "count": 10}