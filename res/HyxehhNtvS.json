{"notes": [{"id": "HyxehhNtvS", "original": "ryglnG_WvH", "number": 176, "cdate": 1569438887993, "ddate": null, "tcdate": 1569438887993, "tmdate": 1577168216160, "tddate": null, "forum": "HyxehhNtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dYBT7SGqoB", "original": null, "number": 1, "cdate": 1576798689436, "ddate": null, "tcdate": 1576798689436, "tmdate": 1576800945708, "tddate": null, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "invitation": "ICLR.cc/2020/Conference/Paper176/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies the problem of optimization for neural networks, by comparing the optimization problem in parameter space with the corresponding problem in function space. It argues that overparametrised models leads to a convex problem formulation leading to global optimality. \n\nAll reviewers agreed that this paper lacks mathematical rigor and novelty relative to the current works on overparametrised neural networks. Its arguments need to be substantially reworked before it can be considered for publication, and as a consequence the AC recommends rejection. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730258, "tmdate": 1576800283015, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper176/-/Decision"}}}, {"id": "SJxjTucgcr", "original": null, "number": 1, "cdate": 1572018370770, "ddate": null, "tcdate": 1572018370770, "tmdate": 1572972629254, "tddate": null, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "invitation": "ICLR.cc/2020/Conference/Paper176/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper claims to analyze the global convergence of large-scale neural networks (NNs). Their analysis relies on the idea of \u201ccanonical space\u201d, which upon further reading, is nothing more than multi-dimensional Fourier analysis.\n\nWhile the analysis of NNs is an extremely important problem, it is not clear what results are actually presented here nor how their results relate to other theoretical analyses of NNs. The introduction of Fourier analysis as a new mathematical tool, and devotion of a whole page to this \u201ccanonical space model\u201d is almost laughable. Furthermore, this statement in the introduction is not only incorrect but it is absurd. To make such a claim would require a significant amount of background and review of literature to even place these statements in context.\n        \u201cNo matter what structures are used in a large scale neural network, either feed-forward or recurrent, either convolutional or fully-connected, either ReLU or sigmoid, the simple first-order methods such as stochastic gradient descent and its variants can consistently converge to a global minimum of zero loss no matter what type of labelled training samples are used.\u201d \n\nSpecific comments:\n\u201cto derive theoretical proofs under a very general setting without any unrealistic assumption on the model structure and data distribution.\u201d - how is this possible?\n\u201cA new mathematical tool called canonical space\u201d = Fourier analysis (??) this is not a new mathematical tool\nSection 2 = a standard formulation of neural networks, followed by undergraduate level Fourier analysis.\nIn 2.1. What is a \u201cwell structured\u201d network? Unclear from the notation and definitions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575774487258, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper176/Reviewers"], "noninvitees": [], "tcdate": 1570237755932, "tmdate": 1575774487272, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper176/-/Official_Review"}}}, {"id": "rken2bFEcS", "original": null, "number": 3, "cdate": 1572274612020, "ddate": null, "tcdate": 1572274612020, "tmdate": 1572972629211, "tddate": null, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "invitation": "ICLR.cc/2020/Conference/Paper176/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper presents an argument that massively overparameterized neural networks trained with gradient descent on a supervised learning problem with convex loss function will converge. It argues that by mapping each model to a truncated Fourier series we can recover a canonical representation for the functions in the model space. Viewing this as a linear map at each point in parameter space and assuming that this matrix is always full rank, they claim convergence of gradient descent.\n\n\nDecision:\nThis paper should be rejected because it lacks originality, the assumptions are often too strong, and the rigor of some claims is questionable.\n\nMain argument:\nOriginality:\nThis is the main issue with the paper. Several papers, for example the ones from Allen-Zhu et al. and Du et al. cited in the paper, have shown that gradient descent on supervised learning provably converges for massively overparameterized neural networks. Not only that, but those works give rigorous proofs of their claims and even handle issues like step size, convergence rates, and precise conditions on the size and architecture of the neural network. This paper does not attempt to handle any of these details, claiming to simplify the proof. To me the proof is only simplified since the details are all omitted.\n\nAssumptions:\nThe main way that details are avoided is by adding a strong assumption that the \u201cdisparity matrix\u201d remains full rank over the course of training. This assumption is not supported in an empirical or formal way, but rather by appeal to some vague argument in section 3 that since the matrix is likely to be full rank at initialization, it is likely to remain so for large enough networks. \n\nRigor:\nAs explained above, many details are omitted in favor of strong assumptions, but there are also some technical details that I think may be wrong. On page 2, it is claimed that the mapping from Lambda_M to L^1 is surjective, this is not true. Every function in L^1 can be approximated by some function in Lambda^M, but for finite M this map cannot be surjective. Another similar issue arises in the proof of Theorem 1 when it is claimed that there is a unique theta_epsilon corresponding to each f. This is again false since by truncating the Fourier series, infinitely many functions will get mapped to the same truncation (when they only differ in the higher coefficients). These issues make me question the validity as well as the originality of the arguments presented.\n\n\nAdditional feedback:\n- I saw a few spelling and grammatical errors. For example: in the abstract the tenses alternate between present and past almost every sentence (\u201cwe have proved\u201d should be \u201cwe prove\u201d),  page 2 \u201cparametarize\u201d, page 3 \u201cserie\u201d, page 5 \u201cdistint\u201d, page 7 \u201cLiptschitz\u201d, section 3.3 title should be \u201cwhen does\u201d not \u201cwhen do\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575774487258, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper176/Reviewers"], "noninvitees": [], "tcdate": 1570237755932, "tmdate": 1575774487272, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper176/-/Official_Review"}}}, {"id": "ryl74Xw_cr", "original": null, "number": 4, "cdate": 1572528939370, "ddate": null, "tcdate": 1572528939370, "tmdate": 1572972629163, "tddate": null, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "invitation": "ICLR.cc/2020/Conference/Paper176/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the problem of optimization for neural networks. It compares the optimization problem in parameter space with the corresponding problem in function space. In particular, it parameterizes function space using Fourier coefficients, so that the new problem is convex. When the \"disparity matrix\" (Jacobian) of the mapping from parameter space into function space has full rank, then critical points in parameter spaces are critical points in function space and hence global minima. The paper concludes by stating that \"an over-parameterized neural network is randomly initialized [...] will converge to a global minimum of zero loss in probability one\".\n\nI believe that the paper in its current form should be rejected. The main reason is that the second part of the paper is not rigorous, and the results that are shown do not imply that the optimization process will converge to a global optimum (contrary to what is explicitly stated). Indeed, while it is reasonable to assume that the \"disparity matrix\" should have full rank at initialization, this may not remain true throughout the training process. \nThere are also other technical problems, discussed below.\n\nSome comments:\n\n* The notation Q(.) is used inconsistently (Q(f), Q(w) and Q(\\theta)). The same is true for f_*, (f_w and f_\\theta).\n* I believe the disparity map is simply the Jacobian of the mapping w->theta.\nAlso, there are missing gradient symbols in eq. 8.\n* The \"canonical model space\" is not clearly defined. I believe it is simply L1(U) parameterized in a certain way, but this should be stated.\n* By considering the mapping into the truncated \"canonical model space\" (i.e., using a finite number of Fourier coefficients) then f_theta is not exactly equal to f_w. In particular, if f_theta has zero loss, then the same is not necessarily true for f_w. Thus, we cannot conclude that f_w is a global minimum. \n* The last statement in Theorem 2 is not clear (\"the trajectory [...] behaves in the same as those in typical convex optimization problems\"). From the proof, I believe it should mean that the disparity map never vanishes, but I don't understand why this is relevant (we would like for it to have full rank).\n* The main result of the paper rests on the claim \"When we use the gradient descent algorithms in Algorithm 1 to update the model, the chance to derive any new dead or duplicated neurons is extremely slim because it is unlikely for all parameters to simultaneously satisfy a large number of equality constraints\" but this is not rigorous and wrong.\n* Some typos: \"pointwise distint\", \"suface\""}, "signatures": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575774487258, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper176/Reviewers"], "noninvitees": [], "tcdate": 1570237755932, "tmdate": 1575774487272, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper176/-/Official_Review"}}}, {"id": "SJgepihG9H", "original": null, "number": 2, "cdate": 1572158392106, "ddate": null, "tcdate": 1572158392106, "tmdate": 1572972629120, "tddate": null, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "invitation": "ICLR.cc/2020/Conference/Paper176/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper looks at the neural net training problem in a \"canonical space\" which is parameterized by the Fourier coefficients of the function. This canonical space is a bijection of the function space L^1([0, 1]^K), and if we allow an epsilon approximation of the function we can truncate the Fourier coefficients so that the canonical space is finite-dimensional. The paper shows that in the canonical space, the training problem is always convex. Going back to the literal space (original parameter space for a neural network), it is shown that as long as a \"disparity matrix\" remains full rank, gradient descent will converge to a global minimum.\n\nI don't think this paper has anything new or non-trivial. I also don't think it's helpful to look at the canonical space proposed in the paper. In particular, it just transfers the difficulty of the problem into a disparity matrix, which we actually don't have control over. The paper claims that the matrix can be made full rank. This is not correct. Maybe one can prove it's full rank at random initialization, but I don't see how to prove this throughout training. The authors would need to provide a rigorous proof in order to claim this.\n\nIn fact, in non-convex optimization it's easy to arrive at a scenario where you \"only\" need some matrix to remain full rank in order to prove convergence to global minimum. One such example is the recent series of work on neural tangent kernel (NTK). There, as long as the NTK matrix stays full rank (actually, one needs eigenvalues bounded away from 0), one can show convergence to global minimum. However, to actually show this, one needs to apply stringent assumptions on the neural network architecture and to devote dozens of pages to the proof."}, "signatures": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper176/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. ", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization", "keywords": ["function space", "canonical space", "neural networks", "stochastic gradient descent", "disparity matrix"], "pdf": "/pdf/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "authors": ["Hui Jiang"], "TL;DR": "Some theoretical work on why learning of large neural networks converges to a global minimum in probability one", "authorids": ["hj@cse.yorku.ca"], "paperhash": "jiang|why_learning_of_largescale_neural_networks_behaves_like_convex_optimization", "original_pdf": "/attachment/71a15a221e19f7e4386c720c5615ae503fc7b577.pdf", "_bibtex": "@misc{\njiang2020why,\ntitle={Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization},\nauthor={Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxehhNtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxehhNtvS", "replyto": "HyxehhNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575774487258, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper176/Reviewers"], "noninvitees": [], "tcdate": 1570237755932, "tmdate": 1575774487272, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper176/-/Official_Review"}}}], "count": 6}