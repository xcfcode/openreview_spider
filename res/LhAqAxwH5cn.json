{"notes": [{"id": "LhAqAxwH5cn", "original": "lkWaJMe1AcM", "number": 153, "cdate": 1601308025804, "ddate": null, "tcdate": 1601308025804, "tmdate": 1614985658861, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gHv6lOpnwSW", "original": null, "number": 1, "cdate": 1610040503328, "ddate": null, "tcdate": 1610040503328, "tmdate": 1610474110370, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "the paper undoubtedly tackles an interesting problem in the mainstream of learning with partial / unknown / weak / noisy / complementary labels. The authors have had a set of constructive suggestions and questions from the reviewers (and external comments), some positive, some negative. I find it a bit unsettling that to several major questions, the main feedback from the authors was a citation in the paper with no further action; (a) ablation tests of R2 end up in citing papers from a public comment, (b) R4 raised a key point in comment 2, with the links to partial labels learning. The authors\u2019 answer is not satisfying as one would have hoped at least of a partial justification of the author\u2019s approach in this context. The authors would have had time to develop at least elements of a formal comparison. Just citing the work is not enough; \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040503314, "tmdate": 1610474110354, "id": "ICLR.cc/2021/Conference/Paper153/-/Decision"}}}, {"id": "To1JEdyxFiX", "original": null, "number": 2, "cdate": 1605510969868, "ddate": null, "tcdate": 1605510969868, "tmdate": 1606235857536, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "Oe0b3Sobol", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "comment": "1. Answer: Thank you very much for your suggestion. We are studying the two methods in [A] and [B], and considering a more effective model to improve the performance of complementary learning. \nWe have updated the conclusion: More methods should be studied to improve the performance of complementary learning in our future works, such as [A] and [B].  \n[A]: Amid, Ehsan, Manfred K. Warmuth, and Sriram Srinivasan. \"Two-temperature logistic regression based on the Tsallis divergence.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019. \n[B]: Amid, Ehsan, et al. \"Robust bi-tempered logistic loss based on bregman divergences.\" Advances in Neural Information Processing Systems. 2019.\n2. Answer: Thank you very much for your question. In complementary-label learning, there are no ordinary labels in the validation set and the training set. Our goal is to train a classifier to predict the ground-true label for any sample drawn from the same distribution as the training set. So, the lower the validation accuracy, the better the classifier learns from the training set.\n3. Answer: Thank you for your helpful suggestion. The form of a complementary label is that a sample only assigned a label that specifies the class it does not belong to. For example, $(x, 1)\\in \\bar{\\mathcal{S}}$, it means that sample $x$ does not belong to $class-1$.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Paper153/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "WcgomHxZFdu", "original": null, "number": 6, "cdate": 1605511981824, "ddate": null, "tcdate": 1605511981824, "tmdate": 1606235756712, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "1z6RJuECzY-", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "comment": "Thank you for your helpful suggestion. We have referenced the two papers and updated our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper153/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Paper153/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "6T77RkqCXsr", "original": null, "number": 7, "cdate": 1605884763656, "ddate": null, "tcdate": 1605884763656, "tmdate": 1605884878260, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "mrd7htXUnmA", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Thank you.", "comment": "I have read the authors' rebuttal and they have answered my questions.  I have increased my score. Though, I am still on the margin regarding this paper, especially after reading some of the comments from other reviewers regarding the connection of this work with \"Partial Labels\" literature."}, "signatures": ["ICLR.cc/2021/Conference/Paper153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "4fW94VG_Hqa", "original": null, "number": 2, "cdate": 1603892278599, "ddate": null, "tcdate": 1603892278599, "tmdate": 1605884779986, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Review", "content": {"title": "An OK paper. Exposition needs to be improved. Interesting Theory. Results not fully convincing.", "review": "Summary:\n=======\nThis paper deals with the problem of complementary label learning, that is, when we know the set of labels which a given observation does not belong to. In particular, the paper proposes a robust loss function and an algorithm for learning from complimentary labels. Results shown on MNIST and CIFAR datasets indicate the superior accuracy using the proposed loss function.\n\n\nComments:\n==========\nThe paper addresses an important problem but it is written in a hurry which makes it hard to assess its contribution. There are many typos and other writing issues in the paper. The experiments are also weak. Though, the theoretical results are interesting and improve the previous known results for complementary label learning along certain dimensions. \n\n\n1). Typically, in ML robust loss function means a loss function that is robust to outliers, e.g., the Huber Loss. However, the definition of robustness of loss function is different in this paper. However, in this paper it means if the loss function with ordinary and complementary labels has the same minimizer. I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem.\n\n\n2). The results in Table 2 are not an apples-to-apples comparison. The numbers for GA, PC, Fwd are copied directly from other papers. In order to be fair, they should also similar base models as the authors. For instance, GA used used MLP which is less complex than the model used by the authors. So, it is unclear whether the improved performance is due to the difference in base architecture or due to the proposed robust loss function. \n\n\nTypos: \n\nPage 1: \"However, label such a large-scale dataset is time-consuming...\"\nPage 1: \"In the view of label noise, complementary labels can also be view as...\"\nPage 3: \"...only complementary labels that specific the samples does not...\"\nMany others!", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149284, "tmdate": 1606915801050, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Review"}}}, {"id": "KrG1ARNRmMV", "original": null, "number": 5, "cdate": 1605511825563, "ddate": null, "tcdate": 1605511825563, "tmdate": 1605511825563, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "IdSf90F0X_D", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "comment": "1. Answer: Thank you very much for your question. To our best knowledge, complementary-label learning was first proposed by Ishida et al., 2017 ([A]). Lately, complementary-label learning has been applied to many applications:\n\u2022\tOnline learning (Kaneko et al., 2019)\n\u2022\tGenerative discriminative learning (Xu et al., 2019)\n\u2022\tMedical image segmentation (Rezaei et al., 2019)\nThis suggests applying complementary labels to other domains may be useful, such as collecting survey data, which can be an attractive future direction. \n[A]: Ishida et al., 2017, \"Learning from complementary labels.\"\n2. Answer: Thank you very much for your suggestion. We have updated our introduction to connecting to partial label learning. \u201c\u2026 partial label learning~\\cite{cour2011learning, feng2018leveraging, wu2018towards} and others.\u201d\n3. Answer: Thank you very much for your question. Maybe it's a little unclear. $\\theta$ is the arbitrary parameters in the searching space, $\\theta^{\\ast}$ is the parameters of a minimizer of risk in the training set. The difference between the two losses is that the training set is completely different. One contains complementary labels; another contains ordinary labels.\n4. Answer: Thank you very much for your comments. Maybe it's a little unclear. In our paper, $\\theta^{\\ast}$ is the finite samples optimal model, $f$ represents the network architecture, and $\\theta$ is the parameters. In Eq.(14), we prove by contradiction, to indicate that the optimal model $f_{\\theta^{\\ast}}$ supporting the Eq.(15). We have updated our paper, adding more details.\n5. Answer: In the MNIST dataset, we achieved a higher than 95% test accuracy in the complementary-label dataset near the ordinary-label dataset. But in a more complex dataset, such as in cifar10, learning with the complementary-label dataset is far from learning with the ordinary-label dataset since a single ordinary label corresponds to (K-1) complementary labels. It needs more training data and research on complementary-label learning. Moreover, the methods designed for ordinary labels cannot be directly applied to the complementary labels.\n6. Answer: Thank you very much for your comments. We have updated the paper carefully in a new version.\n\u2022\t\"supper\"--> super\n\u2022\t\"A complementary-label is only specific that the pattern\" --> A complementary Label is only indicating that the class label of a sample is incorrect.\n\u2022\t\"in some questions refer to private.\" -->\" For example, on some privacy issues, it is much easier to collect complementary labels than ordinary labels.\"\n\u2022\t\"..can be summarized as..\"\n \n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Paper153/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "mrd7htXUnmA", "original": null, "number": 4, "cdate": 1605511530887, "ddate": null, "tcdate": 1605511530887, "tmdate": 1605511530887, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "4fW94VG_Hqa", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "comment": "1. Answer: Thank you for your question. There may be something we do not express very clear. In ML, a loss function is called noise-tolerant if the minimizer of risk (under that loss function) with noisy labels would be the same as that with noise-free labels [A]. In our definition, when the complementary labels are viewed as the noise labels, there is no difference between our definition and the ML robust loss function. \n[A]. Ghosh, A., Kumar, H., & Sastry, P. S. (2017). Robust loss functions under label noise for deep neural networks. \n2. Answer: Thank you for your comment.  As shown in Table.1, in the first two datasets (MNIST, FASHION), we use a complex model than the MLP model, and our method achieved higher performance than others in the prior work. Surely, the improved performance is unclear. But in CIFAR10, we use a simpler model than ResNet-34 they used and achieved higher performance. In this scenario, it is no doubt that the improved performance is due to our method.\n3. Answer: Thank you very much for your comments. We have updated the paper carefully in a new version.\nPage 1: \"However, labeling such a large-scale dataset is time-consuming...\"\nPage 1: \"In the view of label noise, complementary labels can also be viewed as...\"\nPage 3: \"...only labels indicating that the class label of a sample is incorrect. \"\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "2GIE2YLKvEd", "original": null, "number": 3, "cdate": 1605511259596, "ddate": null, "tcdate": 1605511259596, "tmdate": 1605511338924, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "V_OExJ8mev5", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment", "content": {"title": "Robust Loss Functions for Complementary Labels Learning", "comment": "1. Answer: Thank you for your comment.\n2. Answer: Thank you for your comment.\n3. Answer: Thank you very much for your helpful comments. We added more explanation in Eq.(16) to help to understand in a new version: \u201cAccording to Eq.~(\\ref{sufficient_condition}), $\\bar{\\ell}$ is a monotone increase loss function only on $\\mathbf{u}^{(\\bar{y})}$, then we have \u2026\u201d\n4. Answer: Thank you very much for your helpful comments.  In Eq.(10), it means that $\\bar{\\ell}$ is a monotone increasing loss function \\textbf{only on} $\\mathbf{u}^{(\\bar{y})}$. In Eq.(20), it means that $\\bar{\\ell}$ is a symmetric loss ($\\sum \\ell(\\mathbf{u}, i)=C$), and $\\bar{\\ell}$ is a monotone increasing loss function on any $\\bar{y}$.\nWe have updated in a new version.\n5. Answer: In complementary-label learning, there are no ordinary labels in the validation set and the training set. Our goal is to train a classifier to predict the ground-true label for any sample drawn from the same distribution as the training set. So, the training accuracy decrease with epochs."}, "signatures": ["ICLR.cc/2021/Conference/Paper153/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Paper153/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper153/Authors|ICLR.cc/2021/Conference/Paper153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874062, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Comment"}}}, {"id": "1z6RJuECzY-", "original": null, "number": 1, "cdate": 1605042881845, "ddate": null, "tcdate": 1605042881845, "tmdate": 1605042881845, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Public_Comment", "content": {"title": "Please consider referencing/comparing to these more recent works", "comment": "I would like to point out that our work (Amid et al. 2019a) extends the Generalized CE loss (Zhang and Sabuncu 2018) by introducing two temperatures t1 and t2 which recovers GCE when t1 = q and t2 = 1. Our more recent work, called the bi-tempered loss (Amid et al. 2019b) extends these methods by introducing a proper (unbiased) generalization of the CE loss and is shown to be extremely effective in reducing the effect of noisy examples. Please consider referencing/comparing to these papers.\n\n(Amid et al. 2019a) Amid et al. \"Two-temperature logistic regression based on the Tsallis divergence.\" In The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. \n\n(Amid et al. 2019b) Amid et al. \"Robust bi-tempered logistic loss based on Bregman divergences.\" In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n"}, "signatures": ["~Ehsan_Amid1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ehsan_Amid1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LhAqAxwH5cn", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/Authors", "ICLR.cc/2021/Conference/Paper153/Reviewers", "ICLR.cc/2021/Conference/Paper153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024985157, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper153/-/Public_Comment"}}}, {"id": "IdSf90F0X_D", "original": null, "number": 1, "cdate": 1603834600639, "ddate": null, "tcdate": 1603834600639, "tmdate": 1605024752879, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Review", "content": {"title": "In need of some clarification, as well as a positioning with respect to learning with partial labels", "review": "This paper concerns the problem of learning from single-label supervision, when this label is known not to be the truth. This is called complementary label learning. Some loss functions are proposed that are claimed to have the same theoretical minimizer as the one for standard labelling. \n\nThe research agenda of the paper looks reasonable, even if it can be seen as a very specific instance of partial label learning (where one just considers the complement of the complementary label and tries to learn from it). A positioning with this latter approach therefore seems necessary. Also, after reading the paper, there are some unclarities left about the authors claim. Below are some more specific comments about that:\n\n* Introduction: it is claimed that getting complementary label is easier than getting true labels, however complementary labels have to be certainly false, and while there are indeed theoretically more wrong labels than right ones, it is not entirely clear whether getting certainly false labels is easier than getting true ones in practice. Are there applications or empirical studies demonstrating that? Most mentioned papers do not appear to have actually applied the setting. \n\n* Connection to partial label learning: the current framework can be seen as a peculiar case of partial label learning, as if I take a complementary label $\\overline{y}$, then its complement $\\mathcal{Y}\\setminus\\overline{y}$ is a partial label certainly containing the truth. It would then be necessary to connect the current work to this trend, for instance to Cour et al. \"Learning from partial labels\" (JMLR 2011) or the more recent works of, e.g., X Wu, ML Zhang \"Towards Enabling Binary Decomposition for Partial Label Learning.\". \n\n* Definition 2: I do not really follow definition 2. First, are \\theta^* and \\theta arbitrary parameters values? Why call it \\theta^* (suggesting some kind of optimality)? If   \\theta^* is a minimizer of one of the two losses, then either the premise or the conclusion is a tautology (making the definition kind of meaningless). \n\n* Proof of Theorem 1: I have some trouble with this definition. First, Equation (13) seems trivial if \\theta^* is the finite sample optimal model (also, why not identifying the search space with the space of parameters?). I also do not really follow the next line, as it is unclear how realistic it is to modify the parameters for just one instance? It is also unclear what is to be proven here, as \\inf \\sum \\geq \\sum \\inf, thus allowing for instance-specific parameters would always give something better than a global minimizer. In summary, I am not really convinced by this proof. \n\n* In the experiment, I wold expect a comparison with other approaches (complementary but also partial label learning), but more importantly with the optimal models obtained on learning from the initial true labels, if only to demonstrate that the proposed theorems are valid. The asymptotic accuracies displayed are also very far from state-of-art standards (less than half of it) for CIFAR 10, which seems to contradict the fact that complementary labels have the same minimizer (hence comparable performances) as the one obtained with true labels? \n\n* Finally, the paper contains an important numbers of typos or questionable grammatical structures. For instance in the first two pages only:\n- \"supper\" --> super\n- \"A complementary-label is only specific that the pattern\"\n- \"in some questions refer to private.\"\n- \"the best hyper-parameter by empirical risk since\" (by empirical risk minimisation)\n- \"can be summary as\"", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149284, "tmdate": 1606915801050, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Review"}}}, {"id": "V_OExJ8mev5", "original": null, "number": 3, "cdate": 1603899676998, "ddate": null, "tcdate": 1603899676998, "tmdate": 1605024752750, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Review", "content": {"title": "Simple yet insightful conditions on loss and simple training algorithm for successful complementary label learning", "review": "The paper presents (two) simple yet insightful sufficient conditions for a usual loss to work well as a complementary label loss. Based on this, a simple training procedure, which minimally differs from usual training, is presented. Empirically it is shown that the proposal outperforms state-of-the-art.\n\nComments:\n1. I like the simple idea of CL-loss (8). Also the analysis 4.2, though simple, is elegant and insightful. For example, it helps to identify that MSE is not an ideal CL-loss.\n2. The improvement in performance over GA/PC/Fwd is impressive in table1.\n3. Overall, the write-up is well-organised; however at places I felt the presentation can be simplified a lot. sometimes it is because of grammar, sometime because of confusing notation, and sometimes because of too much reading between lines. For e.g., since (16) is an important step, more explanation seems necessary (notation further complicates the ease), e.g., in (26),(27) one-step reasons for verifying validness/non-validness as cl-loss may be helpful.\n4. Though conditions (10)/(20) are insightful for determining robustness, for a reader who is encountering them for the first time, it may help to intuitively explain the conditions.\n5. In fig1 why does training accuracy decrease with epochs?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149284, "tmdate": 1606915801050, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Review"}}}, {"id": "Oe0b3Sobol", "original": null, "number": 4, "cdate": 1603959759476, "ddate": null, "tcdate": 1603959759476, "tmdate": 1605024752680, "tddate": null, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "invitation": "ICLR.cc/2021/Conference/Paper153/-/Official_Review", "content": {"title": "An interesting work for complementary label learning.", "review": "This paper studied a new problem, that is, learning from complementary labels.  The goal is to predict a correct label for a given sample when only given complementary labels. On the basis of the ordinary-label learning, the authors defined \"robust loss functions\" for complementary-label learning:  a a loss function is called robust  loss function if minimizer of risk with complementary labels would be the same as with ordinary  labels. Then, they provided  two sufficient conditions for the robust loss function and a exclusion algorithm is provided for prediction. Experimental results show that the proposed method outperforms other methods in several datasets.\n\nOverall, the problem is interesting and important, and the proposed algorithm seems reasonable and effective. However, I think the paper could be improved from the following two aspects.\n\n1.I suggest to add ablation test to give a deep analysis about the effectiveness of the proposed algorithm.\n\n2.In Section 5.1, it is better to give more explanations about why the lower the validation accuracy, the better the classifier learns from the training set.\n\n3. What is the exact form of complementary label? Suppose there are k classes, or k labels. If we know that a sample does not belong to given k-1 classes, then we can deduce that it belongs to the rest one class.   So, it is important to specify the exact form of complementary label to show the necessity of complementary-label learning.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper153/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper153/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Loss Functions for Complementary Labels Learning", "authorids": ["~Defu_Liu1", "guowu@uestc.edu.cn"], "authors": ["Defu Liu", "Guowu Yang"], "keywords": ["Complementary Labels", "Robustness", "Machine Learning"], "abstract": "In ordinary-label learning, the correct label is given to each training sample. Similarly, a complementary label is also provided for each training sample in complementary-label learning. A complementary label indicates a class that the example does not belong to. Robust learning of classifiers has been investigated from many viewpoints under label noise, but little attention has been paid to complementary-label learning. In this paper, we present a new algorithm of complementary-label learning with the robustness of loss function. We also provide two sufficient conditions on a loss function so that the minimizer of the risk for complementary labels is theoretically guaranteed to be consistent with the minimizer of the risk for ordinary labels. Finally, the empirical results validate our method\u2019s superiority to current state-of-the-art techniques. Especially in cifar10, our algorithm achieves a much higher test accuracy than the gradient ascent algorithm, and the parameters of our model are less than half of the ResNet-34 they used.", "one-sentence_summary": "We introduce a novel algorithm of complementary-label learning with the robustness of loss function and the empirical results validate the superiority of our method to current state-of-the-art methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|robust_loss_functions_for_complementary_labels_learning", "pdf": "/pdf/d667e6bd9c12fb937174e785f642b38e94180781.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZwyNTsxkS", "_bibtex": "@misc{\nliu2021robust,\ntitle={Robust Loss Functions for Complementary Labels Learning},\nauthor={Defu Liu and Guowu Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=LhAqAxwH5cn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LhAqAxwH5cn", "replyto": "LhAqAxwH5cn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149284, "tmdate": 1606915801050, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper153/-/Official_Review"}}}], "count": 13}