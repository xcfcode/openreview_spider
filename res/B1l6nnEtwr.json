{"notes": [{"id": "B1l6nnEtwr", "original": "H1g9icsfPH", "number": 206, "cdate": 1569438900675, "ddate": null, "tcdate": 1569438900675, "tmdate": 1577168286519, "tddate": null, "forum": "B1l6nnEtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS", "code": "https://github.com/Bill-research/homotopy", "keywords": ["Homotopy training algorithm", "Convergence analysis", "Neural networks"], "authors": ["Qipin Chen", "Wenrui Hao"], "authorids": ["qzc18@psu.edu", "wxh64@psu.edu"], "pdf": "/pdf/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "paperhash": "chen|an_efficient_homotopy_training_algorithm_for_neural_networks", "original_pdf": "/attachment/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={{\\{}AN{\\}} {\\{}EFFICIENT{\\}} {\\{}HOMOTOPY{\\}} {\\{}TRAINING{\\}} {\\{}ALGORITHM{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Qipin Chen and Wenrui Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l6nnEtwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "N49tMPeeBF", "original": null, "number": 1, "cdate": 1576798690278, "ddate": null, "tcdate": 1576798690278, "tmdate": 1576800944910, "tddate": null, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "invitation": "ICLR.cc/2020/Conference/Paper206/-/Decision", "content": {"decision": "Reject", "comment": "The work proposes to learn neural networks using a homotopy-based continuation method. Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results. With no response from the authors, I recommend rejecting the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS", "code": "https://github.com/Bill-research/homotopy", "keywords": ["Homotopy training algorithm", "Convergence analysis", "Neural networks"], "authors": ["Qipin Chen", "Wenrui Hao"], "authorids": ["qzc18@psu.edu", "wxh64@psu.edu"], "pdf": "/pdf/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "paperhash": "chen|an_efficient_homotopy_training_algorithm_for_neural_networks", "original_pdf": "/attachment/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={{\\{}AN{\\}} {\\{}EFFICIENT{\\}} {\\{}HOMOTOPY{\\}} {\\{}TRAINING{\\}} {\\{}ALGORITHM{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Qipin Chen and Wenrui Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l6nnEtwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730136, "tmdate": 1576800282871, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper206/-/Decision"}}}, {"id": "H1xf8Ek2FB", "original": null, "number": 1, "cdate": 1571710026500, "ddate": null, "tcdate": 1571710026500, "tmdate": 1572972625383, "tddate": null, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "invitation": "ICLR.cc/2020/Conference/Paper206/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose the Homotopy Training Algorithm (HTA) for neural network optimization problems. They claim that HTA starts with several simplified problems and tracks the solution to the original problem via a continuous homotopy path. They give the theoretical analysis and conduct experiments on the synthetic data and the CIFAR-10 dataset. \nMy major concerns are as follows.\n1. The authors may want to give more detailed explanations of HTA. For example, they may want to give the pseudocode for HTA and explain its advantages compared to other optimization methods.\n2. The theoretical analysis is trivial. The proof of Theorem 3.1 is to verify Assumptions 4.1 and 4.3 in [1]. Moreover, the proof of Theorem 3.2 is similar to the analysis for the convergence of SGD for convex problems in [2].\n3. The experiments do not show the efficiency of HTA, as the original quasi-newton method is faster than the quasi-newton method with the homotopy setup.\n4. The authors make a mistake in the proof of Theorem 3.1. The claim that \u201c{\\theta_k} is contained in an open set which is bounded. Since that g is continuous, g is bounded.\u201d is incorrect. We can find a counterexample g(x) = \\frac{1}{x}, x\\in (0,1).\n\n[1] L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.\n[2] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009."}, "signatures": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS", "code": "https://github.com/Bill-research/homotopy", "keywords": ["Homotopy training algorithm", "Convergence analysis", "Neural networks"], "authors": ["Qipin Chen", "Wenrui Hao"], "authorids": ["qzc18@psu.edu", "wxh64@psu.edu"], "pdf": "/pdf/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "paperhash": "chen|an_efficient_homotopy_training_algorithm_for_neural_networks", "original_pdf": "/attachment/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={{\\{}AN{\\}} {\\{}EFFICIENT{\\}} {\\{}HOMOTOPY{\\}} {\\{}TRAINING{\\}} {\\{}ALGORITHM{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Qipin Chen and Wenrui Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l6nnEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575248774494, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper206/Reviewers"], "noninvitees": [], "tcdate": 1570237755496, "tmdate": 1575248774509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper206/-/Official_Review"}}}, {"id": "SygsJ2af9H", "original": null, "number": 2, "cdate": 1572162530565, "ddate": null, "tcdate": 1572162530565, "tmdate": 1572972625291, "tddate": null, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "invitation": "ICLR.cc/2020/Conference/Paper206/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The work proposes to learn neural networks using homotopy-based continuation method. The method divides the parameter space into two groups (extendable to multiple groups) and introduces a homotopy function which includes the original optimization problem as an extreme case.  By varying the homotopy parameter, one can construct a continuous path from a supposedly easier to solve optimization problem to the problem of interest. The authors prove convergence in the non-convex case, the existence of solution path in the convex case and demonstrate the effectiveness of the proposed method on synthetic and real datasets.\n\nWhile the idea itself is rather intriguing and seems promising, the current presentation and experimentation does not meet the acceptance threshold.  The writing of the draft needs a lot of improvement, in particular the notations the authors used are not consistent throughout the paper, which is very confusing. \n\nThe synthetic example the authors used in section 4.1 are naturally decoupled among the different dimensions of the parameters, which is no surprise the proposed method would achieve 100% convergence as shown in table 1. \n\nIt seems the division of the parameter space would matter. One would imagine there exists certain division leading to much easier to solve subproblems. Do the authors have any insight or experiments comparing different division strategies?\n\nHere's a very closely-related work that should be cited and discussed:\nWang, Xin. \"An efficient training algorithm for multilayer neural networks by homotopy continuation method.\" Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94). Vol. 1. IEEE, 1994.\n\n\nTypos:\n1) Equation following remark in page 2, should H() be replaced by G() or \\nabla H()?\n2) After equation (7), should G():= \\nabla H instead of F?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS", "code": "https://github.com/Bill-research/homotopy", "keywords": ["Homotopy training algorithm", "Convergence analysis", "Neural networks"], "authors": ["Qipin Chen", "Wenrui Hao"], "authorids": ["qzc18@psu.edu", "wxh64@psu.edu"], "pdf": "/pdf/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "paperhash": "chen|an_efficient_homotopy_training_algorithm_for_neural_networks", "original_pdf": "/attachment/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={{\\{}AN{\\}} {\\{}EFFICIENT{\\}} {\\{}HOMOTOPY{\\}} {\\{}TRAINING{\\}} {\\{}ALGORITHM{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Qipin Chen and Wenrui Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l6nnEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575248774494, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper206/Reviewers"], "noninvitees": [], "tcdate": 1570237755496, "tmdate": 1575248774509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper206/-/Official_Review"}}}, {"id": "Skla9ft35S", "original": null, "number": 3, "cdate": 1572799125105, "ddate": null, "tcdate": 1572799125105, "tmdate": 1572972625246, "tddate": null, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "invitation": "ICLR.cc/2020/Conference/Paper206/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\n\nThis paper proposes an algorithm to address the issue of nonlinear optimization\nin high dimensions and applies it to convolutional neural networks (VGG models) on CIFAR 10. \nThey show 11% relative reduction in error for this particular task with this\nparticular network. In addition, they prove additional theoretical results on\nthe convergence of SGD using their method in the convex case as well as\nconvergence of SGD to a stationary point in the nonconvex case when the homotopy\nparameter is fixed which is not done in practice.\n\nGiven an optimization problem, their method first solves multiple independent\nlower-dimensional optimization problems each with a subset of the parameters and\nthen optimizes a new objective function controlled by a monotonically decreasing\nparameter L that interpolates the original objective function and the\npreviously-solved lower dimensional problems. L can be seen as a regularization\nparameter that is gradually decreased as we optimize the new optimization\nfunction. When L = 0, we recover the original optimization problem.\nThe authors prove that (1) SGD with their procedure will find a stationary point\nunder the Robbins-Monro conditions for a fixed L and (2) SGD with their\nprocedure will converge for convex problems as L is decreased to 0.\n\nDecision and reasoning\n\nThis paper should be rejected because (1) the proposed algorithm attempts to address \nthe original issue of high dimensional nonlinear optimization of neural networks but\nviolates the algorithm's assumption in practice, (2) the\nempirical evaluations are lacking - having only evaluated their method on a toy\nproblem with up to only 6 dimensions and a relatively simple image classification task,\nand (3) the assumption of fixing the homotopy parameter in the theorem on the\nnon-convex case directly violates the intention of the algorithm. \n\nRegarding (1): The proposed procedure requires initializing L at a large value\nand reducing L towards 0 in order to recover the original optimization problem.\nHowever, in practice for CIFAR 10, the authors initialize L to be 0.01 and\ngradually reduces it to 0.005 which is hardly the original intent of the\nalgorithm. There is also no demonstration whether or not this gradual reduction in\nL actually has an effect on the optimization of the new objective function. For\nexample, since the start and end values of L are similar, will we get similar\nresults if we simply fix L to be 0.005 or 0.01? The authors also show that their\nmethod outperforms a quasi-newton method by combining the optimization with\ntheir procedure on a non-convex example by Chow et al. 2013. However, this example\nonly goes up to n=6 dimensions, which is hardly comparable to the original\nproblem of high dimensional non-convex optimization that this paper sought to\naddress.\n\nRegarding (2): The authors evaluated their procedure on CIFAR10, a relatively\nsimple image classification task that modern neural networks can solve easily\nand is not representative of the types of nonlinear optimization problems\nprevalent in deep learning. There's also an issue of using only VGG networks for\ntheir evaluations while VGGs are typically eschewed in favor of ResNets today.\nGiven that the optimization is easier with residual connections, it may be the case that\ntheir procedure does not significantly improve the accuracy of ResNets.\n\nRegarding (3): By fixing L in Theorem 3.1, the authors essentially show that SGD\nconverges to a stationary point for their new objective function which can be\nseen as a regularized version of the original objective function, which is not a\nstrong result. Furthermore, fixing L goes against the original procedure's\nmotivation of recovering the original optimization function as L decreases to 0.\n\nAdditional comments and questions\n\nThere are passages that are difficult to understand because not enough context\nis given. For example in the \"remark\" passage, it is not clear where the\n\"necessary condition\" comes from. In addition it seems like it doesn't even\ntype-check since the first term is 2n dimensional while the second term is 4n\ndimensional. \n\nThere are also many errors in the writing that hinder the presentation. A subset of\nthem includes:\n- \"nerual netowrks on roboticsKonda et al.\" -> \"neural networks on robotics Konda et al.\"\n- \"based on homotopy continuation method\" -> \"based on the homotopy continuation methods\"\n- \"random chosen point\" -> \"randomly chosen point\"\n- \"we choose \\tilde{\\theta} = 0 in the dropout\" -> reword\n- Fourth term in Equation 3 should be \\theta_2 - \\tilde{\\theta_2}\n- \"By gradually increasing parameter L\" -> \"By gradually decreasing parameter L\"\n- \"where \\xi is a random variable due to random algorithms\" -> reword and possibly say the randomness is from SGD\n- After equation 6, should have b_i instead of \\beta_i\n- In equation 20, should be g(\\theta_*^0) instead of g(\\theta_*^1)\n- In theorem 3.2 you never explained what \\theta_*^{L_k} is\n- \"We compared the traditional optimization method (the quasi-Newton method)\" -> which quasi-Newton method?\n- Figures 2 and 3 label the x-axis with \"epochs\". However only 4 epochs were run, so I believe the x-axis should be \"iterations\"\"\n\nBesides improving the quality of writing in the paper, I would strongly suggest that the\nauthors improve their empirical evaluation.  Possibilities include evaluating on\nCIFAR 100 or ImageNet, using a wider variety of networks including ResNets,\nevaluating on tasks other than image classification."}, "signatures": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper206/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS", "code": "https://github.com/Bill-research/homotopy", "keywords": ["Homotopy training algorithm", "Convergence analysis", "Neural networks"], "authors": ["Qipin Chen", "Wenrui Hao"], "authorids": ["qzc18@psu.edu", "wxh64@psu.edu"], "pdf": "/pdf/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "paperhash": "chen|an_efficient_homotopy_training_algorithm_for_neural_networks", "original_pdf": "/attachment/06dc98e361055691dc6e881cf96957cd5ecb8913.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={{\\{}AN{\\}} {\\{}EFFICIENT{\\}} {\\{}HOMOTOPY{\\}} {\\{}TRAINING{\\}} {\\{}ALGORITHM{\\}} {\\{}FOR{\\}} {\\{}NEURAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Qipin Chen and Wenrui Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l6nnEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l6nnEtwr", "replyto": "B1l6nnEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper206/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575248774494, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper206/Reviewers"], "noninvitees": [], "tcdate": 1570237755496, "tmdate": 1575248774509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper206/-/Official_Review"}}}], "count": 5}