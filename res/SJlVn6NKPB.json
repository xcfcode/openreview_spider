{"notes": [{"id": "SJlVn6NKPB", "original": "HJx5PJxuwB", "number": 778, "cdate": 1569439148136, "ddate": null, "tcdate": 1569439148136, "tmdate": 1577168221219, "tddate": null, "forum": "SJlVn6NKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EI5nwSy7W8", "original": null, "number": 1, "cdate": 1576798705790, "ddate": null, "tcdate": 1576798705790, "tmdate": 1576800930349, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Decision", "content": {"decision": "Reject", "comment": "The authors present a method for learning representations of remote sensing images from multiple views. The main ideas is to use the InfoNCE loss to learn from multiple views of the data. \n\nThe reviewers had a few concerns about this work which were not adequately addressed by the authors. I have summarised these below and would strongly recommend that the authors address these in subsequent submissions:\n\n1) Experiments on a single dataset and a very specific task: Authors should present a more convincing argument about why the chosen dataset and task are challenging and important to demonstrate the main ideas presented in their work. Further, they should also report results on additional datasets suggested by the reviewers. \n2) Comparisons with existing works: The reviewers suggested several existing works for comparison. The authors agreed that these were relevant and important but haven't done this comparison yet. Without such a comparison it is hard to evaluate the main contributions of this work. \n\nBased on the above objections raised by the reviewers, I recommend that the paper should not be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710546, "tmdate": 1576800259579, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper778/-/Decision"}}}, {"id": "SylM88mHoB", "original": null, "number": 3, "cdate": 1573365322297, "ddate": null, "tcdate": 1573365322297, "tmdate": 1574479185679, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper presents an approach to create unsupervised representations of remote sensing images. The essential idea is to enforce similarity between representations of multiple views obtained by subsetting channels from multiple co-terminus sensor outputs.  This is implemented by training with the InfoNCE loss on high-level features (last two layers of a ResNet 50) of two views of the same image and multiple view of other images which are passed through the same weight-shared network. The evaluation is based on a custom task of classifying OSM labels. The results show that (a) the learned representation (up to 3 chosen channels) outperform a pre-trained ImageNet fine-tuned for the classification task, (b) multiple low resolution sensors can outperform a higher resolution sensor, and (c) visualisation of images indicates that stable high-level representations across sensors. \n\nThe paper makes a worthwhile contribution in introducing contrastive methods to satellite imagery - a domain well suited for analysing how contrastive methods work and also rich in applications of sensor fusion/augmentation for remote sensing applications. The experiments indicate that this is a promising direction, and the authors have helpfully open-sourced the dataset they used. The paper, however, is short on several accounts. Primarily, the experimental evaluation is sparse in detail and rigour. In addition, the paper makes unsubstantiated claims to \u201cargue out\u201d certain baselines from being compared. Finally, the results indicate modest improvements on a custom task, and thus remain inconclusive. \n\nThe paper bases all experimental evaluation and conclusions thereof on the custom task of classifying 12 classes of distinctive objects on 8400 images obtained from OpenStreetMap (OSM). These include diverse categories spanning generic bodies (water, forest), specific structures (substation, bridge), similar items (farm, farmland). The paper does not discuss why this is a good task (is it challenging, is it representative of remote sensing applications, are these most frequent OSM classes). Indeed, the experimental results provide no intuition on the classification task (eg. confusion matrix is essential especially given the middling accuracy of about 0.5). \n\nThe paper makes a strong claim that the \u201cdistributional hypothesis\u201d (of expecting that spatial or temporal similarity reflect in representation similarity) does not apply to satellite imagery because \u201cremote sensing imagery \u2026 can change abruptly between adjacent patches\u201d. No evidence is provided of what I believe is an unintuitive claim. Indeed, existing work [1] (as cited in the paper) uses this hypothesis to create representations for satellite images. Glaringly, having made the claim, the paper does not compare proposed approach with the representations computed in [1].  The paper also makes the claim that computing representation loss should be done with high-level features rather than individual pixels. Again, this is not substantiated as the paper does not compare the proposed methods with simple auto-encoder baselines. In the absence of these baselines, the results of this paper on a custom data-set remain inconclusive. \n\nThe evaluation leaves out several other expected experiments. A few suggested conditions for ablation tests are listed:\n(a) Different augmentation across channels\n(b) Different values of \\lambda_L (only an extreme case of last two layers has been presented)\n(c) At least one other CNN-backbone, perhaps a deeper ResNet\n(d) Different orderings of introducing the channels (not only by low or high resolution ordering)\n(e) Different sizes of the tiles for learning representations (curiously the paper does not mention the size of the tiles as used currently)\n\nFinally, and more broadly, the proposed approach aims to compensate the lack of supervised labels by exploiting redundancy across multiple sensors/channels. In the chosen setup, this redundancy is very high as 4 different RGBI co-terminus sensors are chosen, thereby requiring augmenting and drop-out to simulate some variation. A more realistic or challenging setup would be required to evaluate the underlying ideas.\n\n[1] Jean, Neal, et al. \"Tile2Vec: Unsupervised representation learning for spatially distributed data.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n\n--\n\nUpdate in response to rebuttal: \nThe authors agreed to most of the points raised, but provided no clear suggestions in addressing them. I re-emphasise that the empirical results on a single custom dataset based on OSM is limited. The author response to this remains vague. Further none of the reasonable baselines have been compared against, a point which the authors ignored in the rebuttal. In light of this, rating remains the same. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714679516, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper778/Reviewers"], "noninvitees": [], "tcdate": 1570237747221, "tmdate": 1575714679534, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Review"}}}, {"id": "B1xNtSz1qr", "original": null, "number": 2, "cdate": 1571919228282, "ddate": null, "tcdate": 1571919228282, "tmdate": 1574340310423, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes an approach for representation learning on remote sensing data/satellite imagery inspired by recent unsupervised contrastive multiview representation learning methods (CPC, DIM, CMC). The method relies on the InfoNCE objective to contrast two different views of the data obtained by randomly cropping image patches, color jittering, channel dropping etc. The proposed method is compared to an ImageNet pretrained network in a classification task based on OpenStreetMap (OSM) features and show to outperform the ImageNet pretrained classifier. \n\nThe paper is well-written and the method is clearly explained. To my knowledge there is little prior work on unsupervised/self-supervised learning on remote sensing/satellite data and this data suits the contrastive/multiview framework very well. In that regard I appreciate the direction explored by the paper.\n\nHere are some questions and concerns:\n\n- I think the paper is lacking some important details, for example how are the unsupervised and ImageNet-pretrained representations transferred? Fine-tuning, or learning a classifier on top of the frozen representation? In the case of fine-tuning, the ImageNet baseline could be extended to more channels.\n\n- As an additional baseline, how does a network trained from scratch on the available labeled training data perform?\n\n- The authors evaluate on a single data set, that seems to not have been used previously. To make the evaluation more solid it would be good to compare on other data sets, for example on EuroSAT [1], and with other, possibly supervised classification methods, see, e.g., [1, 2].\n\n- Did you do ablations on augmentations used? For example, is zeroing out channels more effective than copying other channels instead?\n\n- Both forward and backward prediction losses are used, but it seems that the loss is symmetric. Does adding the backward prediction loss really help?\n\nOverall, I like the direction the paper is exploring, but I think it would greatly benefit from adding detail on the outlined aspects and extending the evaluation.\n\n\n[1] Helber P, Bischke B, Dengel A, Borth D. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 2019 Jun 14;12(7):2217-26.\n[2] Kussul N, Lavreniuk M, Skakun S, Shelestov A. Deep learning classification of land cover and crop types using remote sensing data. IEEE Geoscience and Remote Sensing Letters. 2017 Mar 31;14(5):778-82.\n\n\n\n---\nUpdate after the rebuttal:\nThanks to the authors for their detailed response. While the authors agree that some points should be improved, no attempts were made to actually strengthen the method in a revision. In particular, I do think evaluating the proposed method on prior benchmarks that were mostly used in the context of supervised methods (such as EuroSAT) is very important. Also, I still think comparison to from scratch training is crucial. Both comparisons are common practice in prior work such as CPC, DIM, and CMC (specifically, ImageNet). I therefore do not increase my rating.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714679516, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper778/Reviewers"], "noninvitees": [], "tcdate": 1570237747221, "tmdate": 1575714679534, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Review"}}}, {"id": "H1xLeX13iB", "original": null, "number": 3, "cdate": 1573806830491, "ddate": null, "tcdate": 1573806830491, "tmdate": 1573806830491, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "H1e3XLhVtr", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Anonymous reviewer #2,\n\nThank you for writing an extensive and detailed review.\n\n> \u201cThe high-level goal, to train an encoder that always gives a similar encoding [...] independent of the sensor channels used, is certainly appealing\u201d\n\nWe are glad that you see the appeal, but disagree that we do not make progress towards the goal. Fig 5 demonstrates that directions in representation space correspond to the same concepts, regardless of input sensor. Even if \u201cthe learned similarities are not \u2018high level\u2019,\u201d the property of sensor choice invariance is both useful and, to our knowledge, unique to our work.\n\n> \u201cNovelty is limited, the work is a variation of the CMC method\u201d\n\nOur main result is training a network to fuse any subset of its inputs without additional training. We do not claim dramatically improved representation quality over other contrastive unsupervised learning methods; indeed our approach is philosophically similar to CMC. Sensor choice invariance is the useful new contribution.\n\n> \u201cit does in my view not make sense to use pre-trained ImageNet features as the only baseline. [...] A more natural baseline would be to pre-train on the same type of imagery, with a proxy task\u201d\n\nPrior work has found contrastive multiview learning to outperform other self-supervised approaches relying on proxies (e.g. colorization).\n\n> \u201cit feels like making the task harder if one demands not only that a patch of water can be decoded from the embedding, but that it should be distinguishable from any other patch of water that resembles it even in raw pixel space.\u201d\n\nWe\u2019d expect that pixel-space loss would work worse than representation-space loss, as prior work (CPC) suggests. Sometimes we do penalize the model for representing patches of one class the same way, but in practice the majority of batch examples correspond to distinct classes. A main challenge of unsupervised learning is that distinct identities aren\u2019t clearly indicated in the data.\n\n> \u201cThe data used in the experiments is [...] the opposite of diverse, multi-modal remote sensing channels.\u201d\n\nSince our approach maximizes the mutual information across views, our initial experiments use data with high mutual information. We expect that fusing more diverse data sources will improve representation quality, since it makes the contrastive task more difficult; this is a great direction for future work.\n\n> \u201cthe gains are disappointingly small\u201d\n\nWe emphasize that our approach is trained purely unsupervised, whereas the ImageNet-trained network is trained with many labels. This experiment is a demonstration of good quality representations even when input channels are missing, not of significant improvements over supervised pre-training.\n\n> \u201cThe classification accuracy of kNN does improve with the proposed embedding [...] but that experiment is unnatural. If one has the labels to do kNN, [...] train a proper classifier\u201d\n\nWe agree that fine-tuning would be a valuable experiment, though we believe the $k$ nearest neighbors fraction does measure representation quality.\n\n> \u201cthe channels in Fig. 4 were added in a fixed order - why?\u201d\n\nWe did try other combinations of channels and found that they performed comparably. We found it easiest to visualize just a few of the possible combinations of channels. We\u2019ll mention the other combinations we\u2019ve tried in the paper.\n\n> \u201cFurthermore, Fig. 5 suggests to me that the learned similarities are in fact not \u2018high-level\u2019 [...]. I am not sure what suggests to the authors that there is much sensor invariance.\u201d\n\nOur approach is trained unsupervised, so we\u2019d be surprised to find directions in output space corresponding to complicated, multi-part objects. Rather, we\u2019ve learned a distributed representation which groups scenes into roughly the same categories humans do. Whether the representations are high-level is distinct from sensor-invariance. For each sensor, the same direction in output space corresponds to the same concept.\n\n> \u201cAnd the \u2018distributional hypothesis\u2019 certainly also holds for remote sensing images, just at different scale\u201d\n\nWhile the distributional hypothesis does hold for remote sensing images at a small scale, to learn representations of large objects using \u201cadjacent-patch\u201d contrastive predictive coding we\u2019d need to use image patches of many pixels, at which point the hypothesis breaks down. We miscommunicated in saying the distributional hypothesis does not hold; we\u2019ll fix that.\n\n> \u201cOverall, while the paper brings up a valid question, it does not give a convincing answer.\u201d\n\nWe\u2019d like to reiterate that our main result is learning a network that can represent arbitrary subsets of its input channels, allowing a practitioner to fuse arbitrary sensors at inference-time. We know this to be useful in our field, and our approach is a step towards learning high-quality sensor-invariant features."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlVn6NKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper778/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper778/Authors|ICLR.cc/2020/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166362, "tmdate": 1576860558672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment"}}}, {"id": "HJxXkzajir", "original": null, "number": 2, "cdate": 1573798363467, "ddate": null, "tcdate": 1573798363467, "tmdate": 1573798363467, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "SylM88mHoB", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment", "content": {"title": "Response to review #4", "comment": "Anonymous reviewer #4,\n\nThanks for taking the time to write a thorough review. We\u2019d like to respond to some parts of your review below.\n\n> \u201cThe essential idea is to enforce similarity between representations of multiple views obtained by subsetting channels from multiple co-terminus sensor outputs.  This is implemented by training with the InfoNCE loss on high-level features (last two layers of a ResNet 50) of two views of the same image and multiple view of other images which are passed through the same weight-shared network.\u201d\n\nThis is a good summary of our training procedure, but we\u2019d like to emphasize that subsetting channels at training time serves not only to create multiple views for use with InfoNCE loss, but also trains the network to operate with channels missing at inference time. \n\n> \u201cThe results show that [ . . . ] multiple low resolution sensors can outperform a higher resolution sensor\u201d\n\nWe hadn\u2019t emphasized this as a main result of the paper, but it is indeed interesting; we\u2019ll update the paper to reflect this.\n\n> \u201cThe paper does not discuss why this is a good task (is it challenging, is it representative of remote sensing applications, are these most frequent OSM classes).\u201d\n\nThank you for pointing this out. We believe that the OSM classification task demands that a representation understand the kinds of objects and relationships useful for a broad range of remote sensing applications. We\u2019ll add a section to the paper discussing this explicitly. \n\n> \u201cThe paper makes a strong claim that the \u201cdistributional hypothesis\u201d (of expecting that spatial or temporal similarity reflect in representation similarity) does not apply to satellite imagery because \u201cremote sensing imagery \u2026 can change abruptly between adjacent patches\u201d. No evidence is provided of what I believe is an unintuitive claim. Indeed, existing work [1] (as cited in the paper) uses this hypothesis to create representations for satellite images. Glaringly, having made the claim, the paper does not compare proposed approach with the representations computed in [1].\u201d\n\nAs we mention in our response to reviewer #2, we miscommunicated by saying that the distributional hypothesis does not hold at all for this imagery. Rather, it holds at a smaller scale than is useful to learn high-level representations; nearby pixels correspond to the same object on a scale of roughly meters, which means that these correlations occupy only a few pixels for even high-resolution satellite imagery. We\u2019ll update our paper to make this more clear.\n\n> \u201cThe paper also makes the claim that computing representation loss should be done with high-level features rather than individual pixels. Again, this is not substantiated as the paper does not compare the proposed methods with simple auto-encoder baselines.\u201d\n\nPrior work in representation learning (such as CPC, CMC, and AMDIM) has shown that computing loss from high-level features is much more effective than using a pixel-level loss. You\u2019re right that this section of our paper is lacking justification, though; we\u2019ll add these references and discussion to that section. \n\n> \u201cThe evaluation leaves out several other expected experiments. A few suggested conditions for ablation tests are listed:\n(a) Different augmentation across channels\n(b) Different values of \\lambda_L (only an extreme case of last two layers has been presented)\n(c) At least one other CNN-backbone, perhaps a deeper ResNet\n(d) Different orderings of introducing the channels (not only by low or high resolution ordering)\n(e) Different sizes of the tiles for learning representations (curiously the paper does not mention the size of the tiles as used currently)\u201d\n\nWe agree that these ablation tests would be helpful to understand how to get maximally discriminative features. However, as stated above, our main result is learning features which are stable across every subset of input sensors, and our experiments are targeted at demonstrating that result. \n\n> \u201cFinally, and more broadly, the proposed approach aims to compensate the lack of supervised labels by exploiting redundancy across multiple sensors/channels. In the chosen setup, this redundancy is very high as 4 different RGBI co-terminus sensors are chosen, thereby requiring augmenting and drop-out to simulate some variation. A more realistic or challenging setup would be required to evaluate the underlying ideas.\u201d\n\nWe agree that adding more distinct sensors would be beneficial, and believe this represents an exciting direction for future research. In particular, we think that adding sensor diversity should improve representation quality. As mentioned in our response to reviewer #2, we chose data sources with high mutual information for this research because our approach relies on maximizing mutual information across views.\n\n[1] Jean, Neal, et al. \"Tile2Vec: Unsupervised representation learning for spatially distributed data.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlVn6NKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper778/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper778/Authors|ICLR.cc/2020/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166362, "tmdate": 1576860558672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment"}}}, {"id": "H1gDeb6sjH", "original": null, "number": 1, "cdate": 1573798126684, "ddate": null, "tcdate": 1573798126684, "tmdate": 1573798193393, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "B1xNtSz1qr", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment", "content": {"title": "Response to reviewer #1", "comment": "Anonymous reviewer #1,\n\nThank you for your constructive review. We\u2019re glad that you also see the need for approaches to unsupervised learning for remote sensing data, and that contrastive multiview learning is a good fit for this problem. We\u2019d like to respond to some of your points below.\n\n > \u201cThe paper proposes an approach for representation learning on remote sensing data/satellite imagery inspired by recent unsupervised contrastive multiview representation learning methods (CPC, DIM, CMC). The method relies on the InfoNCE objective to contrast two different views of the data obtained by randomly cropping image patches, color jittering, channel dropping etc.\u201d\n\nWe\u2019d like to emphasize that channel dropping is not only a way to strengthen the InfoNCE loss by making the learning task more difficult, but also allows the practitioner to pick the sensors to be fused at inference-time rather than training-time, by producing a network that works with many of its inputs missing. We consider this property, which we believe is unique to our work, to be central to our results.\n\n> \u201cI think the paper is lacking some important details, for example how are the unsupervised and ImageNet-pretrained representations transferred? Fine-tuning, or learning a classifier on top of the frozen representation? In the case of fine-tuning, the ImageNet baseline could be extended to more channels.\u201d\n\nWe think there may be some misunderstanding as to our evaluation section. In our experiments, the representations are transferred as-is, with no fine-tuning or stacking additional models. We will update our paper to clarify this point, and to reflect that experiments with fine-tuning may be valuable further work.\n\n> \u201cAs an additional baseline, how does a network trained from scratch on the available labeled training data perform?\u201d\n\nThe same network trained from scratch on the OpenStreetMap data, with access to labels, would undoubtedly produce much better representations for the task we measure (clustering points by category). However, we\u2019re aiming to demonstrate that CSF learns representations that are useful for many tasks, especially those for which large quantities of data are not available. We\u2019d like to point out that recent work in semi-supervised learning (e.g. [TODO: cite CPC v2 paper]) has achieved results competitive with supervised learning only by dramatically changing the network architectures between the two evaluations (in this case, from AlexNet to a very large ResNet). \n\n> \u201cThe authors evaluate on a single data set, that seems to not have been used previously. To make the evaluation more solid it would be good to compare on other data sets, for example on EuroSAT [1], and with other, possibly supervised classification methods, see, e.g., [1, 2].\u201d\n\nThank you for pointing this out. We agree that evaluation on more standard benchmarks could strengthen our results. However, due to the aforementioned shortage of prior work on unsupervised learning in remote sensing, we did not find a comparable unsupervised benchmark using the data sources we had available \u2014 this is part of why we have open-sourced our data. While there are numerous benchmarks for supervised classification of remote sensing data, as mentioned above we do not think that comparison to a supervised baseline would be particularly helpful.\n\n > \u201cDid you do ablations on augmentations used? For example, is zeroing out channels more effective than copying other channels instead?\u201d\n\nWhile ablation tests on augmentation strategies would be helpful for improving the quantitative performance of our method, as mentioned above we see our main contribution as the ability to train a network to operate with missing inputs. \n\n> \u201cBoth forward and backward prediction losses are used, but it seems that the loss is symmetric. Does adding the backward prediction loss really help?\u201d\n\nBecause the two views are sampled independently, the loss is not actually symmetric. The loss is computed from a similarity matrix with row $i$ corresponding to view 1 of scene $i$ in the batch, and column $j$ corresponding to view 2 of scene $j$ in the batch \u2014 the rows and columns are not interchangeable. Using both the forward and backward loss adds supervisory signal, and it is a minor computational cost relative to that of the entire ResNet encoder.\n\n[1] Helber P, Bischke B, Dengel A, Borth D. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 2019 Jun 14;12(7):2217-26.\n[2] Kussul N, Lavreniuk M, Skakun S, Shelestov A. Deep learning classification of land cover and crop types using remote sensing data. IEEE Geoscience and Remote Sensing Letters. 2017 Mar 31;14(5):778-82."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlVn6NKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper778/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper778/Authors|ICLR.cc/2020/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166362, "tmdate": 1576860558672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper778/Authors", "ICLR.cc/2020/Conference/Paper778/Reviewers", "ICLR.cc/2020/Conference/Paper778/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Comment"}}}, {"id": "H1e3XLhVtr", "original": null, "number": 1, "cdate": 1571239460359, "ddate": null, "tcdate": 1571239460359, "tmdate": 1572972553493, "tddate": null, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "invitation": "ICLR.cc/2020/Conference/Paper778/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an unsupervised method to embed (remotely sensed) image patches such that they are clustered by high-level \"image content\" rather than by low-level pixel statistics. The argument is that in remote sensing one often has access only to a subset of all possible images/channels, so it would be useful to get the same embedding for each subset depicting the same scene. The basic idea is to generate, during training, two versions of the same patch by randomly dropping out channels, and to penalise the difference between the two corresponding embeddings, using the InfoNCE loss. The latter ensures that the patch remains \"recognisable\" among a set of random non-matching patches, so as to avoid learning a trivial embedding that is the same for any input.\n\nThe high-level goal, to train an encoder that always gives a similar encoding (and consequently similar downstream results) for a given patch, independent of the sensor channels used, is certainly appealing. It is also true that one would like to do this with unsupervised learning or transfer learning, since labelled data is scarce in remote sensing. Unfortunately, in my understanding the paper does not make much progress towards that goal.\n\nNovelty is limited, the work is a variation of the CMC method. Yes, it splits channels randomly, not by views; and uses a Siamese architecture, not separate encoders; which it can afford to do since there are no viewpoint differences. And it computes the loss at more than one layer - which is a fairly trivial and popular thing to do. Moreover it is not well justified in this case, since all that matters in a feed-forward architecture is that the final embeddings are similar. Constraining lower layers might empirically help - in particular speed up learning - but conceptually there is no reason to reduce the lower layers' capacity to somehow turn different inputs into the same output. If they are really very different (think RADAR images) this could in principle even hurt. \n\nBoth in the discussion and in the experiments, I miss crucial and obvious baselines. First, it does in my view not make sense to use pre-trained ImageNet features as the only baseline. Yes, they are known to work surprisingly well also for remote sensing data - but, unsurprisingly, not nearly as well as pre-training on similar remote sensing images. A more natural baseline would be to pre-train on the same type of imagery, with a proxy task for which it is easy to get labels automatically  - actually the paper implicitly suggests that labels from OpenStreetMap might be a good candidate. Beyond reducing the domain gap, this also has the advantage that one is not limited to 3 channels, and that one can use an architecture suitable for the task (e.g., standard architecture with a lot of pooling are known to not perform well for semantic segmentation of remote sensing data). Labels for supervised learning might in many remote sensing tasks be scarce, but to get any old labels and pre-train a reasonable proxy task is still easy, as map data is abundant.\n\nPerhaps even more important is another natural baseline. In spirit, the proposed method is closer to a generative embedding, since the supervision signal is not a discriminative task, but rather encourages embeddings to stay unique for each patch, and hence in principle fully decodable. Arguably, the most obvious way to achieve the same thing is a Siamese pair of auto-encoders for the two training patches, with an additional loss on the difference between the latent representations. Without that baseline it is impossible to tell whether the InfoNCE loss does a good job - in some sense it feels like making the task harder if one demands not only that a patch of water can be decoded from the embedding, but that it should be distinguishable from any other patch of water that resembles it even in raw pixel space.\n\nThe data used in the experiments is somewhat unconvincing, as it is really the opposite of diverse, multi-modal remote sensing channels. The same four R-G-B-NIR channels from sensors with not so different resolutions. The claim that the method homes in on high-level similarity and overcomes low-level radiometric differences would be much more credible if one uses short-wave infrared, thermal, perhaps even Radar images. Moreover, the gains are disappointingly small: even in the best case of 12 channels, the improvement over ImageNet pre-training is roughly from 4.5/10 correct neighbours to 5/10 correct neighbours. The even lower gains with fewer channels underline the need to have a pre-trained baseline without a massive domain gap. The classification accuracy of kNN does improve with the proposed embedding (at least for some combinations - for Pleiades RGB it is the same), but that experiment is unnatural. If one has the labels to do kNN, one might as well use them to train a proper classifier, i.e., add a couple of fully connected layers and fine-tune in a supervised fashion.\n\nA number of technical details in the experiments remain unclear. Section 4.2 suggests the channels in Fig. 4 were added in a fixed order - why? There are many potentially viable combinations of, say, 4 channels, not just one. Furthermore, Fig. 5 suggests to me that the learned similarities are in fact not \"high-level\", but (up to rotation invariance) fairly low-level texture properties like \"homogeneous green patch\", \"bright straight line on dark background\", etc. I am not sure what suggests to the authors that there is much sensor invariance. Again, channels with really different wavelength would be more convincing here.\n\nIn terms of presentation, the paper repeatedly makes strong, but unsupported claims about the special properties of remote sensing images.  It makes no sense to state that standard pictures often have \"one subject\" - ImageNet does, but a look at standard datasets like Cityscapes or DepthInTheWild shows that it is not true in general. In the same vein, I don't see why the content changes more unpredictably at the same location across time - arguably the rate of change is much lower, because only large-scale changes impact the image, and because there aren't many dynamic occluders. And the \"distributional hypothesis\" certainly also holds for remote sensing images, just at different scale - otherwise people would not use all sorts of smoothness priors, super-pixel segmentations, etc. when working with them. And I do not understand why the adjacent-patch approach of CPC needs nearby patches to be similar - in my view it only requires that nearby patches are not conditionally independent, so basically any type of patterns.\n\nA small detail: some references, while not wrong, seem rather arbitrary. E.g., (Wu 2018) is a little-known random example. Why not use earlier, more standard references - for semantic segmentation of remote sensing data with deep learning one could for example think of (Maggiori, Audebert, Marmanis, Kampffmeyer, Sherrah, ...).\n\nOverall, while the paper brings up a valid question, it does not give a convincing answer. The justification of the method and its novelty is to some degree contrived, the dataset is ill-suited to really prove the point, and the natural baselines are missing."}, "signatures": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper778/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach", "authors": ["Aidan M. Swope", "Xander H. Rudelis", "Kyle T. Story"], "authorids": ["aidanswope@gmail.com", "xander@descarteslabs.com", "kyle@descarteslabs.com"], "keywords": ["unsupervised learning", "representation learning", "deep learning", "remote sensing", "sensor fusion"], "TL;DR": "Multiple sensor views imply a self-supervised task for learning what things are in aerial imagery without labels", "abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.", "pdf": "/pdf/0f1c988cd122ae0c3c762f0ed8b9bc7e5cb58404.pdf", "code": "https://storage.cloud.google.com/public-published-datasets/csf_code.zip", "paperhash": "swope|representation_learning_for_remote_sensing_an_unsupervised_sensor_fusion_approach", "original_pdf": "/attachment/65ff48adf5958a015554c813672bb6e12cf57ae9.pdf", "_bibtex": "@misc{\nswope2020representation,\ntitle={Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},\nauthor={Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlVn6NKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlVn6NKPB", "replyto": "SJlVn6NKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714679516, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper778/Reviewers"], "noninvitees": [], "tcdate": 1570237747221, "tmdate": 1575714679534, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper778/-/Official_Review"}}}], "count": 8}