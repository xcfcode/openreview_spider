{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519431588326, "tcdate": 1509135337433, "number": 814, "cdate": 1518730164157, "id": "H1WgVz-AZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "H1WgVz-AZ", "original": "Bkgl4f-Ab", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260099191, "tcdate": 1517249289643, "number": 85, "cdate": 1517249289628, "id": "Bkzf71pBG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference.  The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference.\n\nThe submission provides links between two seemingly different frameworks: SPENs and GANs.  By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.  ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642515910, "tcdate": 1511742627981, "number": 1, "cdate": 1511742627981, "id": "H12sn0dlf", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Amortized inference for SPENs", "rating": "7: Good paper, accept", "review": "= Quality = \nOverall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well. \n\n= Clarity = \n\nOverall, the exposition regarding the method is good. I found the setup for the sequence tagging experiments confusing, tough. See more comments below.\n\n= Originality / Significance = \n\nThe paper presents a clever idea that could help make SPENs more practical. The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.\n\n= Major Comment =\n\nI'm concerned by the quality of your results and the overall setup of your experiments. In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper. \n\nMost of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc. The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce. In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives.\n\nIt seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important. The idea of amortizing inference is perhaps more general. My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions.\n\n\n= Minor Comments = \n\n* You should mention 'Energy Based GANs\"\n\n* I don't understand \"This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting.\" Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?\n\n* You spend too much space talking about specific hyperparameter ranges, etc. This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body.\n\n* Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out \"Regularizing neural networks by penalizing confident output distributions.\"\n\n* You should add citations for the statement \"In these and related settings, gradient descent has started to be replaced by inference networks.\"\n\n* I didn't find Table 1 particularly illuminating. All of the approaches seem to perform about the same. What conclusions should I make from it?\n\n* Why not use KL divergence as your \\Delta function?\n\n* Why are the results in Table 5 on the dev data?\n\n* I was confused by Table 4. First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network. This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper. Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy). Why is this?\n\n* I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?\n\n\n\n\n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642515818, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3", "ICLR.cc/2018/Conference/Paper814/AnonReviewer2", "ICLR.cc/2018/Conference/Paper814/AnonReviewer1"], "reply": {"forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642515818}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642515871, "tcdate": 1511757014439, "number": 2, "cdate": 1511757014439, "id": "Sk0CEftxG", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "New idea for training structured predictors, but unclear motivation and evaluation", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes training ``inference networks,'' which are neural network structured predictors. The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network.\n\nThe idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization. However, it remains unclear why SPENs are the right choice for an energy function.\n\nExperiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent. However, the experimental results are not clearly presented. The clarity is poor enough that the paper might not be ready for publication.\n\nComments and questions:\n\n1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors. The setup focuses on using SPENs as an inference network, but this seems inessential. Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below).\n\n2) The confusion over the motivation is confounded by the fact that the experiments are very unclear. Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6). In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS? It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator).\n\n3) The third and fourth columns of Table 5 are identical. The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers.\n\n4) It is also unclear how to compare Tables 4 and 5. The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance. What is the takeaway from these two tables supposed to be?\n\n5) Part of the motivation for the work is said to be the increasing interest in inference networks: \"In these and related settings, gradient descent has started to be replaced by inference networks. Our results below provide more evidence for making this transition.\" However, no other work on inference networks is directly cited.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642515818, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3", "ICLR.cc/2018/Conference/Paper814/AnonReviewer2", "ICLR.cc/2018/Conference/Paper814/AnonReviewer1"], "reply": {"forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642515818}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642515834, "tcdate": 1511844977311, "number": 3, "cdate": 1511844977311, "id": "HytdnPcgG", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting reinterpretation of SPENs", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs.\n\nSPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network. The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy. At inference time y can be optimized by gradient descent steps. SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y' = argmin_y E(f(x), y).\n\nThe key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization. The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).\n\nThe paper explores multiple loss functions and techniques to train these models. They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models. The improved understanding of SPENs and potential for further work justify accepting this paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642515818, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3", "ICLR.cc/2018/Conference/Paper814/AnonReviewer2", "ICLR.cc/2018/Conference/Paper814/AnonReviewer1"], "reply": {"forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642515818}}}, {"tddate": null, "ddate": null, "tmdate": 1515527943652, "tcdate": 1515527888852, "number": 5, "cdate": 1515527888852, "id": "rJKCC5GNM", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "forum": "H1WgVz-AZ", "replyto": "H12sn0dlf", "signatures": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper814/AnonReviewer3"], "content": {"title": "Response to the authors' comments and version 2 of the paper.", "comment": "The new experiments sections is substantially better. It does a good job of providing separate analyses of the various contributions of the paper. Overall, there is definitely a wealth of follow-on work to be done in this area, and the ICLR community will appreciate this paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727286, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1WgVz-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper814/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper814/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper814/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers", "ICLR.cc/2018/Conference/Paper814/Authors", "ICLR.cc/2018/Conference/Paper814/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727286}}}, {"tddate": null, "ddate": null, "tmdate": 1515002899158, "tcdate": 1515002899158, "number": 4, "cdate": 1515002899158, "id": "rJsf2997M", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "forum": "H1WgVz-AZ", "replyto": "H12sn0dlf", "signatures": ["ICLR.cc/2018/Conference/Paper814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper814/Authors"], "content": {"title": "Response", "comment": "Thanks very much for the thoughtful review!\n\nRegarding your major comment, we will first mention that the revised version includes additional experimental results when using our framework to train a SPEN with a global energy that includes the tag language model (TLM) energy. These results are described in Sec. 7.2.4. \n\nWe agree that the original submission suffered from a bit of an identity crisis. As you mentioned, \u201cThe idea of amortizing inference is perhaps more general\u201d and we intend to develop this direction in future work. Also, in the revised version, we restructured the sequence labeling section so as to more cleanly separate the discussion of training SPENs (Sec. 7.2.3) and exploring richer energy functions (Sec. 7.2.4) from the discussion of amortizing inference for pretrained structured predictors (Sec. 7.2.5). \n\nReplies to your minor comments are below:\n\n\u201cEnergy Based GANs\u201d\nThanks -- we added a mention and citation to the Related Work section.\n\n\u201cWhy would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?\u201d\n\nWe should have provided a citation for this. The github page hosting the SPEN code includes the claim: \u201cthe end-to-end approach fitting the training data much better is that it is more prone to overfitting\u201d. As that method is from prior work, we do not know exactly what the cause is of the observed overfitting. It may be that it is caused by the increased capability of calculating precise gradients obtained by unrolling gradient descent into a computation graph, rather than merely performing gradient descent for inference in an offline manner. We clarified the above in Sec. 7.1.\n\n\u201cMove hyperparameter ranges, etc to the appendix. Add summary of TLM architecture to the main paper\u201d\n\nWe moved several tuning details to the appendix and moved the TLM description to the main body (Sec. 7.2.4).\n\n\u201cfootnote on using positive vs. negative sign on entropy regularization term\u201d\n\nThanks for the pointer!  We added a citation.\n\n\u201cadd citations for \u2018gradient descent has started to be replaced by inference networks.\u2019\u201d\n\nGood point. We added relevant citations to that claim. \n\n\u201cTable 1 not particularly illuminating. All of the approaches seem to perform about the same.\u201d\n\nUsually, using cost-augmented inference for testing (with an SVM) gives really bad predictions.  We were surprised to see that the final cost-augmented inference network performs well as a test-time inference network. This suggests that by the end of training, the cost-augmented network may be approaching the argmin. Nonetheless, since the differences are small there, we moved this table and discussion of this to the appendix.\n\n\u201cWhy are results in Table 5 on dev?\u201d\nWe often reported results only on dev so as to avoid reporting too many configurations on the test set, in order to prevent us (and the community) from learning too much about what works best on the test set. \n\n\u201cWhy not use KL divergence as your \\Delta function?\u201d\n\nIn classic max-margin structured prediction, \\Delta is a symmetric function, so we didn\u2019t consider using KL divergence. But we could use JS divergence and we think an exploration of the choices here would be interesting future work. (Also, we could try using asymmetric \\Delta functions as there does not appear to be any strong theoretical motivation to use symmetric \\Delta functions (in our view); it appears to be mostly just a convention.) \n\n\u201cconfused by Table 4\u201d\n\nThanks to the comments by you and the other reviewers, we heavily modified Table 4, splitting it into multiple simpler tables (see the new tables 4, 6, and 9).\n\n\u201cvery surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy). Why is this?\u201d\n\nThis is a good question.  We added some speculation to Sec. 7.2.3 that we think is relevant to this question as well.  In particular, the stabilization terms used when training the inference network may be providing a regularizing effect for the model. \n\n\u201cconfused by difference between Table 6 and Table 4\u201d\n\nYes, we agree that was confusing. We restructured both tables. Please see the new tables 4, 5, and 6."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727286, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1WgVz-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper814/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper814/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper814/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers", "ICLR.cc/2018/Conference/Paper814/Authors", "ICLR.cc/2018/Conference/Paper814/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727286}}}, {"tddate": null, "ddate": null, "tmdate": 1515002840290, "tcdate": 1515002840290, "number": 3, "cdate": 1515002840290, "id": "rJl1nqq7f", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "forum": "H1WgVz-AZ", "replyto": "Sk0CEftxG", "signatures": ["ICLR.cc/2018/Conference/Paper814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper814/Authors"], "content": {"title": "Response", "comment": "Thanks for the questions. We agree with you that the experiment description was unclear in many places and we think the revised version is much improved in this regard. Specific answers to your numbered questions are below:\n\n1) The primary goal of the paper is to propose a framework to do training and inference with SPENs.  We have rewritten the experimental results section to focus on evaluating this SPEN training/inference framework.  It turns out that the framework can also be applied to simpler families of structured prediction models, so we also include experimental results for applying inference network training to CRFs (see Sec. 7.2.5 in the revised version).  In the new version, we have tried to more cleanly separate the contributions for SPENs from the contributions to structured prediction more generally, by relegating the latter results to Sec. 7.2.5 only. \n\n2) All good points. We hope the revised version will help resolve all of these confusions. Please let us know if anything is still unclear.\n\n3) We restructured this table to remove redundant columns and make the presentation simpler.\n\n4) Thanks to the comments by you and the other reviewers, we heavily modified Table 4, splitting it into multiple simpler tables (see the new tables 4, 6, and 9).  \n\n5) Good point. We added relevant citations to that claim.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727286, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1WgVz-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper814/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper814/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper814/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers", "ICLR.cc/2018/Conference/Paper814/Authors", "ICLR.cc/2018/Conference/Paper814/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727286}}}, {"tddate": null, "ddate": null, "tmdate": 1515002610689, "tcdate": 1515002610689, "number": 2, "cdate": 1515002610689, "id": "B1ixj9cmM", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "forum": "H1WgVz-AZ", "replyto": "HytdnPcgG", "signatures": ["ICLR.cc/2018/Conference/Paper814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper814/Authors"], "content": {"title": "Thanks", "comment": "Thank you for the comments and the support!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727286, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1WgVz-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper814/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper814/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper814/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers", "ICLR.cc/2018/Conference/Paper814/Authors", "ICLR.cc/2018/Conference/Paper814/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727286}}}, {"tddate": null, "ddate": null, "tmdate": 1515002489024, "tcdate": 1515002489024, "number": 1, "cdate": 1515002489024, "id": "H1-Y5qqXM", "invitation": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "forum": "H1WgVz-AZ", "replyto": "H1WgVz-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper814/Authors"], "content": {"title": "We posted a revised version of paper and add some comments.", "comment": "Thanks to the reviewers for the many comments and questions. We just posted a revised version that we think addresses many of them. In particular, we rewrote the sequence labeling experimental section (Sec. 7.2).  We simplified the experimental settings in each table to make the results easier to understand.  The results were admittedly very confusing, as we were combining different energy functions, training objectives, and inference network architectures, all in the same table.  We hope we have corrected that with the new rewrite.  We\u2019ll give a quick summary of our changes below:\n\nTable 4 compares our tuned SPEN configuration (which we are now calling \u201cSPEN (InfNet)\u201d throughout) to off-the-shelf BLSTM and CRF baselines.  The SPEN and CRF in that table use the same energy, namely the energy given in Eq. (13).  These experiments allow us to show the impact of differences in training and the use of inference networks while keeping the form of the energy function fixed. \n\nBut, as multiple reviewers pointed out, the goal of SPENs is to use energies that go beyond what\u2019s possible with traditional models like chain CRFs.  We definitely agree with this.  While we intend to pursue this more thoroughly in future work, we do feel that the tag language model (TLM) results are a promising step in this direction.  In Sec. 7.2.4, we describe the tag language model energy and present results when adding it to the energy in Eq. (13) and training with our framework. \n\nThen, in Sec. 7.2.5, we describe experiments in training inference networks to do test-time inference for a pretrained, off-the-shelf CRF.  These results were admittedly confusing in the  original submission, but hopefully by separating them out and moving them to the end of the paper, it is now more clear.  We agree with the reviewers that the approach described (of training inference networks to approximate prediction problems) does indeed apply beyond SPENs. While we did not have space to thoroughly explore this application in this submission, we hope that this small section of promising experimental results will help other researchers to see the potential of inference networks for structured prediction more broadly. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This\n\u201cinference network\u201d outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups\nof 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \u201clabel language model\u201d that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "pdf": "/pdf/a6086ae3c4ca658b234e86580ada2ff2e95c8452.pdf", "paperhash": "tu|learning_approximate_inference_networks_for_structured_prediction", "_bibtex": "@inproceedings{\ntu2018learning,\ntitle={Learning Approximate Inference Networks for Structured Prediction},\nauthor={Lifu Tu and Kevin Gimpel},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=H1WgVz-AZ},\n}", "keywords": ["Approximate Inference Networks", "Structured Prediction", "Multi-Label Classification", "Sequence Labeling"], "authors": ["Lifu Tu", "Kevin Gimpel"], "authorids": ["lifu@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727286, "id": "ICLR.cc/2018/Conference/-/Paper814/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1WgVz-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper814/Authors|ICLR.cc/2018/Conference/Paper814/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper814/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper814/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper814/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper814/Reviewers", "ICLR.cc/2018/Conference/Paper814/Authors", "ICLR.cc/2018/Conference/Paper814/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727286}}}], "count": 10}