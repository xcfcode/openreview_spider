{"notes": [{"id": "B1l6e3RcF7", "original": "SJgwzJRcFm", "number": 1122, "cdate": 1538087925320, "ddate": null, "tcdate": 1538087925320, "tmdate": 1545355409898, "tddate": null, "forum": "B1l6e3RcF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJxmhd7kgE", "original": null, "number": 1, "cdate": 1544661163166, "ddate": null, "tcdate": 1544661163166, "tmdate": 1545354504234, "tddate": null, "forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1122/Meta_Review", "content": {"metareview": "The reviewers agree that the paper needs significantly more work to improve presentation and is not fully empirically and conceptually convincing.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1122/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1122/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1122/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352958358, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1122/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1122/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1122/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352958358}}}, {"id": "ryeCmbgA2Q", "original": null, "number": 3, "cdate": 1541435686209, "ddate": null, "tcdate": 1541435686209, "tmdate": 1541533403505, "tddate": null, "forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "content": {"title": "Important line of research and novel ideas that lack precision and is limited by mere presentation of observations", "review": "The subject of how a given algorithm explores the landscape is still a poorly understood area in training neural networks. There is a large body of recent work that attempts to shed light on this puzzle, and each one tries to claim their share in the furthering of the understanding of the relationship between the geometry of the landscape and the dynamics that one chooses in optimization. The present paper is a fine addition to the literature with interesting observations and novel questions, however, it falls short in many core areas: An apparent work in progress that has a great potential. \n\nIt is safe to say that \"A walk with SGD\" has an important single focus in mind: Does the SGD cross over barriers in the weight space of the underlying neural network? This question, at its heart, is intimately linked with the many properties that are attributed to the modern algorithm of the choice and the way it navigates a given non-convex landscape. The paper claims to provide an almost negative answer to the question and thereby busting several myths that are attributed to the \"trick\" part of SGD algorithm. As good as it sounds, unfortunately, the paper falls short of providing a convincing evidence (be it theoretical or empirical), and the way it tries to frame itself unique and different in relation to related works only indicate a lack of deep understanding of the existing literature. Therefore, I think there are several ways the paper should be improved before it is ready.\n\nA major question (that I hope will easily be addressed) is on the definition of the barrier itself. According to the text, a barrier is defined judging by the minima of two 1-dimensional segments that connect weights connecting three consecutive steps: if the minimum of the line segment defined by the latter step is larger than the former, then it declared that a barrier is crossed. In a low dimensional world, this makes total sense, however, I fail to understand what kind of barrier it implies on the geometry of the landscape: Can the 1-dimensional lines be on the sides of a valley? Can one find *another* 1-dimensional projection for which the inequality is broken? How do such dependencies change the understanding of the problem? And if one is indeed only interested in the flat line segments (since SGD is making discrete steps), then one can, in principle, observe barrier crossing in a convex problem, as well? Is there an argument for otherwise? Or if it is a notion that applies equally well in a convex case then how should we really think about the barrier crossing? On the opposite point of view, can one not imagine a barrier crossing that doesn't appear in this triangular inequality above?\n\nThe paper is full of empirical evidence that is guided by a simple observable that is very intuitive, however, it lacks a comprehensive discussion on the new quantity they propose that I consider a major flaw, but that I think (hope) that the authors can fix very easily. Some minor points that would improve the readability and clarity for the reader:\n- The figures are not very reader-friendly, this can be improved by better using the whitespaces in the paper but it can also be improved by finding further observables that would summarize the observations instead of showing individual consecutive line interpolations.\n- What are the values of the y-axis in Figure 5 and 6? Are they the top eigenvalues of the Hessian?\n- In the models that are compared in Figure 7, what are their generalization properties (early stopping and otherwise)?\n- The interpretation at the end of p. 6 may be a good motivation for the reader if it had been introduced earlier for that section.\n\nFinally, Section 5 reads very strangely, I have hard times understanding why certain phrases exist the way they are in this part. Here are further notes on Section 5:\n- Why the whole first paragraph focused on hot the current paper is different than Goodfellow et al (2014) (it is obvious that it is for a different purpose), or why do we read the sentence \"Li et al (2017b) also visualize....\" What way do they visualize, is it the only paper that does visualization, what's the relation with the current paper and barrier crossing? \n- For the second paragraph, I can suggest another paper, https://arxiv.org/abs/1803.06969, that was at ICML which also looks at the diffusion process through the parameter distance at different times which is similar to Hoffer et al. which also claims no barrier crossing similar to the present paper. \n- However, my main issue is the exact connection between diffusion and no barrier crossing and it's connection to SGD preferring wide local minima instead of a narrow one. The second paragraph of the conclusion touches upon this subject. But it is not entirely clear how they are linked (except for the brittle SDE approximation at Li et al (see https://arxiv.org/abs/1810.00004)). Overall, the paper would benefit a lot from the discussion on why it is preferable to have SGD choose one basin over another in the beginning, as it is, it looks like the paper has another agenda behind the scenes.\n- In the fourth paragraph of the conclusion, the paper refers to three papers that link DNN to spin glasses, in two of the (older) references the networks are far from what we have today, and the third one is far from \"showing\" anything between DNN and spin glass. In any case, what's the link between the aspects studied there with the present paper?\n- Finally, the paper claims at the last few sentences that the works referred a little bit earlier look at the loss surface \"in isolation from the optimization dynamics\", however, many of those works cited have their empirical observations much like the current paper, and clearly they all \"study the DNN loss surface along the trajectory of SGD\" necessarily as it is the way to find local minima, saddle points, paths, curvature etc... The present paper is already very interesting and full of novel insight, I fail to see the value of struggling to stand out like this.\n\nOverall, I think the paper is a very interesting step forward in understanding SGD dynamics on the DNN landscape. And, even though it has many shortcomings as it currently stands, I think it has a lot of room to improve.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1122/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "cdate": 1542234301038, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1122/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335877513, "tmdate": 1552335877513, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1122/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eM35A_h7", "original": null, "number": 2, "cdate": 1541102249901, "ddate": null, "tcdate": 1541102249901, "tmdate": 1541533403264, "tddate": null, "forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "content": {"title": "Two interesting ideas but somewhat disconnected", "review": "This paper explores the idea that mini-batch SGD rarely *crosses* barriers during DNN optimization, but rather uses a 'seemingly' or 'alternate' mechanism, as the authors somewhat mysteriously call it on the first page. In the second part of the paper,  they also investigate why the loss surface is explored more slowly when the batch size increases. \n\nI found both parts of the paper reasonably interesting but not too surprising. My main concern is that both parts are , in themselves, not strong enough to warrant publication at ICLR, and the connection between them is rather weak. The authors write 'to complement this finding'  to connect the first to the second investigation, but that's not connecting them very closely is it?\nI think it would be better to work out both insights in more detail and publish them in separate papers. \nEspecially the second insight should be explored more thoroughly. For example, the authors write 'in convex optimization theory, when gradients point along the sharp directions of the loss surface, optimization exhibits under-damped convergence'. This is repeated later in different wordings.  But no reference to this result (I presume it's a mathematical theorem?) is given, neither here nor later when it is said again. The link from the convex to the nonvex DNN case could also be established more convincingly. Everything became quite (too) heuristic at some point...\n\nA few small remarks (which did not influence my judgement):\n- while in general (with the exception of the too-fast move from convex to nonconvex that I just explained) the paper is written quite clearly, the prose could be made significantly tighter. For example, the definition of what 'crossing a barrier' means is given three times (!) in the paper (two times in a figure, once in section 2). BTW, isn't it better to say 'moving *around* barriers' rather than 'over' barriers? You now use 'over' but still sounds very similar to just 'crossing'. \n- plural nouns are often combined with singular vers ('measurements that ensures'). This happens not just once but all the time...\n\nPROS:\n- two nice little ideas; esp. the first one is well-explained\n- easy to read\nCONS:\n- ideas are not very surprising; and just tested on a few data sets; things could be more  robust. \n- second idea not fully convincingly explained\n- (most important): the two ideas are not closely connected, making this a somewhat strange paper. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1122/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "cdate": 1542234301038, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1122/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335877513, "tmdate": 1552335877513, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1122/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylzRFgP2Q", "original": null, "number": 1, "cdate": 1540979145967, "ddate": null, "tcdate": 1540979145967, "tmdate": 1541533403054, "tddate": null, "forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "content": {"title": "Qualitative analysis lacking key insights and not comprehensive enough?", "review": "I like the idea of trying to qualitatively illustrate the behavior of SGD when optimizing parameters of complex models, such as Deep and Conv Nets, but I think that the contribution is not very substantial. The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. \n\nIt would helpful to illustrate the difference between \"crossing\" and \"moving over\" a barrier with a simple figure. \n\nThe experimental validation is interesting, although I think it is limited and perhaps the conclusions that can be drawn from it are not so surprising. I believe it would have been interesting to study other important factors that affect the behavior of SGD, such as learning rate and type of momentum. For example, a larger learning rate might allow for more crossing of barriers. Also, different SGD algorithms (ADAGRAD, ADAM, etc...) would behave considerably differently I expect. At the moment these important factors are overlooked. \n\nIt is not clear to me why we would want to avoid larger batch sizes. A larger batch size allows for a lower variance of stochastic gradients, and therefore faster convergence. I think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful SGD works, such as SAGA and the like. I agree that a smaller batch-size is preferable at the beginning of the optimization, but again this is a well known fact (again, see SAGA) and it is for computational reasons mostly (being far away from the (local) mode, a noisy gradient is enough to move in the right direction - no need to spend computations to use an accurate gradient). There is no guarantee that the local optimum close to initialization is a bad local optimum in general, so I don't think that using a large batch size at the beginning is a bad idea for this reason - again it is just computational. \n\nAnother thing missing I think is the discussion around why it is potentially a good thing to cross the barrier, either at the beginning of the exploration or towards convergence to a local optimum. At the moment, the paper seems to report the behavior of SGD without key insights on the importance of crossing or avoiding crossing barriers.\n\nAs a concluding remark - there has been a lot of work on the connections between diffusions and MCMC algorithms (see e.g., the Metropolis Adjusted Langevin Algorithm - MALA) and a lot of the considerations made in the paper are somewhat known. That is, random walk/diffusion type MCMC (and even gradient-based MCMC like Hybrid Monte Carlo) struggle a lot in non-convex problems and they hardly move across modes of a posterior distribution (equivalent to crossing barriers of potential). So I'm not at all surprised that SGD does not cross barriers during optimization and I would challenge the statement in the introduction saying \"Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process.\"", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1122/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1122/Official_Review", "cdate": 1542234301038, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l6e3RcF7", "replyto": "B1l6e3RcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1122/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335877513, "tmdate": 1552335877513, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1122/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygeI1GRiQ", "original": null, "number": 1, "cdate": 1540394824003, "ddate": null, "tcdate": 1540394824003, "tmdate": 1540394824003, "tddate": null, "forum": "B1l6e3RcF7", "replyto": "Syl1IJEujm", "invitation": "ICLR.cc/2019/Conference/-/Paper1122/Official_Comment", "content": {"title": "Re: What does \"moving over a barrier\" mean?", "comment": "Apologies for the late reply, open review did not notify us with an email. Let us first try to describe \"moving over a barrier\" in a completely non-mathematical and purely layman terms. To visualize what we mean by it, consider when flying in an airplane and looking out the widow below the airplane. While the height of the airplane above sea level (denoting training loss) may be fixed, the landscape below can have ups and downs (with respect to sea level). These ups and downs in the landscape below is what we refer to as \"moving over the barriers\" instead of crossing them. Here sea level acts as a reference and denotes 0 training loss.\n\nIn the last paragraph of page 2 (section 3), we described the above phenomenon using a geometrical construct where the model parameters go from state A to B to C due to 2 consecutive SGD update steps. When interpolating the loss between parameters A and B along the straight line connecting them, the interpolation looks like a convex quadratic with a minimum in between. The same thing happens between points B and C. Hence no barriers are crossed during SGD update from A to B, and update from B to C. However, since the minimum between A and B is lower than the minimum between B and C in the construct, there must exist a barrier along any path from the minimum between A and B, to the minimum between B and C. This is what we referred to as \"moving over a barrier\" when SGD goes from A to B to C in this construct.\n\nThis behavior is in contrast with the traditional intuition that noise in SGD helps in exploring different regions of the non-convex loss surface by entering and then escaping local minima by jumping out of them, which would require SGD to cross barriers and is a slow process."}, "signatures": ["ICLR.cc/2019/Conference/Paper1122/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1122/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1122/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Walk with SGD: How SGD Explores Regions of Deep Network Loss?", "abstract": "The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.", "keywords": [], "authorids": ["xingchen1113@gmail.com", "devansharpit@gmail.com", "tsirif@gmail.com", "yoshua.umontreal@gmail.com"], "authors": ["Chen Xing", "Devansh Arpit", "Christos Tsirigotis", "Yoshua Bengio"], "pdf": "/pdf/f47da99158df06a898f20d9f1e18c0175bb96ef2.pdf", "paperhash": "xing|a_walk_with_sgd_how_sgd_explores_regions_of_deep_network_loss", "_bibtex": "@misc{\nxing2019a,\ntitle={A Walk with {SGD}: How {SGD} Explores Regions of Deep Network Loss?},\nauthor={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6e3RcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1122/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625530, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1l6e3RcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1122/Authors", "ICLR.cc/2019/Conference/Paper1122/Reviewers", "ICLR.cc/2019/Conference/Paper1122/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1122/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1122/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1122/Authors|ICLR.cc/2019/Conference/Paper1122/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1122/Reviewers", "ICLR.cc/2019/Conference/Paper1122/Authors", "ICLR.cc/2019/Conference/Paper1122/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625530}}}], "count": 6}