{"notes": [{"id": "SyglyANFDr", "original": "SylJa7f_vr", "number": 879, "cdate": 1569439191576, "ddate": null, "tcdate": 1569439191576, "tmdate": 1577168233352, "tddate": null, "forum": "SyglyANFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "faLD4p_Kv", "original": null, "number": 1, "cdate": 1576798708570, "ddate": null, "tcdate": 1576798708570, "tmdate": 1576800927808, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a modification of SGD to do distributionally-robust optimization of deep networks.  The main idea is sensible enough, however, the inadequate handling of baselines and relatively toy nature of the experiments means that this paper needs more work to be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713862, "tmdate": 1576800263576, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper879/-/Decision"}}}, {"id": "Hkx76YqLqr", "original": null, "number": 1, "cdate": 1572411835080, "ddate": null, "tcdate": 1572411835080, "tmdate": 1574188423986, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a method for Distributionally Robust Optimization (DRO). DRO has recently been proposed (Namkoong and Duchi 2016 and others) as a robust learning framework compared to Empirical Risk Minimization (ERM).  My analysis of this work in short is that the problem that this paper addresses is interesting and not yet solved. This paper proposes a simple and efficient method for DRO. However, convergence guarantees are weak which is reflected in their weak experimental results. And for a 10-page paper, the writing has to improve quite a lot.\n\nSummary of contributions:\nThe phi-divergence variation of DRO introduces a min-max objective that minimizes a maximally reweighted empirical risk. The main contribution of this work is as follows:\n- Show that for two common phi-divergences (KL-divergence and Pearson-chi2 divergence), the inner maximization has a simple solution for the weights that depends on the value of the loss on the training set.\n- Use the above algorithm and modify SGD by changing the sampling of data according to the weights.\n- Convergence proofs for the above algorithm for wide networks (not necessarily infinite-width).\n\n\nCons:\n- Fig 1, ERM should not be so bad as if predicting randomly. This suggests a problem in the experimental setting. At least more ablation study is needed, e.g. try varying the percentage of data kept and show the final test accuracy as a function of this percentage for ERM. In the worst case, the accuracy for ERM should be ~90% on 9 classes and zero on 10th which is ~80% for uniform test accuracy. Another point, what is the train accuracy on cats? ERM should be overfitting to those few cats in the training set and achieve some non-zero accuracy at test. But it seems the test accuracy is almost exactly 0 at test time.\n- In Theorem 5.1 that provides convergence proofs, the learning rate eta_exact has a dependence on 1/beta, which means much smaller learning rates should be used for the proposed method. However, in the experiments it seems that the same learning rate is used for all methods. This seems to trouble the convergence very clearly as the curves start to fluctuate considerably by increasing for larger betas. There is no bounds on beta which in practice can force us to use orders of magnitude smaller learning rates.\n- Section 4.3 aims at linking hard negative mining to the proposed method. What they actually do is propose a new definition for hard-negative mining which is satisfied by the proposed method. There is little basis for suggesting this definition. There are myriads of work on hard-negative mining and suggesting a new definition needs a more thorough study of related works.\n- Section A.1.1 argues we would stop ERM when it plateaus, but it doesn't look like it has plateaued in fig 2 left.\n- Section A.3, I'm not sure lr=1 would be stable wide resnet. Maybe there is a problem with the experimental setting.\n- There is such much non-crucial details in the main body that increased the main text to 10 pages. At least 2 pages could be saved by moving details of theorems and convergence results to the appendix.\n\nAfter rebuttal:\nI'm keeping more score. The manuscript has been improved quite a lot.  My main concern remains the experiments. Both texts and mathematical statements still need more edits.\n\nThe authors have completely removed experiments on cifar10. Paper now has only one set of experiments on MNIST with little ablation study. I still think my suggested ablation study is interesting. As a minimal sanity check, I'd like to see the performance of DRO on the original MNIST dataset. Basically, how much do we lose by using a robust method on a balanced dataset? Even with full ablation studies, I don't think MNIST is not enough for testing methods based on hard-negative mining.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper879/Reviewers"], "noninvitees": [], "tcdate": 1570237745652, "tmdate": 1574723085087, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Review"}}}, {"id": "rJgnJ3NojB", "original": null, "number": 7, "cdate": 1573764067807, "ddate": null, "tcdate": 1573764067807, "tmdate": 1573764067807, "tddate": null, "forum": "SyglyANFDr", "replyto": "Hkx76YqLqr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "CIFAR10 experiment and Writing", "comment": "[Writing] \nWe have worked on the writing and have now reduced the main text to 8 pages by moving the details of the convergence theorems to the appendix as you suggested.\n\n[CIFAR10]\nPlease see our common reply \"Experiments on CIFAR10 and DRO with momentum\".\nThank you."}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "r1lhujNssH", "original": null, "number": 6, "cdate": 1573763955841, "ddate": null, "tcdate": 1573763955841, "tmdate": 1573763955841, "tddate": null, "forum": "SyglyANFDr", "replyto": "BklCcLSacS", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "Experiment on CIFAR10", "comment": "Please see our common reply \"Experiments on CIFAR10 and DRO with momentum\".\nThank you."}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "SylmXo4joB", "original": null, "number": 5, "cdate": 1573763866922, "ddate": null, "tcdate": 1573763866922, "tmdate": 1573763866922, "tddate": null, "forum": "SyglyANFDr", "replyto": "HklDPH9QsS", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "Reply to your comments", "comment": "Thank you for taking the time to review our submission and for your insights.\n\nWe have grouped the reply to your questions on the momentum and our reply to the common concern of the reviewers regarding our results on CIFAR10 in the common reply \"Experiments on CIFAR10 and DRO with momentum\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "BJer0FEjjr", "original": null, "number": 4, "cdate": 1573763533070, "ddate": null, "tcdate": 1573763533070, "tmdate": 1573763533070, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "Experiments on CIFAR10 and DRO with momentum", "comment": "In this comment we reply to the concerns of the reviewers regarding the experiment on CIFAR10.\nWe also feedback on the concern of AnonReviewer3 regarding the introduction of momentum in the proposed algorithm.\n\n[CIFAR10 training issues for the baseline] \nThe baseline approach [1] shuffles the training dataset at each epoch. By default, it uses the last batch as is even if it is much smaller than the standard batch size. For the CIFAR10 training set with 10% of the cats, it turned out that the last batch was very small, which in turn led to unstable training.  \n\nAfter changing this behavior so as to drop the last batch, the baseline now trains more smoothly even with the large learning rate used in our experiments. \n\nAs such, even though figure 1 of our first submission showed an accurate depiction of the behavior of the baseline method using all the default parameters except for the learning rate, the effect measured was not related to our method. The difference mostly came from the noisy gradients occurring from the small last batch. As a result, this experiment has been removed from the manuscript. \n\n[Momentum] \nIn the baseline approach [1], the best performance is obtained with SGD with momentum and a learning rate decay schedule. \n\nFollowing preliminary experiments, we have found that using momentum updates with DRO is not straightforward. We think that this is due to the inner maximization of DRO (as suggested by AnonReviewer3), and that specific momentum updates for DRO are needed. \n\nFor large values of beta (beta>10) the WideResNet_28_10 with DRO does not train properly with momentum, except if we reduce the learning rate by two orders of magnitude. In the latter case, it trains very slowly and cannot be properly tested given the duration of the rebuttal period. It is possible that tuning the momentum parameter for DRO could tackle this issue, but we have not yet had the opportunity to evaluate it. \n\nFor low values of beta (beta<=1), we can train the WideResNet_28_10 with DRO with momentum and using the same hyperparameters as the baseline without DRO. With such small betas, we obtain the same accuracy as the baseline since the distributionally robust loss is very close to the mean loss (ERM case). \n\nInvestigating efficient means of introducing momentum within DRO requires further work. We have commented on this in the conclusion.  \n\n\n[1] Wide residual networks. Zagoruyko, S., & Komodakis, N. (2016). "}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "HklDPH9QsS", "original": null, "number": 3, "cdate": 1573262687085, "ddate": null, "tcdate": 1573262687085, "tmdate": 1573262747841, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Disclaimer: I was able to read the other reviews and the author\u2019s responses before finalizing this review.\n\nThe paper proposes an easy-to-implement algorithm for DRO on example weights, with the same computational cost as SGD.  The algorithm is based on hardness weighted sampling and links are shown to hard example mining.  Convergence results are shown for (finitely) wide networks.  Additionally, the paper claims to demonstrate the usefulness of the algorithm in deep learning.\n\nI am unable to assess how the convergence results place in the literature, but I believe they motivate the algorithm.  The claims of practical usefulness in deep learning do not seem supported by the provided empirical evidence.\nFor the CIFAR experiments, the ERM baseline does not seem to train - probably due to the choice of learning rate.  This makes it unclear how the proposed algorithm compares.  They claim the algorithm is more robust to the learning rate, so is it possible to train with the original learning rate used for the ERM baseline?  Why was a different learning rate chosen?\n\nIf the experimental results showed an improvement over a properly trained ERM baseline on CIFAR I would lean toward weak accept.\n\nMany state-of-the-art deep learning pipelines do not use plain SGD - for example, the WideResNet you used on CIFAR. How is Algorithm 1 used on these? Do we only make changes to the update on line 24?  Using momentum with nested optimization can introduce instabilities. Perhaps combining momentum with nested optimization of loss weights and parameters is why the baseline does not train?  Maybe you could try an architecture that trained using vanilla SGD, so you can better leverage your theoretical results?\n\nIt would provide evidence of the usefulness of the algorithm if we could take various pipelines and just drop their optimizer updates into algorithm 1 - hopefully without having to spend time re-tuning their optimizer parameters. It would also be nice to see the training loss/accuracy - perhaps over all classes and just cats - in the appendix.\n\n\nThings to improve that did not impact the score:\n\n(Page 1) \u201cin term\u201d -> \u201cin terms\u201d\n(Page 1 & 2) \u201can history\u201d -> \u201ca history\u201d\n(Page 2) \u201cOptmization\u201d -> \u201cOptimization\u201d\n(Page 5) \u201callows to link\u201d -> \u201callows us to link\u201d\n(Page 7) \u201cwe focus at\u201d -> \u201cwe focus on\u201d\n(Page 8) \u201callows to guarantee\u201d -> \u201callows us to guarantee\u201d\n(Page 19) \u201ccontinous\u201d -> \u201ccontinuous", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper879/Reviewers"], "noninvitees": [], "tcdate": 1570237745652, "tmdate": 1574723085087, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Review"}}}, {"id": "H1g4j-CzoB", "original": null, "number": 3, "cdate": 1573212572428, "ddate": null, "tcdate": 1573212572428, "tmdate": 1573212572428, "tddate": null, "forum": "SyglyANFDr", "replyto": "BklCcLSacS", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "First feedbacks", "comment": "Thank you for taking the time to review our submission and for your insights. \n\nIn this first answer to your review, to foster rapid discussion iterations, we feedback on some of your concerns below. We will come back to you shortly regarding your other concerns, as we are running additional experiments and working on the manuscript. \n\n[On the novelty of the paper] \nTo the best of our knowledge, our paper is the first to propose a weighted sampling strategy for deep DRO. In addition to offer a pragmatic algorithm, we also mathematically prove the convergence of our approach. Research on DRO in the non-convex setting has only started to appear recently. The differences between existing recent works and our algorithm are discussed in section 4 of our submission. The timeliness of our work is further confirmed by the fact that another work on a similar topic was independently submitted at ICLR this year [1]. \n \n[Clarification following your comment: \u201cThe DRO problem studied in this paper is relatively easy, in the sense that the inner maximization has close form solution\u201d] \nIt is correct that there is a closed form solution for the inner maximization problem, provided we have access to the full exact per-sample loss history.  However, this is not possible to evaluate efficiently in practice. To address this limitation, we propose to exploit a stale loss history that is continuously updated. One of our main contributions is to show that this approach is not only intuitive but also leads to provable convergence. \n\n[Contributions compared to Allen-Zhu et al. 2019] \nThere were several important technical challenges that made the extension of the result of Allen-Zhu et al. 2019 from ERM to our algorithm for DRO non-trivial. We used the framework of Allen-Zhu as a backbone for our proofs, but in contrast to the ERM case of Allen-Zhu: \n    - The DRO loss is not linear with respect to the per-sample loss because of the max operator. This linearity property is used many times in the proofs of Allen-Zhu et al. 2019. \n    - The sampling distribution is in our case dynamic and not uniform. \n    - Another substantial challenge is the fact that our sampling used with the stale loss only approximates the closed form solution of the internal max problem of the DRO problem. \n\nAddressing these points required substantial work, as can be found in appendix C.6 and C.7 (pages 21-33 in the first version of the submission) \n\n \n[Question 1: about the link between dynamic sampling (our approach) and re-weighting] \nReweighting while sampling according to the uniform distribution is of course possible. This would correspond to using importance sampling (up to a multiplicative factor n=number of training samples).  \nImportance sampling is often used when we cannot sample with respect to the desired distribution or to reduce the variance of the stochastic estimation of the gradient [2,3]. \n\nIn our case, the proposed hardness weighted sampler is a better approximation of the target distribution than the uniform distribution, which motivates its use. \n \n\n[Question 2: about training longer in the MNIST experiment] \nPlease see our common comment about our experiments on MNIST. \n \n[Question 3: about a link between the Focal Loss and DRO] \nIt is true that both our hardness weighted sampler and the Focal Loss are motivated by hard examples mining. However, the weights used in the Focal Loss were defined heuristically. In addition, those weights are specific to the cross entropy, while our optimization method can be used with any loss function.  \n\nEven though the weights of the Focal Loss can be expressed as a function of the cross entropy loss, It is not clear how these weights can be justified a posteriori as the result of the inner maximization problem of DRO with the cross entropy loss and a given phi-divergence. In particular, the vector of the weights of the Focal Loss is not a probability vector. Therefore, it does not trivially belong to the space of admissible solutions for the inner maximization optimization problem of DRO. More work would be required to establish if deeper connections exists between cross-entropy based DRO and the Focal Loss. \n\n\nReferences:\n[1] Distributionally Robust Neural Networks. (under submission at ICLR 2020. Paper1796). https://openreview.net/forum?id=ryxGuJrFvS&noteId=BJexEXB09r  \n\n[2] An introduction to graphical models. MI Jordan, 2005. \nhttp://www.cs.cmu.edu/~lebanon/pub/book/ \n\n[3] Methods of Reducing Sample Size in Monte Carlo Computations. H. Khan et al, 1953. "}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "Hygk1ZCGjB", "original": null, "number": 2, "cdate": 1573212374939, "ddate": null, "tcdate": 1573212374939, "tmdate": 1573212374939, "tddate": null, "forum": "SyglyANFDr", "replyto": "Hkx76YqLqr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "First feedbacks", "comment": "Thank you for taking the time to review our submission and for your insights. \n\nIn this first answer to your review, to foster rapid discussion iterations, we feedback on some of your concerns below. We will come back to you shortly regarding your other concerns, as we are running additional experiments and working on the manuscript. \n\n[Definition of Hard Example Mining Sampling] \nTo the best of our knowledge, there is no widely accepted definition for Hard Example Mining in the literature. Yet, we need a definition to study the link between DRO and Hard Example Mining. We do not pretend to propose a definition for all possible Hard Example Mining methods, which is why we used the term \u201cHard Example Mining Sampling\u201d rather than only \u201cHard Example Mining\u201d. \n\nOur definition further address only Online Hard Example Mining as defined in [1] since there is no discrete switching between training and hard examples mining. To make it clearer that our definition corresponds only to a subset of Hard Example Mining methods, we have now renamed it as \u201cOnline Hard Example Mining Sampling\u201d in the revised version of the submission. \n \n[The learning rate is too high for the large values of beta] \nPlease see our common comment about our experiments on MNIST. \n\n[When does ERM plateaus in fig.2 left?] \nPlease see our common comment about our experiments on MNIST. \n\n[Reducing the size of the main text] \nWe are working on moving the details of section 5 to the appendix as you suggested to improve clarity. \n\n \nReferences:\n[1] Training Region-based Object Detectors with Online Hard Example Mining. A. Srivastava et al. 2016 "}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "B1ggdeAfiS", "original": null, "number": 1, "cdate": 1573212263535, "ddate": null, "tcdate": 1573212263535, "tmdate": 1573212263535, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment", "content": {"title": "Experiments on MNIST", "comment": "We thank the reviewers for taking the time to review our submission and for their insights. \n\nIn this comment, we feedback on the concerns of the reviewers regarding our experiments on MNIST.\n\n[Training longer in the MNIST experiment] \nWe have expanded our experiments with three times more training epochs as per the reviewer suggestions (please see the updated Fig. 2 in the revised submission). \n\nAs suggested by AnonReviewer2 \u201cI'm not sure they will finally achieve the same accuracy (the same optimum)\u201d, the updated Fig. 2 suggests that after a large number of iterations, ERM and DRO converge to different local minima, and that DRO leads to a better generalization on the under-represented class. \n\n[On Early-stopping: AnonReviewer4 \u201cSection A.1.1 argues we would stop ERM when it plateaus, but it doesn't look like it has plateaued in fig 2 left.\u201d] \nWe agree that our statement could be made more accurate. When we used the term \u201cplateau\u201d it was related to the choice of the patience parameter in early-stopping. In the left panel of Fig.2, there is no improvement of ERM of more than 0.04% in accuracy between epoch 20 and 30. For a patience lesser than 10 epochs, one would stop training for ERM before epoch 30. To clarify this point, we rephrased or statement as: \u201cif the patience parameter is too low, ERM might be considered to plateau at an epoch at which it has an accuracy of 0 on the under-represented class\u201d.  \n\nIt is also worth noting, that this experiment corresponds to the best-case scenario in which the testing set is used for choosing the optimal number of epochs (i.e. there is no bias at all between the validation and the testing distributions). \n\n[On instabilities in the learning curve on the testing set] \nAnonReviewer4 suggested that the instabilities observed during the first epochs in the learning curves on the testing set for large values of beta could be due to a value of the learning rate that is too large: \u201cin the experiments it seems that the same learning rate is used for all methods. This seems to trouble the convergence very clearly as the curves start to fluctuate considerably by increasing for larger betas.\u201d \n\nFollowing this suggestion, we tried to divide the learning rate by 10 or 100. However, we observed no reduction of the instabilities on the testing set. \n\nLooking at the learning curves **on the training set**, we found that the loss curves for beta=10 were actually stable there. However, we have observed that during the iterations for which instabilities appears **on the testing set**, the standard deviation of the loss **on the training set** is relatively high (i.e. the hardness weighted probability is further away from the uniform distribution). \n\nThis suggests that the apparent instabilities **on the testing set** are not related to a too high learning rate, but to differences between the DRO loss and the ERM loss. \n\nFollowing this observation, we increased beta to 100 for the same learning rate and it led to higher accuracy on the testing set (see the updated fig. 2). "}, "signatures": ["ICLR.cc/2020/Conference/Paper879/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyglyANFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper879/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper879/Authors|ICLR.cc/2020/Conference/Paper879/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164803, "tmdate": 1576860553898, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper879/Authors", "ICLR.cc/2020/Conference/Paper879/Reviewers", "ICLR.cc/2020/Conference/Paper879/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Comment"}}}, {"id": "BklCcLSacS", "original": null, "number": 2, "cdate": 1572849301822, "ddate": null, "tcdate": 1572849301822, "tmdate": 1572972540771, "tddate": null, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "invitation": "ICLR.cc/2020/Conference/Paper879/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the Distributionally Robust Optimization (DRO), in the sense that the weights assigned to the training data can change, but the training data itself remains unchanged. They demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. On the theoretical side, they prove the convergence of our DRO algorithm for over-parameterized\ndeep learning networks with ReLU activation and finite number of layers and parameters.\n\nThe DRO problem studied in this paper is relatively easy, in the sense that the inner maximization has close form solution (at least for KL divergence). Therefore, the proposed method is straightforward. All the derivations in Section 3 and 4 are strainghtforward and sensible. I do not know any paper that has proposed Algorithm 1 (or something similar) before, but I will be surprised there is not. I do not check the derivations in Section 5, but I suppose that it is a small modification of the proof in Allen-Zhu et al., 2019. I suppose that the theorem is correct, but it provides nearly no practical guidance.\n\nIn Algorithm 1 Line 18-19, the algorithm proposes to do sampling with replacement using the softmax probability \\hat{p}. How about directly multipling the weights \\hat{p} to the current minibatch of samples. (i.e., re-weighting the samples instead of re-sampling the samples). What's the difference between these two choices?\n\nSecond, both experiments are not convincing enough. In the CIFAR10 experiment (Figure 1), the baseline method ERM is too low. I guess that this low number is due to the large initial learning rate 1. The authors should provide the best performance IRM can achieve, and compare with the best performance DRO can achieve. Although the authors are claiming that the proposed DRO works with larger learning rate, Figure 1 (Left) simply gives readers the wrong information that ERM does not work. \"Figure 2 suggests that if we train ERM long enough it will converge to the same accuracy as DRO.\" The objectives of ERM and DRO are different. Their accuracy may become closer to each other, but I'm not sure they will finally achieve the same accuracy (the same optimum). Can the authors elaborate on this?\n\nFinally, the focal loss has achieved good empirical results in object detection. I feel that it can be formulated as a special case of the Equation (2) and Algorithm 1, by picking up some proper \\phi-divergence and using the (un-normalized) re-weighting scheme. It will good if the authors can provide a principled view of focal loss through the lens of DRO. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper879/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lucas.fidon@kcl.ac.uk", "sebastien.ourselin@kcl.ac.uk", "tom.vercauteren@kcl.ac.uk"], "title": "SGD with Hardness Weighted Sampling for Distributionally Robust Deep Learning", "authors": ["Lucas Fidon", "Sebastien Ourselin", "Tom Vercauteren"], "pdf": "/pdf/606d6bade0185bfa838098ad646418e247c6cd69.pdf", "TL;DR": "An SGD-based method for training deep neural networks with distributionally robust optimization", "abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.", "keywords": ["distributionally robust optimization", "distributionally robust deep learning", "over-parameterized deep neural networks", "deep neural networks", "AI safety", "hard example mining"], "paperhash": "fidon|sgd_with_hardness_weighted_sampling_for_distributionally_robust_deep_learning", "original_pdf": "/attachment/a1b5f4fd1bf854f39b7a812c9f70e7bd49a80f6b.pdf", "_bibtex": "@misc{\nfidon2020sgd,\ntitle={{\\{}SGD{\\}} with Hardness Weighted Sampling for Distributionally Robust Deep Learning},\nauthor={Lucas Fidon and Sebastien Ourselin and Tom Vercauteren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyglyANFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyglyANFDr", "replyto": "SyglyANFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper879/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper879/Reviewers"], "noninvitees": [], "tcdate": 1570237745652, "tmdate": 1574723085087, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper879/-/Official_Review"}}}], "count": 12}