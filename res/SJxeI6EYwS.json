{"notes": [{"id": "SJxeI6EYwS", "original": "Syln7l_wDS", "number": 547, "cdate": 1569439048009, "ddate": null, "tcdate": 1569439048009, "tmdate": 1577168259287, "tddate": null, "forum": "SJxeI6EYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "pgy2MAgq8L", "original": null, "number": 1, "cdate": 1576798699438, "ddate": null, "tcdate": 1576798699438, "tmdate": 1576800936417, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to use stacked layers of Gaussian latent variables with a maxent objective function as a regulariser. I agree with the reviewers that there is very little novelty and the experiments are not very convincing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710205, "tmdate": 1576800259155, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper547/-/Decision"}}}, {"id": "BkeJQaCz9B", "original": null, "number": 3, "cdate": 1572166934954, "ddate": null, "tcdate": 1572166934954, "tmdate": 1574289887846, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper presents a simple stochastic neural network, which makes each neuron output Gaussian random variables. The model is trained with reparameterization trick. The authors advocates the adoptation of a non-informative prior, and shows that learning with the prior equals with an entropy-maximization regularization term. The paper presents the design of the regularization term for pruning, learning with label noise, and defensing with adversarial examples. The claims are well supported: the model is indeed simple, and the effectiveness is well supported by experimental results. \n\nI however think *efficiency* of the proposed model needs to be studied as well. Since the proposed algorithm is an approximate inference algorithm via reparametrization trick, it is necessary to see how fast does the approximate algorithm converge. The experiments don't report any convergence curve, or performance under limited time budget. I think there need to be some related results.\n\nAnother question is, what is the original inference problem of the designed regularization for pruning, label noise, and adversarial attack, respectively?\n\nUpdate\n====\n\nThanks for the additional experiments! After reading the rebuttal and other reviewer's comments I decide to keep my score. (Though I am less positive due to the concern of novelty raised by other reviewers.)", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575830871028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper547/Reviewers"], "noninvitees": [], "tcdate": 1570237750556, "tmdate": 1575830871042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Review"}}}, {"id": "SkeD9je3jB", "original": null, "number": 4, "cdate": 1573813135075, "ddate": null, "tcdate": 1573813135075, "tmdate": 1573813135075, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "HJgsg7xnYH", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment", "content": {"title": "Response to Review#1:", "comment": "We thank the reviewer for the comments. We have revised the paper as suggested and would like to clarify several things:\n\nQ1. More related work:\nWe have added the suggested work in the related work section.\n\nQ2. Standard Deviation in Table 1 and different baseline in task of MNIST and CIFAR: \nWe cropped the results from (Dai et al., 2018), in which they did not show the standard deviation. Note that different papers used different network backbones. For example, the L0 method (Louizos et al., 2018) uses WRN on CIFAR datasets and SBP (Neklyudov et al., 2017) used a variant of VGG. Therefore we consider it to be unfair for direct comparison with them in Table 2.\n\nQ3. VGG rather than WRN:\nWe use the same setting as in (Dai et al., 2018), which is the most relevant paper. In (Dai et al., 2018), VGG is also used as the backbone, which has also been commonly used in other works. Indeed, having a stronger backbone would have a big impact on the absolute number of the error rate. However, to show the effectiveness of a model compression algorithm, the backbone has been the same for fair comparison among competitors. Having said that, we have now obtained a new result using WRN-28-10. The error rate is much lower, but the compression rate is also much lower: Our method reached the accuracy of 3.82% while the model size compressed to 81.97% and the FLOPs size decreased to 4.16 billion from 5.24 billion. In comparison, from Figure 4(a) and (b) in (Louizos et al., 2018), the FLOPs is reduced from 350 billion to 325 billion \u2013 clearly a different (nonstandard) way to calculate FLOPs is used in (Louizos et al., 2018) because most recent projects report similar FLOPs number as ours (e.g., Park et al., 2018, open source codes such as https://github.com/osmr/imgclsmob/tree/master/pytorch). But the relative compression performance is clearly weaker than ours.\n\nQ4. More attacks: \nWe add the latest black-box adversarial attack method NATTACK (Li et al., 2019). The results below show that our SE-SNN is clearly superior to the compared methods. \n+--------------+-------------+--------------+--------------+------------+\n|                  | Baseline | Adv-BNN | Deep VIB | SE-SNN |\n+--------------+-------------+--------------+--------------+------------+\n| NATTACK |    5.94%  |    56.44%  |   82.18%   |  95.00% |\n+--------------+-------------+--------------+--------------+------------+\n\n[1] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In-So Kweon. BAM: Bottleneck Attention Module. In BMVC 2018.\n[2] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks. In ICML, 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper547/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxeI6EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper547/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper547/Authors|ICLR.cc/2020/Conference/Paper547/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169814, "tmdate": 1576860543625, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment"}}}, {"id": "SkeJwme_jS", "original": null, "number": 3, "cdate": 1573548887462, "ddate": null, "tcdate": 1573548887462, "tmdate": 1573548887462, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "rkgLYY6cKH", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the comments. We have revised the paper accordingly and would like to clarify several things:\n\nQ1. Novelty:\nThough the structure of our proposed method is similar with DeepVIB [Alemi et al., 2017], the motivation is very different. In DeepVIB, there is an intuitive explanation that the regularizer corresponds to a sparsity prior on the weights. In contrast, the regularizer introduced in our model enforces uncertainty on activations rather than weights. Besides, our key contribution is to introduce a non-informative prior instead of using the standard Gaussian as an informative prior. For the three applications of network compression by pruning, adversarial defense and learning with label noise, the standard Gaussian is not the best prior, as we have empirically observed in this paper:  tuning the covariance matrix usually leads to better performance. On the contrary, using our non-informative prior stably produces better performance without extra tunings.\n\n\nQ2. Infinite-variance Gaussian\nIndeed, the proposed informative prior is improper, and is designed to be so. Importantly, it does not necessarily mean the posterior distribution is also improper in practice, as this depends on the exact process of optimisation. As we mentioned in page 3, Eq (4). The gradient is -1/\\simga, which means that the increment for gradient shrinks when \\sigma increases. So it never reaches infinity with a finite number of training updates. Furthermore, we have a margin loss the stops increasing \\sigma when it reaches a certain finite value.\n\n\nQ3. Eq2\nIt is clear from Eq 2 that \\sigma2 is irrelevant to the final objective, so it can be disregarded for whatever value it is.\n\n\nQ4. Theoretical Justification\nWe can think of margin loss as an early-stopping mechanism \u2013 it stops pushing the entropy larger when it surpasses a certain threshold. For different problems, we design different aggregation methods (across sample and/or feature axis) which are intuitively motivated by the specific problem settings. We have explained those deigning choices in page 3 for pruning, label noise, and adversarial defense respectively. \n\nMinor:\nWe have revised the paper to differentiate variance and standard deviation. Predicted standard deviation is parameterized by \\theta as Figure 1 shows.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper547/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxeI6EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper547/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper547/Authors|ICLR.cc/2020/Conference/Paper547/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169814, "tmdate": 1576860543625, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment"}}}, {"id": "HJeRf7lOsH", "original": null, "number": 2, "cdate": 1573548821541, "ddate": null, "tcdate": 1573548821541, "tmdate": 1573548821541, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "BkeJQaCz9B", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\nQ1. Convergence Curve:\nWe have added a convergence curve for label noise setting in Appendix B.4.\n\nQ2. Original Inference Problem: \nWe have asked for clarification, and are waiting for responses.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper547/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxeI6EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper547/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper547/Authors|ICLR.cc/2020/Conference/Paper547/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169814, "tmdate": 1576860543625, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper547/Authors", "ICLR.cc/2020/Conference/Paper547/Reviewers", "ICLR.cc/2020/Conference/Paper547/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Comment"}}}, {"id": "HJgsg7xnYH", "original": null, "number": 2, "cdate": 1571713778672, "ddate": null, "tcdate": 1571713778672, "tmdate": 1572972581740, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed SE-SNN, a type of stochastic neural networks that maximize the entropy in stochastic neurons along with the prediction accuracy. The authors argue that maximizing the entropy operates as a form of regularization to force the entropy into the least significant neurons, and that Increasing the diversity/randomness results in more robust models. Experiments show that SE-SNN outperform several baselines tasks such as network pruning, adversarial defense, and learning with noisy lables.\n\nSeveral closely related references are missing. The idea of producing distributions in each layer (i.e., using stochastic layers) is not new and is closely related to the work on local reparameterization trick and variational dropout [1] (predecessor of the cited sparse variational dropout), and various works that directly model neurons as distribution [2, 3]. \n\nBuilt on top of the reparameterization trick, the idea of maximizing the entropy of neurons to regularize the network is interesting. Such regularization is somewhat similar to sparsity regularization on neurons, which forces the information to concentrate on a small portion of the neurons. \n\nThe numbers for different methods in Table 1 are very close to each other. Without standard deviation it is difficult to evaluate the performance.\n\nNote that in Table 1 the best accuracy is actually achieved by SBP and L0. However, in Table 2 and 3 (experiments on CIFAR-10 and CIFAR-100), these two baselines are missing. Is there a reason why these baselines are excluded?\n\nAlso, looking at the results from Louizos et al., 2018, which is the most recent baseline among those chosen by the authors, they actually use WideResNet rather than VGG. Comparing to VGG, WRN performs significantly better. For example, the compressed network by Louizos et al., 2018 achieves an error rate of 3.83% on CIFAR-10, versus 8% from SE-SNN pruned VGG. \n\nFor the experiments on adversarial defense, the two attacks used seem rather weak (FGS from 2015 and CW-L2 from 2017). It may be the state-of-the-art attack around the time of Alemi et al., 2017, which the author claim to follow, but not now.\n\nMinor:\n\nP2: instead -> instead of\n\n[1] Variational Dropout and the Local Reparameterization Trick, NIPS 2015\n[2] Natural-Parameter Networks: A Class of Probabilistic Neural Networks, NIPS 2016\n[3] Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation, ICCV 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575830871028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper547/Reviewers"], "noninvitees": [], "tcdate": 1570237750556, "tmdate": 1575830871042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Review"}}}, {"id": "rkgLYY6cKH", "original": null, "number": 1, "cdate": 1571637630075, "ddate": null, "tcdate": 1571637630075, "tmdate": 1572972581706, "tddate": null, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "invitation": "ICLR.cc/2020/Conference/Paper547/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: \nThis paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. Compared with other existing approaches, such as Bayesian neural networks and variational information bottleneck, the proposed architecture is simpler to implement and faster to train. The authors also achieve state of the art results in various fields, including network compression by pruning, adversarial defense and learning with label noise.\n\nMajor comments: \n- Overall, I find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to VIBNet.\n- The derivation of the max-entropy term is somewhat unclear and I think the paper needs a major revision on this part. The authors suggest using a Gaussian with a finite mean and an infinite variance as the non-informative prior for the produced Gaussian random variable z (in Eq. 1), and then minimize the KL divergence between the produced Gaussian and the infinite-variance Gaussian. However, this may be questionable from a Bayesian viewpoint, in the sense that the infinite variance leads to an improper prior as the variance increases without bound and thus may produce an improper posterior distribution. This is not discussed and needs to be clarified in Sect. 2 (Max-entropy Regularization).\n- There are things unclear in the derivation (last line of Eq. 2), since log(\\sigma_2) trends to infinity.\n- In addition, the penalty terms for different types of tasks are directly given only based on some of the assumptions that the authors have made, there does not seem to be any theoretical justification for such choices.\n\nMinor comments:\n- Some of the notations used in this paper seem a bit confusing, which may hinder readability. For example, on page 3, in \u201cThe non-informative prior is a Gaussian with arbitrary mean (\\mu_1) and infinite variance (\\sigma_1)\u201d, I guess \\sigma means the standard deviation? I would like to recommend using N(\\mu, \\sigma^2) to denote a Gaussian distribution, where \\sigma means the standard deviation and \\sigma^2 the variance.\n- On page 3, in \u201cwhere \\sigma(h|\\theta) denotes the predicted standard deviation of hidden unit h given the neuron uncertainty prediction parameter \\theta\u201d, there is no discussion on the neuron uncertainty prediction parameter. Does it means the predicted standard deviation is again parameterized by \\theta?"}, "signatures": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper547/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple and Effective Stochastic Neural Networks", "authors": ["Tianyuan Yu", "Yongxin Yang", "Da Li", "Timothy Hospedales", "Tao Xiang"], "authorids": ["tianyuan.yu@surrey.ac.uk", "yongxin.yang@surrey.ac.uk", "dali.darren@hotmail.com", "t.hospedales@ed.ac.uk", "t.xiang@surrey.ac.uk"], "keywords": ["stochastic neural networks", "pruning", "adversarial defence", "label noise"], "TL;DR": "In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability.", "abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.", "pdf": "/pdf/c8db792ae0818842b887f606a81491ce5ab0fcfc.pdf", "paperhash": "yu|simple_and_effective_stochastic_neural_networks", "original_pdf": "/attachment/97fdb5a608b5801ab84c7ccaf079879baca46023.pdf", "_bibtex": "@misc{\nyu2020simple,\ntitle={Simple and Effective Stochastic Neural Networks},\nauthor={Tianyuan Yu and Yongxin Yang and Da Li and Timothy Hospedales and Tao Xiang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxeI6EYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxeI6EYwS", "replyto": "SJxeI6EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper547/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575830871028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper547/Reviewers"], "noninvitees": [], "tcdate": 1570237750556, "tmdate": 1575830871042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper547/-/Official_Review"}}}], "count": 8}