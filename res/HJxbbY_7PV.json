{"notes": [{"id": "HJxbbY_7PV", "original": "B1e-WFOQvE", "number": 6, "cdate": 1552283897425, "ddate": null, "tcdate": 1552283897425, "tmdate": 1562082109801, "tddate": null, "forum": "HJxbbY_7PV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "ONLY SPARSITY  BASED  LOSS FUNCTION FOR  LEARNING  REPRESENTATIONS", "authors": ["Vivek Bakaraju", "Kishore Reddy Konda"], "authorids": ["vivek.bakaraju@insofe.edu.in", "konda.kishorereddy@gmail.com"], "keywords": ["Sparsity", "Unsupervised Learning", "Single Layer Models"], "abstract": "We study the emergence of sparse representations in neural networks. We show that in unsupervised\nmodels with regularization, the emergence of sparsity is the result of the input data samples being\ndistributed along highly non-linear or discontinuous manifold. We also derive a similar argument\nfor discriminatively trained networks and present experiments to support this hypothesis. Based\non our study of sparsity, we introduce a new loss function which can be used as regularization\nterm for models like autoencoders and MLPs. Further, the same loss function can also be used\nas a cost function for an unsupervised single-layered neural network model for learning efficient\nrepresentations.", "pdf": "/pdf/a3f6e59530e1de38ca75899a2411107bab4ada62.pdf", "paperhash": "bakaraju|only_sparsity_based_loss_function_for_learning_representations"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "ICLR.cc/2019/Workshop/LLD"}, {"id": "Skl3LnM_KE", "original": null, "number": 1, "cdate": 1554685011814, "ddate": null, "tcdate": 1554685011814, "tmdate": 1555511886296, "tddate": null, "forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Official_Review", "content": {"title": "The paper proposes an interesting loss ruction that relies on independence of datapoints, unfortunately, it also depends on the data to work.", "review": "The authors propose a loss function that encourages different samples to be mapped to different feature vectors. The loss is analyzed both in the context of an autoencoder and a single layer supervised neural network.\nOne issue the authors would consider is introducing more clarity in the exposition of the work. For instance, you never really specify the exact inputs and outputs of your autoencoder. \nMy main criticism would be that OVR overly relies on the quality of the neighboring samples in a minibatch to identify the loss. if you have a mini-batch of datapoints of the same class it would not work at all. However, even in the case where you have a minibatch of 128 and an expected 12 data points of each class for CIFAR-10 I would expect a lot of \"noisy information\" steering the loss towards the wrong result. That problem might alleviate in the  case of many labels and highly diverse datasets.\nI think in total the work leaves a lot to be desired. 1. The loss can be broken by the right construction of the dataset. 2. The loss might do better with a rich dataset which is probably not the case for most applications discussed in the LLD workshop.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ONLY SPARSITY  BASED  LOSS FUNCTION FOR  LEARNING  REPRESENTATIONS", "authors": ["Vivek Bakaraju", "Kishore Reddy Konda"], "authorids": ["vivek.bakaraju@insofe.edu.in", "konda.kishorereddy@gmail.com"], "keywords": ["Sparsity", "Unsupervised Learning", "Single Layer Models"], "abstract": "We study the emergence of sparse representations in neural networks. We show that in unsupervised\nmodels with regularization, the emergence of sparsity is the result of the input data samples being\ndistributed along highly non-linear or discontinuous manifold. We also derive a similar argument\nfor discriminatively trained networks and present experiments to support this hypothesis. Based\non our study of sparsity, we introduce a new loss function which can be used as regularization\nterm for models like autoencoders and MLPs. Further, the same loss function can also be used\nas a cost function for an unsupervised single-layered neural network model for learning efficient\nrepresentations.", "pdf": "/pdf/a3f6e59530e1de38ca75899a2411107bab4ada62.pdf", "paperhash": "bakaraju|only_sparsity_based_loss_function_for_learning_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Official_Review", "cdate": 1553713421406, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421406, "tmdate": 1555511819226, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HJgoCcOJ5E", "original": null, "number": 2, "cdate": 1555167954896, "ddate": null, "tcdate": 1555167954896, "tmdate": 1555511874752, "tddate": null, "forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Official_Review", "content": {"title": "Review of \"ONLY SPARSITY BASED LOSS FUNCTION FOR LEARNING REPRESENTATIONS\"", "review": "The paper discusses a regularisation function that encourages sparsity. Then, the authors use this criterion as a loss to train neural networks in the unsupervised setting. Numerical experiments comparing the approach with other regularization functions are provided.\n\nFirst of all, it seems there are strong formatting issues with the paper. The conclusion is on the 5th page, and the margins are significantly larger than other submitted paper.\n\nGlobally, all the choices of the paper are poorly motivated, as well as the assumptions. It is hard for the reader to be convinced by the (many) claims in this paper. On top of it, the experiments are not convincing at all, as the dropout seems to be as efficient as the proposed approach (in terms of the tradeoff between accuracy and sparsity).\n\nI have many concerns about the author's motivations for the chosen loss. As far as I understand, they penalize the matrix H using a quadratic function (sum of elements in HH^T). However, this seems to me not straightforward why it should encourage sparsity, since\n1) the diagonal elements are taken into account in the regularisation and\n2) the function is quadratic.\n\nFinally, the writing of the paper seems to have been done in haste, as the explanations are not clear at all, and the math formulas are incomplete.\n", "rating": "1: Strong rejection", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ONLY SPARSITY  BASED  LOSS FUNCTION FOR  LEARNING  REPRESENTATIONS", "authors": ["Vivek Bakaraju", "Kishore Reddy Konda"], "authorids": ["vivek.bakaraju@insofe.edu.in", "konda.kishorereddy@gmail.com"], "keywords": ["Sparsity", "Unsupervised Learning", "Single Layer Models"], "abstract": "We study the emergence of sparse representations in neural networks. We show that in unsupervised\nmodels with regularization, the emergence of sparsity is the result of the input data samples being\ndistributed along highly non-linear or discontinuous manifold. We also derive a similar argument\nfor discriminatively trained networks and present experiments to support this hypothesis. Based\non our study of sparsity, we introduce a new loss function which can be used as regularization\nterm for models like autoencoders and MLPs. Further, the same loss function can also be used\nas a cost function for an unsupervised single-layered neural network model for learning efficient\nrepresentations.", "pdf": "/pdf/a3f6e59530e1de38ca75899a2411107bab4ada62.pdf", "paperhash": "bakaraju|only_sparsity_based_loss_function_for_learning_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Official_Review", "cdate": 1553713421406, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper6/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713421406, "tmdate": 1555511819226, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SygMao3GcE", "original": null, "number": 1, "cdate": 1555381177940, "ddate": null, "tcdate": 1555381177940, "tmdate": 1555510978768, "tddate": null, "forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ONLY SPARSITY  BASED  LOSS FUNCTION FOR  LEARNING  REPRESENTATIONS", "authors": ["Vivek Bakaraju", "Kishore Reddy Konda"], "authorids": ["vivek.bakaraju@insofe.edu.in", "konda.kishorereddy@gmail.com"], "keywords": ["Sparsity", "Unsupervised Learning", "Single Layer Models"], "abstract": "We study the emergence of sparse representations in neural networks. We show that in unsupervised\nmodels with regularization, the emergence of sparsity is the result of the input data samples being\ndistributed along highly non-linear or discontinuous manifold. We also derive a similar argument\nfor discriminatively trained networks and present experiments to support this hypothesis. Based\non our study of sparsity, we introduce a new loss function which can be used as regularization\nterm for models like autoencoders and MLPs. Further, the same loss function can also be used\nas a cost function for an unsupervised single-layered neural network model for learning efficient\nrepresentations.", "pdf": "/pdf/a3f6e59530e1de38ca75899a2411107bab4ada62.pdf", "paperhash": "bakaraju|only_sparsity_based_loss_function_for_learning_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper6/Decision", "cdate": 1554736070392, "reply": {"forum": "HJxbbY_7PV", "replyto": "HJxbbY_7PV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736070392, "tmdate": 1555510968357, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}