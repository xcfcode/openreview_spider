{"notes": [{"tddate": null, "replyto": null, "ddate": null, "writable": true, "tmdate": 1487903055966, "tcdate": 1478289214574, "number": 392, "replyCount": 0, "id": "Bk8BvDqex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bk8BvDqex", "signatures": ["~Jessica_B_Hamrick1"], "readers": ["everyone"], "content": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396561028, "tcdate": 1486396561028, "number": 1, "id": "Bkt5nzLul", "invitation": "ICLR.cc/2017/conference/-/paper392/acceptance", "forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper fairly clearly presents a totally sensible idea. The details of the method presented in this paper are clearly preliminary, but is enough to illustrate a novel approach.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396561538, "id": "ICLR.cc/2017/conference/-/paper392/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396561538}}}, {"tddate": null, "tmdate": 1484163398117, "tcdate": 1484163398117, "number": 4, "id": "H1RHtWNIg", "invitation": "ICLR.cc/2017/conference/-/paper392/public/comment", "forum": "Bk8BvDqex", "replyto": "H1sj61Qrx", "signatures": ["~Jessica_B_Hamrick1"], "readers": ["everyone"], "writers": ["~Jessica_B_Hamrick1"], "content": {"title": "Response to reviewer 3", "comment": "Thank you very much for your detailed comments! We have uploaded a new version of the paper with these issues addressed. We have also added a section in the appendix about training and convergence issues (Section D.3). Briefly, we did encounter some difficulties with training (particularly with high numbers of ponder steps or with high ponder costs), which we largely attribute to the entropy penalty that we added to the REINFORCE loss.\n\nRegarding the impact and scope of our work, we would like to emphasize that our approach can potentially be applied to *any* reinforcement learning or control task, and evaluating how well our approach works on a wide range of specific RL and control problems is an important direction for future research. If our metacontroller architecture does scale well to these other domains, it has the potential for significant impact. For example, our approach may greatly reduce the amount of hand-tuning of architectures by researchers (by automatically identifying what, and how many, computations to perform). It also provides a means for addressing the important and open problem of how to best use inaccurate models.\n\nAdditionally, we note that although the domain we evaluated in this paper is constrained, it allowed us to experimentally test out hypotheses about our architecture and draw clear conclusions that would otherwise be difficult to draw in a more complex domain with confounding factors. For example, by using an object-centric approach rather than training the network directly on pixel data, we were able to conclude that the five-planet scenes were genuinely more difficult to predict than the one-planet scenes. This would have been impossible to do if we had trained directly on pixel data because of confounds: with increasing numbers of planets, there are an increasing number of latent factors that need to be inferred, which also likely makes the task more difficult. Further, by choosing the domain that we did, we were able to vary the number and location of the planets, thus manipulating the difficulty of the task and demonstrating that the metacontroller architecture does in fact learn to spend more resources on difficult scenes.\n\nHere are responses to the specific questions:\n\n1) The different thicknesses of the lines have no significance, and we have updated the figure so that all the lines are the same length. We have also updated the figure caption to explain differences in the styles (e.g. arrowhead vs. circle).\n\n2) The different colors refer to different settings of \ud835\uded5 (ponder cost). We have updated the figure and caption to make the different colors clearer.\n\n3) All of the regression lines indeed have a slope and Pearson correlation that is significantly greater than zero. We have included these statistics in the figure.\n\n4) Yes, there is some variance in the fractions from experiment to experiment which we believe are due to the entropy penalty added to the REINFORCE loss in order to discourage the policy from becoming deterministic. We have added some discussion about this point to the appendix in Section D.3.\n\n5) Thank you, we are glad you found the supplemental material helpful!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287593341, "id": "ICLR.cc/2017/conference/-/paper392/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk8BvDqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper392/reviewers", "ICLR.cc/2017/conference/paper392/areachairs"], "cdate": 1485287593341}}}, {"tddate": null, "tmdate": 1484163355117, "tcdate": 1484163355117, "number": 3, "id": "HkQ7t-4Ug", "invitation": "ICLR.cc/2017/conference/-/paper392/public/comment", "forum": "Bk8BvDqex", "replyto": "BJN6F-dBx", "signatures": ["~Jessica_B_Hamrick1"], "readers": ["everyone"], "writers": ["~Jessica_B_Hamrick1"], "content": {"title": "Response to reviewer 4", "comment": "Thank you very much for your review! We have uploaded a new version of the paper which addresses the issues you brought up.\n\nWe are not entirely sure what you have in mind when you mention the applicability to other types of optimization. Broadly, our approach applies to any form of iterative optimization procedure. The acquisition/selection function in our approach (i.e., the controller) does not directly correspond to that used in traditional iterative procedures (e.g. Bayesian optimization, gradient descent), but could easily be replaced by these methods as long as they are differentiable. Other iterative procedures such as value iteration could also be used with our approach, for example by using the manager in conjunction with a value iteration network (Tamar et al., NIPS 2016).\n\nThe only method we used to ameliorate problems with REINFORCE was the entropy term discussed in Section B.4. In general, we did not have many problems with REINFORCE because both the size of the action space and the length of the horizon were quite small (maximum 3 and 11, respectively). We believe the entropy term may have caused other problems, however, and have added a discussion about this and other convergence details in Section D.3.\n\nResponses to the specific points indicated above:\n\na) Thank you for the suggestion of more clearly demarcating what is part of the metacontroller. We have updated Figure 1a to include a box around the metacontroller components (which is everything except the scene and the world).\n\nb) We have added x-axis tick marks to Figure 3, as suggested. Unfortunately the confidence intervals in the top row of subplots are already at the thinnest width. However, we have updated the line colors to match the color of the points to make them a little easier to tell apart. We have also changed the colors of the plot to make it clearer which lines correspond to which values of \ud835\uded5 (ponder cost).\n\nc) We have fixed this typo, thank you for catching it!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287593341, "id": "ICLR.cc/2017/conference/-/paper392/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk8BvDqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper392/reviewers", "ICLR.cc/2017/conference/paper392/areachairs"], "cdate": 1485287593341}}}, {"tddate": null, "tmdate": 1484163313691, "tcdate": 1484163313691, "number": 2, "id": "HJ5gYZVIe", "invitation": "ICLR.cc/2017/conference/-/paper392/public/comment", "forum": "Bk8BvDqex", "replyto": "r1rWhbJLl", "signatures": ["~Jessica_B_Hamrick1"], "readers": ["everyone"], "writers": ["~Jessica_B_Hamrick1"], "content": {"title": "Response to reviewer 5", "comment": "Thank you for your comments!\n\nWe agree that the formalism is difficult to understand without a specific example. To address this, we have updated the caption of Figure 1A to give specific examples of what all the components are. We have also provided algorithm boxes as suggested in the appendix.\n\nAs far as we are aware, there are no existing models outside the metareasoning framework that explicitly reason about all three of the following: *which* computations to perform (controller), *how long* to compute for (manager), and *how* to actually do the computation (manager + experts). There are, of course, many existing models that address one or more of these. For example, in Bayesian optimization, the controller is the selection criterion (e.g., expected improvement, probability of improvement, UCB); there is no manager; and there is one expert, which is the function to be optimized. In gradient descent, the controller is the gradient step; there is no manager; and there is one expert, which is the gradient of the function to be optimized.\n\n\nThank you for pointing us to the idea of optimizing with respect to expected improvement (EI) per second. This is certainly a related idea, in that by minimizing for time cost our metacontroller is also essentially trying to improve as quickly as possible (though the metacontroller does not explicitly choose those actions which have the highest rate of improvement -- it performs this optimization over the entire sequence of actions).\n\nWe are also excited about doing more experiments with more experts in future work!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287593341, "id": "ICLR.cc/2017/conference/-/paper392/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk8BvDqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper392/reviewers", "ICLR.cc/2017/conference/paper392/areachairs"], "cdate": 1485287593341}}}, {"tddate": null, "tmdate": 1483836413256, "tcdate": 1483836413256, "number": 4, "id": "r1rWhbJLl", "invitation": "ICLR.cc/2017/conference/-/paper392/official/review", "forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "signatures": ["ICLR.cc/2017/conference/paper392/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper392/AnonReviewer5"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "A well written paper and an interesting construction - I thoroughly enjoyed reading it. \n\nI found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well.\n\nIn Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework.\n\nExperimental results were presented clearly and well illustrated the usefulness of the metacontroller. I'm curious to see the results of using more metaexperts.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483836413934, "id": "ICLR.cc/2017/conference/-/paper392/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper392/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper392/AnonReviewer1", "ICLR.cc/2017/conference/paper392/AnonReviewer3", "ICLR.cc/2017/conference/paper392/AnonReviewer4", "ICLR.cc/2017/conference/paper392/AnonReviewer5"], "reply": {"forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483836413934}}}, {"tddate": null, "tmdate": 1483377084005, "tcdate": 1483377084005, "number": 3, "id": "BJN6F-dBx", "invitation": "ICLR.cc/2017/conference/-/paper392/official/review", "forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "signatures": ["ICLR.cc/2017/conference/paper392/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper392/AnonReviewer4"], "content": {"title": "Using metacontroller optimization produces more efficient learning on one-shot control task", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Pros (quality, clarity, originality, significance:):\n\nThis paper presents a novel metacontroller optimization system that learns the best action for a one-shot learning task, but as a framework has the potential for wider application. The metacontroller is a model-free reinforcement learning agent that selects how many optimization iterations and what function or \u201cexpert\u201d to consult from a fixed set (such as an action-value or state transition function). Experimental results are presented from simulation experiments where a spacecraft must fire its thruster once to reach a target location, in the presence of between 1 and 5 heavy bodies.\n\nThe metacontroller system has a similar performance loss on the one-shot learning task as an iterative (standard) optimization procedure. However, by taking into account the computational complexity of running a classical, iterative optimization procedure as a second \u201cresource loss\u201d term, the metacontroller is shown to be more efficient. Moreover, the metacontroller agent successfully selects the optimal expert to consult, rather than relying on an informed choice by a domain-expert model designer. The experimental performance is a contribution that merits publication, and it also exhibits the use of an interaction network for learning the dynamics of a simulated physical system. The dataset that has been developed for this task also has the potential to act as a new baseline for future work on one-shot physical control systems. The dataset constitutes an ancillary contribution which could positively impact future research in this area.\n\nCons:\nIt's not clear how this approach could be applied more broadly to other types of optimization. Moreover, the REINFORCE gradient estimation method is known to suffer from very high variance, yielding poor estimates. I'm curious what methods were used to ameliorate these problems and if any other performance tricks were necessary to train well. Content of this type this could form a useful additional appendix.\n\nA few critiques on the communication of results:\n\n- The formal explication of the paper\u2019s content is clear, but Fig.\u2019s 1A and 3 could be improved. Fig. 1A is missing a clear visual demarcation of what exactly the metacontroller agent is. Have you considered a plate or bounding box around the corresponding components? This would likely speed the uptake of the formal description.\n\n- Fig. 3 is generally clear, but the lack of x-axis tick marks on any subplots makes it more challenging than necessary to compare among the experts. Also, the overlap among the points and confidence intervals in the upper-left subplot interferes with the quantitative meaning of those symbols. Perhaps thinner bars of different colors would help here. Moreover, this figure lacks a legend and so the different lines are impossible to compare with each other.\n\n- Lastly, the second sentence in Appendix B. 2 is a typo and terminates without completion.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483836413934, "id": "ICLR.cc/2017/conference/-/paper392/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper392/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper392/AnonReviewer1", "ICLR.cc/2017/conference/paper392/AnonReviewer3", "ICLR.cc/2017/conference/paper392/AnonReviewer4", "ICLR.cc/2017/conference/paper392/AnonReviewer5"], "reply": {"forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483836413934}}}, {"tddate": null, "tmdate": 1483042211148, "tcdate": 1483042211148, "number": 2, "id": "H1sj61Qrx", "invitation": "ICLR.cc/2017/conference/-/paper392/official/review", "forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "signatures": ["ICLR.cc/2017/conference/paper392/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper392/AnonReviewer3"], "content": {"title": "Pondering on expert advice for control", "rating": "7: Good paper, accept", "review": "This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory \u00ab experts \u00bb is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (\u00ab pondering \u00bb) even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.\n\nThe paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.\n\nHere are a few specific comments, questions and suggestions:\n\n1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style \u2014 do these mean different things? \n\n2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the \u00ab bottom row \u00bb part. This makes understanding this figure difficult.\n\n3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). \n\n4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?\n\n5) the supplementary materials are very helpful. Thank you for all these details.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483836413934, "id": "ICLR.cc/2017/conference/-/paper392/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper392/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper392/AnonReviewer1", "ICLR.cc/2017/conference/paper392/AnonReviewer3", "ICLR.cc/2017/conference/paper392/AnonReviewer4", "ICLR.cc/2017/conference/paper392/AnonReviewer5"], "reply": {"forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483836413934}}}, {"tddate": null, "tmdate": 1482957467341, "tcdate": 1482957467341, "number": 1, "id": "S1QsfoWrg", "invitation": "ICLR.cc/2017/conference/-/paper392/public/comment", "forum": "Bk8BvDqex", "replyto": "SkVdaZRNl", "signatures": ["~Jessica_B_Hamrick1"], "readers": ["everyone"], "writers": ["~Jessica_B_Hamrick1"], "content": {"title": "Response to reviewer 1", "comment": "We thank the reviewer for the comments and are glad that they enjoyed reading the paper! We agree this approach bears some similarities to ensemble learning methods. However, we also note ours is quite distinct from typical ensemble learning approaches in that we use a different expert depending on the input, rather than aggregating or selecting from the results of multiple ensemble models.\n\nWe also agree that it would be interesting to see how our approach handles more than two experts. Indeed, one can imagine many potential experts that might be available for a given problem beyond the MLP and interaction net experts used in the paper: for example, a \u201ccoarse\u201d simulation expert (which uses an accurate dynamics model but which also uses a larger step size for time integration, \u025b, than the true simulation), a vanilla LSTM expert (rather than an interaction network), an \u201cepisodic\u201d expert (which has an episodic memory to remember previous actions it has tried in the world), etc. Having more than just one or two experts might be particularly useful when those experts specialize in predictions of different parts of the state space, or at different levels of specificity. For example, the metacontroller could potentially learn to execute a \u201ccoarse-to-fine\u201d search by utilizing a number of simulation experts with different granularities of time integration, where the \u201ccoarser\u201d experts have lower ponder costs. Exploring what policies the metacontroller learns when it has access to more diverse experts such as these is an important future direction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287593341, "id": "ICLR.cc/2017/conference/-/paper392/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk8BvDqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper392/reviewers", "ICLR.cc/2017/conference/paper392/areachairs"], "cdate": 1485287593341}}}, {"tddate": null, "tmdate": 1482722668057, "tcdate": 1482722668057, "number": 1, "id": "SkVdaZRNl", "invitation": "ICLR.cc/2017/conference/-/paper392/official/review", "forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "signatures": ["ICLR.cc/2017/conference/paper392/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper392/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Thank you for an interesting read on an approach to choose computational models based on kind of examples given.\n\nPros\n- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. \n\n- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.\n\n\nCons\n- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "pdf": "/pdf/da8a506167be0f352375d2aeb90379f95feebe76.pdf", "TL;DR": "We present a \"metacontroller\" neural architecture which can adaptively decide how long to run an model-based online optimization procedure for, and which models to use during the optimization.", "paperhash": "hamrick|metacontrol_for_adaptive_imaginationbased_optimization", "keywords": ["Deep learning", "Reinforcement Learning", "Optimization"], "conflicts": ["berkeley.edu", "google.com", "mit.edu"], "authors": ["Jessica B. Hamrick", "Andrew J. Ballard", "Razvan Pascanu", "Oriol Vinyals", "Nicolas Heess", "Peter W. Battaglia"], "authorids": ["jhamrick@berkeley.edu", "aybd@google.com", "razp@google.com", "vinyals@google.com", "heess@google.com", "peterbattaglia@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483836413934, "id": "ICLR.cc/2017/conference/-/paper392/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper392/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper392/AnonReviewer1", "ICLR.cc/2017/conference/paper392/AnonReviewer3", "ICLR.cc/2017/conference/paper392/AnonReviewer4", "ICLR.cc/2017/conference/paper392/AnonReviewer5"], "reply": {"forum": "Bk8BvDqex", "replyto": "Bk8BvDqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper392/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483836413934}}}], "count": 10}