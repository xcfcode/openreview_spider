{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396334941, "tcdate": 1486396334941, "number": 1, "id": "HJPnsfIue", "invitation": "ICLR.cc/2017/conference/-/paper63/acceptance", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396335478, "id": "ICLR.cc/2017/conference/-/paper63/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396335478}}}, {"tddate": null, "tmdate": 1485188872390, "tcdate": 1481818285725, "number": 1, "id": "ByI2eHlNx", "invitation": "ICLR.cc/2017/conference/-/paper63/official/review", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/conference/paper63/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper63/AnonReviewer1"], "content": {"title": "Final Review", "rating": "3: Clear rejection", "review": "In the question response the authors mention and compare other works such as \"Learning to Learn by Gradient Descent by Gradient Descent\", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.\nThe network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.\nAs discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.\nIn summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711243, "id": "ICLR.cc/2017/conference/-/paper63/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper63/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper63/AnonReviewer1", "ICLR.cc/2017/conference/paper63/AnonReviewer3", "ICLR.cc/2017/conference/paper63/AnonReviewer2"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711243}}}, {"tddate": null, "tmdate": 1482255316802, "tcdate": 1482255316802, "number": 3, "id": "SyaCjJv4x", "invitation": "ICLR.cc/2017/conference/-/paper63/official/review", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/conference/paper63/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper63/AnonReviewer2"], "content": {"title": "Interesting application of actor-critic methods, but difficult to assess relative to other adaptive learning algorithms", "rating": "5: Marginally below acceptance threshold", "review": "The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:\n\n-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.\n-Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.\n-Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?\n-In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711243, "id": "ICLR.cc/2017/conference/-/paper63/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper63/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper63/AnonReviewer1", "ICLR.cc/2017/conference/paper63/AnonReviewer3", "ICLR.cc/2017/conference/paper63/AnonReviewer2"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711243}}}, {"tddate": null, "tmdate": 1481838537172, "tcdate": 1481838537166, "number": 2, "id": "S1bRJcgEg", "invitation": "ICLR.cc/2017/conference/-/paper63/official/review", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/conference/paper63/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper63/AnonReviewer3"], "content": {"title": "No comparisons to recent alternatives", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\nthere wouldn't be a question of how well they are being tuned. Beating a previously published result on a well known architecture would be much more convincing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711243, "id": "ICLR.cc/2017/conference/-/paper63/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper63/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper63/AnonReviewer1", "ICLR.cc/2017/conference/paper63/AnonReviewer3", "ICLR.cc/2017/conference/paper63/AnonReviewer2"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711243}}}, {"tddate": null, "tmdate": 1481729119608, "tcdate": 1481729119603, "number": 3, "id": "HyOv41JEl", "invitation": "ICLR.cc/2017/conference/-/paper63/public/comment", "forum": "Sy7m72Ogg", "replyto": "By8BKf17x", "signatures": ["~Chang_Xu1"], "readers": ["everyone"], "writers": ["~Chang_Xu1"], "content": {"title": "New version uploaded", "comment": "We have uploaded a new version. In the appendix, we introduce a \"multi-run\" method which learns actor network from several training runs. The learned actor network is fixed and is applicable to a new run."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287743940, "id": "ICLR.cc/2017/conference/-/paper63/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy7m72Ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper63/reviewers", "ICLR.cc/2017/conference/paper63/areachairs"], "cdate": 1485287743940}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481728489256, "tcdate": 1478177562729, "number": 63, "id": "Sy7m72Ogg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sy7m72Ogg", "signatures": ["~Chang_Xu1"], "readers": ["everyone"], "content": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481258514869, "tcdate": 1480822939917, "number": 2, "id": "ByEjgfZmx", "invitation": "ICLR.cc/2017/conference/-/paper63/public/comment", "forum": "Sy7m72Ogg", "replyto": "By8BKf17x", "signatures": ["~Chang_Xu1"], "readers": ["everyone"], "writers": ["~Chang_Xu1"], "content": {"title": "Re: Pre review questions", "comment": "We appreciate your professional questions and advices.\n\n- The differences between \u201cLearning step size controllers for robust neural network training (Daniel et al.)\u201d and our method are listed below\ni) Input: In the work of Daniel et al., prior knowledge of the learning problem is used as input such as \u201cPredictive change in function value\u201d, \u201cDisagreement of function values\u201d. Our method takes the loss of the training model as input.  \nii) Model: Daniel et al used Relative Entropy Policy Search which is an RL method from the robot learning literature. This method constrains consecutive policy updates should remain \u2018close\u2019 to each other. In our work, we employ the actor-critic algorithm from RL literature, and we propose an improved method to avoid overfitting.\niii) Optimization Objective:\nIn the work of Daniel et al., their method tries to learn a policy which maximizes the average reward. In our method, the goal of the actor is to maximize the expected long term reward, and the objective of the critic is to evaluate the goodness of an actor network.\n\n- In Table 3, the learning rate of SGD follows a standard learning rate decay schedule of the form: \\eta (t) = \\eta_0 (1 + \\gamma t)^{-1}. The best setting for \\eta_0 and \\gamma are chosen by grid search among a candidate set. Experimental results show that our method outperforms SGD with decay schedules.\n\n- The x-axis in figure 2 and 3 represents the number of mini-batches used to train the optimizee model. The data we used to train optimize model and actor critic networks are from the same training set.\n\n- Actually, we are now working on an improved algorithm which basically takes experience from several training runs to update controller. Then fix the controller and apply it to a new training run. This improved algorithm has achieved better experimental results than what is reported in this version. We will introduce this method in detail in the next version.\n\n- Since we aimed to test the effectiveness of our method on different architectures and datasets, in our experiments, we just adopted the two classic structures for MNIST and CIFAR-10 on TensorFlow website without modification. Thank you very much for the suggestion. We will conduct experiments on some state-of-the-art architectures.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287743940, "id": "ICLR.cc/2017/conference/-/paper63/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy7m72Ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper63/reviewers", "ICLR.cc/2017/conference/paper63/areachairs"], "cdate": 1485287743940}}}, {"tddate": null, "tmdate": 1480727701737, "tcdate": 1480665281706, "number": 1, "id": "B1q6dsAfg", "invitation": "ICLR.cc/2017/conference/-/paper63/public/comment", "forum": "Sy7m72Ogg", "replyto": "SJrvO9pze", "signatures": ["~Chang_Xu1"], "readers": ["everyone"], "writers": ["~Chang_Xu1"], "content": {"title": "Re: The actor-crtic networks still need learning rate search", "comment": "Thanks for your very insightful question. Yes, learning rates for actor and critic networks can be learned by using additional actor and critic networks, which seems to be a recurrent problem logically. This is unavoidable, given that all the machine learning algorithms have hyper-parameters. Some other related works face similar problems. Bayesian optimization for learning hyper-parameters also introduce its own hyper-parameters. In another recent work \u201cLearning to Learn by Gradient Descent by Gradient Descent (NIPS 2016)\u201d, the model used to learn gradients also has gradients itself. Those works set their hyper-parameters directly without additional learning. In our paper, we followed this common practice. As we mentioned in section 4.1, we used Adam with the default setting in TensorFlow optimizer toolbox to train the actor and critic networks in all the experiments, without learning the learning rates for the actor and critic networks. We have demonstrated that learning rate of DNNs can be learned automatically, and produced some positive results. If we automatically learn the learning rates for actor and critic networks, it is likely to achieve better results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287743940, "id": "ICLR.cc/2017/conference/-/paper63/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy7m72Ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper63/reviewers", "ICLR.cc/2017/conference/paper63/areachairs"], "cdate": 1485287743940}}}, {"tddate": null, "tmdate": 1480694077603, "tcdate": 1480694077598, "number": 2, "id": "By8BKf17x", "invitation": "ICLR.cc/2017/conference/-/paper63/pre-review/question", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/conference/paper63/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper63/AnonReviewer3"], "content": {"title": "Pre review questions", "question": "- Can you elaborate on how your work differs from \"Learning step size controllers for robust neural network training\"? They also use RL to train learning rate controllers.\n- The schedule learned by your method seems to mostly decay the learning rate. Have you compared to any standard learning rate decay schedules for SGD? e.g. exponential or linear.\n- Is the separate data used to train the critic included in what is reported on the x-axis in figures 2 and 3?\n- Have you tried taking the learning rate controller from one run, fixing it, and then applying it to a new training run (without updating the controller)?\n- This is more of a comment, but it would be good to see experiments with architectures that are more similar to the current state-of-the-art? This should be very easy to do given that you are working in TensorFlow."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959483577, "id": "ICLR.cc/2017/conference/-/paper63/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper63/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper63/AnonReviewer1", "ICLR.cc/2017/conference/paper63/AnonReviewer3"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959483577}}}, {"tddate": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1480605913402, "tcdate": 1480595548878, "number": 1, "replyCount": 0, "id": "SJrvO9pze", "invitation": "ICLR.cc/2017/conference/-/paper63/pre-review/question", "forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "signatures": ["ICLR.cc/2017/conference/paper63/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper63/AnonReviewer1"], "content": {"title": "The actor-crtic networks still need learning rate search", "question": "Interesting idea, but doesn't the actor and critic networks still needs hyper-parameter search for their learning rate? As the authors mentioned, they did the grid search for learning rate for those models. So seems like a circular problem, or am I missing something?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "pdf": "/pdf/7092cca2d341f4546ef5dd82256bdaf0045a988d.pdf", "TL;DR": "We propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning.", "paperhash": "xu|an_actorcritic_algorithm_for_learning_rate_learning", "conflicts": ["nankai.edu.cn", "microsoft.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Chang Xu", "Tao Qin", "Gang Wang", "Tie-Yan Liu"], "authorids": ["changxu@nbjl.nankai.edu.cn", "taoqin@microsoft.com", "wgzwp@nbjl.nankai.edu.cn", "tie-yan.liu@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959483577, "id": "ICLR.cc/2017/conference/-/paper63/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper63/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper63/AnonReviewer1", "ICLR.cc/2017/conference/paper63/AnonReviewer3"], "reply": {"forum": "Sy7m72Ogg", "replyto": "Sy7m72Ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper63/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959483577}}}], "count": 10}