{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028595829, "tcdate": 1490028595829, "number": 1, "id": "Hk2VdYTje", "invitation": "ICLR.cc/2017/workshop/-/paper96/acceptance", "forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028596417, "id": "ICLR.cc/2017/workshop/-/paper96/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028596417}}}, {"tddate": null, "tmdate": 1489519111930, "tcdate": 1489519111930, "number": 2, "id": "S1gMz6rjl", "invitation": "ICLR.cc/2017/workshop/-/paper96/public/comment", "forum": "rJeYrsEYg", "replyto": "ry_pSceie", "signatures": ["~Matthias_Meyer1"], "readers": ["everyone"], "writers": ["~Matthias_Meyer1"], "content": {"title": "Response", "comment": "Thank you for your review and your feedback. The missing citation will be corrected in the next paper revision.\n\nWe understand your point that the paper is quite specific in its application, which might not be the preferred application for some people at ICLR, but we submitted the paper to ICLR Workshop Track because the conference topics in the Call for Abstracts include\n\n+ Unsupervised, semi-supervised, and supervised representation learning\n+ Applications in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487349113187, "tcdate": 1487349113187, "id": "ICLR.cc/2017/workshop/-/paper96/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper96/reviewers"], "reply": {"forum": "rJeYrsEYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487349113187}}}, {"tddate": null, "tmdate": 1489519069696, "tcdate": 1489519069696, "number": 1, "id": "rJ8yzarjg", "invitation": "ICLR.cc/2017/workshop/-/paper96/public/comment", "forum": "rJeYrsEYg", "replyto": "S1Lv_ZMjx", "signatures": ["~Matthias_Meyer1"], "readers": ["everyone"], "writers": ["~Matthias_Meyer1"], "content": {"title": "Response", "comment": "Thank you very much for the valuable feedback.\n\nIn contrast to images or videos, acoustic events are almost solely characterized by temporal changes. Considering this temporal change is necessary for a good classification (see references in the paper) whereas much information about a video can already be identified from still images. The predictive autoencoder was used instead of a normal autoencoder to exploit this time dependency and has shown better results. These experiments are not part of the current version of the paper due to the page limit. \n\nThe submitted paper reflects the state of the work and its core idea and thus has been submitted to the Workshop Track. Therefore the comparison to other methods is missing but indeed necessary for a full evaluation of the proposed method. This is being worked on at the moment. However, we see a fundamental difference to the mentioned approaches (VAE, ladder networks). Due to the pairwise loss an inter-sample comparison is achieved, while the mentioned methods only optimize for the current input sample. From the paper it can be seen that due to this inter-sample comparison we can not only extract features but can make them distinct, which helps for the intended exploration of a dataset. Having said this, a comparison to these other approaches will definitely strengthen the paper.\n\nThe dataset has been chosen to be close to the designated application. Therefore key aspects of the presented work rely on the specific application scenario (e.g. time-dependency, variety of sound sources). Available references like for the TIMIT dataset are not suitable for a fair comparison, since the applied algorithms are optimized for speech/phonetic classification while our proposed approach is designed to work for general audio analysis without prior knowledge.\n\nDespite its relation to the application the used AED dataset has been chosen because it contains a large number of samples per category (around 20 minutes/category), which is beneficial to train the network, whereas ESC-50 ( https://github.com/karoldvl/ESC-50 ) and DCASE2016 ( http://www.cs.tut.fi/sgn/arg/dcase2016/ ) contain less training samples (3 minutes/category and <1 minute/category, respectively). Therefore a meaningful comparison between the different datasets with the settings from the current paper was not possible. However, the recently released AudioSet ( https://research.google.com/audioset/ ) can fill this gap."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487349113187, "tcdate": 1487349113187, "id": "ICLR.cc/2017/workshop/-/paper96/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper96/reviewers"], "reply": {"forum": "rJeYrsEYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487349113187}}}, {"tddate": null, "tmdate": 1489274974046, "tcdate": 1489274974046, "number": 2, "id": "S1Lv_ZMjx", "invitation": "ICLR.cc/2017/workshop/-/paper96/official/review", "forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "signatures": ["ICLR.cc/2017/workshop/paper96/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper96/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a convLSTM based audio frame prediction approach as a method of unsupervised learning of representative audio features.  Proposed model is  trained using a combination of mean squared error as well as a pair wise similarity measure.  Model and the training approach are evaluated on the task of audio event classification.\n\nWhile the combination of the ideas is novel, the individual elements model and training approach are previously known.  It is also not intuitively clear why a predictive auto-encoding would be a good unsupervised feature learning approach for the task of audio event classification, thus I\u2019d like to see comparisons with some well known basic unsupervised feature learning approaches (e.g. VAE, ladder networks, etc.).\n\nResults are presented on an audio event detection dataset which is relatively new and not many reference comparisons are available.  To make the paper stronger I\u2019d also advise authors provide comparisons with other known results on this task, and also to apply their feature learning approach to other well established sound classification tasks (e.g. phone classification in TIMIT).\n\nOverall I feel the paper is not strong enough in current shape for ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489274974893, "id": "ICLR.cc/2017/workshop/-/paper96/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper96/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper96/AnonReviewer2", "ICLR.cc/2017/workshop/paper96/AnonReviewer1"], "reply": {"forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper96/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper96/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489274974893}}}, {"tddate": null, "tmdate": 1489180095608, "tcdate": 1489180095608, "number": 1, "id": "ry_pSceie", "invitation": "ICLR.cc/2017/workshop/-/paper96/official/review", "forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "signatures": ["ICLR.cc/2017/workshop/paper96/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper96/AnonReviewer2"], "content": {"title": "review", "rating": "5: Marginally below acceptance threshold", "review": "This paper combines several ideas, ConvLSTM autoencoders and a pairwise lose. The idea is to do sound classification/clustering.\n\nI feel this paper is more suited towards the signal processing community (i.e., ICASSP/INTERSPEECH). The main problem I have with this paper/task it seems too specific and there isn't enough core-ML contributions for this round of ICLR workshop acceptance. Sequence autoencoders (see Dai et al.,) and ConvLSTM (as cited by authors Zhang et al.,) and pair wise losses (see SIGIR) are not new. Merging all these ideas together is a contribution, but I am not sure it would generate a lot of interest in the ICLR community.\n\nNote:\nThis reviewer is unfamiliar w/ the \"acoustic event dataset (AED) from Takahashi et al. (2016)\" used in evaluation.\n\nCitations Missing:\nhttps://arxiv.org/pdf/1511.01432.pdf (for sequence autoencoders which this model is quite similar).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489274974893, "id": "ICLR.cc/2017/workshop/-/paper96/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper96/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper96/AnonReviewer2", "ICLR.cc/2017/workshop/paper96/AnonReviewer1"], "reply": {"forum": "rJeYrsEYg", "replyto": "rJeYrsEYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper96/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper96/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489274974893}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487360285443, "tcdate": 1487349112519, "number": 96, "id": "rJeYrsEYg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJeYrsEYg", "signatures": ["~Matthias_Meyer1"], "readers": ["everyone"], "content": {"title": "Unsupervised Feature Learning for Audio Analysis", "abstract": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.", "pdf": "/pdf/814eb4492508dc2d750bc3c27521f1e74e364be1.pdf", "TL;DR": "Novel method to train autoencoders by introducing pairwise loss. Use case: Unsupervised audio analysis with an audio frame predictor using Convolutional LSTM layers", "paperhash": "meyer|unsupervised_feature_learning_for_audio_analysis", "keywords": [], "conflicts": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"], "authors": ["Matthias Meyer", "Jan Beutel", "Lothar Thiele"], "authorids": ["matthias.meyer@tik.ee.ethz.ch", "beutel@tik.ee.ethz.ch", "thiele@tik.ee.ethz.ch"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}