{"notes": [{"id": "S1eRbANtDB", "original": "B1lsXdNuDH", "number": 985, "cdate": 1569439238069, "ddate": null, "tcdate": 1569439238069, "tmdate": 1583912048071, "tddate": null, "forum": "S1eRbANtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "KpX5uMudwp", "original": null, "number": 1, "cdate": 1576798711489, "ddate": null, "tcdate": 1576798711489, "tmdate": 1576800924889, "tddate": null, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "All reviewers come to agreement that this is a solid paper worth publishing at ICLR; the authors are encouraged to incorporate additional comments suggested by reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727956, "tmdate": 1576800280268, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper985/-/Decision"}}}, {"id": "HkgPT3nJqH", "original": null, "number": 2, "cdate": 1571962047064, "ddate": null, "tcdate": 1571962047064, "tmdate": 1574411785693, "tddate": null, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Summary:\n\nThis paper proposed a data-driven method of selecting a linkage-based clustering algorithm from a large space. The space of algorithms is parameterized by two sets of parameters which indicate the convex combinations of metrics and merge functions. They analyze the sample complexity for small generalization error. An efficient algorithm for searching an empirically optimal algorithm is proposed.\n\nComments:\n\nIn general, I think this is a good quality paper. \n- Selecting a clustering algorithm from a large space by a data-driven method is an interesting and sound idea, which makes a lot of sense to me. \n- The theorem for generalization error is strong.\n\nIt can be further improved in the following aspects (mainly the experiments).\n- The curves in Fig 3 all look smooth, so I wondered whether one can simply apply a grid search on [0.1,0.2,...,1.0], the obtained algorithm should also be very good. To demonstrate the advantage and necessity of the proposed search algorithm, I think it better to either conduct an experiment with a higher dimensional search space (instead of only searching \\alpha) or demonstrate a case when there is a sharp turn near the optimal point, so that grid search won't work well.\n- Although the authors have proved the generalization error, it is still better to empirically validate the theoretical result, by showing the training and testing errors along with varying sample sizes.\n\nOverall I like the idea and the theoretical analysis in this paper, but the experimental results could be further improved. Therefore I vote for weak acceptance.\n\n\n----- after reading the response --\n\nI'd like to thank the authors for giving more explanations. Theoretically, I understand the advantages of the proposed algorithm, but still, it is more convincing if stronger experiments can be conducted.\n\nMy score does not change, but overall I advocate to accept this paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576557178051, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper985/Reviewers"], "noninvitees": [], "tcdate": 1570237744072, "tmdate": 1576557178070, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Review"}}}, {"id": "HJx7F1_hoS", "original": null, "number": 3, "cdate": 1573842810611, "ddate": null, "tcdate": 1573842810611, "tmdate": 1573847213274, "tddate": null, "forum": "S1eRbANtDB", "replyto": "HkgY83ahtr", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your careful review and thoughtful comments.\n\n- The key insight behind our sample complexity guarantee is that for the proposed family of algorithms and any clustering instance, we can partition the parameter space into a small number of simple regions where the algorithm output is constant. We rely on the results of Balcan et al. (2019) only for the last step in the proof to convert this structural property into a sample complexity guarantee. In the camera ready version we will clarify this and provide a direct proof of our sample complexity bound - the proof for our specific setting does not really need the subtle machinery of  that work.\n\n- The algorithms we propose in Section 3 completely enumerate the leaves of the execution tree (i.e., they don't stop early once they've found a promising leaf), and therefore the number of nodes visited by DFS and BFS are identical. The key difference is that DFS only keeps one path from root to leaf in memory at a time, and for this algorithm family we are guaranteed that the depth of the tree is $|S|-1$. On the other hand, BFS keeps one level of the tree in memory at a time, which can be significantly larger (e.g., for interpolating between single and complete linkage, our best bound on the width of a level is $O(|S|^8)$. We will clarify this in the camera ready version of the paper.\n\n- We briefly describe a few examples of where the prerequisites of our work are met in the introduction. In general, we have in mind any application where we face a sequence of clustering tasks and we can ask a human to provide the target clusterings for those tasks (e.g., clustering the news articles that appear day to day by topic). In these situations, having a low sample complexity is crucial because we do not want to ask for many target clusterings. We will expand and emphasize the examples in the camera ready version of the paper.\n\n- While our results do not apply to the $k$-means algorithm or Gaussian mixture models, we do not view this as a shortcoming given that hierarchical clustering is widely used and a classic research area. Our related work discusses relationships between our work and a related paper for learning initialization procedures for Lloyd's method for $k$-means clustering. We will include further discussion of these relationships and the applicability of our results.\n\n- We agree that it would be interesting to validate our sample complexity results empirically. See our response to R3's similar comment.\n\n- We will correct the typos and ambiguities you found, thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper985/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eRbANtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper985/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper985/Authors|ICLR.cc/2020/Conference/Paper985/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163065, "tmdate": 1576860546904, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment"}}}, {"id": "r1xup6d2or", "original": null, "number": 4, "cdate": 1573846463921, "ddate": null, "tcdate": 1573846463921, "tmdate": 1573846463921, "tddate": null, "forum": "S1eRbANtDB", "replyto": "HJx7F1_hoS", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment", "content": {"title": "RE: Author response", "comment": "I have read the other reviews and authors' responses. They do not change my view of the paper (\"weak accept\"). That is, I appreciate the theoretical contribution (from a non-expert perspective), but like the other reviewers, I believe stronger empirical results would really strengthen the arguments presented in this work."}, "signatures": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eRbANtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper985/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper985/Authors|ICLR.cc/2020/Conference/Paper985/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163065, "tmdate": 1576860546904, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment"}}}, {"id": "HJgvQy_hjS", "original": null, "number": 2, "cdate": 1573842718638, "ddate": null, "tcdate": 1573842718638, "tmdate": 1573842718638, "tddate": null, "forum": "S1eRbANtDB", "replyto": "HkgPT3nJqH", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your careful review and thoughtful comments.\n\n- Yes, for the specific distributions in our experiments section, using grid search with a sufficiently fine grid would find nearly optimal parameters. However, there are clustering distributions for which the expected cost is not smooth and the set of approximately optimal parameters is an arbitrarily small interval (we will include such an example in the camera ready version). Our proposed methods have two significant advantages over grid search. First, they are guaranteed to return empirically optimal parameters even when the performance is not smooth. Second, they are more efficient than running grid search with a very fine grid. To see this, observe that if the grid has $G$ points, we must run the clustering algorithm $G$ times on every training clustering instance. In contrast, the cost of our proposed methods scales with the number of discontinuities for each instance. If $G$ is bigger than the average number of discontinuities, then the grid search is actually more computationally expensive.\n\n- We agree that it would be interesting to validate our theoretical claims empirically by showing that the performance of parameters are similar across training and testing datasets of various sizes. Our current experimental results support our theory by showing that for several natural distributions over clustering tasks, we can obtain large improvements in performance by combining a pair of merge functions or a pair of metrics. They also show that our proposed optimization algorithms are efficient enough to run on realistically sized clustering instances."}, "signatures": ["ICLR.cc/2020/Conference/Paper985/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eRbANtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper985/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper985/Authors|ICLR.cc/2020/Conference/Paper985/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163065, "tmdate": 1576860546904, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment"}}}, {"id": "B1xD6Cv3iS", "original": null, "number": 1, "cdate": 1573842623215, "ddate": null, "tcdate": 1573842623215, "tmdate": 1573842623215, "tddate": null, "forum": "S1eRbANtDB", "replyto": "rJlGe3uIqS", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your careful review and thoughtful comments.\n\n- At the end of Section 2 we discuss how our analysis can be extended to include merge functions beyond 2-point-based merges. In the camera ready version of the paper we will include clarifications and additional details. We only use the 2-point-based property in Lemma 2. First, we show that restricted to beta belonging to any region of the partition constructed in Lemma 1, any 2-point-based merge function is a linear function of the metric parameter $\\beta$. Second, we use it to count the total number of quadratic functions that must be included in the set Q. We can extend the result to also hold when one of the merge functions is chosen to be average-linkage using the following insights: first, the average-linkage distance between a pair of clusters is always a linear function of the metric parameter $\\beta$. Second, we can replace the bound of $O(|S|^{4L'})$ on the size of $\\mathcal{Q}$ by $O(3^{|S|})$ since that is a bound on the number of ways to choose two clusters from $|S|$ points. This exponential increase in the size of $\\mathcal{Q}$ corresponds to a linear dependence on $|S|$ in our final sample complexity, since we depend only on the log size of $\\mathcal{Q}$. \n\n- The motivating setting for our experiments is described in the introduction. We are thinking about situations where we encounter a collection of related clustering tasks and where the target clustering is consistent from task to task (e.g., each day we cluster the articles appearing in a newspaper and our goal is always to cluster them by topic). The distributions in our experiments model this type of situation. Each instance includes a random set of points (drawn from a larger classification dataset) and our goal is to find an algorithm that best recovers the target clustering given by the ground-truth labels. If we think of the points as being news articles and the class labels being the unknown topics, then this fits well with our formal problem setup and is consistent with our theoretical results. In contrast, if we were to keep the points fixed but vary the target clustering, no single algorithm will have good performance, since each algorithm can produce only one clustering for the points. "}, "signatures": ["ICLR.cc/2020/Conference/Paper985/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eRbANtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper985/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper985/Authors|ICLR.cc/2020/Conference/Paper985/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163065, "tmdate": 1576860546904, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper985/Authors", "ICLR.cc/2020/Conference/Paper985/Reviewers", "ICLR.cc/2020/Conference/Paper985/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Comment"}}}, {"id": "HkgY83ahtr", "original": null, "number": 1, "cdate": 1571769424757, "ddate": null, "tcdate": 1571769424757, "tmdate": 1572972527081, "tddate": null, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose an approach to learning combinations of (instance-wise) distance metrics and (cluster-wise) merge functions to optimally cluster instances from a  particular data distribution. In particular, given a set of clustering instances (each of which is a set of instances from the domain and their cluster assignment), a set of distance metrics, and a set of merge functions, the proposed approach aims to learn a convex combination of the distance metrics and merge functions to reconstruct the given clusterings.\n\nThe paper has two main contributions. First, a PAC learning type of guarantee is given on the quality of the learned clustering approach. Second, an efficient data structure for identifying the convex combinations is given. A small set of experiments suggests that, in practice, the learned combinations can outperform using single distance metrics and merge functions.\n\nComments\n\nI am not an expert in this area; I had trouble following the details of the theoretical developments. However, I appreciated that intuition was given on both what the theorems and lemmas were showing as well as the main steps of the proofs.\n\nConcerning Theorem 1, it is not exactly clear to me what the contribution is on top of [Balcan et al., 2019]. The text mentions that they already give sample complexity guarantees in what seems like the same setting (piecewise-structured cost function).\n\nThe authors point out that depth-first traversal is a good choice here due to its memory efficiency. However, in cases where the search space is a graph rather than a tree (i.e., there are multiple paths to some nodes), then DFS can exponentially increase the work compared to breadth-first or other search strategies (e.g., [Edelkamp and Schroedl, 2012]). While the name suggests that the \u201cexecution tree\u201d is, indeed, a tree, is this guaranteed to be the case? or could multiple paths lead to the same partition?\n\nFor the experimental evaluation, it seems as though there is no \u201ctest\u201d set of clustering instances. It would be helpful to also include performance of the learned combinations on some test clustering instances to give an idea of how generalizable to approach is to other instances within the data distribution. (Of course, the main contributions of this work are the theoretical developments, so just one or two examples would be sufficient.)\n\nFor motivation, it would be helpful to give some examples where the prerequisites of this work are actually met; that is, cases where sufficiently large number of labeled cluster instances are available, but the generative mechanism of the clusters is not.\n\nFor context, it could be helpful to briefly mention how, if at all, the current results apply to widely-used clustering algorithms such as k-means or Gaussian mixture models.\n\nTypos, etc.\n\nThe references are somewhat inconsistently formatted. Also, some proper nouns in titles are not capitalized (e.g., \u201clloyd\u2019s families\u201d).\n\n\u201cleaves correspond to\u201d -> \u201cleaves corresponding to\u201d\n\nWhat does the \u201cbig-Oh tilde\u201d notation in Theorem 1 mean?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576557178051, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper985/Reviewers"], "noninvitees": [], "tcdate": 1570237744072, "tmdate": 1576557178070, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Review"}}}, {"id": "rJlGe3uIqS", "original": null, "number": 3, "cdate": 1572404201954, "ddate": null, "tcdate": 1572404201954, "tmdate": 1572972526997, "tddate": null, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "invitation": "ICLR.cc/2020/Conference/Paper985/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of learning both the distance metric and a linkage rule from clustering examples. Suppose we have L metrics d_1, \u2026, d_L and L\u2019 linkage rules for hierarchical agglomerative clustering, D_1, \u2026, D_L\u2019 where each rule is a 2-point-based merge function (i.e. computes the distance between some two points in the clusters, examples of such functions are single-linkage and complete-linkage). The paper considers the problem of finding the convex combination of the distance functions and linkage rules which best fits the data. The main result (Theorem 1) is an \\tildeO((L\u2019 + L)^2 L\u2019 /eps^2) uniform convergence bound on the number of clustering instances which are required to learn up to expected loss \\eps the best possible convex combination. The key technical part of the proof is showing that for any fixed clustering the loss function is piecewise-constant with a small number of simple pieces. The overall approach is based on Balcan et al.\u201917 who solve the case when the distance metric is known but the linkage rule is to be learned and Balcan et al. \u201819 who give techniques for the piecewise constant case. Some further results are given which are specific to learning a mix of two merge functions under a single distance metric and the best combination of two metrics when using the complete linkage merge function. Experimental results are given on MNIST, CIFAR-10 and some other fairly small datasets.\n\nThe paper makes a somewhat interesting contribution to the area, but I think can only be seen as a basic step in the general direction. Most of the interesting merge functions used for HAC don\u2019t boil down to simple 2-point-merge rules (average-linkage, Ward\u2019s method, etc.). The sample complexity of the problem is rather prohibitive. In particular, it is unclear to me why the experimental setup in the paper is consistent with the theoretical model -- when i.i.d. clusterings should be sampled from a distribution, why is it ok to just sample 5 random classes from MNIST a bunch of times? In this case the ground truth clustering is fixed and you sample some subset of classes from it each time. This seems like a much simpler setup compared to the general setting considered in the paper.  I would expect a real experimental setup to have all n points be fixed, then you have a distribution over different clusterings on the same set of points which you sample from each time.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper985/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Link", "authors": ["Maria-Florina Balcan", "Travis Dick", "Manuel Lang"], "authorids": ["ninamf@cs.cmu.edu", "tdick@ttic.edu", "manuel.lang@student.kit.edu"], "keywords": ["Data-driven Algorithm Configuration", "Metric Learning", "Linkage Clustering", "Learning Algorithms"], "TL;DR": "We show how to use data to automatically learn low-loss linkage procedures and metrics for specific clustering applications.", "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets.", "pdf": "/pdf/9938c3f64cb34c88f61529dd494643322a66a067.pdf", "paperhash": "balcan|learning_to_link", "_bibtex": "@inproceedings{\nBalcan2020Learning,\ntitle={Learning to Link},\nauthor={Maria-Florina Balcan and Travis Dick and Manuel Lang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eRbANtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/bc96cf892d10855a4c00649ce1a8d728c20a611e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eRbANtDB", "replyto": "S1eRbANtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper985/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576557178051, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper985/Reviewers"], "noninvitees": [], "tcdate": 1570237744072, "tmdate": 1576557178070, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper985/-/Official_Review"}}}], "count": 9}