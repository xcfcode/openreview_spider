{"notes": [{"id": "rke-f6NKvS", "original": "BJgQVDtUvB", "number": 401, "cdate": 1569438984707, "ddate": null, "tcdate": 1569438984707, "tmdate": 1583912029575, "tddate": null, "forum": "rke-f6NKvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "5qyN6HfWO1", "original": null, "number": 1, "cdate": 1576798695353, "ddate": null, "tcdate": 1576798695353, "tmdate": 1576800940226, "tddate": null, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper introduces Value Iteration with Negative Sampling (VINS) algorithm as a method to accelerate RL using expert demonstrations. VINS learns an initial value function that has a smaller value at states not encounter during the demonstrations.\n\nThe reviewers raised several issues regarding the assumptions, theoretical results, and experiments. The method seems to be most natural for robotic control problems. Nonetheless, it seems that the rebuttal addressed most of the concerns, and two of the reviewers increased their scores accordingly. Since we have three Weak Accepts, I believe this paper can be accepted at the conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709645, "tmdate": 1576800258455, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper401/-/Decision"}}}, {"id": "H1gCYBFkcB", "original": null, "number": 3, "cdate": 1571947909527, "ddate": null, "tcdate": 1571947909527, "tmdate": 1574367812359, "tddate": null, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "\nThis paper tackles an issue imitation learning approaches face. More specifically, policies learned in this manner can often fail when they encounter new states not seen in demonstrations. The paper proposes a method for learning value functions that are more conservative on unseen states, which encourages the learned policies to stay within the distribution of training states. Theoretical results are derived to provide some support for the approach. A practical algorithm is also presented and experiments on continuous control tasks display the effectiveness of the method, with particularly good results on imitation learning followed by reinforcement learning.\n\nThe proposed algorithm makes use of a natural intuition, that states visited by the expert probably have higher values, and the paper generally does a good job of supporting the approach through theory and experiments. Although the experiments seem sound, certain experimental details are not completely clear. The theory may also have some restrictive assumptions, limiting its significance.\nOverall, I am divided about this paper. While this submission has the elements of a good paper and the presentation is great, certain concerns make me hesitant to recommend acceptance. I would be willing to increase my score if those points are addressed. \n\nTheory:\n1) My main concern is the applicability of the theorem, due to Assumption 4.2. While the intuition is that there is an action that corrects the next state towards the demonstration states, the theoretical condition is more restrictive. In particular, the following part (paraphrasing): \"there exists an action a_cx that is close to a^bar and that makes a correction towards U\". This condition implies that there are correcting actions near any action a^bar, which sems unrealistic in most cases. For example, in a driving task, say s^bar is a state such that moving back to U require the vehicle to move to the left. Then, consider the action a^bar of steering towards the right (with some angle). There could be no action near a^bar that makes the vehicle turn left towards U. Note that this is not necessarily a pathological situation as described in the text.\n\n2) Also concerning assumption 4.2, I do not see why s^bar' is included in the quantifier of the statement since it is not used afterwards; after \"there exists an action...\" no mention of s^bar' is made.\n\n3) The projection function may not be well-defined if there are multiple states that are closest to the one being projected.\n\n4) It could be said explicitly that the expert policy is assumed to be deterministic. Currently, this is not said outright.\n\nExperiments:\n1) It seems like VINS relies heavily on the assumption that the environment is deterministic to learn an effective model. Was VINS tried in stochastic environments? \n\n2) Data augmentation is used for VINS. This seems like an unfair advantage compared to the baseline competitors since sample efficiency is a key concern to reinforcement learning algorithms.  To make the comparisons fair, either it should be removed or the other algorithms should also receive additional data. How is the performance of VINS without this addition?\n\n3) A description of how the hyperparameters were chosen and their final values would be needed for reproducibility. Also, a discussion of the importance of the hyperparameters and their sensitivity would be informative. For example, I was curious to know the value of \\alpha in Algorithm 2 compared to the ranges the actions could take. \n\n4) I am not convinced that using Q functions would necessarily fail. On p.6, the paragraph \"can we learn conservatively-extrapolated Q-function\" gives some reasoning why this could fail, that we may not want to penalize unseen actions. This is in opposition to the BCQ algorithm [1], which explicitly tries to avoid unseen actions and still has good performance. Trying a variant with Q(s,a) could be worthwhile. \nI am not exactly sure if I understood Appendix A properly but, from my understanding, I do not think the argument made there necessarily invalidates using Q functions. It seems to apply mostly to deterministic expert policies and also Q(s,a) could still have reasonable values due to function approximation (even if the particular action 'a' is not seen in demonstrations). \n\n5) Which RL algorithms were used for the imitation learning + RL set of experiments?\n\n6) For table 1, are the results also averaged over different sets of demonstrations? \n\n7) Are error bars one standard deviation or one standard error (divided by sqrt(n))?\n\n8) For figure 3, using RL without imitation learning would serve as a good additional baseline \n\n9) Ablation study: Trying no negative sampling with a perfect model could isolate the effect of negative sampling. \n\n10) Ablation study: What is the no behavior cloning and perfect model experiment trying to show?\n\n11) I think the name of the algorithm should be modified as \"value iteration\" refers to a specific dynamic programming algorithm for learning value functions, while the proposed algorithm does not resemble this at all. \n\nMinor comments and typos (no impact on score):\n- Using the cross-entropy method as in QTOpt [2] could be used to pick actions in a more refined manner.\n- There is a large amount of blank space on p.8\n- p.3 \"At the test time\" -> \"At test time\"\n- p.4 \"entire states space\" -> \"entire state space\", \"burned to warm up them\" -> \"burned to warm them up\"\n- p.9  \"option 2 by search the action uniformly.\"  -> \"option 2 to search the actions uniformly\"\n\n[1] \"Off-Policy Deep Reinforcement Learning without Exploration\" by Fujimoto et al.\n[2] \"QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation\" by Kalashnikov et al.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877433566, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper401/Reviewers"], "noninvitees": [], "tcdate": 1570237752684, "tmdate": 1575877433579, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Review"}}}, {"id": "S1liW_icKH", "original": null, "number": 1, "cdate": 1571629058726, "ddate": null, "tcdate": 1571629058726, "tmdate": 1574264378410, "tddate": null, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This work tried to address the covariate shift problem in imitation learning, which is due to the mismatch between training and test state distribution and may cause compounding errors. \n\nThe authors proposed the algorithm called value iteration with negative sampling (VINS) of which the main ideas can be summarized as follows. First, value iteration is used on expert trajectories with negative sampling. Specifically, the states that are randomly perturbed from expert\u2019s states were used to enforce (4.1) and (4.2) in the submission (called *conservative extrapolation* requirements in the submission). By doing so, the state values outside the support of expert state visitation distribution become less than those inside the support. In the meantime, temporal-difference (TD) error was minimized to satisfy Bellman consistency among state values. The second main idea of this work is to use *self-correctable policy*, where the approximate dynamics and behavioral-cloning (BC) policy were used to select the action which is expected to give higher value at the successor states. \n\nTo consolidate their ideas, the authors proved Theorem 4.4 showing that state visitation distribution of resultant policy is approximately close to that of an expert under a few assumptions. Empirically, they considered two experiments. In the first experiment, assuming that the environment simulation is not allowed, the performance of VINS was compared with that of BC over 5 tasks, and VINS achieved higher success rates. In the second experiment, assuming the simulation is allowed, VINS was compared with existing methods such as HER+BC, GAIL and Nair et al \u201818 and shown to be much more sample-efficient compared to the selected baselines. \n\nAlthough the theoretical and empirical contributions of this work are clear to me when the environment simulation is not allowed (as shown in the first experiment in Table 1), I think the second experiment, which allows the environment simulation (as shown in Figure 3), is misleading, and this is why I vote weak reject for this work. For instance, we can simply think about GAIL with BC initialization, but it seems to me that GAIL with random initialization was used in the second experiment (since authors mentioned GAIL in OpenAI baselines was used without hyperparameter tuning (https://github.com/openai/baselines/blob/master/baselines/gail/run_mujoco.py#L53)). In addition to it, there have been some recent works on the sample efficiency of imitation learning with environment simulation which are not included as baselines in this work:\n\n[1] GMMIL (Kim and Park, 2018, \u201cImitation Learning via Kernel Mean Embedding\u201d) - cost learning with maximum mean discrepancy minimization leads to sample-efficient training\n\n[2] BGAIL (Jeon et al, 2019, \u201cBayesian Approach to Generative Adversarial Imitation Learning\u201d) - Bayesian cost is helpful for sample-efficient imitation learning\n\n[3] DAC (Kostrikov et al, 2019, \u201cDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\u201d) - Solving reward bias in imitation learning with simulation and using off-policy RL algorithm to enhance the sample efficiency\n\n[4] Sasaki et al, 2019, \u201cSample Efficient Imitation Learning for Continuous Control\u201d - Bypassing cost learning and introducing off-policy RL to enhance the sample efficiency. \n\nEspecially, off-policy imitation learning methods [3], [4] are shown to be extremely sample-efficient compared to GAIL. I think the authors should have compared the performance of VINS + RL with those baselines in the second experiment if they tried to emphasize the sample efficiency of VINS + RL. Otherwise, they should have focused more on the initialization effect of VINS and BC. For example, one can consider the convergence speed of GAIL to the expert performance when policies were initialized with either VINS or BC.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877433566, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper401/Reviewers"], "noninvitees": [], "tcdate": 1570237752684, "tmdate": 1575877433579, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Review"}}}, {"id": "rkxZvWG3iH", "original": null, "number": 6, "cdate": 1573818713390, "ddate": null, "tcdate": 1573818713390, "tmdate": 1573818713390, "tddate": null, "forum": "rke-f6NKvS", "replyto": "Bylgr7yRFr", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We thank the anonymous reviewer 2 for the helpful comments. The review noted that our method is \u201ca novel and highly effective approach\u201d and \u201chas a very significant advantage both in terms of sample complexity and final performance\u201d. The reviewer also has many constructive questions and comments which we will address below. \n\n\n> \u201cFor one, its theoretical guarantees depend on the local reversibility of the dynamics, that is, for small deviations from the desired state, it is possible to return to that state in a single step.  This isn't too significant a restriction, as the ability to recover quickly from small mistakes would seem to be a necessary for any method to be able to provide similar guarantees about its behavior.  \u201c\n \nWe agree with the reviewer on the point that the local-correctability is key to VINS and is suitable for robotic control problems. \n\n> \u201cIt is unclear that VINS would generalize well beyond robotic control domains. \u2026 The bigger issue is the use of the Euclidean metric (or any fixed metric) in the definition of conservative extrapolation.\u201d\nWe agree with the reviewer Euclidean metric are not applicable to certain situations such as pixel space. We design the algorithm to learn self-correctable policies in the robotic control domain. Indeed, we do not expect the technique can directly work in a pixel space as it is.  However, we believe that applying perturbation to the latent representation of images would be a very  promising and interesting direction for future work. Concretely, it\u2019s conceivable that the metric can be the Euclidean distance between the latent representations of the states. (Our theory still applies to this metric, though the metric needs to be learnt.) \n> \u201cIt would be helpful to see how well VINS compares against the alternatives for a much smaller number of demonstrations, say 5-20, a regime where we would expect initial performance to be poor.\u201d\n\nThanks for the suggestions. For the Reach environment, we run our algorithm on 10 demonstration trajectories. And the result is: \n\nVINS = 99.3 +/- 0.1%, BC = 98.6 +/- 0.1%. \n\nFor Push and PickAndPlace environment, we found BC (and VINS) have very low success rate (below 20%) with less than 20 trajectories, and therefore we suspect the results are not very meaningful.  Thus, we conducted experiments on 40 demonstration trajectories. The results are:\n\nPickAndPlace-40: VINS = 40.0 +/- 0.9%, BC = 36.3 +/- 1.7%\nPush-40: VINS = 26.5 +/- 0.6%, BC = 23.5 +/- 0.7%\n\n(Please see Table 4 in Appendix E of the revision for a formal table.)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rke-f6NKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper401/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper401/Authors|ICLR.cc/2020/Conference/Paper401/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172016, "tmdate": 1576860530836, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment"}}}, {"id": "rJlyzZGnjB", "original": null, "number": 5, "cdate": 1573818631382, "ddate": null, "tcdate": 1573818631382, "tmdate": 1573818631382, "tddate": null, "forum": "rke-f6NKvS", "replyto": "S1liW_icKH", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We thank the reviewer for the helpful comments. The review noted that VINS \u201cachieved higher success rates\u201d and is \u201cshown to be much more sample-efficient compared to the selected baselines\u201d. We have updated with the references that the reviewer kindly suggested, and added additional experiments in the revision.  We\u2019ll address the comments below. \n\n---  \u201cAlthough the theoretical and empirical contributions of this work are clear to me when the environment simulation is not allowed (as shown in the first experiment in Table 1), I think the second experiment, which allows the environment simulation (as shown in Figure 3), is misleading, and this is why I vote weak reject for this work. For instance, we can simply think about GAIL with BC initialization, but it seems to me that GAIL with random initialization was used in the second experiment (since authors mentioned GAIL in OpenAI baselines was used without hyperparameter tuning (https://github.com/openai/baselines/blob/master/baselines/gail/run_mujoco.py#L53)). \n\u201c\n\nIn the work of Sasaki et al, 2019 [4], section 5.3, the authors have empirically showed, somewhat surprisingly, that BC-initialized GAIL does work not better than BC itself. We also run the experiment on our tasks and find out that the performance of BC initialized GAIL is also not better than vanilla GAIL. Please see Figure 3 of the revision for the curve of BC+GAIL. \n\nIn general, we do think the baselines that we compare are a set of strong baselines for the manipulation tasks that we are working on. We also note that because we are working with the setting where a sparse reward function is available when environment interaction is allowed, there are a broader set of RL algorithms to use with environment interactions beyond GAIL-related approaches.  We focus on the sparse reward setting because we believe that in many real-world robotic applications a sparse reward is available, and the main goal of imitation learning in this case is to improve the sample efficiency.  In this setting,  GAIL is apparently not very competitive (partly because it does not use the sparse reward). This is also a reason why we are not centering  our work around comparing with GAIL (and its related work), because we are using more information than GAIL uses (and thus achieve better performance). Instead, we focus more on comparing with the strong baselines reported in Nair et al. \n\n---  \u201cI think the authors should have compared the performance of VINS + RL with those baselines in the second experiment if they tried to emphasize the sample efficiency of VINS + RL. Otherwise, they should have focused more on the initialization effect of VINS and BC. For example, one can consider the convergence speed of GAIL to the expert performance when policies were initialized with either VINS or BC. In addition to it, there have been some recent works on the sample efficiency of imitation learning with environment simulation which are not included as baselines in this work. \u201d \n\nThanks for the suggestion of the references and baselines. We have cited these papers. Following the suggestion, we run the official code of  the DAC algorithm in [3] on our environments. The result is formally reported in Figure 3 of the revision. Unfortunately, DAC couldn\u2019t get any non-trivial reward within 100K samples. \n\nWe would like to note that VINS cannot and perhaps should not be used to initialize GAIL because the strength of VINS is to provide a good initial value function. If we only use the policy learned from VINS to initialize GAIL or other RL algorithms, then very likely it won\u2019t be better than using BC to initialize them. The strength of VINS is that it  provides a good value function and dynamics so that a value-based RL algorithm can be initialized with a good initialization of value function, Q-function, and policy. Please see Appendix C for the precise RL algorithm we use, and how we make it compatible with VINS. We cannot initialize the policy of a generic RL algorithm to see the benefit of VINS. We consider this as the main contribution of VINS --- it allows us to fully initialize value-based RL algorithms. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rke-f6NKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper401/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper401/Authors|ICLR.cc/2020/Conference/Paper401/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172016, "tmdate": 1576860530836, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment"}}}, {"id": "HJlj1VxhoH", "original": null, "number": 4, "cdate": 1573811170777, "ddate": null, "tcdate": 1573811170777, "tmdate": 1573811170777, "tddate": null, "forum": "rke-f6NKvS", "replyto": "H1gCYBFkcB", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment", "content": {"title": "Response to R3, Part 3/3", "comment": "Q5: \u201cWhich RL algorithms were used for the imitation learning + RL set of experiments?\u201d\n\nWe use a simple variant of model-based value iteration algorithm, the detailed description of which is deferred to Appendix C due to page limit. We design this new algorithm so that we can use the value and the dynamics learned from VINS more easily.  (We also briefly mentioned it at the last paragraph of Section 5. ) \n\nQ6: \u201cFor table 1, are the results also averaged over different sets of demonstrations? \u201c\n\nNo, we only tried one set of demonstration following the setting in  [1].\n\nQ7: \u201cAre error bars one standard deviation or one standard error (divided by sqrt(n))?\u201d\n\nIt\u2019s one standard error. We have made this more explicit in the revision. \n\nQ8: \u201cFor figure 3, using RL without imitation learning would serve as a good additional baseline \u201c\n\nThanks for the suggestion. Our understanding is that for the sparse reward tasks, the best RL algorithms are HER [2] that we used to generate the expert policy. As reported by Andrychowicz et al. [2], Figure 3, these algorithms typically require 2 million steps to achieve near optimal success rate (e.g., 95%), whereas our algorithm needs about 100K.  \n\nQ9: \u201cAblation study: Trying no negative sampling with a perfect model could isolate the effect of negative sampling. \u201c\n\nThanks for the suggestions. This is indeed a very useful ablation study in addition to those in Table 2. We conducted the suggested experiment and found that indeed no negative sample (NS) with oracle mode is significantly worse than negative sample with oracle model as shown below. \n(Here PickAndPlace-100 means that we have 100 trajectories in the demonstration.) \n\nPickAndPlace-100: \nOracle model w/o NS: 47.2 +/- 1.9%; oracle model w/  NS: 76.3 +/- 1.4%\nPickAndPlace-200:\nOracle model w/o NS: 74.3 +/- 1.0%; oracle model w/ NS: 87.0 +/- 0.7% \nPush-100: \nOracle model w/o NS: 30.7 +/- 0.8%; oracle model w/ NS: 48.7 +/- 1.2%\nPush-200: \nOracle model w/o NS: 41.3 +/- 0.8%; oracle model w/NS: 63.8 +/- 1.3%\n(Please see Table 3 in Appendix E in the revision for a formal table.) \n\nQ10: \u201cAblation study: What is the no behavior cloning and perfect model experiment trying to show?\u201d\n\nBy comparing VINS w/ perfect  w/o BC and VINS w/o BC, we can see how a better model improves the performance (even if BC is not used.) In reality, it\u2019s conceivable that the dynamics can be learned more accurately through other means (by using prior knowledge, or by collecting more data with random exploration, etc.), and thus these experiments also suggest that our methods have a potential to perform better if the dynamics can be more accurate.\n\n\nQ11: \u201cI think the name of the algorithm should be modified as \"value iteration\" refers to a specific dynamic programming algorithm for learning value functions, while the proposed algorithm does not resemble this at all. \u201c\n\nOur algorithm VINS does learn value functions with iterative Bellman equation updates. We note that in line 4 of Algorithm 2, when we minimize the $\\mathcal{L}_{\\textrm{td}}$ part of the loss, we minimize the Bellman error. In our view, this can be viewed as an approximate dynamic programming. (But we are also happy to consider other names if the reviewer has some suggestions.) \n\nMinor comments:\nThanks for pointing out the typos. We\u2019ll fix them in the next version. \n\nFor QT-Opt: Thanks for your kind suggestion. Yes, we agree that better optimization algorithms might result  in superior performance. We will mention this as a potential extension of our algorithm. \n\n[1] Overcoming Exploration in Reinforcement Learning with Demonstrations, Nair et al.\n[2] Hindsight Experience Replay, Andrychowicz et al. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rke-f6NKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper401/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper401/Authors|ICLR.cc/2020/Conference/Paper401/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172016, "tmdate": 1576860530836, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment"}}}, {"id": "BkxaamgniH", "original": null, "number": 3, "cdate": 1573811140837, "ddate": null, "tcdate": 1573811140837, "tmdate": 1573811140837, "tddate": null, "forum": "rke-f6NKvS", "replyto": "H1gCYBFkcB", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment", "content": {"title": "Response to R3, Part 2/3", "comment": "Q2:  \u201cData augmentation is used for VINS. This seems like an unfair advantage compared to the baseline competitors since sample efficiency is a key concern to reinforcement learning algorithms.  To make the comparisons fair, either it should be removed or the other algorithms should also receive additional data. How is the performance of VINS without this addition? \u201c\n\nThanks for the suggestions. We think our comparison is arguably fair, because we only use data augmentation to train our value function.  BC is part of VINS, and the BC component in VINS is not trained with any data augmentation. If it were the case that BC with data augmentation can outperform the vanilla BC, then using the data-augmented version of BC will also likely improve the performance of VINS. \n\nHowever, unfortunately, we observed that BC cannot be improved by any data augmentation that we are aware of. Concretely, we tried the following data augmentation for BC, which is similar to that for the value function in VINS: given two consecutive state-action pairs (s, a) and (s\u2019, a\u2019), where s\u2019 = M(s, a), we sample t ~ Uniform[0, 1] and construct a new data point pair (t s + (1 - t) s\u2019, t a + (1 - t) a\u2019) as the augmented data. After we use this data-augmentation for BC, we found out that the success rate of BC is even worse as shown below (see Table 3 in Appendix E for a formal table): \n\nPickAndPlace-100: with data aug = 58.8 +/- 1.3%, without data aug = 66.8 +/- 1.1%\nPickAndPlace-200: with data aug = 77.4 +/- 0.9%, without data aug = 82.0 +/- 0.8%\nPush-100: with data aug = 30.1 +/- 0.8%, without data aug = 37.3 +/- 1.1%\nPush-200: with data aug = 41.4 +/- 1.6%, without data aug = 51.3 +/- 0.6%\n\nWe suspect that the biases/errors introduced by the data augmentation is particularly harmful for BC, because it lacks a mechanism of self-correction that VINS has. \n\n\nQ3: \u201cA description of how the hyperparameters were chosen and their final values would be needed for reproducibility. Also, a discussion of the importance of the hyperparameters and their sensitivity would be informative. For example, I was curious to know the value of \\alpha in Algorithm 2 compared to the ranges the actions could take. \u201c\n\nFor $\\alpha$, we tried 0.05, 0.1 and 0.2 and found out 0.1 works best. Each entry of the action is within [-1, 1]. The results of  0.05 and 0.1 don\u2019t differ much, while the result for 0.2 is slightly worse.  For $\\lambda$, we tried 15, 20, 25, and found out 20 works best. \nWe found $\\lambda$ is the most sensitive hyper-parameter ---  large $\\lambda$ leads to difficult optimization and small $\\lambda$ leads to not enough negative sampling. We will also make the code of the paper public.  \n\nQ4: \u201cI am not convinced that using Q functions would necessarily fail. On p.6, the paragraph \"can we learn conservatively-extrapolated Q-function\" gives some reasoning why this could fail, that we may not want to penalize unseen actions. This is in opposition to the BCQ algorithm [1], which explicitly tries to avoid unseen actions and still has good performance\u2026 I am not exactly sure if I understood Appendix A properly but, from my understanding, I do not think the argument made there necessarily invalidates using Q functions. \u201d\n\nWe would like to clarify that we are not saying that we shouldn\u2019t penalize unseen actions. We are saying that those unseen actions that can correct the trajectory shouldn\u2019t be penalized. In other words, they are two types of unseen actions. Type I is those actions that are not seen because they are completely wrong actions. These should be penalized, and that\u2019s the key idea of BCQ and a few related approach. The second type is those actions that are only useful to correct the trajectories when the state starts to leave the demonstration set. These actions would have not been useful if all the previous actions are exactly optimal, whereas otherwise they are very useful to prevent the cascading errors. Therefore, penalizing unseen actions may be useful, but it wouldn\u2019t encourage the self-correction behavior that this paper is aiming to learn.  We suspect that the self-correction behavior is very important for imitating perfect demonstration. (Empirically, in the early stage of the project, we have also  implemented negative sampling on Q-function and found out that it does not provide the correction effect even for the toy grid-world  environment in Figure 1.)\n\nWe clarify that Appendix A is not meant to invalidate using Q-functions. The first paragraph in Appendix argues that the false extrapolation issue (without any fix) exists with Q-function as well, which is perhaps a common consensus. The section A.1 only meant to justify that our VINS idea doesn\u2019t suffer from the same extrapolation problem, but it doesn\u2019t invalidate the use of Q-functions or other methods that addresses the use of extrapolation issue of Q-functions. (To some extent, VINS also uses Q-functions --- the Q-function in VINS is a composition of V and M.) \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rke-f6NKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper401/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper401/Authors|ICLR.cc/2020/Conference/Paper401/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172016, "tmdate": 1576860530836, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment"}}}, {"id": "Hyg0iQghsS", "original": null, "number": 2, "cdate": 1573811110431, "ddate": null, "tcdate": 1573811110431, "tmdate": 1573811110431, "tddate": null, "forum": "rke-f6NKvS", "replyto": "H1gCYBFkcB", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment", "content": {"title": "Response to R3, Part 1/3", "comment": "We thank the Anonymous Reviewer 3 for the detailed and helpful comments. The review noted that \u201cgood results on imitation learning followed by reinforcement learning\u201d and \u201ctheoretical results are derived to provide some support for the approach\u201d. The reviewer also has many constructive questions and comments which we will address below. \n\nRegarding the theoretical part of the paper: \n\nQ1: \u201cMy main concern is the applicability of the theorem, due to Assumption 4.2. While the intuition is that there is an action that corrects the next state towards the demonstration states, the theoretical condition is more restrictive. In particular, the following part (paraphrasing): \"there exists an action a_cx that is close to a^bar and that makes a correction towards U\"...\u201d\n\nWe first note that we only require a local correctability,  that is, when we make a minor mistake (an action that is close but not exactly the same as the optimal action), then there is an action that can drive the state closer to the demonstration set. (Please also see the response for Q2 below as well on a more technical level.) We consider this assumption relatively mild. \n\n We agree that the correctability assumption may not apply to every environment, but we would like to argue that it\u2019s close to necessary for imitation learning to work. Consider an environment where it\u2019s possible to make a minor mistake to reach a state that is not correctable. Then this environment is so non-robust that we need to follow exactly the path of the expert policy, and this seems to be fundamentally challenging for learning algorithms without any prior knowledge. \n\nQ2: \u201cAlso concerning assumption 4.2, I do not see why s^bar' is included in the quantifier of the statement since it is not used afterwards; after \"there exists an action...\" no mention of s^bar' is made\u201d.\n\nThe reviewer is right that $\\bar{s}$ is not mentioned after \u201cthere exists an action...\u201d, but it\u2019s mentioned right before it in the phrase \u201cany $\\varepsilon_0$-perturbation s of $\\bar{s}$\u201d to quantify the scope of $s$. In other words, we want to assume correctability, but we don\u2019t need correctability of every state $s$ because that might be unrealistic. Instead, we only require correctability of  a state $s$ that is close to some state $\\bar{s}$ in the demonstrate set. \n\nQ3: \u201cThe projection function may not be well-defined if there are multiple states that are closest to the one being projected.\u201d\n\nFor simplicity, we can (and should) use some arbitrary tiebreaker to define the projection. We\u2019ve clarified this in the revision.   (A slightly mathematically better but involved way would be to define the projection as the set of closest points. Then we need to measure the distance between projections by some distance between the sets, which seems unnecessarily complicated for the purpose of this paper.) \n\nQ4:  \u201cIt could be said explicitly that the expert policy is assumed to be deterministic. Currently, this is not said outright.\u201d\n\nThanks for pointing it out. We are indeed assuming a deterministic expert policy, and we\u2019ve clarified this in the revision. \n\nQuestions regarding the experiments:\n\nQ1: \u201cIt seems like VINS relies heavily on the assumption that the environment is deterministic to learn an effective model. Was VINS tried in stochastic environments? \u201c\n\nTo learn the value function, the implementation of VINS does not require a deterministic model (although the analysis is only for deterministic model for the moment.) With the value function, suppose we also have a good estimator for the stochastic model, we can still use a variant of 4.3 to compute the action. (Precisely, we maximize $\\mathbb{E}[V(M(s,a))]$ where the randomness is from the model $M$.) However, learning a stochastic environment is often challenging, due to various issues such as calibration. \n\nWe also conducted a proof-of-concept experiment on stochastic environments by modifying the Push environment into a stochastic one. (Precisely, we create a stochastic Push environment by which takes in s, and a, and returns the Push(s,a+noise) as the stochastic outcome.) For simplicity, we learn the value function the same as in VINS; but  we only learn a deterministic dynamics model  to approximate the stochastic one, because we consider learning stochastic dynamics  with good calibration is an orthogonal issue and is beyond the scope of this paper. However, even with a learned deterministic dynamics, we found that VINS outperforms BC. As shown below (or formally in Table 4 in Appendix E of the revision), on the stochastic version of Push environment, VINS outperforms BC by about 9% with with both 100 or 200 demonstration trajectories. \n\nStochasticPush-100: VINS = 38.4 +/- 0.9%, BC = 29.6 +/- 1.4%\nStochasticPush-200: VINS = 51.5 +/- 0.7%, BC = 42.5 +/- 0.9%\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rke-f6NKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper401/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper401/Authors|ICLR.cc/2020/Conference/Paper401/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172016, "tmdate": 1576860530836, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper401/Authors", "ICLR.cc/2020/Conference/Paper401/Reviewers", "ICLR.cc/2020/Conference/Paper401/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Comment"}}}, {"id": "Bylgr7yRFr", "original": null, "number": 2, "cdate": 1571840824356, "ddate": null, "tcdate": 1571840824356, "tmdate": 1572972599919, "tddate": null, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "invitation": "ICLR.cc/2020/Conference/Paper401/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work presents the value iteration with negative sampling (VINS) algorithm, a method for accelerating reinforcement learning using expert demonstrations.  In addition to learning an expert policy through behavioral cloning, VINS learns an initial value function which is biased to assign smaller expected values to states not encountered during demonstrations.  This is done by augmenting the demonstration data with states that have been randomly perturbed, and penalizing the value targets for these states by a factor proportional to their Euclidean distance to the original state.  In addition to the policy and value function, VINS also learns a one-step dynamics model used to select actions against the learned value function.  As the value function learned in VINS is only defined with respect to the current state, action values are estimated by sampling future states using the learned model, and computing the value of these sampled states.\n\nEmpirical results on a set of robotic control tasks demonstrate that VINS requires significantly less interaction with the environment to learn a good policy than existing, state of the art approaches given the same set of demonstrations.  While the paper presents a novel and highly effective approach, there are some apparent limitations to the algorithms which should be highlighted, and there is room for improvement in the empirical evaluations.\n\nIt is unclear that VINS would generalize well beyond robotic control domains.  For one, its theoretical guarantees depend on the local reversibility of the dynamics, that is, for small deviations from the desired state, it is possible to return to that state in a single step.  This isn't too significant a restriction, as the ability to recover quickly from small mistakes would seem to be a necessary for any method to be able to provide similar guarantees about its behavior.  The bigger issue is the use of the Euclidean metric (or any fixed metric) in the definition of conservative extrapolation.  Basically, a state is said to be similar to the states observed during the demonstrations if it is close, under the Euclidean metric, to at least one demonstrated state.  This is a reasonable approach in robotic control tasks, where Euclidean distance is a good measure of how similar two configurations are to one another, but it would seem to be unsuitable for domains where the observation space consists of images or other high-dimensional representations.  In those cases, a useful notion of similarity would likely have to be learned from the data.  In such domains, one might imagine that the conservative value function would simply learn to distinguish between real observations, and those that have been perturbed by random noise, which would never be observed in the actual task.\n\nWhile experimental results demonstrate a very significant advantage for VINS both in terms of sample complexity and final performance, results are presented for only two tasks, 'pick-and-place' and 'push', while VINS outperforms the alternatives on these tasks, it is worth noting that its initial performance (without additional environment interact) is not dramatically superior to pure behavioral cloning.  It would be helpful to see how well VINS compares against the alternatives for a much smaller number of demonstrations, say 5-20, a regime where we would expect initial performance to be poor."}, "signatures": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper401/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yupingl@cs.princeton.edu", "huazhe_xu@eecs.berkeley.edu", "tengyuma@stanford.edu"], "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling", "authors": ["Yuping Luo", "Huazhe Xu", "Tengyu Ma"], "pdf": "/pdf/ffd4070fdb6f1ce2691d35def62153b18778730d.pdf", "TL;DR": "We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.", "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.", "keywords": ["imitation learning", "model-based imitation learning", "model-based RL", "behavior cloning", "covariate shift"], "paperhash": "luo|learning_selfcorrectable_policies_and_value_functions_from_demonstrations_with_negative_sampling", "_bibtex": "@inproceedings{\nLuo2020Learning,\ntitle={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},\nauthor={Yuping Luo and Huazhe Xu and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rke-f6NKvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d0ccc952bd716c96138a8473b3cec67fd0c19317.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rke-f6NKvS", "replyto": "rke-f6NKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877433566, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper401/Reviewers"], "noninvitees": [], "tcdate": 1570237752684, "tmdate": 1575877433579, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper401/-/Official_Review"}}}], "count": 10}