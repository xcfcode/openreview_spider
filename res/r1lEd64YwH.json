{"notes": [{"id": "3Qh1t9HZNZ", "original": null, "number": 9, "cdate": 1578305699406, "ddate": null, "tcdate": 1578305699406, "tmdate": 1578305699406, "tddate": null, "forum": "r1lEd64YwH", "replyto": "H1eyyIUhsS", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Justification for the Experiment Used", "comment": "Dear Reviewer 2,\n\nthe reason that we incorporated the results of an auto encoder instead of your proposed solution is not that we are incapable of understanding or implementing your suggestion but, as already said in the previous comment, that we could not perform this analysis during the two week review period. I was on my honeymoon during this time and did not have access to hardware capable of training any kind of new setup. If you didn't like the idea of the auto encoder it would have been nice to know that soon after I suggested it, before I waste days of my honeymoon stressing about adapting the paper to it. It would have also been nice to receive your comment less close to the deadline so we have time to respond to it and justify our decision. We chose the auto encoder as it is a very commonly used way of training a neural network in an unsupervised fashion and we wanted to point out the difference of our approach to other commonly used approaches. Of course we will also perform an analysis on your suggested training set up and I will update the paper as soon as I have the results."}, "signatures": ["ICLR.cc/2020/Conference/Paper628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "r1lEd64YwH", "original": "B1eEn1hDDr", "number": 628, "cdate": 1569439083869, "ddate": null, "tcdate": 1569439083869, "tmdate": 1577168251131, "tddate": null, "forum": "r1lEd64YwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CcBVtUNuwD", "original": null, "number": 1, "cdate": 1576798701761, "ddate": null, "tcdate": 1576798701761, "tmdate": 1576800934241, "tddate": null, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Decision", "content": {"decision": "Reject", "comment": "What is investigated is what kind of representations are formed by embodied agents; it is argued that these are different than from non-embodied arguments. This is an interesting question related to foundational AI and Alife questions, such as the symbol grounding problem. Unfortunately, the empirical investigations are insufficient. In particular, there is no comparison with a non-embodied control condition. The reviewers point this out, and the authors propose a different control condition, which unfortunately is not sufficient to test the hypothesis.\n\nThis paper should be rejected in its current form, but the question is interesting and hopefully the authors will do the missing experiments and submit a new version of the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710436, "tmdate": 1576800259440, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper628/-/Decision"}}}, {"id": "H1eyyIUhsS", "original": null, "number": 6, "cdate": 1573836247407, "ddate": null, "tcdate": 1573836247407, "tmdate": 1573836247407, "tddate": null, "forum": "r1lEd64YwH", "replyto": "Bkg8A4k7iB", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Response", "comment": "I do not think that the autoencoder experiments are a useful comparison because the autoencoder is trained on a completely different task than the RL agent: image reconstruction vs. value/action prediction. The autoencoder is trained to encode a lot of information that is irrelevant to the RL agent.\n\nInstead of training an autoencoder on image reconstruction, it would have been more informative to train a supervised version of the network to predict values and actions generated by the RL agent. This would have a similar computational cost (probably less) than training the autoencoder, so resource limitations cannot have prevented this (no re-training of the RL agent would have been necessary).\n\nI agree with Reviewer #3 that this work is not ready for publication at this point, but that the authors should be encouraged to continue with this important research as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "B1el1NKijH", "original": null, "number": 5, "cdate": 1573782487702, "ddate": null, "tcdate": 1573782487702, "tmdate": 1573782487702, "tddate": null, "forum": "r1lEd64YwH", "replyto": "rJxQskxsir", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Regarding concerns", "comment": "We agree with the concerns regarding the objective optimized by the autoencoder. However, it is clearly a better comparison than the previous baseline. Further, as the reviewer points out, there is no canonical choice of a baseline for comparison. Therefore, we consider the comparison to the auto encoder as valuable. We phrased our conclusions carefully and point out that they are obtained by comparison to the autoencoder. In this way we think that our claims are carefully and correctly phrased. \n\nIn addition, we would like to state that obviously any setup different from the embodied agent setup will enforce different types of representations. This is unavoidable and one of the main points of the paper. We like to show that the way a network is trained has a huge impact on the kind of representations learned and that the representations learned with a closed loop between action and perception are closer to representations found in nature than the ones learned in other classical training setups without dense semantic labeling such as an auto encoder. We therefore do not agree that that the autoencoder enforcing a denser representation is a reason for rejection as it is a key fact that we are presenting and part of the main message. We will try to formulate this more clearly in the paper. \n\nWe agree that there are other setups out there that would isolate embodiment more clearly and we will explore this further in the future. However, we think that already the representation analysis in itself and the comparison to a standard architecture like the autoencoder present very valuable insights, motivating many further research directions.\n\nSincerely,\nThe authors\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "rJxQskxsir", "original": null, "number": 4, "cdate": 1573744539428, "ddate": null, "tcdate": 1573744539428, "tmdate": 1573744539428, "tddate": null, "forum": "r1lEd64YwH", "replyto": "BylBESy7iH", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thank you for the corrections and for the additional experiment. Unfortunately I think there's a confounding factor with the auto-encoder experiment in that the autoencoder objective (and presumably the network architecture) explicitly encourages a dense representation. This makes it difficult to isolate embodiment as the critical differentiating factor. Sadly I do not have a concrete suggestion regarding an appropriate baseline, and I think it will require a significant amount of thought and effort. But setting the correct baseline is the only way to validate the claims about embodiment, and is the only route to showing its importance. Unfortunately I cannot recommend publication at this time, but I encourage the authors to explore these issues further as they are definitely important to the field. "}, "signatures": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "Skxrl81XsB", "original": null, "number": 3, "cdate": 1573217773115, "ddate": null, "tcdate": 1573217773115, "tmdate": 1573217773115, "tddate": null, "forum": "r1lEd64YwH", "replyto": "SJgrTBn1KS", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Reviewer #1 - Reply", "comment": "Dear Reviewer #1,\n\nThank you for your very insightful feedback. We greatly appreciate your concerns about the paper and have thought a lot about how to better demonstrate the strength of the results and their novelty. Based on your comments we decided to take these actions:\n\n-\tWe agree that the comparison to a random agent is a rather soft baseline. Therefore, we now include further analysis comparing the agent trained in an embodied set up to an auto encoder trained on the same visual stimuli. The results from this analysis show that the representations of the visual input in the embodied agent are very different to the representation learned by the auto encoder. There is a much stronger encoding of actions in the embodied agent and no more action encoding in the trained action encoder than in the random network. \n\n-\tThis observation might be considered as obvious. However, the difference of representations in the visual module under different training conditions is an important factor when analyzing representation learning. We think that this insight is important to keep in mind when studying the brain and the mechanisms of what kind of representations are learned under which conditions are interesting to study. As the learning conditions of an embodied network are closer to the ones of humans and animals we think it is interesting to investigate these representations further.\n\n-\tYour concern for section 3.3 that many CNNs are over-parameterized, regardless of embodiment, will be addressed by the addition of the results from the trained auto encoder. It is true that many CNNs trained in a supervised setup are over-parameterized, however, those CNNs are usually bigger than two layers and often have input of a smaller dimensionally. In our setup the input has dimensionality 84.672 and is compressed to a vector of size 256 which is only about 0.3% of the original input size. When training the autoencoder with a network of the same structure it learns no sparse representations of the input and uses most neurons in 100% of the input frames with varying strengths in the activations while the embodied network activates its neurons much more selectively.\n\n-\tIn your last comment you point out that it is obvious that similar images will be next to each other in the t-SNE visualization. We apologize for this misunderstanding. It addresses the core message of the manuscript that conceptually similar images, not perceptually similar images, are close in the representational space. This is the very advantage of reinforcement learning by the embodied agent. We will revise and precisely phrase the respective sections and figure 7. When looking at the representations of the auto encoder such conceptual similarities are not as strongly encoded, and focus is more on pure perceptual similarities such as overall illumination.\n\nAfter adding these changes and additional results we would be very happy if you could take another look and evaluate our results under the light that this is the first step of a bigger project investigating the relationship between training environment, training conditions, and learned representations. Even though this paper alone might not be ground breaking, it lays the foundations of many more results to come investigating no reward tasks using curiosity, hierarchical representations and different learning schemes to make network training more biologically inspired.\n\nSincerely,\nthe authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "BylBESy7iH", "original": null, "number": 2, "cdate": 1573217581414, "ddate": null, "tcdate": 1573217581414, "tmdate": 1573217581414, "tddate": null, "forum": "r1lEd64YwH", "replyto": "Hylx6OChtB", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Reviewer #3 - Reply", "comment": "Dear reviewer #3,\n\nWe are very grateful for your comments and feedback on the paper and will work them into the revised paper. Originally the random agent presented itself as a good comparison as we could use the exact same network structure, input and representation dimensionality. However, we understand that this is not the most convincing comparison we can make and have already trained an auto encoder on the static observations collected by the embodied agent, using the same network structure up to the encoding layer. We will include these results in the revised version of the paper and contrast them with the representations learned by the embodied agent. We hope that these additional results will help us make a stronger point than the comparison to a random agent.\n\nAdditionally, we apologize for the use of he/him to refer to the agent. This was not supposed to be any kind of politically statement but was a simple mistranslation from the authors native tongue where the noun for agent is grammatically male. We will of course change this in the revised version.\n\nSincerely,\nthe authors\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "Bkg8A4k7iB", "original": null, "number": 1, "cdate": 1573217485685, "ddate": null, "tcdate": 1573217485685, "tmdate": 1573217485685, "tddate": null, "forum": "r1lEd64YwH", "replyto": "rkgKdL1RtS", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment", "content": {"title": "Reviewer #2 - Reply", "comment": "Dear reviewer #2,\n\nThank you very much for your valuable input. We appreciate your ideas for further experiments very much and think that both experiments you propose are very interesting. We will definitely run the controlled experiment isolating embodiment that you suggested as well as the experiment using the two different learned representations in the RL task and comparing them. \nHowever, seeing the tight deadline of the paper submission and the fact that the training of these agents can take up to a month on our available hardware we propose to add a slightly modified control experiment from which we already have results. We have trained an autoencoder with the exact same network structure before the embedded layer on the visual input collected by the embodied agent. We will add the analysis of the representations learned in the autoencoder and compare them to the representations of the embodied agent. We think that this comparison will make a much stronger case for the action oriented and robust encoding learned in the embodied setup compared to the non-embodied setup. \n\nWe have also removed the video link from the website.\n\nSincerely,\nthe authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEd64YwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper628/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper628/Authors|ICLR.cc/2020/Conference/Paper628/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168632, "tmdate": 1576860546889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper628/Authors", "ICLR.cc/2020/Conference/Paper628/Reviewers", "ICLR.cc/2020/Conference/Paper628/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Comment"}}}, {"id": "Hylx6OChtB", "original": null, "number": 2, "cdate": 1571772600094, "ddate": null, "tcdate": 1571772600094, "tmdate": 1572972571849, "tddate": null, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work builds on the embodied cognition literature, hypothesizing that representations learned in embodied agents will be of improved \u201cquality\u201d compared to non-embodied models, such as neural networks trained on static supervised datasets. \n\nThe authors provide an excellent motivation to the work, with the introduction nicely laying the groundwork for their hypothesis. In general the motivation is quite strong, and research along these directions will no doubt be of value to the field.\n\nTo assess their hypothesis, the authors compare the representations learned in trained vs. random agents on the Unity Obstacle Tower Challenge, and demonstrate that trained agents develop semantically meaningful, sparse representations, without explicit regularization. \n\nUnfortunately, the work doesn\u2019t provide adequate baselines to properly assess the hypothesis. With the data presented, we can only make claims about *trained* vs. *untrained* agents (both of which are embodied!). In other words, an embodied agent with a random policy is not equivalent to a non-embodied agent. Thus, the data only support the conclusion that performance and task-relevant policies drive good representation learning in embodied agents, which is altogether not surprising, as one wouldn\u2019t expect representations to be good in a randomly initialized network. \n\nTo assess wither *embodiment* is a critical factor for driving good representations, the authors should compare to a model that learns from a static supervised dataset. Curiously, this idea is alluded to in the introduction, but not followed up on.\n\nOverall, there are some nicely presented ideas but the work is unfortunately incomplete, and the results cannot support the hypothesis laid out.\n\nAs a final note, the authors are encouraged to remove any assignments of gender to the agent (\u201che/him\u201d is unnecessarily used throughout).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575871558124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper628/Reviewers"], "noninvitees": [], "tcdate": 1570237749390, "tmdate": 1575871558141, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Review"}}}, {"id": "SJgrTBn1KS", "original": null, "number": 1, "cdate": 1570911677193, "ddate": null, "tcdate": 1570911677193, "tmdate": 1572972571807, "tddate": null, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nPaper Summary: The goal of the paper is to analyze what information is encoded in the representation learned using RL for a specific game. The paper shows that the activations are sparse and the activation patterns are distinct and shows that the conceptually similar images are clustered together in t-sne visualization.\n\nPaper Strengths:\n\nThe paper starts with a nice introduction that embodiment is useful for perception. However, the main content of the paper is very different from the introduction.\n\nPaper Weaknesses:\n\nThe conclusions of the paper are either already known or very trivial. So there is nothing new for the community to benefit from. Please refer to my comments below for more information.\n\nThe conclusion of section 3.3 is that \"the agent learns to use sparse activation patterns and even leaves some of the available neurons completely unused\". This has nothing to do with embodiment. The same patterns are observed in non-dynamic tasks such as image classification. The CNNs are usually over-parameterized.\n\nThe conclusion of section 3.4 is that \"When comparing this with the encodings of a random untrained agent one can see that there is a clear association between the learned image encoding and the actions\". That is the whole point of training. We train the models to find correlations between actions and observations. It is obvious that there is more correlation compared to a random agent.\n\nIt is shown in section 3.5 that similar images will be next to each other in the t-sne visualization. It is obvious that this happens.\n\nDue to the issues mentioned above, I do not think there is anything new in the paper and I vote for rejection.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575871558124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper628/Reviewers"], "noninvitees": [], "tcdate": 1570237749390, "tmdate": 1575871558141, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Review"}}}, {"id": "rkgKdL1RtS", "original": null, "number": 3, "cdate": 1571841649449, "ddate": null, "tcdate": 1571841649449, "tmdate": 1572972571765, "tddate": null, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "invitation": "ICLR.cc/2020/Conference/Paper628/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Paper summary:\n\nThis is an empirical study of the representations learned by a reinforcement learning agent. An agent is trained, using a standard RL algorithm, to solve puzzles by navigating through a 3D visual environment (Unity obstacle tower challenge). The analyses in the paper show that the visual representations of the trained agents are sparse and cluster according to the actions performed by the agent. The goal of the paper is to show that these features are due to the embodied nature of the agent. Specifically, the paper states that \u201cthe quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability\u201d.\n\nDecision:\n\nI suggest to reject this paper. While the topic is interesting and the paper is clearly written, there is a lack of control/comparison experiments, such that the paper\u2019s conclusions are not backed up by the analyses. However, with more experiments, I think this line of research has large potential.\n\nFurther justification for the decision:\n\nMy main criticism is that the paper claims to show that embodiment is important for representation learning, but never actually compares representations learned by an embodied agent to representations learned in some other way.\n\nComparing to a random network is a sanity check, but not sufficient to support the paper\u2019s claims.\n\nOne way to experimentally dissociate embodiment/agency from supervised learning would be to train two separate models, one that is \u201cembodied\u201d/active, and another that gets the same sensory input, but without embodiment (\u201cpassive\u201d). The passive network could be trained using the sensory inputs recorded while training the active network. The passive network could be trained to predict the value and/or the actions of the active network. Thus, the passive network would be trained in a supervised way, whereas the active network would be trained by RL (i.e. in an embodied way).\n\nThe representations of the two networks could then be compared using the analyses used in the paper. This setup would experimentally isolate the effect of embodiment. Without such comparisons, it is unclear whether representations learned in a supervised way would be any different from those learned by RL.\n\nIn addition to the descriptive analyses presented in the paper, a transfer learning approach could test whether there is actually a functional difference between the \u201cembodied\u201d and the supervised representations: Take the representations of the \u201cactive agent\u201d and the \u201cpassive agent\u201d and freeze the weights. Then re-initialize and re-train only the dense layer before the action probabilities on the RL task, leaving everything else frozen. Does the model from the \u201cactive agent\u201d do better on the RL task? This would suggest that the embodied agent learned better representations.\n\nMinor comment: The website contains a link to a YouTube profile that is not completely anonymous (it contains the first name and a profile photo). While I did not identify the authors when visiting the paper website, I recommend removing links to personal YouTube profiles and create submission-specific anonymous accounts."}, "signatures": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper628/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantically Meaningful Representations Through Embodiment", "authors": ["Viviane Clay", "Peter K\u00f6nig", "Kai-Uwe K\u00fchnberger", "Gordon Pipa"], "authorids": ["vkakerbeck@uos.de", "pkoenig@uos.de", "kkuehnbe@uos.de", "gpipa@uos.de"], "keywords": ["reinforcement learning", "deep learning", "embodied", "embodiment", "embodied cognition", "representation learning", "representations", "sparse coding"], "TL;DR": "We show how a deep neural network can learn meaningful and robust representations of visual input when trained in an embodied framework.", "abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.", "code": "https://github.com/EmbodiedLearning/ICLR-Submission-2020", "pdf": "/pdf/25d28cb99974a04ccf184c5c229597fc51429db1.pdf", "paperhash": "clay|learning_semantically_meaningful_representations_through_embodiment", "original_pdf": "/attachment/d621d58b981f4eba779ce4aa3a4b45ded32aed5f.pdf", "_bibtex": "@misc{\nclay2020learning,\ntitle={Learning Semantically Meaningful Representations Through Embodiment},\nauthor={Viviane Clay and Peter K{\\\"o}nig and Kai-Uwe K{\\\"u}hnberger and Gordon Pipa},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEd64YwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEd64YwH", "replyto": "r1lEd64YwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper628/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575871558124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper628/Reviewers"], "noninvitees": [], "tcdate": 1570237749390, "tmdate": 1575871558141, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper628/-/Official_Review"}}}], "count": 12}