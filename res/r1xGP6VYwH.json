{"notes": [{"id": "r1xGP6VYwH", "original": "HJeRfq5PDr", "number": 588, "cdate": 1569439066071, "ddate": null, "tcdate": 1569439066071, "tmdate": 1583912040883, "tddate": null, "forum": "r1xGP6VYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "w1e9xMTmlT", "original": null, "number": 1, "cdate": 1576798700562, "ddate": null, "tcdate": 1576798700562, "tmdate": 1576800935375, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper propose a scheme to enable optimistic initialization in the deep RL setting, and shows that it's helpful.\n\nThe reviewers agreed that the paper is well-motivated and executed, but had some minor reservations (e.g. about the proposal scaling in practice). In an example of a successful rebuttal two of the reviewers raised their scores after the authors clarified the paper and added an experiment on Montezuma's revenge.\n\nThe paper proposes a useful, simple and practical idea on the bridge between tabular and deep RL, and I gladly recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721591, "tmdate": 1576800272709, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper588/-/Decision"}}}, {"id": "rJeE7x9-5r", "original": null, "number": 3, "cdate": 1572081692257, "ddate": null, "tcdate": 1572081692257, "tmdate": 1574357512601, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "#rebuttal responses\n I am pleased by the authors' responses. Thus I change the score to weak accept.\n\n#review\nThis paper presented OPIQ, a model-free algorithm that does not rely on an optimistic initialization\nto ensure efficient exploration. OPIQ augments the Q-values with a new count-based optimism bonus. \nOPIO is ensured with good sample efficiency in the tabular setting. Experimental results show that OPIQ drives\nbetter exploration than DQN variants that utilize a pseudo count-based intrinsic motivation in the randomized chain and the maze environment.\n\nThe new optimism bonus is interesting and convincing with a good theoretical guarantee. The paper would be more clear if the authors add a motivating example in a tabular environment. That is, why does this extra optimism bonus help to predict optimistic estimates for novel state and action pairs. \n\nI appreciate that the authors compare extensive DQN variants using count-based explorations. But the experimental results are somewhat weak, as there are no comparison results on hard Atari games, such as freeway and Montezuma's revenge.\n\nI am willing to improve the score if the authors show better motivation or results on Atari games.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575809967481, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper588/Reviewers"], "noninvitees": [], "tcdate": 1570237749973, "tmdate": 1575809967494, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Review"}}}, {"id": "S1g-THFhiB", "original": null, "number": 9, "cdate": 1573848505150, "ddate": null, "tcdate": 1573848505150, "tmdate": 1573848505150, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "HkeeoJ-iir", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Reviewer", "comment": "Thank you for your comments, and for raising your score.\n\n\"The choice of \"count\" as the baseline for exploration is still not examined. Count seems like a proxy for \"uncertainty\" which is valid in the tabular setting, but not valid with generalization.\"\n> Using a different method to produce uncertainty estimates, and using those in place of the counts in OPIQ would be a very interesting avenue for future research. Counts were chosen to stay as close to the tabular theory/inspirations as possible, and because they have demonstrated strong empirical performance (especially on Atari [1,2]).\n\n\"Randomized prior p(state) where each p(state) is for example N(0,1) based on the hash(state).\"\n> Thanks for clarifying, unfortunately, there was not enough time to run these experiments on the maze environment in time for the rebuttal. \n\n[1] Bellemare, Marc, et al. \"Unifying count-based exploration and intrinsic motivation.\" Advances in Neural Information Processing Systems. 2016.\n[2] Ostrovski, Georg, et al. \"Count-based exploration with neural density models.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "SkxvBHthjS", "original": null, "number": 8, "cdate": 1573848383388, "ddate": null, "tcdate": 1573848383388, "tmdate": 1573848383388, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "rJe0fZhKiS", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Reviewer", "comment": "Thank you for your comments, we are pleased that you like the new results on Montezuma and find the motivation more compelling.\nFreeway can be easily solved by simple epsilon greedy [1, 2], so it doesn\u2019t provide much of a challenge in the way of exploration. Unfortunately, there was not enough time to run experiments on Gravitar in time for the rebuttals.\n\n[1] Bellemare, Marc, et al. \"Unifying count-based exploration and intrinsic motivation.\" Advances in Neural Information Processing Systems. 2016.\n[2] Machado, Marlos C., Marc G. Bellemare, and Michael Bowling. \"Count-based exploration with the successor representation.\" arXiv preprint arXiv:1807.11622 (2018)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "rke3ol-ItS", "original": null, "number": 1, "cdate": 1571324067999, "ddate": null, "tcdate": 1571324067999, "tmdate": 1573748658601, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Optimistic Exploration even with a Pessimistic Initialisation\n================================================================\n\nThis paper presents an exploration algorithm based on \"optimism in the face of uncertainty\" via count-based bonus.\nThe authors observe that typical neural net initializations close to zero can be pessimistic, but show that augmenting a count-based bonus for acting and bootstrapping can overcome this.\nThe authors support their claim with an adaptation of a regret bound for the tabular case, and a series of didactic experiments with neural net models.\n\n\nThere are several things to like about this paper:\n- Exploration with generalization in Deep RL is a large outstanding problem, with few effective options and none really commonly used in the field beyond epsilon-greedy or Boltzmann.\n- This algorithm is reasonably well thought out, building on an established literature of exploration bonuses, but with a slightly different take on the structure of the bonus.\n- The paper is well structured, building from intuition to theory to toy examples in DQN setting.\n- Overall the algorithm appears to perform well against a wide variety of related variants (although that presentation is a little confusing / overwhelming).\n\n\nThere are several places the paper might be improved:\n- I don't think the authors make a clear enough case for why this method is *better* than the other optimistic bonus approaches listed... Yes there is a regret bound, but this is not as good as some other methods... Yes there are ablations... but they're not really clear about what the mechanism that makes this method better than others!\n- Although this algorithm is motivated by applications to *deep* RL, the key choice of the \"count\" (and thus the method for optimism bonus) is mostly sidestepped. It amounts to an essentially tabular bonus in the space of the hashing function... and it's not clear why this approach should work any better or worse than other similar approaches that the paper complains about. For example, if you used \"Randomized Prior Functions\" or \"Random Network Distillation\" with that same hashing functions you would likely end up with similar results?\n- The comparison to benchmark algorithms seems quite confusing and I'm not sure if it's really presented well. It might be good to focus on fewer comparisons at a time and push remaining less important ones to the appendix.\n- It would be great to get an evaluation of these algorithms on a standardized and open-source benchmark... and I think that bsuite could be a really good candidate for this paper https://github.com/deepmind/bsuite particularly the \"deep sea\" experiments.\n\n\nOverall I think this is a reasonable paper, and I expect it to improve during the review process.\nAt the moment I have to say that I don't think there is a clear enough case for why this method is preferable to other similar approaches, or enough insight into the pros and cons to accept.\n\n============\n\nUpdating to \"weak accept\" as part of rebuttal.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575809967481, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper588/Reviewers"], "noninvitees": [], "tcdate": 1570237749973, "tmdate": 1575809967494, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Review"}}}, {"id": "HkeeoJ-iir", "original": null, "number": 7, "cdate": 1573748631841, "ddate": null, "tcdate": 1573748631841, "tmdate": 1573748631841, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "BJe-11eDoH", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Clarifying some points", "comment": "Thank you for your response!\n\nI think it's clear that the authors have made an effort to update their paper in response to the reviewers.\nIt will take me some time to go through the new paper fully, but it does look better.\nMy feeling is that I should increase my score to \"weak accept\" to reflect this, but also will need time to look at it properly.\n\nResponding to each point:\n\n- I'm still not convinced to this as an improved method for optimism, but I think that the paper now makes a clearer case in terms of the ablations.\n\n- The choice of \"count\" as the baseline for exploration is still not examined. Count seems like a proxy for \"uncertainty\" which is valid in the tabular setting, but not valid with generalization.\n\n- Randomized prior p(state) where each p(state) is for example N(0,1) based on the hash(state).\n\n- The experiments do look more clear.\n\n- bsuite I think would be a natural fit for this paper, and it would be a nice complementary analysis that *should* be quite easy to run. Opensourcing the code is good, but usually is difficult for people to bring into like for like methodology. I agree that the chain is similar to deep sea, which is why I believe your algorithm *should* be able to perform well... plus we'll get some insight into its *scaling* as you increase the size N."}, "signatures": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "rJe0fZhKiS", "original": null, "number": 6, "cdate": 1573662998336, "ddate": null, "tcdate": 1573662998336, "tmdate": 1573662998336, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "SJxJIRJDor", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Authors", "comment": "I am pleased that the authors show the comparison results on Montezuma\u2019s Revenge and show better motivation. I would like to see more results on other hard Atari games, such as freeway and Gravitar."}, "signatures": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "BJe-11eDoH", "original": null, "number": 5, "cdate": 1573482201477, "ddate": null, "tcdate": 1573482201477, "tmdate": 1573482201477, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "rke3ol-ItS", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Reviewer #2", "comment": "Thank you for your review. \n\n\u201cI don't think the authors make a clear enough case for why this method is *better* than the other optimistic bonus approaches listed\u2026\u201d\n>OPIQ provides an incentive for the agent to try and explore new state-action pairs by using optimistic Q-Value estimates during action selection and bootstrapping. Simply adding an intrinsic reward in novel states, after visiting them, merely provides an incentive to return to those states, but does not provide any incentive to try a new action during action selection. Additionally, an agent only using intrinsic rewards can prematurely stop exploring states that it has visited a number of times already, even if there are still actions it has never tried before in those states. OPIQ ensures the agent is incentivised to do both of these by explicitly ensuring that optimistic Q-Values are used during action selection and bootstrapping.\nWe have updated the Motivations subsection (3.1) to include a paragraph about why optimistic estimates lead to better exploration.\n\nThe ablation OPIQ w/o OB (light green in Figures 4 and 5) only differs from a DQN with pseudo-count based intrinsic motivation (DQN + PC) during action selection, but performs significantly better showing the importance of optimism (particularly during action selection in this particular environment).\n\nThe regret bound is not designed to be as competitive as possible with other tabular algorithms. It is a sanity check that our modifications to UCB-H in the tabular setting maintain the same efficiency guarantees, and provides a firm theoretical foundation for OPIQ. \n\n\u201cthe key choice of the \"count\" (and thus the method for optimism bonus) is mostly sidestepped\u201d\n> We believe that the shortcomings and limitations that OPIQ aims to address are orthogonal to the choice of pseudocount. There has been a large amount of work proposing new methods for obtaining approximate counts (either directly or through density models) to use as intrinsic motivation bonuses that we build on. \n\n\u201cFor example, if you used \"Randomized Prior Functions\" or \"Random Network Distillation\" with that same hashing functions you would likely end up with similar results?\u201d\n> Could you please clarify how you intend the hashing function to be used with these approaches? \n\n\u201cThe comparison to benchmark algorithms seems quite confusing and I'm not sure if it's really presented well. It might be good to focus on fewer comparisons at a time and push remaining less important ones to the appendix.\u201d\n> The experimental results are indeed quite crowded, but we believe it is important to compare against a wide variety of algorithms to provide strong experimental results.\nWe have uploaded a revision of the paper with a revised results section that should address your concerns. \n\n\u201cIt would be great to get an evaluation of these algorithms on a standardized and open-source benchmark... and I think that bsuite could be a really good candidate for this paper https://github.com/deepmind/bsuite particularly the \"deep sea\" experiments.\u201d\n> The \u201cdeep sea\u201d environment is very similar to the Chain environment we tested our algorithm on (although bsuite uses a \u2018length\u2019 N up to 50, whereas we tested N=100). \nThank you for the suggestion, we will look into bsuite.\nAdditionally, we plan on open-sourcing our code so that the environments can be used in future research. \n\nWe have also updated the results section to include some experiments on Montezuma\u2019s Revenge showing that OPIQ is able to scale to these complex environments and continue to provide exploration advantages.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "S1ljc0kPir", "original": null, "number": 4, "cdate": 1573482131217, "ddate": null, "tcdate": 1573482131217, "tmdate": 1573482131217, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "BJgEwAgZ5S", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Reviewer #1", "comment": "Thank you for your review.\n\n\u201cUltimately, the proposed approach relies strongly on good pseudo-count estimates in high dimensional state spaces, which is still an open problem.\u201d\n> Whilst we agree that producing good pseudo-count estimates in high dimensional state spaces is still an open problem, there has been a considerable amount of work attempting to address this with some good success [1,2,3].\nThe aim of this paper is to show the importance of optimism during both action-selection and bootstrapping even when using count-based intrinsic rewards. OPIQ builds upon the existing work on producing and using pseudo-counts for exploration, and leads to significantly improved exploration in sparse reward environments.\nOur contributions in this paper are orthogonal to the problem of producing appropriate pseudo-counts. \n\n\u201c...this approach may be applicable to actually high dimensional state spaces where pseudo counts do not work well.\u201d\n> We have uploaded a revision of the paper containing some experiments on Montezuma\u2019s Revenge showing that OPIQ is able to scale to these complex environments and continue to provide exploration advantages, even with a very simplistic pseudo-counting scheme.\n\n[1] Bellemare, Marc, et al. \"Unifying count-based exploration and intrinsic motivation.\" Advances in Neural Information Processing Systems. 2016.\n[2] Ostrovski, Georg, et al. \"Count-based exploration with neural density models.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n[3] Tang, Haoran, et al. \"# Exploration: A study of count-based exploration for deep reinforcement learning.\" Advances in neural information processing systems. 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "SJxJIRJDor", "original": null, "number": 3, "cdate": 1573482055098, "ddate": null, "tcdate": 1573482055098, "tmdate": 1573482055098, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "rJeE7x9-5r", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment", "content": {"title": "Replying to Reviewer #3", "comment": "Thank you for your review.\n\n\u201cThe paper would be more clear if the authors add a motivating example in a tabular environment. That is, why does this extra optimism bonus help to predict optimistic estimates for novel state and action pairs.\u201d\n> The MDP in Figure 1, which is explored in more detail in Appendix G, provides intuition for why optimism (particularly during action selection) is extremely important even in such a simple MDP. \n\nIf a state-action pair has never been visited, then its count will be 0 and hence its optimism bonus will be high. The Q+ values (Q + Optimism Bonus) for unvisited state-action pairs will then also be high. Crucially, we use these Q+ and not just Q during action selection and bootstrapping which ensures the optimism. Being optimistic during action selection in particular encourages the agent to try actions that it has never tried before. Without it, an agent with pessimistically initialised Q-Values could get stuck taking the wrong action indefinitely even on the simple 2-action MDP from Figure 1.\nWe have updated section 3.1 to include these motivations/justifications. \n\nIn the Deep RL setting, using these optimistic bonuses during action selection and bootstrapping is the crucial difference between OPIQ and other pseudo-count based methods (DQN + PC). \nFigure 6 shows the Q-Values alone are not optimistic for novel state-action pairs, which leads to insufficient exploration as shown by our experimental results in Figures 3 and 4.\n\n\u201cBut the experimental results are somewhat weak, as there are no comparison results on hard Atari games, such as freeway and Montezuma's revenge.\u201d\n> We have uploaded a revision of the paper containing some experiments on Montezuma\u2019s Revenge showing that OPIQ is able to scale to these complex environments and continue to provide exploration advantages.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xGP6VYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper588/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper588/Authors|ICLR.cc/2020/Conference/Paper588/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169208, "tmdate": 1576860536820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper588/Authors", "ICLR.cc/2020/Conference/Paper588/Reviewers", "ICLR.cc/2020/Conference/Paper588/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Comment"}}}, {"id": "BJgEwAgZ5S", "original": null, "number": 2, "cdate": 1572044380294, "ddate": null, "tcdate": 1572044380294, "tmdate": 1572972576629, "tddate": null, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "invitation": "ICLR.cc/2020/Conference/Paper588/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to optimistically initialize Q-values for unseen state actions for the case of Deep Q-Networks (DQNs) with high dimensional state representations, in order to step closer towards the optimistic initializations in the tabular Q-learning case which have proven guarantees of convergence. The paper shows that simple alternatives such as adding a bias to a DQN do not help as the generalization to novel states usually reduces the optimism of unvisited state-actions. Instead, a separate optimistic Q function is proposed that is an addition of two parts - a Deep Q-network which may be pessimistically initialized (in the worst case) and a term with pseudo-counts of state-actions that together form an optimistic estimate of novel state-actions. For the tabular case, the paper shows a convergence with guarantees similar to UCB-H (Jin et. al., 2018). This optimistic Q-function is used during action selection as well as bootstrapping in the n-step TD update.\n\nI vote for weak accept as this paper does a great job at demonstrating the motivation, simple examples and thorough comparisons for their proposed \u201cOPIQ\u201d model on simple environments such as the Randomized Chain (from Shyam et. al., 2019) and a 2D maze grid. While the experiments are in toy settings, the connection to UCB-H and the novel optimistic Q-function and it\u2019s training formulation make the contributions of this paper significant.\n\nHowever, my confidence on this rating is low as I have not gone through the theorem in the appendix and I may be wrong in judging the amount of empirical evidence required for the approach.\n\nWhile the paper does cover a lot of ground with important details and clear motivation, a lot of the desired experiments have been left to future work as mentioned in the paper. This, in addition to the experiments on just toy settings, is not sufficient to conclude that this approach may be applicable to actually high dimensional state spaces where pseudo counts do not work well. Ultimately, the proposed approach relies strongly on good pseudo-count estimates in high dimensional state spaces, which is still an open problem.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper588/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Exploration even with a Pessimistic Initialisation", "authors": ["Tabish Rashid", "Bei Peng", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["tabish.rashid@cs.ox.ac.uk", "bei.peng@cs.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "keywords": ["Reinforcement Learning", "Exploration", "Optimistic Initialisation"], "TL;DR": "We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.", "abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.", "pdf": "/pdf/4c77de9fe2127ab40ce5141bdcbc2d6f83494302.pdf", "paperhash": "rashid|optimistic_exploration_even_with_a_pessimistic_initialisation", "_bibtex": "@inproceedings{\nRashid2020Optimistic,\ntitle={Optimistic Exploration even with a Pessimistic Initialisation},\nauthor={Tabish Rashid and Bei Peng and Wendelin Boehmer and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xGP6VYwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a2f9dc63cd2594f5eff930e5623e61aa21121f29.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xGP6VYwH", "replyto": "r1xGP6VYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper588/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575809967481, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper588/Reviewers"], "noninvitees": [], "tcdate": 1570237749973, "tmdate": 1575809967494, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper588/-/Official_Review"}}}], "count": 12}