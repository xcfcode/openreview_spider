{"notes": [{"id": "SJGyFiRqK7", "original": "S1ljaQu5Ym", "number": 408, "cdate": 1538087799088, "ddate": null, "tcdate": 1538087799088, "tmdate": 1545355440386, "tddate": null, "forum": "SJGyFiRqK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Decoupling Gating from Linearity", "abstract": "The gap between the empirical success of deep learning and the lack of strong theoretical guarantees calls for studying simpler models. By observing that a ReLU neuron is a product of a linear function with a gate (the latter determines whether the neuron is active or not), where both share a jointly trained weight vector, we propose to decouple the two. We introduce GaLU networks \u2014 networks in which each neuron is a product of a Linear Unit, defined by a weight vector which is being trained, with a Gate, defined by a different weight vector which is not being trained. Generally speaking, given a base model and a simpler version of it, the two parameters that determine the quality of the simpler version are whether its practical performance is close enough to the base model and whether it is easier to analyze it theoretically. We show that GaLU networks perform similarly to ReLU networks on standard datasets and we initiate a study of their theoretical properties, demonstrating that they are indeed easier to analyze. We believe that further research of GaLU networks may be fruitful for the development of a theory of deep learning.", "keywords": ["Artificial Neural Networks", "Neural Networks", "ReLU", "GaLU", "Deep Learning"], "authorids": ["jonathan.fiat@gmail.com", "eran.malach@mail.huji.ac.il", "shais@cs.huji.ac.il"], "authors": ["Yonathan Fiat", "Eran Malach", "Shai Shalev-Shwartz"], "TL;DR": "We propose Gated Linear Unit networks \u2014 a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.", "pdf": "/pdf/692ed15150f994737dc37adfb71c45ac8519e277.pdf", "paperhash": "fiat|decoupling_gating_from_linearity", "_bibtex": "@misc{\nfiat2019decoupling,\ntitle={Decoupling Gating from Linearity},\nauthor={Yonathan Fiat and Eran Malach and Shai Shalev-Shwartz},\nyear={2019},\nurl={https://openreview.net/forum?id=SJGyFiRqK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeEOA9LgE", "original": null, "number": 1, "cdate": 1545150059805, "ddate": null, "tcdate": 1545150059805, "tmdate": 1545354477000, "tddate": null, "forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper408/Meta_Review", "content": {"metareview": "The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper408/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper408/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Gating from Linearity", "abstract": "The gap between the empirical success of deep learning and the lack of strong theoretical guarantees calls for studying simpler models. By observing that a ReLU neuron is a product of a linear function with a gate (the latter determines whether the neuron is active or not), where both share a jointly trained weight vector, we propose to decouple the two. We introduce GaLU networks \u2014 networks in which each neuron is a product of a Linear Unit, defined by a weight vector which is being trained, with a Gate, defined by a different weight vector which is not being trained. Generally speaking, given a base model and a simpler version of it, the two parameters that determine the quality of the simpler version are whether its practical performance is close enough to the base model and whether it is easier to analyze it theoretically. We show that GaLU networks perform similarly to ReLU networks on standard datasets and we initiate a study of their theoretical properties, demonstrating that they are indeed easier to analyze. We believe that further research of GaLU networks may be fruitful for the development of a theory of deep learning.", "keywords": ["Artificial Neural Networks", "Neural Networks", "ReLU", "GaLU", "Deep Learning"], "authorids": ["jonathan.fiat@gmail.com", "eran.malach@mail.huji.ac.il", "shais@cs.huji.ac.il"], "authors": ["Yonathan Fiat", "Eran Malach", "Shai Shalev-Shwartz"], "TL;DR": "We propose Gated Linear Unit networks \u2014 a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.", "pdf": "/pdf/692ed15150f994737dc37adfb71c45ac8519e277.pdf", "paperhash": "fiat|decoupling_gating_from_linearity", "_bibtex": "@misc{\nfiat2019decoupling,\ntitle={Decoupling Gating from Linearity},\nauthor={Yonathan Fiat and Eran Malach and Shai Shalev-Shwartz},\nyear={2019},\nurl={https://openreview.net/forum?id=SJGyFiRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper408/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353227575, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper408/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper408/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper408/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353227575}}}, {"id": "Sygl2gUT3Q", "original": null, "number": 3, "cdate": 1541394600229, "ddate": null, "tcdate": 1541394600229, "tmdate": 1541534020930, "tddate": null, "forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "content": {"title": "Review of \"Decoupling Gating from Linearity\"", "review": "The paper introduces a GaLU activation function, which is the product of a random gate function and a learnable linear function. The authors argue that empirically, neural networks with the GaLU activation is as effective as that with the ReLU activation, but theoretically, the GaLU activation is easier to understand because of the separation of the non-linearity and the learnable parameters. The the paper analyzes neural networks with one GaLU layer. Essentially, the network is a random transformation followed by a linear projection. This property enables analysis that are well known for the linear models.\n\nAlthough the definition of GaLU is new, the idea of combining a non-linear projection with a linear transformation is an old one. [1] shows that many kernel SVM models can be written in this form. [2] [3] show that neural networks with various of activation functions can be relaxed to this form. However, these methods have never achieved performance that is as good as the state-of-the-art CNN models in challenging datasets (ImageNet or even CIFAR-10).\n\nIn section 3, the accuracies on MNIST (98%) and MNIST-fashion (88%) are quite low. They are not even as good as a classical kernel SVM, though the non-linear projection version of the kernel SVM has been well-studied in theory.\n\nIn section 4, the analyses are mostly standard for linear models and convex optimization. To the best of our knowledge, it doesn't introduce new insight on the understanding of non-convex optimization.\n\nOverall, I think the paper and its theoretical analysis is built on an unsolid claim that the GaLU activation is a good replacement for traditional non-linear activation functions. The empirical study doesn't seem to support this claim. I cannot recommend accepting the paper.\n\n[1] Random Features for Large-Scale Kernel Machines \n[2] Learning Kernel-Based Halfspaces with the Zero-One Loss\n[3] Convexified Convolutional Neural Networks", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper408/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Gating from Linearity", "abstract": "The gap between the empirical success of deep learning and the lack of strong theoretical guarantees calls for studying simpler models. By observing that a ReLU neuron is a product of a linear function with a gate (the latter determines whether the neuron is active or not), where both share a jointly trained weight vector, we propose to decouple the two. We introduce GaLU networks \u2014 networks in which each neuron is a product of a Linear Unit, defined by a weight vector which is being trained, with a Gate, defined by a different weight vector which is not being trained. Generally speaking, given a base model and a simpler version of it, the two parameters that determine the quality of the simpler version are whether its practical performance is close enough to the base model and whether it is easier to analyze it theoretically. We show that GaLU networks perform similarly to ReLU networks on standard datasets and we initiate a study of their theoretical properties, demonstrating that they are indeed easier to analyze. We believe that further research of GaLU networks may be fruitful for the development of a theory of deep learning.", "keywords": ["Artificial Neural Networks", "Neural Networks", "ReLU", "GaLU", "Deep Learning"], "authorids": ["jonathan.fiat@gmail.com", "eran.malach@mail.huji.ac.il", "shais@cs.huji.ac.il"], "authors": ["Yonathan Fiat", "Eran Malach", "Shai Shalev-Shwartz"], "TL;DR": "We propose Gated Linear Unit networks \u2014 a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.", "pdf": "/pdf/692ed15150f994737dc37adfb71c45ac8519e277.pdf", "paperhash": "fiat|decoupling_gating_from_linearity", "_bibtex": "@misc{\nfiat2019decoupling,\ntitle={Decoupling Gating from Linearity},\nauthor={Yonathan Fiat and Eran Malach and Shai Shalev-Shwartz},\nyear={2019},\nurl={https://openreview.net/forum?id=SJGyFiRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "cdate": 1542234468083, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper408/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717521, "tmdate": 1552335717521, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper408/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gaTaqY3X", "original": null, "number": 2, "cdate": 1541152196787, "ddate": null, "tcdate": 1541152196787, "tmdate": 1541534020692, "tddate": null, "forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "content": {"title": "neat idea but not convincing enough", "review": "The authors propose a modified ReLU, the GaLU, where the nonlinearity gating role is decoupled from the linear weights. Similar ideas have been previously proposed. For example Tsai et al: http://papers.nips.cc/paper/6516-tensor-switching-networks: \"The TS network decouples a hidden unit\u2019s decision to activate (as encoded by the activation weights) from the analysis performed on the input when the unit is active (as encoded by the analysis weights)\" and Veness et al: Online learning with gated linear networks, https://arxiv.org/abs/1712.01897. \n\nIn short, the paper proposes a tweak to the nonlinearity in neural nets. Since many tweaks have been previously investigated, for such a paper to be worthy of publication, in 2018, the experimental results need to be extremely impressive. The results in this paper, on MNIST and fashion-MNIST are nowhere near sufficient.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper408/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Gating from Linearity", "abstract": "The gap between the empirical success of deep learning and the lack of strong theoretical guarantees calls for studying simpler models. By observing that a ReLU neuron is a product of a linear function with a gate (the latter determines whether the neuron is active or not), where both share a jointly trained weight vector, we propose to decouple the two. We introduce GaLU networks \u2014 networks in which each neuron is a product of a Linear Unit, defined by a weight vector which is being trained, with a Gate, defined by a different weight vector which is not being trained. Generally speaking, given a base model and a simpler version of it, the two parameters that determine the quality of the simpler version are whether its practical performance is close enough to the base model and whether it is easier to analyze it theoretically. We show that GaLU networks perform similarly to ReLU networks on standard datasets and we initiate a study of their theoretical properties, demonstrating that they are indeed easier to analyze. We believe that further research of GaLU networks may be fruitful for the development of a theory of deep learning.", "keywords": ["Artificial Neural Networks", "Neural Networks", "ReLU", "GaLU", "Deep Learning"], "authorids": ["jonathan.fiat@gmail.com", "eran.malach@mail.huji.ac.il", "shais@cs.huji.ac.il"], "authors": ["Yonathan Fiat", "Eran Malach", "Shai Shalev-Shwartz"], "TL;DR": "We propose Gated Linear Unit networks \u2014 a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.", "pdf": "/pdf/692ed15150f994737dc37adfb71c45ac8519e277.pdf", "paperhash": "fiat|decoupling_gating_from_linearity", "_bibtex": "@misc{\nfiat2019decoupling,\ntitle={Decoupling Gating from Linearity},\nauthor={Yonathan Fiat and Eran Malach and Shai Shalev-Shwartz},\nyear={2019},\nurl={https://openreview.net/forum?id=SJGyFiRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "cdate": 1542234468083, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper408/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717521, "tmdate": 1552335717521, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper408/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkge4jSvnX", "original": null, "number": 1, "cdate": 1540999975801, "ddate": null, "tcdate": 1540999975801, "tmdate": 1541534020485, "tddate": null, "forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "content": {"title": "Intellectual exercise of limited value to the community", "review": "The paper proposes an alternative to commonly used ReLU activated networks. The \"gating\" and \"amount\" effects of the weights are decoupled. The authors claim that such architectures are easier to theoretically understand. That might be the case indeed, but I fail to see much value in obtaining such understanding of very contrived objects that are not being used in practice. Unless such architectures can be proven to be interesting from a practical standpoint I do not think there is much of a point in studying them. The argument provided by the authors that they can - in a simple situation - have as much expressive power as a standard ReLU activated architecture is insucfficient, in my opinion, to justify researching them. Also, if a strong, deep theorem was proven using GaLU networks was proven, I would be inclined to recommend the paper to be accepted. As is - I do not find the paper to be a contribution significant enough for ICLR.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper408/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Gating from Linearity", "abstract": "The gap between the empirical success of deep learning and the lack of strong theoretical guarantees calls for studying simpler models. By observing that a ReLU neuron is a product of a linear function with a gate (the latter determines whether the neuron is active or not), where both share a jointly trained weight vector, we propose to decouple the two. We introduce GaLU networks \u2014 networks in which each neuron is a product of a Linear Unit, defined by a weight vector which is being trained, with a Gate, defined by a different weight vector which is not being trained. Generally speaking, given a base model and a simpler version of it, the two parameters that determine the quality of the simpler version are whether its practical performance is close enough to the base model and whether it is easier to analyze it theoretically. We show that GaLU networks perform similarly to ReLU networks on standard datasets and we initiate a study of their theoretical properties, demonstrating that they are indeed easier to analyze. We believe that further research of GaLU networks may be fruitful for the development of a theory of deep learning.", "keywords": ["Artificial Neural Networks", "Neural Networks", "ReLU", "GaLU", "Deep Learning"], "authorids": ["jonathan.fiat@gmail.com", "eran.malach@mail.huji.ac.il", "shais@cs.huji.ac.il"], "authors": ["Yonathan Fiat", "Eran Malach", "Shai Shalev-Shwartz"], "TL;DR": "We propose Gated Linear Unit networks \u2014 a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.", "pdf": "/pdf/692ed15150f994737dc37adfb71c45ac8519e277.pdf", "paperhash": "fiat|decoupling_gating_from_linearity", "_bibtex": "@misc{\nfiat2019decoupling,\ntitle={Decoupling Gating from Linearity},\nauthor={Yonathan Fiat and Eran Malach and Shai Shalev-Shwartz},\nyear={2019},\nurl={https://openreview.net/forum?id=SJGyFiRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper408/Official_Review", "cdate": 1542234468083, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJGyFiRqK7", "replyto": "SJGyFiRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper408/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717521, "tmdate": 1552335717521, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper408/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}