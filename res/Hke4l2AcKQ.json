{"notes": [{"id": "Hke4l2AcKQ", "original": "HyxEZna9Y7", "number": 1064, "cdate": 1538087915631, "ddate": null, "tcdate": 1538087915631, "tmdate": 1546746672083, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gjl5gJzV", "original": null, "number": 15, "cdate": 1546746355180, "ddate": null, "tcdate": 1546746355180, "tmdate": 1546746355180, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Carema-Ready version updated", "comment": "Carema-Ready version of this paper has been uploaed"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "SJxCSBYegV", "original": null, "number": 1, "cdate": 1544750406358, "ddate": null, "tcdate": 1544750406358, "tmdate": 1545354516478, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Meta_Review", "content": {"metareview": "This paper proposes a solution for the well-known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible.\n\nA downside of the proposed method is the introduction of hyper-parameters controlling the degree of regularization. The empirical results show improvements on various baselines.\n\nThe paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1064/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352980631, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352980631}}}, {"id": "HkxNWckD0Q", "original": null, "number": 12, "cdate": 1543072252294, "ddate": null, "tcdate": 1543072252294, "tmdate": 1543072252294, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Byl31zowTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Results no longer look marginal, thanks for the extra work!", "comment": "Thanks to the authors for the work in addressing my questions and comments.\n\n2. That\u2019s interesting to know, makes sense indeed. I would explicitly indicate this in your \u201cMeasure of Smoothness.\u201d section then, as this does not come across in the current text.\nThe new figure in Appendix B.1.3 is interesting to see, but does not seem to indicate such a drastic effect, which I guess might be due to t-SNE \u201cfixing it\u201d, but I am not sure what would be the best way to showcase this effect. \n\n5. Yes sorry what I meant by \u201clatent traversals\u201d is something akin to the single unit clamping done in Beta-VAE (Higgins et al 2017, https://openreview.net/forum?id=Sy2fzU9gl). In your case, given you have latents with 32 dimensions this is harder to do easily, hence interpolations might be interesting to see indeed.\n\nI think the updated results seem to make the model stronger and show visible improvements on VLAE. \nI am still a bit unclear on the exact characteristics of the latent space learnt and I\u2019m looking forward to see more work in that direction. \n\nHence the paper does seem good enough in its current state, so I\u2019d recommend publication as a poster (keeping my score, increasing my confidence).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "SJec11dj3X", "original": null, "number": 3, "cdate": 1541271266227, "ddate": null, "tcdate": 1541271266227, "tmdate": 1543072063085, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "content": {"title": "Interesting paper with marginal results", "review": "This paper proposes changes to the ELBO loss used to train VAEs, to avoid posterior collapse. They motivate their additional components rather differently than what has been done in the literature so far, which I found quite interesting.\nThey compare against appropriate baselines, on MNIST and OMNIGLOT, in a complete way.\n\nOverall, I really enjoyed this paper, which proposed a novel way to regularise posteriors to force them to encode information. However, I have some reservations (see below), and looking squarely at the results, they do not seem to improve over existing models in a significant manner as of now.\n\nCritics:\n1.\tThe main idea of the paper, in introducing a measure of diversity, was well explained, and is well supported in its connection to the Mutual Information maximization framing. One relevant citation for that is Esmaeili et al. 2018, which breaks the ELBO into its components even further, and might help shed light on the exact components that this new paper are introducing. E.g. how would MAE fit in their Table A.2?\n2.\tOn the contrary, the requirement to add a \u201cMeasure of Smoothness\u201d was less clear and justified. Figure 1 was hard to understand (a better caption might help), and overall looking at the results, it is even unclear if having L_smooth is required at all?\n\u2028Its effect in Table 1, 2 and 3 look marginal at best?\u2028\nGiven that it is not theoretically supported at all, it may be interesting to understand why and when it really helps.\n3.\tOne question that came up is \u201chow much variance does the L_diverse term has\u201d? If you\u2019re using a single minibatch to get this MC estimate, I\u2019m unsure how accurate it will be. Did changing M affect the results?\n4.\tL_diverse ends up being a symmetric version of the MI. What would happen if that was a Jensen-Shannon Divergence instead? This would be a more principled way to symmetrically compare q(z|x) and q(z).\n5.\tOne aspect that was quite lacking from the paper is an actual exploration of the latent space obtained. \u2028The authors claim that their losses would control the geometry of the latents and provide smooth, diverse and well-behaved representations. Is it the case?\n\u2028Can you perform latent traversals, or look at what information is represented by different latents?\u2028 \nThis could actually lend support to using both new terms in your loss.\n6.\tReconstructions on MNIST by VLAE seem rather worst than what can be seen in the original publication of Chen et al. 2017? Considering that the re-implementation seems just as good in Table 1 and 3, is this discrepancy surprising?\n7.\tFigure 2 would be easier to read by moving the columns apart (i.e. 3 blocks of 3 columns).\n\nOverall, I think this is an interesting paper which deserves to be shown at ICLR, but I would like to understand if L_smooth is really needed, and why results are not much better than VLAE.\n\nTypos:\n-\tKL Varnishing -> vanishing surely?\n-\tDevergence -> divergence\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "cdate": 1542234314508, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335864837, "tmdate": 1552335864837, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByxYlNP4C7", "original": null, "number": 11, "cdate": 1542906864995, "ddate": null, "tcdate": 1542906864995, "tmdate": 1542906864995, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "H1l3IfiwaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Thanks for your response.", "comment": "The changes to the paper look great, thanks for your updates.  They do not, however, change my basic opinion of the paper and so I will maintain my score as is."}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "HyeQs7536Q", "original": null, "number": 9, "cdate": 1542394779505, "ddate": null, "tcdate": 1542394779505, "tmdate": 1542394779505, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "r1eryMXiTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Response to Review 2 ", "comment": "Thank you for upgrading your score!\nWe really appreciate your suggestion to evaluate learned representations with simple non-linear classifiers.\nWe are performing experiments with SVM using non-linear kernels and will update results soon.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "r1eryMXiTX", "original": null, "number": 8, "cdate": 1542300124971, "ddate": null, "tcdate": 1542300124971, "tmdate": 1542300124971, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "HklIzEowaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Response to author feedback", "comment": "Thank you for your clarifications and the additional experiments. As a result of these, I have increased my score by one point.\n\nI agree with your comments on the importance of learning interpretable and disentangled representation. However notice that this can also be achieved learning simple non-Euclidean spaces, that may require however a simple but non-linear classifiers (e.g. 1-layer neural network with a small number of hidden units, non-linear SVM)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "BJgo0o0NnX", "original": null, "number": 1, "cdate": 1540840403325, "ddate": null, "tcdate": 1540840403325, "tmdate": 1542299922463, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "content": {"title": "Good paper, but the experiments could be improved", "review": "In this paper the authors present mutual posterior divergence regularization, a data-dependent regularization for the ELBO that enforces diversity and smoothness of the variational posteriors. The experiments show the effectiveness of the model for density estimation and representation learning.\nThis is an interesting paper dealing with the important issues of fully exploiting the stochastic part of VAE models and avoiding inactive latent units in the presence of very expressive decoders. The paper reads well and is well motivated. \n\nThe authors claim that their method is \"encouraging the learned variational posteriors to be diverse\". While it is important to have models that can use well the latent space, the constraints that are encoded seem too strong. If two data points are very similar, why should there be a term encouraging their posterior approximation to be different? In this case, their true posteriors will be in fact be similar, so it seems counter-intuitive to force their approximations to be different.\n\nThe numerical results seem promising, but I think they could be further improved and made more convincing.\n- For the density estimation experiments, while there is an improvement in terms of NLL thanks to the new regularizer, it is not clear which is the additional computational burden. How much longer does it takes to train the model when computing all the regularization terms in the experiments with batch size 100? \n- I am not completely convinced by the claims on the ability of the regularizer to improve the learned representations. K-means implicitly assumes that the data manifold is Euclidean. However, as shown for example by [Arvanitidis et al. Latent space oddity: on the curvature of deep generative models, ICLR 2018] and other authors, the latent manifold of VAEs is not Euclidean, and curved riemannian manifolds should be used when computing distances and performing clustering. Applying k-means in the high dimensional latent spaces of ResNet VAE and VLAE does not seem therefore a good idea.\nOne possible reason why your MAE model may perform better in the unsupervised clustering of table 2 is that the terms added to the elbo by the regularizer may force the space to be more Euclidean (e.g. the squared difference term in the Gaussian KL) and therefore more suitable for k-means. \n- The semi-supervised classification experiment is definitely better to assess the representation learning capabilities, but KNN suffers with the same issues with the Euclidean distance as in the k-means experiments, and the linear classifier may not be flexible enough for non-euclidean and non-linear manifolds. Have you tried any other non-linear classifiers?\n- Comparisons with other methods that aim at making the model learn better representation (such as the kl-annealing of the beta-vae) would be useful.\n- The lack of improvements on the natural image task is a bit concerning for the generalizability of the results.\n\nTypos and minor comments:\n- devergence -> divergence in introduction\n- assistant -> assistance in 2.3\n- the items (1) and (2) in 3.1 are not very clear\n- set -> sets in 3.2\n- achieving -> achieve below theorem 1\n- cluatering -> clustering in table 2", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "cdate": 1542234314508, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335864837, "tmdate": 1552335864837, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklIzEowaQ", "original": null, "number": 6, "cdate": 1542071309971, "ddate": null, "tcdate": 1542071309971, "tmdate": 1542071309971, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "BJgo0o0NnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Response to Review 2", "comment": "Thank you for the insightful comments! \n-- For your questions and concerns about the results on CIFAR-10, please see this post:\nhttps://openreview.net/forum?id=Hke4l2AcKQ&noteId=BylQ2fjL6X \nwhere we show stronger performance of our model.\n\n-- For your questions about the motivation of our method:\n \u201cencouraging the learned variational posteriors to be diverse\u201d is the motivation of L_diversity. If we only have L_diversity in our regularization method, it is, as in your comment, counter-intuitive for similar data points. However, by adding the smoothness term L_smooth, we expect that the model itself is able to learn how to balance diversity and smoothness to capture both diverse patterns in different data points and shared patterns in similar ones. And our experimental results show that these two regularization terms together help achieve stronger performance.\n\n-- For your questions about additional computational burden:\nIn order to train the model with large batch size, like 100, it requires more memory. But the computation of all the regularization terms is relatively efficient comparing to the computation of other parts of the objective. And the model converges as fast as that without the regularization.\n\n-- We really appreciate your comments about the evaluation of the learned representations. \nWe agree that the latent manifold of VAEs may not be Euclidean.\nHowever, as discussed in our paper and previous works, good latent representations need to capture global structured information and disentangle the underlying causal factors, tease apart the underlying dependencies of the data, so that it becomes easier to understand, to classify, or to perform other tasks. Evaluating learned representations with unsupervised or semi-supervised methods with limited capacity is a reasonable way and has been widely adopted by previous works. From this perspective, it might be an important advantage of our method if our regularizer can force the space to be more Euclidean, because the learned representations are easier to be interpreted and utilized. Flexible classifiers might favor representations by just memorizing the data, thus not providing fair evaluation of the learned representations.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "H1l3IfiwaQ", "original": null, "number": 5, "cdate": 1542070867718, "ddate": null, "tcdate": 1542070867718, "tmdate": 1542070867718, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "rJgA-rlU37", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Response to Review 3", "comment": "Thank you for the insightful comments! \n\nFor your questions and concerns about the results on CIFAR-10 with more expressive decoders, please see this post:\nhttps://openreview.net/forum?id=Hke4l2AcKQ&noteId=BylQ2fjL6X\nwhere we show stronger performance with more expressive decoders for our model.\n\nFor your specific questions, \n1 & 2. We appreciate your suggestion to perform ablation experiments for the two terms in our regularizer. Actually, both of the regularization terms play important roles. Without L_smooth, the model will easily place different posteriors into isolated points far away from each other, obtaining L_diversity close to zero, and the model performance on both density estimation and representation learning is worse than original VLAE without the regularization. Moreover, removing the L_smooth term, the training of the model becomes unstable.\n\n3. Thanks for your suggestion, we have added samples from VLAE in the updated version.\n\n4. Thanks for your comment, we have revised the paper to fix the grammatical mistakes.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "Byl31zowTX", "original": null, "number": 4, "cdate": 1542070755612, "ddate": null, "tcdate": 1542070755612, "tmdate": 1542070755612, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "SJec11dj3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "Response to Review 1", "comment": "Thank you for the insightful comments! \nFor your questions:\n1. Thanks for pointing out the related work. We cited  Esmaeili\u2019s paper in our updated version. Actually, MAE does not fit anyone in their Table A.2. If we also decompose our objective in the same, our objective is, if we use the original form of MPD and ignore L_sommth, term (1) + (2) + (4\u2019), where (4\u2019) is a modified version of (4).\nThe original (4) is KL(q(z) || p(z)) = E_q(z} [log q(z) - log p(z)], while (4\u2019) is E_{p(x) q(z)} [log q(z|x) - log p(z)]\n\n2. In our experiments, L_smooth plays a very important role. If we remove it, the model will easily place different posteriors into isolated points far away from each other, obtaining L_diversity close to zero. This phenomenon becomes more serious when a more powerful prior is applied, like auto-regressive flow. The unsupervised clustering and semi-supervised classification experiments justified the necessity of L_smooth. We also visualized the latent spaces with different settings in Appendix B.1.3, which might be helpful to understand the effects of the two regularization terms.\n\nFrom the theoretical perspective, we have not provided rigorous support of L_smooth and will leave it to future work.\n\n3. In order to better approximate L_diversity, we used large batch size in our experiments. For binary images, we use batch size 100. For natural images, due to memory limits, we use 64. The details are provided in Appendix. In practice, we found that these batch sizes provide stable estimation of L_diversity.\n\n4. As we discussed in the paper, one advantage of our regularization method is that L_diversity is computationally efficient. Previous works such as InfoVAE and AAE also has considered the  Jensen-Shannon Divergence. But directly optimizing it is intractable, and they applied adversarial learning methods.\n\n5. We plan to show the reconstruction results with linearly interpolated z-vectors in another updated version. We appreciate your suggestions if there are better ways of investigating the latent space in terms of \"latent travelsals\".\n\n6. The possible reason that VLAE obtained worse reconstruction than the original paper is that in our experiments, we used more powerful decoders with more layers and receptive fields. We want to test the performance of our regularizer with sufficiently expressive decoders. With more powerful decoders, our reimplementation of VLAE achieved better NLL but worse reconstruction, showing that VLAE suffers the KL varnishing issue with stronger decoders.\n\n7. Thanks for your suggestion! We will make figure 2 easier to understand and update the revised version later.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "rJgA-rlU37", "original": null, "number": 2, "cdate": 1540912390433, "ddate": null, "tcdate": 1540912390433, "tmdate": 1541533454462, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "content": {"title": "Improving InfoVAE", "review": "This paper presents a new regularization technique for VAEs similar in motivation and form to the work on InfoVAE.  The basic intuition is to encourage different training samples to occupy different parts of z-space, by maximizing the expected KL divergence between pairwise posteriors, which they call Mutual Posterior-Divergence (MPD).  They show that this objective is a symmetric version (sum of the forward and reverse KL) of the Mutual Info regularization used by the InfoVAE.  In practice however, they do not actually use this objective.  They use a different regularization which is based on the MPD loss but they say is more stable because it's always greater than zero, and ensures that all latent dimensions are used.  In addition to the MPD based term, they also add another term which encouraging the pairwise KL-divergences to have a low standard-deviation, to encourge more even spreading over the z-space rather than the clumpy distribution that they observed with only the MPD based term.\n\nThey show state of the art results on MNIST and Omniglot, improving over the VLAE.  But on natural data (CIFAR10), their results are worse than VLAE.  \n\nPros:\n\t1. The technique has a nice intuitive (but not particularly novel) motivation which is kinda-sorta theoretically motivated if you squint at it hard enough.\n\t2. The results on the simple datasets are solid and encouraging.\n\nCons:\n\t1.  The practical implementation is a bit ad-hoc and requires turn two additional hyper parameters (like most regularization techniques).\n\t2. The basic motivation and observations are the same as InfoVAE, so it's not completely novel.\n\t3. The CIFAR10 results are bit concerning, and one can't help but wondering if the technique really only helps when the data has simpler shared structure.\n\nOverall:  I think the idea is interesting enough, and the results encouraging enough to be just above the bar for acceptance at ICLR.\n\nI have the following question for the authors:\n\n\t1. Why do you use the truncated pixelcnn on CIFAR10?  Did you try it with the more expressive decoder (as was used on the binary images) and got worse results?  or is there some other justification for this difference?\n\nI would have like to see the following modifications to the paper:\n\n\t1. The paper essentially presents two related but separate regularization techniques.  It would be nice to have ablation results to show how each of these perform on their own.\n\t2. Bonus points for showing results which combine VLAE (which already has a form of the MPD regularization) with the smoothness regularization.\n\t3. It would be nice to see samples from VLVAE in Figure 3 next to the MAE samples to more easily compare them directly.\n\t4. There are many grammatical and English mistakes.  The paper is still quite readably, but please make sure the paper is proofread by a native English speaker.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Review", "cdate": 1542234314508, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335864837, "tmdate": 1552335864837, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylPAIJZ97", "original": null, "number": 1, "cdate": 1538483919014, "ddate": null, "tcdate": 1538483919014, "tmdate": 1538483919014, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "BJlD3jFg9X", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "content": {"title": "thanks for pointing out missing related work", "comment": "Thanks for pointing out the related work missed in the paper.\nWe will cite and compare with it in our revised version.\n\nWe really appreciate your comments about the notation used in the appendix.\nWe will revise it."}, "signatures": ["ICLR.cc/2019/Conference/Paper1064/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612355, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke4l2AcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1064/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1064/Authors|ICLR.cc/2019/Conference/Paper1064/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612355}}}, {"id": "BJlD3jFg9X", "original": null, "number": 1, "cdate": 1538460591046, "ddate": null, "tcdate": 1538460591046, "tmdate": 1538460705642, "tddate": null, "forum": "Hke4l2AcKQ", "replyto": "Hke4l2AcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1064/Public_Comment", "content": {"comment": " This work is closely related to the following work:\n R. D. Hjelm et al, \"Learning deep representations by mutual information estimation and maximization\", https://arxiv.org/abs/1808.06670.\n\n  I would suggest the authors cite the latest work and compare the performance between the two methods. \n\n  By the way, in the appendix, it mentions that the KL divergence is equal to H(.,.) - H(.), where H(.,.) denotes the relative entropy. Note that relative entropy is actually the KL divergence. Please use a proper name to define H(.,.). The different information measures can be found in \n  1. Cover and Thomas, \"Elements of Information Theory\".\n  2. Raymond Yeung, \"Information Theory and Network Coding\"\n  3. Robert Gallager, \"Information Theory and Reliable Communication\"", "title": "suggestion"}, "signatures": ["~Hello_Kitty2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted"], "writers": ["~Hello_Kitty2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders", "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.", "keywords": ["VAE", "regularization", "auto-regressive"], "authorids": ["xuezhem@cs.cmu.edu", "ctzhou@cs.cmu.edu", "ehovy@cs.cmu.edu"], "authors": ["Xuezhe Ma", "Chunting Zhou", "Eduard Hovy"], "pdf": "/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf", "paperhash": "ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders", "_bibtex": "@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1064/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311687382, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hke4l2AcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1064/Authors", "ICLR.cc/2019/Conference/Paper1064/Reviewers", "ICLR.cc/2019/Conference/Paper1064/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311687382}}}], "count": 15}