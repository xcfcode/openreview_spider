{"notes": [{"tddate": null, "ddate": null, "tmdate": 1524531659648, "tcdate": 1524531659648, "number": 2, "cdate": 1524531659648, "id": "SyVCZb2nz", "invitation": "ICLR.cc/2018/Workshop/-/Paper87/Public_Comment", "forum": "B1_u3cRUG", "replyto": "Hk-NTfU2z", "signatures": ["~Tianyun_Zhang1"], "readers": ["everyone"], "writers": ["~Tianyun_Zhang1"], "content": {"title": "We have added a citation in the Arxiv version", "comment": "Thanks for the comment. We were unaware of this manuscript during our submission. Now we have added a citation to yours in the Arxiv version.\n\n\nThese two papers have different focuses. Your submission uses ADMM to solve DNN training with regularization, which can result in sparsity. On the other hand, this work directly targets at optimizing the sparsity level and can thereby achieve higher sparsity degree.\n\n\nIn fact our results in the ICLR workshop are also outdated. The new results, algorithm extensions, and model releases are summarized in our new Arxiv report\n\nhttps://arxiv.org/pdf/1804.03294.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712626865, "id": "ICLR.cc/2018/Workshop/-/Paper87/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper87/Reviewers"], "reply": {"replyto": null, "forum": "B1_u3cRUG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712626865}}}, {"tddate": null, "ddate": null, "tmdate": 1524145563752, "tcdate": 1524145448996, "number": 1, "cdate": 1524145448996, "id": "Hk-NTfU2z", "invitation": "ICLR.cc/2018/Workshop/-/Paper87/Public_Comment", "forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Similar paper submitted at ICLR 2017 and on arXiv", "comment": "A very similar work was submitted at ICLR 2017 by my team.\n\nhttps://openreview.net/forum?id=rye9LT8cee\nhttps://arxiv.org/abs/1611.01590\n\nIt is a pity to see that it was not properly cited.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712626865, "id": "ICLR.cc/2018/Workshop/-/Paper87/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper87/Reviewers"], "reply": {"replyto": null, "forum": "B1_u3cRUG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712626865}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582655709, "tcdate": 1520781721556, "number": 1, "cdate": 1520781721556, "id": "HyGoF6fFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper87/Official_Review", "forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "signatures": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer3"], "content": {"title": "Intersting empirical study", "rating": "6: Marginally above acceptance threshold", "review": "This paper uses ADMM to justify hard thresholding heuristic, which thresholds out DNN's weights if the value is less than some amount.\nI think the empirical case study is well carried, and recommend this for the workshop to have a discussion. Note that the convergence of non-convex ADMM is unknown, and this paper does not use ADMM in a strict sense. ADMM splits the optimization problem to multiple instances, and we need to solve those subproblems EXACTLY. This paper worked around this by obtaining very crude approximation of solution for the subproblem. But for practical purposes, I think this maybe enough.\n\nAlso I would encourage the authors to report training AND TESTING loss and accuracy. Non-convex ADMM has stability issue, so it's best to see \n\n1. Run the algorithm 10 times.\n2. Report error bar in training and testing loss and accuracy.\n\nIn the optimization point of view, reducing the number of non-zero weights while preserving training loss makes sense, but in the machine learning point of view, if the procedure hurts the generalization property significantly, it is not useful.\nSo I recommend to the authors to include a performance of weight-pruned classifier on the test set.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582655518, "id": "ICLR.cc/2018/Workshop/-/Paper87/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper87/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper87/AnonReviewer1"], "reply": {"forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper87/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper87/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582655518}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582638506, "tcdate": 1520809961933, "number": 2, "cdate": 1520809961933, "id": "r1MeO47Kf", "invitation": "ICLR.cc/2018/Workshop/-/Paper87/Official_Review", "forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "signatures": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer1"], "content": {"title": "A decent work, but with a few minor issues and unexplained aspects", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes to use an l0 norm regularizer (although the authors do not identify it as such) to achieve weight sparsity in DNN training. The authors propose to upper bound the l0 norm (equivalently, the number of non-zero components) of the weights of each layer, by adding an indicator function to the underlying loss function. The resulting (non-convex) optimization problem is then tackled via ADMM. The experimental results (though not very extensive) show that the approach is able to yield networks with many zero weights.  \n\nThe paper suffers from a few minor issues and unexplained aspects. How is the update with respect to W_i carried out? Is it solved exactly, or simply a few steps of SGD? The notation in Equation (5) is wrong since it is the notation used to represent products; the authors should use \\Pi rather than \\prod. Furthermore, notice that this projection may not be unique, due to the non-convexity of S_i. How are the \\rho_i parameters selected? Does this choice affect the results?\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582655518, "id": "ICLR.cc/2018/Workshop/-/Paper87/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper87/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper87/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper87/AnonReviewer1"], "reply": {"forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper87/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper87/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582655518}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573567836, "tcdate": 1521573567836, "number": 109, "cdate": 1521573567494, "id": "Hk_aCR0Yz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1_u3cRUG", "replyto": "B1_u3cRUG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518413287934, "tcdate": 1518410863685, "number": 87, "cdate": 1518410863685, "id": "B1_u3cRUG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1_u3cRUG", "signatures": ["~Tianyun_Zhang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of Multipliers", "abstract": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a constrained nonconvex optimization problem, and then adopt the ADMM framework for systematic  weight pruning. We show that ADMM is highly suitable for weight pruning due to the computational efficiency it offers. We achieve a much higher compression ratio compared with prior work while maintaining the same test accuracy, together with a faster convergence rate.", "pdf": "/pdf/881d58061d886d80a5409c9298a0ea61d4a75887.pdf", "TL;DR": "We present a systematic weight pruning framework of deep neural networks (DNNs) using the alternating direction method of multipliers (ADMM).", "paperhash": "zhang|systematic_weight_pruning_of_dnns_using_alternating_direction_method_of_multipliers", "keywords": ["systematic weight pruning", "deep neural networks (DNNs)", "alternating direction method of multipliers (ADMM)"], "authors": ["Tianyun Zhang", "Shaokai Ye", "Yipeng Zhang", "Yanzhi Wang", "Makan Fardad"], "authorids": ["tzhan120@syr.edu", "sye106@syr.edu", "yzhan139@syr.edu", "ywang393@syr.edu", "makan@syr.edu"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 6}