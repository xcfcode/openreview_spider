{"notes": [{"id": "HyxlHsActm", "original": "Bkg5Pm9VFQ", "number": 59, "cdate": 1538087736376, "ddate": null, "tcdate": 1538087736376, "tmdate": 1545355440303, "tddate": null, "forum": "HyxlHsActm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Efficient Dictionary Learning with Gradient Descent", "abstract": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. ", "paperhash": "gilboa|efficient_dictionary_learning_with_gradient_descent", "TL;DR": "We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.", "authorids": ["dg2893@columbia.edu", "sdb2157@columbia.edu", "jw2966@columbia.edu"], "authors": ["Dar Gilboa", "Sam Buchanan", "John Wright"], "keywords": ["dictionary learning", "nonconvex optimization"], "pdf": "/pdf/3e45a265d01fd7351c1a3cd99261b0894afae18e.pdf", "_bibtex": "@misc{\ngilboa2019efficient,\ntitle={Efficient Dictionary Learning with Gradient Descent},\nauthor={Dar Gilboa and Sam Buchanan and John Wright},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxlHsActm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skgshcq8e4", "original": null, "number": 1, "cdate": 1545149106810, "ddate": null, "tcdate": 1545149106810, "tmdate": 1545354477082, "tddate": null, "forum": "HyxlHsActm", "replyto": "HyxlHsActm", "invitation": "ICLR.cc/2019/Conference/-/Paper59/Meta_Review", "content": {"metareview": "It seems that the reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper59/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper59/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Dictionary Learning with Gradient Descent", "abstract": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. ", "paperhash": "gilboa|efficient_dictionary_learning_with_gradient_descent", "TL;DR": "We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.", "authorids": ["dg2893@columbia.edu", "sdb2157@columbia.edu", "jw2966@columbia.edu"], "authors": ["Dar Gilboa", "Sam Buchanan", "John Wright"], "keywords": ["dictionary learning", "nonconvex optimization"], "pdf": "/pdf/3e45a265d01fd7351c1a3cd99261b0894afae18e.pdf", "_bibtex": "@misc{\ngilboa2019efficient,\ntitle={Efficient Dictionary Learning with Gradient Descent},\nauthor={Dar Gilboa and Sam Buchanan and John Wright},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxlHsActm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper59/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353351888, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxlHsActm", "replyto": "HyxlHsActm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper59/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper59/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper59/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353351888}}}, {"id": "SklG6dwsnX", "original": null, "number": 3, "cdate": 1541269689852, "ddate": null, "tcdate": 1541269689852, "tmdate": 1541534321089, "tddate": null, "forum": "HyxlHsActm", "replyto": "HyxlHsActm", "invitation": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "content": {"title": "Needs some improvement.", "review": "The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. I have three major concerns:\n\n(1) The optimization problem for complete orthogonal dictionary learning in this paper is very different from overcomplete dictionary learning in practice. It is actually more similar to tensor decomposition-type problems, especially after smoothing. From this point of view, it is not as interesting as the optimization problem.\n\nArora et al. Simple, Efficient, and Neural Algorithms for Sparse Coding, 2015\n\n(2) Some recent works focus on analyzing gradient descent for phase retrieval and matrix sensing. These obtained results are significantly improved and near-optimal. However, the convergence rate in this paper is very loose. Besides, the paper even does not look into the last phase of gradient descent, when there exists restricted strong convexity. Thus, only sublinear convergence rate is presented.\n\nChen et al. Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval, 2018\n\nThe quality of this paper could be improved, if the author could sharpen their analysis.\n\n(3) The analysis for the manifold gradient methods is something new, but not very significant. There have already been some works on manifold gradient methods. For example, the following paper has established convergence rates to second order optimal solutions for general nonconvex function over manifold.\n\nBoumal et al. Global rates of convergence for nonconvex optimization on manifolds. 2016.\n\nThe following paper has established the asymptotic convergence to second order optimal solutions for general nonconvex function over manifold.\n\nLee et al. First-order Methods Almost Always Avoid Saddle Points, 2017.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper59/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Dictionary Learning with Gradient Descent", "abstract": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. ", "paperhash": "gilboa|efficient_dictionary_learning_with_gradient_descent", "TL;DR": "We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.", "authorids": ["dg2893@columbia.edu", "sdb2157@columbia.edu", "jw2966@columbia.edu"], "authors": ["Dar Gilboa", "Sam Buchanan", "John Wright"], "keywords": ["dictionary learning", "nonconvex optimization"], "pdf": "/pdf/3e45a265d01fd7351c1a3cd99261b0894afae18e.pdf", "_bibtex": "@misc{\ngilboa2019efficient,\ntitle={Efficient Dictionary Learning with Gradient Descent},\nauthor={Dar Gilboa and Sam Buchanan and John Wright},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxlHsActm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "cdate": 1542234547197, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxlHsActm", "replyto": "HyxlHsActm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper59/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639936, "tmdate": 1552335639936, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper59/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygOCLP527", "original": null, "number": 2, "cdate": 1541203663527, "ddate": null, "tcdate": 1541203663527, "tmdate": 1541534320889, "tddate": null, "forum": "HyxlHsActm", "replyto": "HyxlHsActm", "invitation": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "content": {"title": "Iteration complexity analysis of Riemannian gradient descent for orthogonal dictionary learning with sparse factors.", "review": "The authors analyze the convergence performance of Riemannian gradient descent based algorithm for the dictionary learning problem with orthogonal dictionary and sparse factors. They demonstrate a polynomial time convergence from a random initialization for a smooth surrogate objective for the original non-smooth one. The problem and the analysis are of interest, but I have several questions regarding the paper as follows.\n\nMy first concern is that the analysis is on a smooth surrogate of the non-smooth sparse minimization for solving the dictionary learning problem, so it is not clear what is relationship between the global minimizer of the smooth problem to the underlying true dictionary. More specifically, how far is the global minimizer of problem (1) or (2) to the true dictionary parameter, and whether they share (approximately) the space or components regarding the sparse factors. Without clarifying this, it is not well motivated why we are interested in studying the problem considered in this paper at the beginning. Intuitively, since the recovered factors are not sparse anymore, it will impact the dictionary accordingly due to the linear mapping, which may lead to a very different set of dictionary components. Thus, explicit explanation is necessary here to avoid such degenerate case.\n\nMy second concern is the eligibility of assuming the dictionary A to be an identity matrix and extending it to the general orthogonal matrix case. The analysis uses the fact that rows of A are canonical basis, i.e., each row only has one non-zero entry. I do not see a straightforward extension by replacing A to be an orthogonal matrix as the authors claimed on page 3, since then the inner product of one row of A and one column of X is not just the corresponding entry of the column of X. It will be helpful if the authors can explain this explicitly or adjust the analysis accordingly to make this valid.\n\nAnother issue is the clarity of the paper. Some statements in the paper are not very clear. Form example, on page 3, third paragraph of Section 3, what does row(Y) = row(X_0) mean? Also, in eqn (1), y_k means k-th column of Y, and in eqn (2), q_i means i-th entry of q? Since both are bold lower case letters, clear distinction will help. Moreover, the reference use (. ) instead [ .], which can be confusing sometimes. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper59/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Dictionary Learning with Gradient Descent", "abstract": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. ", "paperhash": "gilboa|efficient_dictionary_learning_with_gradient_descent", "TL;DR": "We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.", "authorids": ["dg2893@columbia.edu", "sdb2157@columbia.edu", "jw2966@columbia.edu"], "authors": ["Dar Gilboa", "Sam Buchanan", "John Wright"], "keywords": ["dictionary learning", "nonconvex optimization"], "pdf": "/pdf/3e45a265d01fd7351c1a3cd99261b0894afae18e.pdf", "_bibtex": "@misc{\ngilboa2019efficient,\ntitle={Efficient Dictionary Learning with Gradient Descent},\nauthor={Dar Gilboa and Sam Buchanan and John Wright},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxlHsActm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "cdate": 1542234547197, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxlHsActm", "replyto": "HyxlHsActm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper59/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639936, "tmdate": 1552335639936, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper59/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1emP_AJ3Q", "original": null, "number": 1, "cdate": 1540511835392, "ddate": null, "tcdate": 1540511835392, "tmdate": 1541534320683, "tddate": null, "forum": "HyxlHsActm", "replyto": "HyxlHsActm", "invitation": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "content": {"title": "I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. Therefore, I have reservations about the paper.", "review": "This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The analysis relies on the negative curvature in the directions normal to the stable manifolds of all critical points that are not global minimizer.\n\nExploring the surface of a function and analyzing the structure of the negative curvature normal to the stable manifolds is an interesting idea. However, I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. See details below. Therefore, I have reservations about the paper.\n\n*) The terminology \"stable manifolds\" is used from the first page, while its formal definition is given on page 4.\n*) P3, the dictionary learning problem is not formally given. It is stated in the paper that the task is to find A and X, given Y. However, what optimization problem does the author consider? Is it \\min_{A, X} \\|Y - A X\\|_F^2? assuming both dictionary A and sparse code X are unknown or \\min_{A} \\|Y - A X\\|_F^2 assuming only dictionary is unknown?\n*) P3, second paragraph in Section 3: what is the variable q? It is not defined before.\n*) P3, third paragraph in Section 3: What is the function row()? Why does row(Y) equal row(X)?\n*) P3: How does the dictionary learning problem reformulate into the problem in the third paragraph of Section 3? If I understand correctly, the task is to find A, X such that A^* Y = X since A is orthogonal. Consider the first column in A and denote it by q. Then the first column of X is approximated by q^* Y. Since X is sparse, the task is to find q so that q^* Y as sparse as possible. But how about the other columns in matrix $A$? \n*) The Riemannian gradient algorithm is not stated in this paper.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper59/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Dictionary Learning with Gradient Descent", "abstract": "Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. ", "paperhash": "gilboa|efficient_dictionary_learning_with_gradient_descent", "TL;DR": "We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.", "authorids": ["dg2893@columbia.edu", "sdb2157@columbia.edu", "jw2966@columbia.edu"], "authors": ["Dar Gilboa", "Sam Buchanan", "John Wright"], "keywords": ["dictionary learning", "nonconvex optimization"], "pdf": "/pdf/3e45a265d01fd7351c1a3cd99261b0894afae18e.pdf", "_bibtex": "@misc{\ngilboa2019efficient,\ntitle={Efficient Dictionary Learning with Gradient Descent},\nauthor={Dar Gilboa and Sam Buchanan and John Wright},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxlHsActm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper59/Official_Review", "cdate": 1542234547197, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxlHsActm", "replyto": "HyxlHsActm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper59/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639936, "tmdate": 1552335639936, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper59/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}