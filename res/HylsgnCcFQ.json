{"notes": [{"id": "HylsgnCcFQ", "original": "HJg-np25K7", "number": 1110, "cdate": 1538087923362, "ddate": null, "tcdate": 1538087923362, "tmdate": 1545355390129, "tddate": null, "forum": "HylsgnCcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ByxtAy0gxV", "original": null, "number": 1, "cdate": 1544769489269, "ddate": null, "tcdate": 1544769489269, "tmdate": 1545354520763, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Meta_Review", "content": {"metareview": "This paper proposes a self-attention based approach for learning representations for the vertices of a dynamic graph, where the topology of the edges may change. The attention focuses on representing the interaction of vertices that have connections. Experimental results for the link prediction task on multiple datasets demonstrate the benefits of the approach. The idea of attention or its computation is not novel, however its application for estimating embeddings for dynamic graph vertices is new.\nThe original version of the paper did not have strong baselines as noted by multiple reviewers, but the paper was  revised during the review period. However, some of these suggestions, for example, experiments with larger graph sizes and other related work i.e., similar work on static graphs are left as a future work.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Novel application of self attention for estimating dynamic graph embeddings"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1110/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352962477, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352962477}}}, {"id": "rylCJmlmkV", "original": null, "number": 8, "cdate": 1543860965591, "ddate": null, "tcdate": 1543860965591, "tmdate": 1544327581197, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Feedback after revisions", "comment": "\nDear Reviewers and ACs,\n\nWe thank you once again for the time and effort to review our paper, and appreciate the valuable questions and suggestions. We have made several improvements to our paper, and hope that they sufficiently address your key concerns. We would greatly value any additional comments on the revised paper, for further discussion and improvement.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "BkgzUr-9CQ", "original": null, "number": 7, "cdate": 1543275850282, "ddate": null, "tcdate": 1543275850282, "tmdate": 1543275850282, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Summary of revisions ", "comment": "\nWe hope our revisions to the paper have adequately addressed the comments of the reviewers, and other interested anonymous researchers.  We would like to thank everyone for their insightful comments on our paper and sincerely believe that it has helped improve the overall quality and contribution.\n\n* We have added new experiments on dynamic new link prediction, to compare different graph representation learning methods on link prediction focused on (a) new links, and (b) previously unseen new nodes. Our results (summarized in Appendices B and D) indicate similar performance improvements for DySAT over existing methods.\n\n* We have conducted additional experiments to evaluate DySAT on multi-step link prediction/forecasting (Appendix C) where the node embeddings trained until G_t, are used to predict the links at future snapshot G_{t+n}. DySAT achieves significant improvements over all baselines and maintains a highly stable link prediction performance over future time steps.\n\n* In all of our experimental results, we additionally compare against two static graph embedding baselines, GCN and GAT trained for link prediction, denoted by GCN-AE and GAT-AE respectively. As expected, their performance is typically close to the corresponding GraphSAGE variants.\n\n* We have added an experiment in Appendix A.2 to visualize the distribution of attention weights learned by the temporal attention layers, over multiple time steps. Our results indicate a mild bias towards recent snapshots while exhibiting significant variability across different nodes in the graph.\n\n* In Section 2, we have added appropriate references to discuss relevant related work on dynamic attributed graphs and continuous-time dynamic graphs.\n\n* In section 5.4, we have added details on the running time of DySAT, comparing the relative costs of structural and temporal attention.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "rkgbrNZ5R7", "original": null, "number": 6, "cdate": 1543275577362, "ddate": null, "tcdate": 1543275577362, "tmdate": 1543275577362, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "rylPJV-cRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Reply to AnonReviewer2 (Part 2) ", "comment": "\nQ2: There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work.\n\nWe thank you for the useful references on dynamic graphs. We are aware of these papers and do agree that they are related to our work. Consequently, we have revised Section 2 (Related work) in the revised paper to reflect the same. While we agree on the relevance of these works, we list below our reasons for not including experimental comparisons:\n\n>> Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17 [2]: \nThis paper learns node embeddings in dynamic attributed graphs by initially training an offline model, followed by incremental updates over time. \n\nFirst, their key focus is online learning to improve efficiency over retraining static models, while our goal is to improve representation quality by capturing temporal evolutionary patterns in graph structure. This implies that their model can at best reach the performance of a statically re-trained method (as demonstrated in their paper), while we achieve significant improvements over static methods.\n\nSecond, their proposed model DANE is designed for attributed graphs with evolving structure and attributes, while our model is designed for dynamic non-attributed graphs. A direct application of DANE to non-attributed graphs would not be optimal and indeed our initial experiments on using DANE indicate significantly inferior performance even versus simplest static embedding methods. Thus, to avoid an unfair comparison, we exclude DANE in our experimental results.\n\n>> Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18 [3]: \nThis paper focuses on link prediction in dynamic attributed graphs, but does not learn latent representations for nodes, hence directly orthogonal and does not address our problem of dynamic graph representation learning. We mention a few other differences to support our choice:\n\nFirst, they once again focus on online learning to enable scaling to large-scale streaming networks. Their key objective is on efficiency to support streaming graphs, while our goal is to learn latent node representations that capture evolutionary graph structures.\n\nSecond, although their method might be relevant for comparison with our incremental variant IncSAT, we were unable to obtain the implementation even after contacting the authors. Since their method falls under the category of streaming graphs with a focus on efficiency and scalability, we believe an experimental comparison is outside the scope of our work.\n\n>> Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18 [4]: \nThis paper learns dynamic graph embeddings on temporal graphs with continuous time-stamped links. \n\nFirst, this paper operates under the assumption of continuous time-stamped links, which is often not realistic and distinct from the most established problem setup of using dynamic graph snapshots at discrete time steps. Thus, a direct comparison may not be fair.\n\nSecond, this paper assumes a continuous-time dynamic graph with the restriction that each link occurs *only* once. This is an unrealistic assumption, which prevents applicability to  most real-world dynamic graphs including all of our considered datasets where each link typically occurs in multiple snapshots. Thus considering all these factors, we exclude a comparison with this method in our experiments.\n\nQ3: The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently?\n\nWe thank you for your question on the analysis of temporal attention weights. We have conducted a preliminary study to analyze the distribution of attention weights learned by the temporal attention layers, over multiple time steps as requested. The results are reported in Appendix A.2. We choose the Enron dataset for this experiment, and present a heatmap of the normalized attentional coefficients with mean and standard deviation, over multiple time steps. Figure 3 in Appendix A.2 illustrates a mild bias towards recent snapshots while we observe significant variance in the attentional coefficients across different nodes in the graph.  This may indicate that the learned temporal attention weights capture historical context well, and vary to a considerable degree across different nodes in the graph.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "rylPJV-cRX", "original": null, "number": 5, "cdate": 1543275486526, "ddate": null, "tcdate": 1543275486526, "tmdate": 1543275507657, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "Bkg9hJLDnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Reply to AnonReviewer2 (Part 1)", "comment": "\nWe would like to thank you for the in-depth questions on our experimental results! Please refer to our global comment above for a list of all revisions to the paper -- we hope they have appropriately addressed your comments.\n\nWe respond to each of your comments below as follows:\n\nQ1: The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction.\n\nWe agree with you on the observation that static methods often match or beat existing dynamic embedding methods. Our initial experiments contain comparison to GraphSAGE - an unsupervised representation learning framework that supports various neighborhood aggregation functions, including GCN and GAT aggregators.\n\nWe thank you for the valuable pointer [1] which trains GCN/GAT as a graph autoencoder directly for link prediction. While conceptually similar to GCN/GAT variants of GraphSAGE, two key differences include (a) lack of neighborhood sampling, and (b) link prediction objective instead of random walk samples. To examine the effect of these differences on link prediction performance, we used the aforementioned implementation [1] to train autoencoder models of GCN and GAT, denoted by GCN-AE and GAT-AE in our experiments. Our experimental results have been updated to include these as static embedding methods for comparison. From the results, we find the performance of these methods to be mostly similar to their corresponding GraphSAGE variants, which is consistent with our expectation.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "Hyx3dG-cRm", "original": null, "number": 4, "cdate": 1543275124344, "ddate": null, "tcdate": 1543275124344, "tmdate": 1543275124344, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "S1g39zdt3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "We would like to thank you for the insightful questions on our experiments! Please refer to our global comment above for a list of all revisions to the paper.\n\nWe respond to each of your comments below as follows:\n\nQ1: What will happen if a never-seen node appears at t+1? The model design seems to be compatible with this case. The structural attention will still work, however, the temporal attention degenerates to a \u201cstatic\u201d result - all the attention focus on the representation at t+1. I am curious about the model performance in this situation, since nodes may arise and vanish in real applications.\n\nWe agree with the apt observation on the capability of DySAT to handle new nodes, and have conducted additional experiments to examine model performance in such situations. To compute the representation of a new node v at time step t+1, the only available information is the local structure around v at t+1. Although temporal attention will focus on the latest representation due to absence of history, it however does not degenerate to a \u201cstatic\u201d result. The temporal attention applied on the neighboring nodes of v (say N_v), indirectly contribute towards the embedding of v, through backpropagation updates. Specifically, the structural embedding of v is computed as a function of N_v, whose structural embeddings receive backpropagation signals through the temporal attention layers (assuming they are not all new nodes). Thus, temporal attention indirectly affects the final embedding of node v.\n\nAs suggested, we empirically examine the performance of DySAT on \u201cnew\u201d previously unseen nodes. We report link prediction performance *only* on the new nodes at each time step using the same experimental setup, i.e., a test example (node pair) is reported for evaluation only if it contains at least one new node. Due to the significant variance in the number of new nodes at each step, we report the performance (AUC) at each time step, along with a mention on the corresponding number of new nodes. The results are available in Figure 5 of the Appendix D. From Figure 5, we observe consistent gains of DySAT over other baselines, similar to our main results.\n\nQ2: What is the performance of the proposed algorithm for multi-step forecasting? In the experiments, graph at t+1 is evaluated using the model trained up to graph_t. However, in real applications we may don\u2019t have enough time to retrain the model at every time step. If we use the model trained up to graph_t to compute node embedding for the graph_{t+n}, what is the advantage of DySAT over static methods?\n\nWe agree with your view on the importance of not re-training the model at every time step in real-world applications. Multi-step forecasting is typically achieved either by (a) designing a model to predict multiple steps into the future, or by (b) recursively feeding next predictions as input, for a desired number of future steps. In case of dynamic graphs, events correspond to link occurrences, which renders forecasting different from conventional time-series, due to the occurrence of new nodes in each time step. Due to this key distinction, we list below, two possibilities for forecasting in dynamic graphs: (a) Link prediction at future step t+n (on all nodes) by incrementally updating the model on new snapshots. (b) Link prediction at future step t+n (among nodes present at t) based on dynamic embeddings learned at t, followed by a downstream classifier to predict the links at t+n. Note that (b) does not involve model re-training or updating while (a) requires incremental model updates.\n\nIn our paper, we have examined (a) by proposing an incremental variant named IncSAT and report the performance in Table 5 of Appendix E.\n\nWe have now added an additional experiment in Appendix C to evaluate forecasting using strategy (b), which enables direct evaluation of DySAT on multi-step link prediction. Here, each method is trained for a fixed number of time steps, and the latest embeddings are used to predict links at multiple future steps. In each dataset, we choose the last 6 snapshots to evaluate multi-step link prediction where we create examples from the links in G_{t+n} and an equal number of randomly sampled pairs of unconnected nodes (non-links). Our experimental results (Figure 4) indicate significant improvements for DySAT over all baselines and a highly stable link prediction performance over future time steps.\n\nQ3: What is the running time for a single training process?\n\nWe have revised Section 5.4 to add the running time information. Specifically, we report the runtime of DySAT on a machine with Nvidia Tesla V100 GPU and 28 CPU cores. The runtime per mini-batch of DySAT with batch size of 256 nodes on the ML-10M dataset, is 0.72 seconds. In comparison, the model variant without the temporal attention (No Temporal) takes 0.51 seconds. Thus, structural attention constitutes a major fraction of the overall runtime, while the cost of temporal attention is relatively lower."}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "Skg4XcgqC7", "original": null, "number": 3, "cdate": 1543272987919, "ddate": null, "tcdate": 1543272987919, "tmdate": 1543272987919, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "S1eSaFecR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Reply to AnonReviewer1 (Part 2) ", "comment": "Q3: The selected graphs are very small comparing to the dynamic graphs available here http://konect.uni-koblenz.de/networks/. \n\nWe thank the reviewer for the useful pointer to an extensive collection of real-world dynamic graphs.\n\nFirst, our experiments are conducted on real-world communication and rating networks with over 20,000 nodes and nearly 100,000 edges, which we believe constitute a diverse and representative sample of real-world dynamic graphs. Due to lack of established benchmark datasets for dynamic graphs, we choose Enron, UCI, Yelp, and MovieLens which have been widely in analysis of dynamic graphs.\n\nSecond, among the 7 compared baseline models on graph representation learning, 5 of them (GCN, GAT, node2vec, GraphSAGE, and DynGEM) choose their experiment datasets with comparable or smaller sizes. As mentioned in Section 6, our current implementation requires storing the sparse adjacency matrices of each snapshot in GPU memory, which limits scaling to graphs with over millions of nodes. This is a common issue faced by many successful graph neural network architectures such as GCN, GAT, etc.\n\nSince DySAT builds on the same framework as GCN and GAT, we foresee a direct extension to incorporate efficient neighborhood sampling strategies (similar to GraphSAGE and others), thus scaling to larger-scale dynamic graphs and we leave it as our future work.\n\nFinally, we would like to point out that the sizes of graph used in our experiments are comparable and often larger than the widely established benchmark citation networks Cora, Citeseer, and, Pubmed datasets for node classification in static graphs.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "S1eSaFecR7", "original": null, "number": 2, "cdate": 1543272892521, "ddate": null, "tcdate": 1543272892521, "tmdate": 1543272892521, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "SkgpdOcs27", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "Reply to AnonReviewer1 (Part 1)", "comment": "We would like to thank you for your review with thoughtful questions on the experiments! Please refer to our global comment above for a list of all revisions to the paper.\n\nWe respond to each of your comments below as follows:  \n\nQ1: I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph, however, working with a real-world dynamic graph, you can compute the difference between G_t and G_{t+1} as the changes that occur in G_{t+1} 1) Why are you not trying to predict these changes? \n\nWe agree with the observation that the differences between graphs G_t and G_{t+1} can be computed in real-world dynamic graphs, and we have included additional experiments on new link prediction in Appendix B.\n\nFirst, we would like to clarify our view of dynamic link prediction based on our understanding of existing literature. The goal of dynamic link prediction is to predict the set of future links (or interactions) based on historically observed graph snapshots. In practice, this can be realized as predicting future user interactions in email communication networks or user-item ratings in recommender systems. In such scenarios, dynamic link prediction aims to predict the set of \u201call\u201d future links (at time step t+1) given history until time step t. To the best of our knowledge, this evaluation approach has been widely adopted in our surveyed literature on dynamic link prediction in Section 2 (Related Work) of the paper. Our compared dynamic graph embedding baselines DynamicTriad, DynGEM, and Know-Evolve also adopt the same convention, by evaluating the predicted links at (t+1) through classification and ranking metrics.\n\nOn the other hand, we do agree with your perspective that a dynamic graph representation should be evaluated in its ability to predict \u201cnew\u201d links. We have added an additional experiment (Appendix B) where evaluation examples comprise \u201cnew\u201d links at G_{t+1} (which have not been observed in G_t), and an equal number of randomly sampled pairs of unconnected nodes (non-links). We use a similar evaluation methodology to evaluate the performance of dynamic link prediction through AUC scores. This experiment specifically evaluates the ability of different methods to predict new links at (t+1).\n\nThough the overall prediction accuracies are generally lower in comparison to the previous experimental setting, we observe consistent gains of 3-5% over the best baseline similar to our earlier results. The new results can be found in Table 4 of Appendix B, along with accompanying discussion. We hope that the addition of this experiment further showcases the capability of DySAT for dynamic link prediction.\n\nQ2: Why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph?\n\nFirstly, the training step for all models only utilizes the snapshots up to t to compute the embeddings for all nodes, which can subsequently be used in different downstream tasks such as link prediction, classification, clustering, etc. No data from snapshot t+1 are utilized in training the node embedding model. Since we focus on dynamic link prediction as the primary task for evaluation, the goal to predict future links (at time step t+1) given history until time step t. Thus, the evaluation set consists of examples from snapshot t+1.\nSecondly, the examples from time snapshot t+1 are *only* used to train a downstream logistic regression classifier for evaluating link prediction performance. Since the evaluation set comprises the links at t+1, we choose a small percentage of those examples (20%) for training, which is consistent with standard evaluation procedures. We follow the same setup for all the compared methods. In case of a different task such as multi-step forecasting to predict links at t+n, we similarly use 20% of examples at t+n for training the downstream classifier. We have revised the draft to make the experiment setup clearer.\n\nMeanwhile, we also describe the reason for using a downstream classifier to evaluate link prediction. Arguably, link prediction can also be evaluated by applying a sigmoid function on the inner product computed on pairs of node embeddings at time step t. However, we instead choose to train a downstream classifier (as done in node2vec, DynamicTriad etc.) to provide a fair comparison against baselines (such as DynamicTriad), which use other distance metrics (L_1 distance, etc.) for link prediction. We believe this evaluation methodology provides a more flexible framework to fairly evaluate various methods which are trained using different distance/proximity metrics.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "SkgpdOcs27", "original": null, "number": 3, "cdate": 1541281908748, "ddate": null, "tcdate": 1541281908748, "tmdate": 1541533414097, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "content": {"title": "Dynamic graph representation learning with self-attention", "review": "This paper describes learning representation for dynamic graphs using structural and temporal self-attention layers. They applied their method for the task of link-prediction. However, I have serious objections to their experimental setup. I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph, however, working with a real-world dynamic graph, you can compute the difference between G_t and G_{t+1} as the changes that occur in G_t+1 1) Why are you not trying to predict these changes?  Moreover, 2) why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph? \n3) The selected graphs are very small comparing to the dynamic graphs available here http://konect.uni-koblenz.de/networks/.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "cdate": 1542234303866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335874785, "tmdate": 1552335874785, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1g39zdt3m", "original": null, "number": 2, "cdate": 1541141140482, "ddate": null, "tcdate": 1541141140482, "tmdate": 1541533413891, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "content": {"title": "Dynamic Self-Attention Network", "review": "This paper proposes a model for learning node embedding vectors of dynamic graphs, whose edge topology may change. The proposed model, called Dynamic Self-Attention Network (DySAT), uses attention mechanism to represent the interaction of spatial neighbouring nodes, which is closely related to the Graph Attention Network. For the temporal dependency between successive graphs, DySAT also uses attention structure inspired by previous work in machine translation. Experiments on 4 datasets show that DySAT can improve the AUC of link prediction by significant margins, compared to static graph methods and other dynamic graph methods. Though the attention structures in this paper are not original, combining these structures and applying them on dynamic graph embedding is new.\n\nHere are some questions:\n\n1. What will happen if a never-seen node appears at t+1? The model design seems to be compatible with this case. The structural attention will still work, however, the temporal attention degenerates to a \u201cstatic\u201d result --- all the attention focus on the representation at t+1. I am curious about the model performance in this situation, since nodes may arise and vanish in real applications.\n\n2. What is the performance of the proposed algorithm for multi-step forecasting? In the experiments, graph at t+1 is evaluated using the model trained up to graph_t. However, in real applications we may don\u2019t have enough time to retrain the model at every time step. If we use the model trained up to graph_t to compute node embedding for the graph_{t+n}, what is the advantage of DySAT over static methods?\n\n3. What is the running time for a single training process?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "cdate": 1542234303866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335874785, "tmdate": 1552335874785, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkg9hJLDnQ", "original": null, "number": 1, "cdate": 1541001138342, "ddate": null, "tcdate": 1541001138342, "tmdate": 1541533413686, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "content": {"title": "Good paper, lacks comparison against a few key baselines.", "review": "This is a well-written paper studying the important problem of dynamic network embedding. Please find below some pros and cons of this paper.\nPros:\n\n* Studies the important problem of network embedding under a more realistic setting (i.e., nodes & edges evolve over time).\n* Introduces an interesting architecture that uses two forms of attention: structural and temporal.\n* Demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot.\n\nCons:\n\n* The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction.\n* There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work.\n* The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently?\n\n[1] Modeling Polypharmacy Side Effects with Graph Convolutional Networks. Zitnik et. al. BioInformatics 2018. \n[2] Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17. \n[3] Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18. \n[4] Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Review", "cdate": 1542234303866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335874785, "tmdate": 1552335874785, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eLDjjAtX", "original": null, "number": 1, "cdate": 1538337630510, "ddate": null, "tcdate": 1538337630510, "tmdate": 1538337630510, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "Skeoa8zRFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "content": {"title": "re: many prior works missing ", "comment": "Thank you very much for providing a detailed description of prior work on deep learning on graphs. \nWe are aware of most of the works that you have mentioned, which fall into the category of static graph representation learning. Due to our focus on the dynamic graph setting and limited space, we limit our attention mainly on most recent and related state-of-the-art works GCN (Kipf & Welling), GraphSAGE (Hamilton et al.) and GAT (Veli\u010dkovi\u0107 et al.). However, we agree that the mentioned papers are relevant and we will be sure to cite and discuss them in a subsequent version of our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1110/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608547, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylsgnCcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1110/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1110/Authors|ICLR.cc/2019/Conference/Paper1110/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608547}}}, {"id": "Skeoa8zRFX", "original": null, "number": 1, "cdate": 1538299586695, "ddate": null, "tcdate": 1538299586695, "tmdate": 1538299586695, "tddate": null, "forum": "HylsgnCcFQ", "replyto": "HylsgnCcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1110/Public_Comment", "content": {"comment": "I would like to draw the authors' attention to multiple recent works on deep learning on graphs directly related to their work. Among spectral-domain methods, the fundamental work of Bruna et al. [1] has started the recent interest in deep learning on graphs. Replacing the explicit computation of the Laplacian eigenbasis of the spectral CNNs in [1] with polynomial [2] and rational [3] filter functions is a very popular approach (the cited method of Kipf&Welling is a particular setting of [1]). On the other hand, there are several spatial-domain methods that generalize the notion of patches on graphs. These methods originate from works on deep learning on manifolds in computer graphics and recently applied to graphs, e.g. the Mixture Model Networks (MoNet) [4] (Note that the cited Graph Attention Networks (GAT) of Veli\u010dkovi\u0107 et al. are a particular setting of [4]). MoNet architecture was generalized in [5] using more general learnable local operators and dynamic graph updates. A further generalization of GAT is the dual graph attention mechanism [6]. Finally, the authors may refer to a review paper [7] on non-Euclidean deep learning methods. \n\n\n1. Spectral Networks and Locally Connected Networks on Graphs, arXiv:1312.6203.\n\n2. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, arXiv:1606.09375\n\n3. CayleyNets: Graph convolutional neural networks with complex rational spectral filters, arXiv:1705.07664,\n\n4. Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017. \n\n5. Dynamic Graph CNN for learning on point clouds, arXiv:1712.00268\n\n6. Dual-Primal Graph Convolutional Networks, arXiv:1806.00770.\n\n7. Geometric deep learning: going beyond Euclidean data, IEEE Signal Processing Magazine, 34(4):18-42, 2017\n", "title": "many prior works missing"}, "signatures": ["~Michael_Bronstein1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1110/Reviewers/Unsubmitted"], "writers": ["~Michael_Bronstein1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Graph Representation Learning via Self-Attention Networks", "abstract": "Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.", "keywords": ["Graph Representation Learning", "Dynamic Graphs", "Attention", "Self-Attention", "Deep Learning"], "authorids": ["asankar3@illinois.edu", "yanwu@visa.com", "ligou@visa.com", "wzhan@visa.com", "haoyang@visa.com"], "authors": ["Aravind Sankar", "Yanhong Wu", "Liang Gou", "Wei Zhang", "Hao Yang"], "TL;DR": "A novel neural architecture named DySAT to learn node representations on dynamic graphs by employing self-attention along two dimensions: structural neighborhood and temporal dynamics, achieves state-of-the-art results in dynamic link prediction.", "pdf": "/pdf/634cdf82116d590023e523e6dba913ed9f67d62f.pdf", "paperhash": "sankar|dynamic_graph_representation_learning_via_selfattention_networks", "_bibtex": "@misc{\nsankar2019dynamic,\ntitle={Dynamic Graph Representation Learning via Self-Attention Networks},\nauthor={Aravind Sankar and Yanhong Wu and Liang Gou and Wei Zhang and Hao Yang},\nyear={2019},\nurl={https://openreview.net/forum?id=HylsgnCcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1110/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311676261, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylsgnCcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1110/Authors", "ICLR.cc/2019/Conference/Paper1110/Reviewers", "ICLR.cc/2019/Conference/Paper1110/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311676261}}}], "count": 14}