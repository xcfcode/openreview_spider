{"notes": [{"id": "H1MgjoR9tQ", "original": "SJefsz4KYX", "number": 591, "cdate": 1538087831924, "ddate": null, "tcdate": 1538087831924, "tmdate": 1550470597152, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HyerSQiexV", "original": null, "number": 1, "cdate": 1544758076898, "ddate": null, "tcdate": 1544758076898, "tmdate": 1545354518527, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Meta_Review", "content": {"metareview": "This paper presents CMOW\u2014an unsupervised sentence representation learning method that treats sentences as the product of their word matrices. This method is not entirely novel, as the authors acknowledge, but it has not been successfully applied to downstream tasks before. This paper presents methods for successfully training it, and shows results on the SentEval benchmark suite for sentence representations and an associated set of analysis tasks.\n\nAll three reviewers agree that the results are unimpressive: CMOW is no better than the faster CBOW baseline on most tasks, and the combination of the two is only marginally better than CBOW. However, CMOW does show some real advantages on the analysis tasks. No reviewer has any major correctness concerns that I can see.\n\nAs I see it, this paper is borderline, but narrowly worth accepting: As a methods paper, it presents weak results, and it's not likely that many practitioners will leap to use the method. However, the method is so appealingly simple and well known that there is some value in seeing this as an analysis paper that thoroughly evaluates it. Because it is so simple, it will likely be of interest to researchers beyond just the NLP domain in which it is tested (as CBOW-style models have been), so ICLR seems like an appropriate venue. It seems like it's in the community's best interest to see a method like this be evaluated, and since this paper appears to offer a thorough and sound evaluation, I recommend acceptance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Clear study of an important problem, though improvements limited"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper591/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353161610, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353161610}}}, {"id": "Hyld2_TykV", "original": null, "number": 8, "cdate": 1543653551746, "ddate": null, "tcdate": 1543653551746, "tmdate": 1543653551746, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "H1e3m8gvRm", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "post-response note response", "comment": "Dear Reviewer,\n\nDue to its brevity, your comment leaves a lot of room for interpretation, and we are not sure if we understood your concerns correctly. Nevertheless, we would like to address them in the following.\n\nTo our understanding, your main concerns with our paper now is that\n\n1. We study a problem that is too \u201cspecific\u201d, i.e., not of interest for a broad audience at ICLR.\n2. Our contribution is too \u201cspecialized\u201d, i.e., not of large value for the considered problem.\n3. The technique is too \u201cspecific\u201d, i.e., not well studied and thus does not integrate well with common approaches.\n\nWe strongly disagree with you on all three of these points. \n\n1.\nSentence representation learning has been widely studied in recent years, mainly because it has many important applications. In our paper alone we consider more than 10 supervised downstream tasks and several unsupervised downstream tasks.\nThe importance of this topic for ICLR can also be seen from the fact that more than a dozen submissions have \u201cSentence embedding\u201d, \u201csentence representation\u201d, or \u201csentence encoding\u201d in their title alone!\nObviously, being concerned with unsupervised representation learning for NLP, our paper is also very well in line with ICLR 2019\u2019s CfP.\n\n2.\nOur contribution is to introduce order awareness to efficient, linear word embedding based models. Given that semantics of natural language are inherently order dependent, this has considerable consequences for the expressiveness of the resulting model. \nHence, our paper can not be considered a \u201cspecialized\u201d contribution.\n\n3.\nCMOW is based on the Compositional Matrix Space Model, which is a rather old idea that indeed has not been studied much. However, due to its conceptual similarity to CBOW, some knowledge is transferable, such that it can be trained in a similar fashion as CBOW. \n\nOur hybrid CBOW-CMOW model extends and improves upon CBOW, which is arguably the most popular baseline for sentence representation. It consists of a simple concatenation of CBOW and CMOW at the embedding level. As such, CMOW can be integrated with existing CBOW approaches easily.\n\nTo get a word order aware text embedding model, the obvious choice would have been some kind of RNN. This would probably be considered less \u201cspecialized\u201d or \u201cspecific\u201d. But keep in mind that science demands not only pushing the limits of already well-established techniques, but also following underexplored research paths. Considering the reviewer guidelines, https://iclr.cc/Conferences/2019/Reviewer_Guidelines , ICLR seems to be a place that welcomes these efforts.\n\nAlthough we would appreciate a higher rating, we would like to thank you for advocating our paper!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "H1e3m8gvRm", "original": null, "number": 6, "cdate": 1543075363800, "ddate": null, "tcdate": 1543075363800, "tmdate": 1543075363800, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "rkxCqDVshm", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "post-response note", "comment": "I understand the intent/value of doing controlled experiments. While I said the main weakness was weak experimental results, now I think a bigger issue is the impact of the work on the broader ICLR community. It is indeed a rather specialized contribution about a specific problem and technique, so while I like the paper I'm a bit hesitant to advocate it more strongly. Hence I'll keep my rating. "}, "signatures": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "Byg69SmWhQ", "original": null, "number": 1, "cdate": 1540597141503, "ddate": null, "tcdate": 1540597141503, "tmdate": 1542649938823, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "content": {"title": "A way to actually train sequential embedding models", "review": "The main contribution of this paper in practice seems to be a way to initialize the Continuous Matrix Space Model so that training actually converges, followed by a slightly different contrastive loss function used to train these models. The paper explores the pure matrix model and a mixed matrix / vector model, showing that both together improve on simpler methods on many benchmark tasks.\n\nMy main concern is that the chained matrix multiplication involved in this method is not substantially simpler than an RNN or LSTM sentence encoding model, and there are no comparisons of training and inference cost between the models proposed in this paper and conceptually simpler RNNs and LSTMs. The FastSent paper, used here as a baseline, does compare against some deep models, but they choose far more complex baselines such as the NMT encoding, which is trained on a very different loss function. Indeed the models proposed here do not seem to outperform fasttext and fastsent despite having fairly similar computational costs.\n\nI think this paper could use a little more justification for when it's appropriate to use the method proposed here versus more straightforward baselines.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "cdate": 1542234424829, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759080, "tmdate": 1552335759080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1eTBddx0X", "original": null, "number": 4, "cdate": 1542649925469, "ddate": null, "tcdate": 1542649925469, "tmdate": 1542649925469, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "SJe4Q3-x07", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "response to author response", "comment": "After reading the author response I'm revising my scores upward."}, "signatures": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "SJe4Q3-x07", "original": null, "number": 3, "cdate": 1542622235991, "ddate": null, "tcdate": 1542622235991, "tmdate": 1542622235991, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "Byg69SmWhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "Our model is more efficient than RNNs", "comment": "Dear reviewer,\n\nThank you for your helpful comments. From my understanding, your main critique is:\n\n1. \u201cNo comparison in terms of encoding speed with RNNs\u201d\n2. \u201cOur model does not yield as good performance as fastText and fastSent\u201d\n\nLet me respond to these points one by one.\n\n1.\nWe did not do compare against RNNs in terms of encoding speed, because it is already clear from the corpus of related work that embeddings from RNNs are much slower to compute than CBOW embeddings (see for instance Hill et al. (2016), to which you also refer).  Since we report that CMOW is approximately as fast as CBOW, we thought that this is clear, i.e., that our method is much more efficient than RNNs.\n\nHowever, it seems that this is not clear from our paper. Thus, we performed some measurements of our own and added the results to the paper (last paragraph of Discussion section). We had already reported the encoding speeds of CBOW and CMOW at test time (61k and 71k sentences per second, respectively). We also added the results of an Elman RNN in order to show that CMOW is substantially faster: In our experiments, the Elman RNN encodes 12k sentences per second, which is 5 times slower than our CMOW encoder. This corresponds almost exactly to the results also observed by Hill et al. at test time.\n\nPlease note, CMOW and CBOW are based on matrix multiplication and addition, respectively, which are associative operations. As such, CMOW and CBOW have substantial parallelization capacities: For a sequence of length n, only log(n) sequential steps are required, and the rest can be computed in parallel. On the other hand, an RNN is not associative and cannot be parallelized in the same manner: It requires n sequential steps.\n\n2.\nFirst, I would like to point out that our Hybrid method DOES outperform the results from fastSent on all supervised datasets that they report results on. On TREC and MRPC, the differences are even as large as 14.1% and 8.3% improvement, respectively.\n\nHowever, the training settings are too different to be considered fair, which we point out in the paper repeatedly: FastSent is trained on a corpus that is three times smaller (1B tokens vs 3B tokens in our case).\nIt is important to understand that we do not consider fastSent as a baseline to directly compare to. We report their results merely to show that our methods (which perform better than fastSent) produce embeddings that are reasonably useful.\nThe same holds for fastText: This was trained on a much larger corpus than our methods (600B tokens as opposed to 3B tokens) and its vocabulary has 2M words as opposed to 30k in our case. Hence, again, this comparison is by no means fair. \nWe perform a controlled study which allows us to identify exactly where the differences in performance come from. Hence, the only direct baseline is the CBOW model from our paper.\n\nHill et al. (2016) : Learning Distributed Representations of Sentences from Unlabelled Data, NAACL 2016"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "BkxNis-xC7", "original": null, "number": 2, "cdate": 1542622108221, "ddate": null, "tcdate": 1542622108221, "tmdate": 1542622108221, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "Bkgah1-i37", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "Enhancing simple word embedding models is an common, important topic of research", "comment": "Dear reviewer,\n\nThanks for your review! To my understanding, your main concerns with our paper are:\n\n1. \u201cThe improvements of the Hybrid model over CBOW are not large enough\u201d\n2. \u201cThere are other, more powerful models (RNNs) that achieve much better results, so there is not enough justification.\u201d\n\nLet me address these issues one at a time:\n\n1.\nWe conducted a study in the field of learning universal sentence embeddings. Obviously, an embedding that doesn\u2019t have a notion of word order should not be considered \"universal\". The goal of our research is thus to push the limits of what simple word aggregation methods are capable of encoding. Finding some empirical evidence, Henao et al. (2018) hypothesize that the main difference of simple word embedding methods to RNNs may be their inability to capture word order.\n\nWe successfully propose a way to diminish that difference. Our hybrid CBOW-CMOW model is not only able to capture word order information, it scores 8% better on average on the linguistic probing tasks than CBOW. Even if we disregard the benefit from BShift, the improvement is still large (~4%). From the perspective of learning linguistically informed universal sentence embeddings, this is an important result, especially at a conference that is all about learning representations.\n\nIt is true that the results on linguistic probing tasks do not transfer to the same extent to the downstream tasks, achieving an average improvement of \"only\" 1.2%. We have added this in the revised version of the paper. \nWe evaluate our models on the SentEval benchmark. This framework is the de facto standard for evaluating sentence embeddings, and thus we should evaluate our models this way as well. Most tasks in SentEval depend heavily on word content memorization (Conneau et al., 2018). Thus, the selection of downstream tasks rather disfavors our model, since it improves in every aspect but Word Content memorization.\nRecently, more doubt has been cast repeatedly whether the selection of tasks in SentEval is sufficient to test the generality of sentence embeddings (\u201cAnonymous ICRL Submission\u201d, 2018), especially their compositionality (Dasgupta et al., 2018).\n\nIn summary, considering the strong results on linguistic probing tasks, and the nature of the SentEval framework, we believe that the results obtained by our hybrid CBOW-CMOW model are already strong evidence that our method produces more general, robust sentence embeddings.\n\n2.\n\nThe research community in sentence embedding learning has paid a lot of attention to baselines based on word embedding aggregation methods (such as the one presented in this paper) that are conceptually simple, e.g., Henao et al. (2018), Pagliardini et al. (2018), Rueckle et al. (2018), including important work presented at ICLR (Wieting et al (2016), Arora et al. (2017).\nThe reasoning is two-fold: i) Aggregated word embeddings are computationally inexpensive compared to RNNs (see Hill et al. (2016), and the measurements in our work). ii) Pushing the limits of conceptually simple encoders helps to identify the benefit introduced by more sophisticated encoders, which has also been a recurring topic of interest ( Adi et al. (2016), Conneau et al. (2018), Zhu et al. (2018), Anonymous (2018) ).\nOur paper is clearly motivated by reason i), since our method is computationally as inexpensive as CBOW. It is also motivated by reason ii): The conceptual difference between CBOW and CMOW boils down to using matrix multiplication instead of addition, followed by simple adaptations to the training procedure. Yet, these changes substantially improve the model's ability to learn linguistic properties such as word order, which were formerly left up to more sophisticated RNNs.\n\nAdi et al. (2016) : Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks, ICLR 2017\nAnonymous (2018) : No Training Required: Exploring Random Encoders for Sentence Classification. URL: https://openreview.net/forum?id=BkgPajAcY7 , ICLR 2019 Submission\nArora et al. (2017) : A Simple But Tough-to-Beat Baseline for Sentence Embeddings, ICLR 2017\nConneau et al. (2018): What you can cram into a single vector, ACL 2018\nHenao et al. (2018) : Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms, ACL 2018\nHill et al. (2016) : Learning Distributed Representations of Sentences from Unlabelled Data, NAACL 2016\nPagliardini et al. (2018) : Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features, NAACL 2018\nRueckle et al. (2018) : Concatenated Power Mean Word Embeddings as Universal Cross-Lingual Sentence Representations, arXiv:1803.01400\nWieting et al. (2016) : Towards Universal Paraphrastic Sentence Embeddings, ICLR 2016\nZhu et al. (2018) : Exploring Semantic Properties of Sentence Embeddings"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "rJl-95bx0X", "original": null, "number": 1, "cdate": 1542621832882, "ddate": null, "tcdate": 1542621832882, "tmdate": 1542621832882, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "rkxCqDVshm", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "content": {"title": "The hybrid CBOW-CMOW model makes CBOW a more robust baseline", "comment": "Dear reviewer,\n\nThank you for your comments! To my understanding, your main concerns with our paper are the following:\n\n1. \u201cfastText embeddings are clearly better than our approach.\u201d\n2. \u201cThe improvements of the Hybrid model over CBOW are small\u201d.\n\nI would like to address these concerns in the following.\n\n1.\nWe aim at conducting a controlled study, where we have full control over the independent variables. This allows me to precisely measure the effect of our changes/extensions to the CBOW model. Therefore, our baseline is the CBOW we trained ourselves, not fastText or fastSent. We report the scores of fastText and fastSent merely to show that our models produce useful embeddings and are therefore worth studying in the first place. FastText and FastSent are NOT the baselines we compare against.\n\nLet me elaborate why the scores we report for fastText are not comparable to our approach. The scores achieved by fastText are based on the implementation by Mikolov et al. (2018). Like our baseline, fastText is based on the CBOW objective, i.e., predicting the center word from the sum of its context word embeddings. However, their model is trained on a much larger corpus (CommonCrawl, 630B tokens, vs UMBC, 3B tokens), and with a much larger vocabulary (2M words vs. 30,000 in our case). \n\nFurthermore, the authors of fastText employ many tricks to enhance the quality of their models (word subsampling, subword-information, phrase representation, n-gram representations, etc.). For simplicity, I focus on the essential part of our models, i.e., the composition function, in order to conduct a fair and scientifically robust comparison of the performance of CBOW with my novel CMOW and finally the hybrid CBOW-CBOW-model. This makes a direct comparison with fastText very difficult, if not entirely unfair.\n\n2.\nMy paper is concerned with learning universal sentence embeddings with simple word embedding methods. Averaging word embeddings already shows good performance on downstream tasks. However, one cannot really expect to obtain a \"universal\" sentence embedding from an encoder that is word-order agnostic like CBOW. In fact, finding some empirical evidence, Henao et al. (2018) recently hypothesized that word-order sensitivity may be the main difference of simple word aggregation methods to RNNs.\nWe successfully propose a method to diminish this difference.\nOur hybrid CBOW-CMOW-model is not only able to capture word order information like RNNs. It also scores on average 8% better on the linguistic probing tasks than CBOW! Even if we disregard the benefit from BShift, the improvement is still large (~4%). From the perspective of learning linguistically informed universal sentence embeddings, this is an important result.\n\nIt is true that the results on linguistic probing tasks do not transfer to the same extent to the downstream tasks, achieving an average improvement of \"only\" 1.2%. We have added this in the revised version of the paper. \nWe evaluate our models on the SentEval benchmark. This framework is the de facto standard for evaluating sentence embeddings, and thus we should evaluate our models this way as well. Most tasks in SentEval depend heavily on word content memorization (Conneau et al., 2018). Thus, the selection of downstream tasks rather disfavors our model, since it improves in every aspect but Word Content memorization.\nRecently, more doubt has been cast repeatedly whether the selection of tasks in SentEval is sufficient to test the generality of sentence embeddings (\u201cAnonymous ICRL Submission\u201d, 2018), especially their compositionality (Dasgupta et al., 2018).\n\nIn summary, considering the strong results on linguistic probing tasks, and the nature of the SentEval framework, we believe that the results obtained by our hybrid CBOW-CMOW model are already strong evidence that our method produces more general, robust sentence embeddings.\n\n\u201cAnonymous ICRL Submission\u201d(2018): No Training Required: Exploring Random Encoders for Sentence Classification. URL: https://openreview.net/forum?id=BkgPajAcY7\nConneau et al. (2018): What you can cram into a single vector, ACL 2018\nDasgupta et al. (2018): Evaluating Compositionality in Sentence Embeddings, arXiv:1802.04302\nHenao et al. (2018) : Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms, ACL 2018\nMikolov et al. (2018): Advances in Pre-Training Distributed Word Representations, LREC 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611611, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MgjoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper591/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper591/Authors|ICLR.cc/2019/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers", "ICLR.cc/2019/Conference/Paper591/Authors", "ICLR.cc/2019/Conference/Paper591/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611611}}}, {"id": "Bkgah1-i37", "original": null, "number": 2, "cdate": 1541242805313, "ddate": null, "tcdate": 1541242805313, "tmdate": 1541533860905, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "content": {"title": " Interesting model to embed words in a way that captures order information", "review": "\nThe authors propose CMOW, an extension of the CBOW model that allows the model to capture word order. Instead of each word being represented as a vector, words are represented by matrices. They extend the CBOW objective to take into account word order by replacing the averaging of vectors to create the context with matrix multiplication (a non-commutative operation). This is the first time this model has been applied in a large scale unsupervised setting. They are able to do this using their objective and an initialization strategy where the matrix embeddings are set to the identity matrix with some Gaussian noise added.\n\nThe results of this paper are its main weakness. I did enjoy reading the paper, and it is nice to see some results using matrices as embeddings and matrix multiplication as a compositional function. They include a nice analysis of how word order is captured by these CMOW embeddings while CBOW embeddings capture the word content, but it doesn't seem to make much of a difference on the downstream tasks where CBOW is better than CMOW and close to the performance of the hybrid combination of CBOW and CMOW.\n\nI think it's clear that their model is able to capture word information to some extent, but other models  (RNNs etc.) can do this as well, that admittedly are more expensive, but also have better performance on downstream tasks. I think a stronger motivation for their method besides an analysis of some phenomena it captures and a slight improvement on some downstream tasks when combined with CBOW is needed though for acceptance. Could it be used in other settings besides these downstream transfer tasks?\n\nPROS:\n- introduced an efficient and stable approach for training CMSM models\n- Show that their model CMOW is able to capture word order information\n- Show that CMOW compliments CBOW and a hybrid model leads to improved results on downstream tasks. \n\nCONS\n- The results on the hybrid model are only slightly better than CBOW. CMOW alone is mostly worse than CBOW.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "cdate": 1542234424829, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759080, "tmdate": 1552335759080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxCqDVshm", "original": null, "number": 3, "cdate": 1541257109885, "ddate": null, "tcdate": 1541257109885, "tmdate": 1541533860696, "tddate": null, "forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "content": {"title": "new training schemes for a matrix-multiplicative variant of CBOW ", "review": "The paper presents new training schemes and experiments for a matrix-multiplicative variant of CBOW. This variant is called a CMSM (Yessenalina and Cardie, 2011; Asaadi and Rudolph, 2017) which swaps the bag of vectors to a product of square matrices for encoding context to incorporate word ordering. It seems this model has not been trained successfully before (at least with a simple approach) due to the vanishing gradient problem.\n\nThe paper's main contributions are an initialization scheme for context matrices (to I + [N(0,0.1)]) to counter the vanishing gradient problem and a modification of the CBOW objective so that the target word is drawn uniformly at random from the context window (rather than the center word). Both are shown to improve the quality of learned representations when evaluated as sentence embeddings. Concatenating CBOW and CMSM architectures is additionally helpful. \n\nI was not aware of the matrix-multiplicative variant of CBOW previously so it's possible that I don't have the expertise to judge the novelty of the approach. But the idea is certainly sensible and the proposed strategies seem to work. The main downside is that for all this work the improvements seem a little weak. The averaged fastText embeddings are clearly superior across the board, though as the authors say it's probably unfair to compare based on different training settings. But this doesn't hurt the simplicity and effectiveness of the proposed method when compared against CBOW baselines. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper591/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.", "keywords": ["Text representation learning", "Sentence embedding", "Efficient training scheme", "word2vec"], "authorids": ["florian.ren.mai@googlemail.com", "lga@informatik.uni-kiel.de", "mail@ansgarscherp.net"], "authors": ["Florian Mai", "Lukas Galke", "Ansgar Scherp"], "TL;DR": "We present a novel training scheme for efficiently obtaining order-aware sentence representations.", "pdf": "/pdf/328057644a45ca77378b39e794b39b2426f1c3aa.pdf", "paperhash": "mai|cbow_is_not_all_you_need_combining_cbow_with_the_compositional_matrix_space_model", "_bibtex": "@inproceedings{\nmai2018cbow,\ntitle={{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},\nauthor={Florian Mai and Lukas Galke and Ansgar Scherp},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MgjoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper591/Official_Review", "cdate": 1542234424829, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MgjoR9tQ", "replyto": "H1MgjoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper591/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759080, "tmdate": 1552335759080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper591/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}