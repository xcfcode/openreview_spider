{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1460649621649, "tcdate": 1460649621649, "id": "81rOkAP49c6O2Pl0UVE2", "invitation": "ICLR.cc/2016/workshop/-/paper/166/comment", "forum": "q7kqBKMN2U8LEkD3t7Xy", "replyto": "k8WD1v14NuOYKX7ji40V", "signatures": ["~Yingyu_Liang1"], "readers": ["everyone"], "writers": ["~Yingyu_Liang1"], "content": {"title": "Thanks for the related works", "comment": "Thanks for point out the related papers! Both papers provide nice results about reconstruction while our focus is more on reversibility and connection to generative models. More precisely, we assume that the data are generated from the top hidden layer and would like to recover the hidden layer from the data (instead of reconstructing the data from the hidden layer). Also, we focus on the case when the generative model and the recovery procedure are coupled (ie, weight tieing). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "244", "title": "Why are deep nets reversible: A simple theory, with implications for training", "abstract": "Generative models for  deep learning are promising  both to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. \nHowever, there is no accompanying \"proof of correctness\" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input.  Furthermore, these models are complicated. \n \n \nThe current paper takes a more theoretical tack. It presents a very simple generative model for ReLU deep nets, with the following characteristics: \n (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.)\n  (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for  recovering the hidden layer. \n\nThe generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels and  include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.\n\nThis extended abstract provides a succinct description of our results while the full paper is available on arXiv.", "pdf": "/pdf/q7kqBKMN2U8LEkD3t7Xy.pdf", "paperhash": "arora|why_are_deep_nets_reversible_a_simple_theory_with_implications_for_training", "conflicts": ["cs.princeton.edu"], "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "authorids": ["arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831131076, "ddate": null, "super": null, "final": null, "tcdate": 1455831131076, "id": "ICLR.cc/2016/workshop/-/paper/166/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "q7kqBKMN2U8LEkD3t7Xy", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1460616887555, "tcdate": 1460616887555, "id": "k8WD1v14NuOYKX7ji40V", "invitation": "ICLR.cc/2016/workshop/-/paper/166/comment", "forum": "q7kqBKMN2U8LEkD3t7Xy", "replyto": "q7kqBKMN2U8LEkD3t7Xy", "signatures": ["~Raja_Giryes1"], "readers": ["everyone"], "writers": ["~Raja_Giryes1"], "content": {"title": "related works", "comment": "There are two strongly related works to this one:\nThe first is the one from ICML 2014\nhttp://yann.lecun.com/exdb/publis/pdf/bruna-icml-14.pdf\nthat shows that it is possible to recover input of a network's layer from its output by drawing connections to phase retrieval theory.\n\nThe second is from last iclr\nhttp://arxiv.org/abs/1504.08291    (this is the iclr version: http://arxiv.org/pdf/1412.5896v3.pdf)\nthat shows that it is possible to recover the input of a network's layer from its output (and therefore the input of the whole network from its output) using assumption of random weights in the network and that the data is low dimensional. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "244", "title": "Why are deep nets reversible: A simple theory, with implications for training", "abstract": "Generative models for  deep learning are promising  both to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. \nHowever, there is no accompanying \"proof of correctness\" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input.  Furthermore, these models are complicated. \n \n \nThe current paper takes a more theoretical tack. It presents a very simple generative model for ReLU deep nets, with the following characteristics: \n (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.)\n  (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for  recovering the hidden layer. \n\nThe generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels and  include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.\n\nThis extended abstract provides a succinct description of our results while the full paper is available on arXiv.", "pdf": "/pdf/q7kqBKMN2U8LEkD3t7Xy.pdf", "paperhash": "arora|why_are_deep_nets_reversible_a_simple_theory_with_implications_for_training", "conflicts": ["cs.princeton.edu"], "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "authorids": ["arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831131076, "ddate": null, "super": null, "final": null, "tcdate": 1455831131076, "id": "ICLR.cc/2016/workshop/-/paper/166/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "q7kqBKMN2U8LEkD3t7Xy", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455831126738, "tcdate": 1455831126738, "id": "q7kqBKMN2U8LEkD3t7Xy", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "q7kqBKMN2U8LEkD3t7Xy", "signatures": ["~Yingyu_Liang1"], "readers": ["everyone"], "writers": ["~Yingyu_Liang1"], "content": {"CMT_id": "244", "title": "Why are deep nets reversible: A simple theory, with implications for training", "abstract": "Generative models for  deep learning are promising  both to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. \nHowever, there is no accompanying \"proof of correctness\" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input.  Furthermore, these models are complicated. \n \n \nThe current paper takes a more theoretical tack. It presents a very simple generative model for ReLU deep nets, with the following characteristics: \n (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.)\n  (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for  recovering the hidden layer. \n\nThe generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels and  include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.\n\nThis extended abstract provides a succinct description of our results while the full paper is available on arXiv.", "pdf": "/pdf/q7kqBKMN2U8LEkD3t7Xy.pdf", "paperhash": "arora|why_are_deep_nets_reversible_a_simple_theory_with_implications_for_training", "conflicts": ["cs.princeton.edu"], "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "authorids": ["arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}