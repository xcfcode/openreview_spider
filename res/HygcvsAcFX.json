{"notes": [{"id": "HygcvsAcFX", "original": "ryx_S9FOY7", "number": 289, "cdate": 1538087778084, "ddate": null, "tcdate": 1538087778084, "tmdate": 1545355425779, "tddate": null, "forum": "HygcvsAcFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BklwhMc7lE", "original": null, "number": 1, "cdate": 1544950447160, "ddate": null, "tcdate": 1544950447160, "tmdate": 1545354490118, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Meta_Review", "content": {"metareview": "The paper proposed an optimal margin distribution loss and applied PAC-Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method. \n\nThe majority of reviewers think the paper\u2019s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors\u2019 response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.  \n\nBased on current ratings, the paper is therefore proposed to borderline lean rejection. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "A new optimal margin distribution loss with some good empirical results, yet premature "}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper289/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353269958, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353269958}}}, {"id": "S1lXsaCBnm", "original": null, "number": 1, "cdate": 1540906395410, "ddate": null, "tcdate": 1540906395410, "tmdate": 1543277147105, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "content": {"title": "just seem interesting", "review": "The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].\nMore precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical \nbounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and \n[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some \nconditions) on the change of the layers (layer and interlayer cushion) as well as the activation \ncontraction. It is also worth noting that the paper is using a differrent loss function comparing \nto [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.\nAlthough the results seem interesting, the analysis is not convincible for me.\nA plus point is that the paper presents interesting numerical experiments showing the promising of the approach.\n\nMajor comments:\n1) The statement of the Theorem 1 is not clear: \nis it just under the assumptions of the lemmas\nor is it under all definitions and lemmas?\n2) The proof of Theorem 1 is not clear:\n how do you get the inequality (5)?\nhow do you get an upper bound on the KL divergence?\n This is not trivial for me!\n3) What is \\rho in Theorem 1 and in Definition 2?\n4) Your remark after Theorem 1 is not clear for me.\n  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?\n a simple counter example would fit better the explanation here, I guest.\n\nMinor comments:\n1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]\nwithout precisely citations. I wonder how do you obtain your Lemma 1?\n2) page3, after formula (1), your loss will first DECREASING, not \"increasing\".\nCheck the sentence \"Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....\"\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "cdate": 1542234495661, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335691060, "tmdate": 1552335691060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gKXwNHCm", "original": null, "number": 5, "cdate": 1542960929508, "ddate": null, "tcdate": 1542960929508, "tmdate": 1542962937467, "tddate": null, "forum": "HygcvsAcFX", "replyto": "S1lXsaCBnm", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\n##The contribution in our paper##\nOur paper theoretically proves that margin distribution plays an important role in the generalization of deep learning. Specifically, we propose a well-designed loss function inspired by [Gao, AIJ 2013; Zhang, ICML 2017], and it can effectively alleviate the overfitting of deep models. To the best of our knowledge, this is the first work that introduces margin distribution into the analysis of deep learning. We notice that some recent works [Barlett, NIPS 2017; Neyshabur, ICLR 2018] have considered this problem, but they only focus on minimum margin, which is significantly different from our paper.\n\nThe PAC-Bayesian framework is a convenient technique for margin theory analysis. In fact, other techniques like Rademacher complexity [Koltchinskii, TIT 2001; Koltchinskii, ANN STAT 2002; Bartlett, MACH LEARN 2002; Barlett, NIPS 2017] could also be used to derive similar results. In our paper the PAC-Bayesian technique is an analysis approach rather than our main contribution. As emphasized before, the theoretical contribution is relating the generalization gap to margin distribution.\n\n##Major comments##\n\nQ1. \u201cThe statement of the Theorem 1 is not clear: is it just \u2026 definitions and lemmas?\u201d:\nA. Theorem 1 is under all the lemmas and definitions in our paper.\n\nQ2. \u201cThe proof of Theorem 1 is not clear: how do you get the inequality (5)? how do you get an upper bound on the KL divergence?\u201d:\nA. The inequality (5) is an extension of matrix version Hoeffding\u2019s inequalities [Tropp, FOCS 2012; Mackey, ANN STAT 2014]. Note that the KL divergence is between two normal distributions with different mean but the same variance, so it can be bounded by |\\vw|^2 / 2 \\sigma^2.\n\nQ3. \u201cWhat is \\rho in Theorem 1 and in Definition 2?\u201d:\nA. As presented at the beginning of the Section 3: \u201cand $\\rho$ be an upper bound on the number of output units in each layer\u201d.\n\nQ4. \u201cyou claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?\u201d:\nA. The impact of norms in deep learning is not similar to the linear models, we can\u2019t directly normalize it to the value $1$. And the most common method to control the norm of weights is called weight decay, which tries to control the weights by a preset decay with a parameter. Because of its data-independence, the performance is not satisfied, and many more efficient methods to preventing the overfitting problem have been proposed, such as dropout and batch normalization. If we try to optimize the product of spectral norms directly as a regularization, we can find that the weights update of each layer is related to the product of spectral norms of other layers, the calculation cost for spectral norm is too large, and the weight decay does not consider this correlation.\n\n##Minor comments\n\nQ1. \u201cThe Lemma 1 and 2 are almost the same to Lemma 1 and 2 \u2026 you obtain your Lemma 1?\u201d:\nA. Lemma 1 is obtained in a similar way with PAC-Bayesian work [Neyshabur, ICLR 2018] and we have cited this paper in Sec. 3: \u201c... we have to relate this PAC-Bayesian bound to the expected perturbed loss just like [Neyshabur, ICLR 2018] derive the Lemma 1 in their paper.\u201d But for Lemma 2, we make a lot of nontrivial modification due to the introduction of margin variance.\n\nQ2. \u201cpage3, after formula (1), your loss will first DECREASING, not \"increasing\".\u201d:\nA. We have refined the misleading description.\n\nThe whole paper has been carefully improved. \n\nThank you very much for your help!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper289/Authors|ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613416}}}, {"id": "BkeJdPEr0Q", "original": null, "number": 6, "cdate": 1542960998725, "ddate": null, "tcdate": 1542960998725, "tmdate": 1542960998725, "tddate": null, "forum": "HygcvsAcFX", "replyto": "r1gKXwNHCm", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "content": {"title": "Reference in our reply", "comment": "Reference:\n[Gao, AIJ 2013] Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence, 203:1\u201318, 2013.\n[Zhang, ICML 2017] Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063\u20134071, 2017.\n[Barlett, NIPS 2017] Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.\n[Neyshabur, ICLR 2018] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.\n[Koltchinskii, TIT 2001] Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902\u20131914, 2001.\n[Koltchinskii, ANN STAT 2002] Vladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30, 2002.\n[Bartlett, MACH LEARN 2002] Peter L. Bartlett, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Model selection and error estimation. Machine Learning, 48:85\u2013113, September 2002a.\n[Tropp, FOCS 2012] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.\n[Mackey, ANN STAT 2014] Lester Mackey, Michael I. Jordan, Richard Y. Chen, Brendan Farrell, Joel A. Tropp. Matrix Concentration Inequalities via the Method of Exchangeable Pairs. The Annals of Probability, 42: 906\u2013945. 2014.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper289/Authors|ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613416}}}, {"id": "HyxjEHVBAm", "original": null, "number": 4, "cdate": 1542960434596, "ddate": null, "tcdate": 1542960434596, "tmdate": 1542960434596, "tddate": null, "forum": "HygcvsAcFX", "replyto": "S1xq2nxq2X", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\nQ1. \u201cwhether this theorem holds for other activations \u2026 instead of only the ReLU activation for all the layers\u201d:\nA. Current analysis only holds for ReLU, however, it\u2019s not difficult to generalize to other activations since only the Lipschitz property is required.\n\nQ2. \u201cCan an activation satisfying Definition 3 have a similar bound to Theorem 1\u201d:\nA. Definition 3 is a necessary but not sufficient condition. If other conditions are also satisfied, a similar bound can be achieved.\n\nQ3. \u201cdoes the convolutional layer simplify the bound in Theorem 1\u201d:\nA. Of course Theorem 1 may be simplified by considering some special cases, but it\u2019s beyond the scope of the paper.\n\nWe have also fixed the typos and the whole paper has been carefully improved.\n\nThank you very much for your help!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper289/Authors|ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613416}}}, {"id": "r1e4WH4S07", "original": null, "number": 3, "cdate": 1542960380339, "ddate": null, "tcdate": 1542960380339, "tmdate": 1542960380339, "tddate": null, "forum": "HygcvsAcFX", "replyto": "ryerWMf93m", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\nQ1. \u201clacks an in-depth study of the properties of this handcrafted loss\u201d and \u201cThe impact of loss hyper-parameters (r, \\gamma, \\mu) should be discussed thoughtfully\u201d:\nA. We have added an extra appendix A to clearly explain the intuition behind this loss function. \n\nQ2. \u201cthe provided PAC-Bayes generalization \u2026 shed no light on the benefit of the hinge and quadratic parts\u201d:\nA. Our paper proves that the generalization of deep learning heavily depends on the margin distribution. To optimize the margin distribution, we designed this ODN loss function. As a result, this well-designed loss function alleviates the overfitting problem of deep models efficiently. And we introduce how the hinge and quadratic part is derived from the intuition of margin distribution in appendix A.\n\n\nQ3. \u201cwould like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written\u201d:\nA. We have explicitly presented the \"(soft) hinge loss\" in section 4.1. \n\nWe have also fixed the typos and the whole paper has been carefully improved.\n\nThank you very much for your help!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper289/Authors|ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613416}}}, {"id": "SJlzk7uV3m", "original": null, "number": 2, "cdate": 1540813529599, "ddate": null, "tcdate": 1540813529599, "tmdate": 1542802430870, "tddate": null, "forum": "HygcvsAcFX", "replyto": "H1eRD8ag3m", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "content": {"title": "Sorry for the missing of this reference, but the contributions of these two works are totally different.", "comment": "Thanks for the comments. Our responses are proved below.\n\nQ1. Regarding the margin definition\nTo our knowledge, the margin definition in section 2 is widely used in multi-class learning setting, just to name a few, the classic book [Mohri M. 2012] (cf. chapter 8.2), the top journal / conference papers [Crammer K. JMLR 2001], [Zhang T. ICML 2017], [Peter L. B. NIPS 2017]. Therefore, the margin definition is not proposed by Elsayed et al., and we treat it as a common knowledge. Note that Elsayed et al. also use this definition without citation.\n\nQ2. Regarding the linear approximation\nThanks for pointing out the missing reference and we will cite Elsayed's paper in the revised version. However, we want to clarify that the novelty of our paper is irrelevant to this linear approximation. To be specific, we summarize the difference between our paper and Elsayed's paper in the next question.\n\nQ3. The contributions of our paper and Elsayed's paper are totally different\nThe main contribution of Elsayed's work is to introduce the hinge loss into deep models across all layers to improve the empirical performance. In contrast, our contribution is:\n1. Our paper proves that the margin distribution plays an important role in the generalization theory of deep learning;\n2. For alleviating the overfitting problem in deep learning, we designed this ODN loss function to optimize the margin distribution.\nTherefore, our paper and Elsayed's work have totally different motivations, as we do theoretical analysis for the margin distribution of deep learning frameworks, while they mainly focus on the empirical improvement.\n\n[Mohri M. 2012] Mohri M, Rostamizadeh A, Talwalkar A. Foundations of machine learning[M]. MIT press, 2012.\n[Crammer K. JMLR 2001] Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based vector machines[J]. Journal of machine learning research, 2001, 2(Dec): 265-292.\n[Zhang T. ICML 2017] Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063\u20134071, 2017.\n[Peter L. B. NIPS 2017] Peter L. B, Dylan J. F, and Matus J. T. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygcvsAcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper289/Authors|ICLR.cc/2019/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613416}}}, {"id": "ryerWMf93m", "original": null, "number": 3, "cdate": 1541181948925, "ddate": null, "tcdate": 1541181948925, "tmdate": 1541534121122, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "content": {"title": "Promising work, but an in-depth study of the handcrafted margin loss function is lacking", "review": "I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results.\n\nThe proposed margin loss (Equation 1) is said to be \"specially adapted for accelerating the convergence velocity of networks by [the authors]\". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \\gamma,\\mu) should be discussed thoughtfully; at some points in the paper, r and \\gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \\mu.\nBy considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\\gamma, r+\\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. \n\nThe empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process.\n\nTypos and minor comments:\n- Abstract: \"And our ODN model also outperforms the other three loss models...\" Which three loss models?\n- Section 3: \"Specially, define L_0 as r=\\theta...\" I think it should be r=0\n- Section 4.1: model-s => models\n- Page 7 (and elsewhere): Table. 2 => Table 2\n- Please specify that \"Xent\" stands for cross-entropy\n- Figure 3: Please use larger font sizes\n- Proof of Lemma 2: Equation. 4 => Equation 4 \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "cdate": 1542234495661, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335691060, "tmdate": 1552335691060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xq2nxq2X", "original": null, "number": 2, "cdate": 1541176498513, "ddate": null, "tcdate": 1541176498513, "tmdate": 1541534120866, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "content": {"title": "PAC-Bayesian analysis for DNNs", "review": "This paper presents a PAC-Bayesian bound for a margin loss.\n\nTheorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1?\n\nThere are some typos in this paper.\n\u201cTo derive a expected risk bound\u201d: a -> an\n\u201cused to formalize error-resilience in Arora et al. (2018) as following:\u201d: following: -> follows.\n\u201cthe deep network from layer i to layer j\u201d, \u201cinjected before level i\u201d: i,j should be in the math mode.\n\u201cdependent on the network structure .\u201d there is an additional blank space after \u2018structure\u2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper289/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Official_Review", "cdate": 1542234495661, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper289/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335691060, "tmdate": 1552335691060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper289/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1eRD8ag3m", "original": null, "number": 2, "cdate": 1540572773738, "ddate": null, "tcdate": 1540572773738, "tmdate": 1540958490916, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Public_Comment", "content": {"comment": "Parts of this submission shows great similarity (and sometimes identical sentences) to the following NIPS paper:\n\u201cLarge Margin Deep Networks for Classification\u201d by Elsayed et al. (arXiv March 2018).\n\nWhile the authors seem to have read the above reference and even adapted portions without much modification, they surprisingly do not mention the source. Below are some examples.\n\n## Section 2 of this submission vs Section 3 of Elsayed\u2019s ##\n\n1. Near the beginning of Section 2 the submission:\n\u201cDefine the decision boundary for each class pair {i, j} as: D{i,j} , {x | fi(x) = fj (x)}\u201d.\nThe English text, the equation and even variable naming is *identical* to Eq(1) in Elsayed\u2019s.\n2. *Immediately* after this equation, the submission continues the same as Elsayed\u2019s, with minor wording change:\nSubmission: \u201cConstructed on this definition, the margin distance of a sample point x to the decision boundary Di,j is defined by the smallest translation of the sample point to establish the equation as:\u201d\nElsayed\u2019s: \u201cUnder this definition, the distance of a point x to the decision boundary D{i,j} is defined as the smallest displacement of the point that results in a score tie:\u201d\nElsayed\u2019s: \u201c\n3. The above sentence is followed by an equation that is the same as Eq 2 of Elsayed\u2019s, including the naming convention (small delta, x, f_i, f_j).\n4. Immediately after this:\nSubmission: \u201cWe present an approximation to \u03b3 by linearizing f_i\u201d.\nElsayed\u2019s: Above their Eq 6: \u201cWe present an approximation to d by linearizing f_i\u201d, and then the same exact formula in Eq 7 of Elsayed\u2019s appears in the submission (even similar notation conventions, like nabla_x).\n\n## Section 4.1 ##\n\nThe middle of the first paragraph read is:\n\u201cthere is a gradient term in the loss itself, which can make the computation expensive. To reduce computational cost, in the backpropagation step we considered the gradient term EQN2 as a constant, so that we recomputed the value of EQN1 at every forward propagation step.\u201d\nElsayed\u2019s Section 4.1, also middle of the first paragraph:\n\u201c....presence of gradients in the loss itself\u2026. The backpropagation step for parameter updates requires the computation of second-order gradients. To further reduce computation cost to a manageable level\u2026. treating the denominator EQN1 in (15) as a constant w.r.t. w for backpropagation. The value of EQN2 is recomputed at every forward propagation step.\u201d\n\nAlso the end of this paragraph in the submission reads as:\n\u201cFurthermore, since the denominator item could be too small, which would cause numerical problem, we added an \\epsilon with small value to the denominator so that clip the loss at some threshold.\u201d\nElsayed\u2019s closing sentence for their Section 4.1: \u201cFinally, to improve stability when the  denominator is small, we found it beneficial to clip the loss at some threshold [denoted by \\epsilon].\u201d\n\n\n", "title": "Extreme similarity to a paper but without mentioning the source."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311874862, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HygcvsAcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311874862}}}, {"id": "Bkes3De1s7", "original": null, "number": 1, "cdate": 1539405746838, "ddate": null, "tcdate": 1539405746838, "tmdate": 1539405746838, "tddate": null, "forum": "HygcvsAcFX", "replyto": "HygcvsAcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper289/Public_Comment", "content": {"comment": "This is an interesting work. Considering margin distribution is more important than the minimum margin to the performance of model in the theoretical research in AdaBoost, applying this margin distribution principle to Deep Neural Network is a novel insight. Moreover, this paper provides a theoretical foundation of generalization bound, which can be used to search the value of hyper-parameters. In the experiment result, consistent with theoretical result , the DNN improves the performance on small sample learning problem, through introducing the margin variance to loss function. I read a similar thought in another paper submitted in this ICLR( R( https://openreview.net/forum?id=HJlQfnCqKX ),  ), it introduces a statistic $\\hat{R}^2$ through a lot of empirical tests, to vertify the importance of the margin distribution. I think $\\hat{R}^2$ is similar to the conception of margin variance in your paper. And the margin bound in your paper implement the future work in their paper, as they write: \"We believe that the method developed here can be used in complementary with existing generalization bound\".\nHere is some suggestion to your paper:\n1. Read the another paper I mentioned, and expand the related works;\n2. In the Section 2, the hyper-parameter $r$ and $\\theta$ representing margin mean and margin variance did not be stated, this trivial mistake makes me confused about this loss function;\n3. Since the margin distribution is so important to get a better \"representation\", did the author consider the application of this ODN to adversary attack problems in the future work?", "title": "Minor comments\uff1a"}, "signatures": ["~XINYI_LIN1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper289/Reviewers/Unsubmitted"], "writers": ["~XINYI_LIN1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "TL;DR": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "pdf": "/pdf/2f891efbe9bb2c8cabf81e4d929a2bfc0224d692.pdf", "paperhash": "lv|optimal_margin_distribution_network", "_bibtex": "@misc{\nlv2019optimal,\ntitle={Optimal margin Distribution Network},\nauthor={Shen-Huan Lv and Lu Wang and Zhi-Hua Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=HygcvsAcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper289/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311874862, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HygcvsAcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper289/Authors", "ICLR.cc/2019/Conference/Paper289/Reviewers", "ICLR.cc/2019/Conference/Paper289/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311874862}}}], "count": 12}