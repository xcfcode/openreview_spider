{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396683194, "tcdate": 1486396683194, "number": 1, "id": "HyQf6GI_l", "invitation": "ICLR.cc/2017/conference/-/paper570/acceptance", "forum": "S1JG13oee", "replyto": "S1JG13oee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper may have an interesting contribution, but at present its motivation, and the presentation in general, are not clear enough. After a re-write, the paper might become quite interesting and should be submitted to some other forum."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396683810, "id": "ICLR.cc/2017/conference/-/paper570/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1JG13oee", "replyto": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396683810}}}, {"tddate": null, "tmdate": 1484323367099, "tcdate": 1484323367099, "number": 4, "id": "BJyNcuULe", "invitation": "ICLR.cc/2017/conference/-/paper570/public/comment", "forum": "S1JG13oee", "replyto": "Bki7uJ84e", "signatures": ["~Masatoshi_Uehara1"], "readers": ["everyone"], "writers": ["~Masatoshi_Uehara1"], "content": {"title": "Thank you for the review ", "comment": "Thank you for the review.\nSorry for reply late.\n\n>> My main problem with this paper is that it is unclear why any of this is useful...This is completely hand-wavey and no further evidence is given to back this claim.\n\nWe tried experiments many times and confirmed  that when using the robust divergence like Pearson divergence, the learning did not tend to stop even if the learning rate is not low. We think it is enough to support our claim, i.e., using Pearson divergence is useful for easing the instability.\n\nWe are sorry that the figure is not easy to understandable.  For further explanation of the figure2, I added the comment below.\n\n>>  why was the normal GAN objective not tried in light of this analysis? \n\nWhen using Jensen-Shannon divergence , b-gan also worked. However, It is difficult to find any special properties to be confirmed in the experiments. \nThat's why we did not do in the paper. \n\n >> it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work.\n\nWe used only one heuristic to run our algorithm. That is bounding the ratio.  We think it is very natural because that kind of heuristic is also used many times in other fields such as classification, covariant shift. On the other hand, the heuristic used in the original GAN is a formulation level and more crucial and fatal. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287516786, "id": "ICLR.cc/2017/conference/-/paper570/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1JG13oee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper570/reviewers", "ICLR.cc/2017/conference/paper570/areachairs"], "cdate": 1485287516786}}}, {"tddate": null, "tmdate": 1482188834711, "tcdate": 1482188834711, "number": 3, "id": "Bki7uJ84e", "invitation": "ICLR.cc/2017/conference/-/paper570/official/review", "forum": "S1JG13oee", "replyto": "S1JG13oee", "signatures": ["ICLR.cc/2017/conference/paper570/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper570/AnonReviewer4"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching. The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective. In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x).\n\nMy main problem with this paper is that it is unclear why any of this is useful. The connection to density estimation is interesting, but any derived conclusions between the two seem questionable. For example, in previous density estimation literature, the Pearson divergence is more stable. The authors claim that the same holds for GANS and try to show this in their experiments. Unfortunately, the experiments section is very confusing with unilluminating figures. Looking at the graph of density ratios is not particularly illuminating. They claim that for the Pearson divergence and modified KL-divergence, \"the learning did not stop\" by looking at the graph of density ratios. This is completely hand-wavey and no further evidence is given to back this claim. Also, why was the normal GAN objective not tried in light of this analysis? Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work.\n\nI think this paper would be much improved if it was rewritten in a clear fashion. As it stands, it is difficult to understand the motivation or intuition behind this work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512537797, "id": "ICLR.cc/2017/conference/-/paper570/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper570/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper570/AnonReviewer1", "ICLR.cc/2017/conference/paper570/AnonReviewer3", "ICLR.cc/2017/conference/paper570/AnonReviewer4"], "reply": {"forum": "S1JG13oee", "replyto": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512537797}}}, {"tddate": null, "tmdate": 1481981830476, "tcdate": 1481981804834, "number": 3, "id": "B1HuJTz4g", "invitation": "ICLR.cc/2017/conference/-/paper570/public/comment", "forum": "S1JG13oee", "replyto": "Sy_oMlGEl", "signatures": ["~Masatoshi_Uehara1"], "readers": ["everyone"], "writers": ["~Masatoshi_Uehara1"], "content": {"title": "The explanation of Figure 2", "comment": "\nThank you for commenting. We are sorry that the experimental section is difficult to understand. \n\n- could you please clarify what you are plotting in left and right plots of Figure 2? What is the message that you are trying to convey from this figure?\n\nIn the left figure, density ratio, i.e., r (x), is plotted. In the right figure, f-divergence, i.e., f (r(x)) is plotted. It should be noted that all values are plotted every G-step and D-step and a single-step gradient method, i.e.,  repeating updating the discriminator once and updating the generator once, is used. By figure2, we shows that some techniques like using Pearson divergence and relative density ratio is useful for learning stability. \n\nIn GAN, it is known that the learning stopped when the gradient is too large. For example, the second right graph shows that situation. We call such a problem \u201cinstability\u201d. Usually, that problem can be solved by lowering the learning rate. However, at the same time, the learning becomes slow. So there is a tradeoff. In our experiment, by fixing the learning rate, we state that the instability problem is also prevented by using two techniques. \n\n\u30fb\u3000Use robust divergence like Pearson divergence not KL divergence and reversed KL divergence. \n\u30fb\u3000Use relative density ratio.\n\nWe showed the above things in Figure2. In fact, when using KL divergence, the learning stopped. (Second right graph)\nOn the other hand, when using Pearson divergence, the learning did not stop. (First right graph)\nAnd the learning did not stop when using KL divergence and relative density ratio. (Third right graph)\n\n- how does this affect your claims of stability / convergence in section 5.2 of the paper?\n\nWe judge the learning would be close to convergence by seeing generated images. \nIn our cases, around 40000 iterations is needed to generate natural images. \nGAN is a mini max problem, so it is difficult to judge by seeing plots like the figure2. \n\nHowever, our touching problem is the stability in the learning process as mentioned above, not convergence.  So we do not think that the difficulty of judging convergence weakens our statement. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287516786, "id": "ICLR.cc/2017/conference/-/paper570/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1JG13oee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper570/reviewers", "ICLR.cc/2017/conference/paper570/areachairs"], "cdate": 1485287516786}}}, {"tddate": null, "tmdate": 1481929376647, "tcdate": 1481929376647, "number": 1, "id": "Sy_oMlGEl", "invitation": "ICLR.cc/2017/conference/-/paper570/official/comment", "forum": "S1JG13oee", "replyto": "Bk1mYHC7g", "signatures": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "content": {"title": "clarification about figure 2", "comment": "\"In Figure 2, f-divergence is recorded in after G-step and after D-step. Sorry for typos.  We would revise it. \"\n- could you please clarify what you are plotting in left and right plots of Figure 2? What is the message that you are trying to convey from this figure?\n\n\"Yes, in fact, the learning did not converge, as in GANs.\nThe assessment of GANs including b-GAN and f-GAN is the future work.\"\n\n- how does this affect your claims of stability / convergence in section 5.2 of the paper?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287516648, "id": "ICLR.cc/2017/conference/-/paper570/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper570/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper570/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper570/reviewers", "ICLR.cc/2017/conference/paper570/areachairs"], "cdate": 1485287516648}}}, {"tddate": null, "tmdate": 1481928931229, "tcdate": 1481928931229, "number": 2, "id": "BkjkZezVg", "invitation": "ICLR.cc/2017/conference/-/paper570/official/review", "forum": "S1JG13oee", "replyto": "S1JG13oee", "signatures": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "content": {"title": "Interesting paper on connections between GANs and density ratio estimation research", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes b-GAN, which trains a discriminator by estimating density ratio that minimizes Bregman divergence. The authors also discuss how b-GANs relate to f-GAN and the original GAN work, providing a unifying view through the lens of density ratio estimation. \n\nNote that the unifying view applies only to GAN variants which optimize density ratios. In general, GANs which use MMD in the discriminator step do not fit in the b-GAN framework except for special choices of the kernel. \n\nI was a bit confused about the dual relationship between f-GAN and b-GAN. Are the conditions on the function f the same in both cases? If so, what's the difference between f-GAN and b-GAN (other than the fact that the former has been derived using f-divergence and the latter has been derived using Bregman divergence)?\n\nOne of the original claims was that b-GANs optimize f-divergence directly as opposed to f-GAN and GAN. However, in practice, the authors optimize an approximation to the f-divergence; the quality of the approximation is not quantified anywhere, so b-GAN doesn't seem more principled than f-GAN and GAN.\n\nThe experiments left me a bit confused and were not very illuminating on the choice of f. \n\nOverall, I liked the connections to the density ratio estimation literature. The appendix seems like a scattered collection right now. Some re-writing of the text would significantly improve this paper. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512537797, "id": "ICLR.cc/2017/conference/-/paper570/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper570/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper570/AnonReviewer1", "ICLR.cc/2017/conference/paper570/AnonReviewer3", "ICLR.cc/2017/conference/paper570/AnonReviewer4"], "reply": {"forum": "S1JG13oee", "replyto": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512537797}}}, {"tddate": null, "tmdate": 1481811090336, "tcdate": 1481811090328, "number": 2, "id": "BJ5qVXgNe", "invitation": "ICLR.cc/2017/conference/-/paper570/public/comment", "forum": "S1JG13oee", "replyto": "SyQ_dsaXe", "signatures": ["~Masatoshi_Uehara1"], "readers": ["everyone"], "writers": ["~Masatoshi_Uehara1"], "content": {"title": "Thank you for comments.", "comment": ">> This submission introduces a formulation of Generativ...\n\nThe motivation of using Bregman divergences is not to obtain an objective function with stronger gradients. The motivation is solving the such heuristics introduced in the f-GAN (including the original GAN) and finding the general right formulation. It is often said that a necessary heuristic, i.e., changing objective functions when updating a generator, is introduced to obtain stronger gradients. However, it would be natural that the original formulation is not precise in fact. There is a possibility that there exists the better formulation beyond the original GAN. \n\n>> First, the exposition of the paper must be significantly improved\u2026.\n\nSorry for unreadable points. I will revise it. \n\n>> Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN\u2026.\n\nWe used only two heuristics. The first heuristic is bounding the density ratio. It is a necessary heuristic. The second heuristic is using a relative-density ratio. It is not a necessary heuristic; however, it is helpful for the stability of GANs.  (The similar heuristic is introduced in [1])\n\nIn addition, our main algorithm used in experiments is only provided in algorithm 1.  The other introduced algorithm is a trivial variable transformation version of f-GAN. We compared these objective functions by clarifying the understanding of GANs. Under our experiment, b-GAN worked by only changing objective functions and the first heuristic from the usual DCGAN. So, it is easy to implement our algorithm. \n\n>> Third, it is next to impossible to interpret the experimental results...\n\nWe compared some divergences by fixing the learning rate and seeing the behaviour. And we confirmed that Pearson divergence and relative-density ratio is useful. That confirmation would me meaningful. \n\nWe do not state that our algorithm is superior to the original GANs from the perspective of the accuracy of the generated distribution. However, our b-GAN is a more natural algorithm that dose not need a mysterious heuristic. As far as I know, the evaluation of variants of GANs is not a settled problem. We believe that providing new insight and algorithm would be definitely meaningful. As far as I mentioned, there is a possibility that there exists the better formulation beyond the original GAN. \n\n[1] Salimans, Tim, Goodfellow, Ian, Zaremba, Wojciech, Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287516786, "id": "ICLR.cc/2017/conference/-/paper570/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1JG13oee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper570/reviewers", "ICLR.cc/2017/conference/paper570/areachairs"], "cdate": 1485287516786}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481801270012, "tcdate": 1478373127141, "number": 570, "id": "S1JG13oee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1JG13oee", "signatures": ["~Masatoshi_Uehara1"], "readers": ["everyone"], "content": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481717589998, "tcdate": 1481689367350, "number": 1, "id": "Bk1mYHC7g", "invitation": "ICLR.cc/2017/conference/-/paper570/public/comment", "forum": "S1JG13oee", "replyto": "Syevf2J7g", "signatures": ["~Masatoshi_Uehara1"], "readers": ["everyone"], "writers": ["~Masatoshi_Uehara1"], "content": {"title": "Thank you for comments.", "comment": "Thank you for comments.\nSorry for late replying. We missed a message. \n\nIn Figure1, we put an image to make it easy to imagine.\nThe comparisons explained in the sentence is not restricted to Pearson divergence. \n\nIn FIgure 2, f-divergence is recorded in after G-step and after D-step. Sorry for typos.  We would revise it. \nf-divergence after G-step is low and f-divergence after D-step is high.\nThat\u2019s why f-divergence is up and down.\nYes, in fact, the learning did not converge, as in GANs.\nThe assessment of GANs including b-GAN and f-GAN is the future work. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287516786, "id": "ICLR.cc/2017/conference/-/paper570/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1JG13oee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper570/reviewers", "ICLR.cc/2017/conference/paper570/areachairs"], "cdate": 1485287516786}}}, {"tddate": null, "tmdate": 1481648234997, "tcdate": 1481648234989, "number": 1, "id": "SyQ_dsaXe", "invitation": "ICLR.cc/2017/conference/-/paper570/official/review", "forum": "S1JG13oee", "replyto": "S1JG13oee", "signatures": ["ICLR.cc/2017/conference/paper570/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper570/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences. Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients. I have three concerns with this submission.\n\nFirst, the exposition of the paper must be significantly improved. The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.\n\nSecond, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN. This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN. Further work is necessary to rule out (in a principled manner!) many of the proposed variants of the algorithm.\n\nThird, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4. The authors claim that these figures show that \"learning does not stop\", but such behavior can also be attributed to the typical chaotic dynamics of GANs. Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).\n\nOverall, I believe this submission calls for significant improvements before being considered for publication.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512537797, "id": "ICLR.cc/2017/conference/-/paper570/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper570/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper570/AnonReviewer1", "ICLR.cc/2017/conference/paper570/AnonReviewer3", "ICLR.cc/2017/conference/paper570/AnonReviewer4"], "reply": {"forum": "S1JG13oee", "replyto": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512537797}}}, {"tddate": null, "tmdate": 1480733272386, "tcdate": 1480733272381, "number": 1, "id": "Syevf2J7g", "invitation": "ICLR.cc/2017/conference/-/paper570/pre-review/question", "forum": "S1JG13oee", "replyto": "S1JG13oee", "signatures": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "content": {"title": "question about figures 1 and 2", "question": "How does figure 1 look like for other values of f?\n\nLooking at figure 2, it seems like the generator loss does not change much? How do you assess convergence in these cases?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "b-GAN: Unified Framework of Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ", "pdf": "/pdf/debdd3674598cc40a4812df3eeaf7e814823d3bd.pdf", "TL;DR": "New Unified Framework of Generative Adversarial Networks using Bregman divergence beyond f-GAN", "paperhash": "uehara|bgan_unified_framework_of_generative_adversarial_networks", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["weblab.t.u-tokyo.ac.jp", "k.u-tokyo.ac.jp", "g.ecc.u-tokyo.ac.jp"], "authors": ["Masatosi Uehara", "Issei Sato", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["uehara-masatoshi136@g.ecc.u-tokyo.ac.jp", "sato@k.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959209012, "id": "ICLR.cc/2017/conference/-/paper570/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper570/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper570/AnonReviewer3"], "reply": {"forum": "S1JG13oee", "replyto": "S1JG13oee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper570/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959209012}}}], "count": 11}