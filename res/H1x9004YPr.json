{"notes": [{"id": "H1x9004YPr", "original": "r1eX6KcdPr", "number": 1440, "cdate": 1569439441646, "ddate": null, "tcdate": 1569439441646, "tmdate": 1577168218544, "tddate": null, "forum": "H1x9004YPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BiZnkni1lo", "original": null, "number": 1, "cdate": 1576798723330, "ddate": null, "tcdate": 1576798723330, "tmdate": 1576800913218, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Decision", "content": {"decision": "Reject", "comment": "With an average post author response score of 4 - two weak rejects and one weak accept, it is just not possible for the AC to recommend acceptance. The author response was not able to shift the scores and general opinions of the reviewers and the reviewers have outlined their reasoning why their final scores remain unchanged during the discussion period.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708926, "tmdate": 1576800257484, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Decision"}}}, {"id": "Byxh9hH2or", "original": null, "number": 5, "cdate": 1573833875987, "ddate": null, "tcdate": 1573833875987, "tmdate": 1573833875987, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment", "content": {"title": "Paper Update", "comment": "We appreciate the constructive feedback of every reviewer. We have thoroughly refined the paper: grammar errors are corrected, sections including abstract, introduction and experiments are retouched, and appendix is added to provide more clear explanations."}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x9004YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1440/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1440/Authors|ICLR.cc/2020/Conference/Paper1440/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155981, "tmdate": 1576860559790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment"}}}, {"id": "Sye-x7zKsH", "original": null, "number": 1, "cdate": 1573622504993, "ddate": null, "tcdate": 1573622504993, "tmdate": 1573786231183, "tddate": null, "forum": "H1x9004YPr", "replyto": "rJlIaog6YS", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 [1/2]", "comment": "First of all, we thank the reviewer for the constructive feedback. \n\n(Q1) Eq. 5. The temperature scalar for each token competes with each other, since they are calculated with a softmax (and then rescaled). Another way is to use, e.g., a sigmoid function. Can the authors explain the motivation behind the use of softmax?\n\n(A1) We agree with the reviewer that using a softmax to control the temperature of each token will make these temperatures compete with each other (since they have to sum up to 1). As the reviewer suggested, we conduct more experiments to compare softmax with tanh and sigmoid. Experiment results show that using softmax achieves the lower perplexity compared to the other two functions\u2014softmax: 54.69, sigmoid: 57.74, and tanh: 58.89 (on the test set of the PTB dataset). We conjecture that the relationship among different tokens represents a certain kind of competitiveness as only a few tokens share similar semantics as the ground-truth token should be generated in a sentence. We appreciate the suggestion from the reviewer. \n\n(Q2) Another view of the proposed method is that it learns a context-dependent weighting of the tokens in the vocabulary, such that \"important\" tokens (those with smaller \\tau) receive more gradient updates. Can the authors comment on this? \n\n(A2) We appreciate the reviewer's insights on the relationship between the importance of tokens and the magnitude of the gradients from the contextual temperature. As each token is represented by an embedding that has multiple dimensions, we believe \u2018more gradient updates\u2019 mentioned by the reviewer actually refers to larger gradient norms. If that's the case, we generally agree with the reviewer's insights. \n\nIn the paper, we observe that common tokens ('<eos>\u2019, \u2018of\u2019, \u2018the\u2019, ...) receive the temperature that increases dramatically during the course of training (please refer to Figure 1a in the paper), which effectively scales down the corresponding logits. We refer these tokens as \u2018unimportant\u2019 tokens to contrast the rest of tokens (referred to as \u2018important\u2019 tokens). We then calculate the average gradient norm of \u2018important tokens\u2019 and repeat the same procedure for \u2018unimportant\u2019 tokens. The results are provided below. \n\nTo confirm the reviewer's conjecture, we calculated the average norm of gradients with respect to the embedding parameters. We calculated the results separately for the case when `important tokens' are the ground truth and the case when `unimportant tokens` are ground truth. Results are shown in the next table. \n\nWe note that, when the ground truth belongs to \u2018important tokens\u2019, the average gradient norm of \u2018important tokens\u2019 is larger. Same story for unimportant tokens: When the ground truth belongs to unimportant tokens, the average gradient norm of unimportant tokens is larger. In other words, depending on the ground truth belonging to important or unimportant tokens, applying contextual temperature seems to make the corresponding type of tokens receive a larger gradient norm. \n\n(1) If the ground truth belongs to \u201cimportant tokens\u201d, then\ngradient norm of \"important\" tokens\t  \tavg. \\tau of \u201cimportant\u201d tokens\t\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n             0.260164  (a)\t\t\t                                            2.0001514\n\ngradient norm of \"unimportant\" tokens\t avg. \\tau of \u201cunimportant\u201d tokens\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n             0.045674  (b)                                                                2.0696018 \n\n(2) If the ground truth belongs to \u201cunimportant tokens\u201d, then\ngradient norm of \"important\" tokens\t\tavg. \\tau of \u201cimportant\u201d tokens\t\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n             0.280090  (c)\t\t\t                                            2.0001671\n\ngradient norm of \"unimportant\" tokens\t avg. \\tau of \u201cunimportant\u201d tokens\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n             0.491596  (d)                                                                2.047023\n\nHere, each of the entries is calculated as the following, where y represents samples, L2(*) represents the norm function, \u2207(y,\u0398) represents the gradient vector of sample y and parameter \u0398, \u0398 represents a parameter and E is the expectation function. \n \na = E_{y in important} E_{\u0398 in important} L2(\u2207(y,\u0398))\nb = E_{y in important} E_{\u0398 in unimportant} L2(\u2207(y,\u0398))\nc = E_{y in unimportant} E_{\u0398 in important} L2(\u2207(y,\u0398))\nd = E_{y in unimportant} E_{\u0398 in unimportant} L2(\u2207(y,\u0398))"}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x9004YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1440/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1440/Authors|ICLR.cc/2020/Conference/Paper1440/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155981, "tmdate": 1576860559790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment"}}}, {"id": "H1xvj9zFjS", "original": null, "number": 3, "cdate": 1573624479334, "ddate": null, "tcdate": 1573624479334, "tmdate": 1573785780689, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1em6VUatr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "First of all, we thank the reviewer for the feedback.\n\n(Q1) The idea of dynamic temperature scaling has been tried in other works and tasks (e.g., attended temperature scaling). The paper parameterizes this mechanism with DNNs for the language model.  Though the idea looks interesting, it fails to explain why the scaling is better than other dynamic temperature scaling frameworks. \n\n(A1) We appreciate the feedback from the reviewer. To the best of our knowledge, we are the first work to learn a different temperature for each token based on the context. We have done a comprehensive survey on related works, but we haven\u2019t find any similar work. To answer the reviewer's question, the proposed method distinguishes from the other attended temperature scaling since attended temperature scaling paper learns a temperature that is universal for all the classes (tokens). Contextual temperature, on the other hand, learns a different temperature for each token and is thus a more general approach. To make it more clear to readers on the differences between our method and other related ones, we plan to modify the paper to emphasize the differences.\n\n(Q2) The experiments are not solid. The baseline only includes Mos, which is not very strong. To validate whether this approach works with other LM of high-order attention or self-attention, a better baseline model is required (e.g., transformer, GPT). \n\n(A2) We would like to point out that MoS is the state-of-the-art model on language modeling on the Penn Treebank dataset and WikiText-2 dataset. The Transformer-XL model, which is based on the transformer architecture, actually performs worse than the MoS model in these two datasets. To make the comparison clear, below we've put a summary of the comparisons between these baselines and our approach. Although the Transformer-XL model performs worse than MoS in the paper, we do agree that it should be added as a comparison to the paper. We will revise it in an updated version. \n\nModel | validation ppl | test ppl\nCT-MoS (ours)                              | 55.31 | 53.20\nMoS                            \t\t\t| 56.54 | 54.44\nTransformer-XL                            | 56.72 | 54.52\nGPT-2 (w/ extra training data and significantly larger model params) |-| 35.76\n\nFinally, the GPT model works on a very different setting than the ones found in the mainstream language model research. In the GPT paper, it utilizes a large dataset that is collected outside the domain of the language modeling. We argue that a comparison between GPT and the other baselines would be unfair as the standard setting of language modeling do not use additional datasets. As the proposed contextual temperature method aims at improving language model in the standard setting without the use of additional dataset, we believe that it would be more appropriate to compare against baselines under the same setting.\n\n(Q3) I would like to see this technique can help either NLU or NLG tasks, instead of just pure modeling.\n\n(A3) We would like to point out that the goal of our paper is to study language modeling other than its performance on the downstreaming NLU or NLG tasks. It is true that many language models such as BERT and XLNet are designed specifically for boosting the performance of downstream NLU and NLG tasks, models that study language modeling such as MoS focus purely on the performance of the language model itself. To this end, we feel it is important to separate these two groups of research as they fundamentally serve as different goals. We will clarify the differences on an updated version of the paper.\n\n(Q4) The case analysis section needs more examples instead of just cherry-picking few.\n\n(A4) We have provided more examples in Appendix A. Hopefully that can provide more insights on the methods. We will opensource the codebase upon the acceptance of this paper to allow the examinations of more examples. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x9004YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1440/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1440/Authors|ICLR.cc/2020/Conference/Paper1440/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155981, "tmdate": 1576860559790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment"}}}, {"id": "Bkx_JJQtor", "original": null, "number": 4, "cdate": 1573625568067, "ddate": null, "tcdate": 1573625568067, "tmdate": 1573785222254, "tddate": null, "forum": "H1x9004YPr", "replyto": "Ske6l8RJjS", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "First of all, we thank the reviewer for the feedback.\n\nTemperature scaling as a technique to control the smoothness of the softmax output is widely used in NLP, as can be seen from the large body of literature that we have surveyed in our paper. We have examined each of them but we found it generally difficult to justify the method from a theoretical perspective: either on its convergence or the property of the loss function that it leads to. However, even if theoretical analysis is difficult, its empirical performance of temperature has been verified by a large body of work in NLP. \n\nSimilar to the situation of the temperature in general, our model, which builds on a highly nonlinear transformation of inputs, is difficult to generate theoretical guarantees. However, given the large amount of empirical evidence, it is unlikely that the effectiveness of this approach is a coincidence. We believe the significance of the proposed contextual temperature is that it provides a more general view of the temperature mechanism. And its effectiveness can be demonstrated in a wide range of NLP tasks. If the reviewer has any further suggestions on the theoretical analysis, we would love to know."}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x9004YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1440/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1440/Authors|ICLR.cc/2020/Conference/Paper1440/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155981, "tmdate": 1576860559790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment"}}}, {"id": "SyeltUfKiH", "original": null, "number": 2, "cdate": 1573623415859, "ddate": null, "tcdate": 1573623415859, "tmdate": 1573623415859, "tddate": null, "forum": "H1x9004YPr", "replyto": "rJlIaog6YS", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 [2/2] ", "comment": "(Q3) Also, I don't see the thermodynamics connection and find calling the proposed method `temperature` a bit misleading.\n\n(A3) We follow the naming convention in related previous works we\u2019ve known [1, 2, 3] and call our method \u201ctemperature\u201d. As mentioned in [2], the connection between temperature scaling (in deep learning domain) and thermodynamics can be found in statistical mechanics [4]. We are willing to hear if there are any advice about the naming. \n\n[1] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. 2015.\n[2] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. 2017.\n[3] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward controlled generation of text. 2018.\n[4] Jaynes, Edwin T. Information theory and statistical mechanics. 1957.\n\n(Q4) Adding onto above. [1] discusses the low-rank bottleneck of using a single softmax. Since elementwise matrix product can blow up the rank, how do the authors think the proposed method can serve as a more efficient way to deal with the softmax bottleneck?\n\n(A4) We thank the reviewer for the great feedback. According to rank inequality( R(A\u0966B)\u2264R(A)R(B) ), element-wise matrix product indeed will potentially increase the rank. This is a new theoretical direction for the proposed contextual temperature, and we will study more in-depth in this direction and update the manuscript when having concrete conclusion and/or findings. Due to the limited time of ICLR rebuttal, we are not able to finish the analysis before the deadline. However, we will keep working on finding the evidence of this conjecture as the reviewer suggested. \n\n(Q5) Last but not least, the paper can be improved a lot if the authors can thoroughly polish the writing.\n\n(A5) Thank you for the advice. We have identified several spots in the paper that we can further polishing. Additionally, we have also improved the introduction section as well as the analysis section. We will keep looking for potential issues in writing. Here we list a few of changes we have made:\nIn Abstract:  co-adopt => co-adapt\nIn Introduction:  exiting work => existing methods\n\"explored the vocabulary differences when adjusting temperature\"  => \"explored the differences among vocabulary tokens when adjusting temperature\"\n\"tends to be heating up\" => \"tends to heat up\"\n\"This suggests that temperature mechanism helps to promote stochasticity early in\nthe sentence while suppressing uncertainties when the context gets longer\" => \"This suggests that the temperature mechanism helps promote stochasticity early in the sentence, and suppress uncertainties when the context gets longer.\"\ndealing with these phenomenons => dealing with these phenomena.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x9004YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1440/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1440/Authors|ICLR.cc/2020/Conference/Paper1440/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155981, "tmdate": 1576860559790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Authors", "ICLR.cc/2020/Conference/Paper1440/Reviewers", "ICLR.cc/2020/Conference/Paper1440/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Comment"}}}, {"id": "Ske6l8RJjS", "original": null, "number": 3, "cdate": 1573017076893, "ddate": null, "tcdate": 1573017076893, "tmdate": 1573017076893, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #3", "review": "This paper presents a strategy to automatically adjust the temperature scaling based on the context of words in a sentence for NLP. Experiments demonstrate that this approach can significantly improve perplexity scores on several datasets popular for NLP.\n\nNLP is not an area of research I'm very familiar with so this review is limited to my understanding of temperature scaling as a general technique to improve learning. As described in the paper, temperature scaling is a type of hyper-parameter estimation that adjusts the sensitivity of the softmax function as training evolves. The paper proposes to learn a function that given context, adjust the temperature automatically. This can be seen as a meta-learning method. \n\nI believe this can be a useful technique but before considering such an approach as a general strategy, more theoretical insights should be provided. The authors report on ablation studies that demonstrate some empirical benefits. However, until I see more theoretical analysis on how the method improves convergence or lead to better losses by smoothing out the output of the objective function, I remain skeptical of the usefulness of this as a general training method.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216428554, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Reviewers"], "noninvitees": [], "tcdate": 1570237737363, "tmdate": 1575216428565, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review"}}}, {"id": "rJlIaog6YS", "original": null, "number": 1, "cdate": 1571781565557, "ddate": null, "tcdate": 1571781565557, "tmdate": 1572972468893, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work proposes a learned and context dependent way to calculate the temperatures for the softmaxes. More specifically, a low-rank affine-transformation, taking the hidden state at the current step as input, is used to calculate scalar weighting for every token in the vocabulary. The method is very general, and can be used in combination with other techniques in tasks such as language modeling and text generation. Experiments on language modeling with Penn TreeBank and WikiText-2 show that the proposed method yields strong performance.\n\nOverall I found the paper well-motivated and easy to follow. The empirical results are solid and strong. The analysis is also interesting. I vote for an acceptance, if the authors can polish the writing.\n\nDetails:\n\n- Eq. 5. The temperature scalar for each token competes with each other, since they are calculated with a softmax (and then rescaled). Another way is to use, e.g., a sigmoid function. Can the authors explain the motivation behind the use of softmax?\n\n- Another view of the proposed method is that it learns a context-dependent weighting of the tokens in the vocabulary, such that \"important\" tokens (those with smaller \\tau) receive more gradient updates. Can the authors comment on this? Also, I don't see the thermodynamics connection and find calling the proposed method `temperature` a bit misleading. \n\n- Adding onto above. [1] discusses the low-rank bottleneck of using a single softmax. Since elementwise matrix product can blow up the rank, how do the authors think the proposed method can serve as a more efficient way to deal with the softmax bottleneck?\n\n- Last but not least, the paper can be improved a lot if the authors can thoroughly polish the writing.\n\n\n[1] Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. https://arxiv.org/abs/1711.03953"}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216428554, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Reviewers"], "noninvitees": [], "tcdate": 1570237737363, "tmdate": 1575216428565, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review"}}}, {"id": "H1em6VUatr", "original": null, "number": 2, "cdate": 1571804346940, "ddate": null, "tcdate": 1571804346940, "tmdate": 1572972468846, "tddate": null, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "invitation": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a contextual temperature scaling to improve language modeling. The temperature model is parameterized using a deep neural network. Experiments on the language modeling datasets show some effects of the method. \n\nThe idea of dynamic temperature scaling has been tried in other works and tasks (e.g., attended temperature scaling). The paper parameterizes this mechanism with DNNs for the language model.  Though the idea looks interesting, it fails to explain why the scaling is better than other dynamic temperature scaling frameworks. \n\nThe experiments are not solid. The baseline only includes Mos, which is not very strong. To validate whether this approach works with other LM of high-order attention or self-attention, a better baseline model is required (e.g., transformer, GPT). I would like to see this technique can help either NLU or NLG tasks, instead of just pure modeling. The case analysis section needs more examples instead of just cherry-picking few. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1440/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["peihsin@gapp.nthu.edu.tw", "steins1111@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "title": "Contextual Temperature for Language Modeling", "authors": ["Pei-Hsin Wang", "Sheng-Iou Hsieh", "Shieh-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "pdf": "/pdf/c87b26edce4d2a4f9bb1446f9dea793ba69142fb.pdf", "TL;DR": "We propose contextual temperature, a mechanism that enables temperature scaling for language models based on the context of each word. Contextual temperature co-adapts with model parameters and can be learned during training.", "abstract": "Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a \ufb01xed value or a dynamically changing temperature but with a \ufb01xed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, we propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signi\ufb01cantly. Our model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis showed that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justi\ufb01ed the need for contextual temperature and explained its performance advantage over \ufb01xed temperature or scheduling.", "keywords": ["natural language processing", "language modeling", "sequence modeling", "temperature scaling"], "paperhash": "wang|contextual_temperature_for_language_modeling", "original_pdf": "/attachment/be1c3add68d12e4b4cce15fb8cb7e59b9c81caaa.pdf", "_bibtex": "@misc{\nwang2020contextual,\ntitle={Contextual Temperature for Language Modeling},\nauthor={Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x9004YPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x9004YPr", "replyto": "H1x9004YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1440/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575216428554, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1440/Reviewers"], "noninvitees": [], "tcdate": 1570237737363, "tmdate": 1575216428565, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1440/-/Official_Review"}}}], "count": 10}