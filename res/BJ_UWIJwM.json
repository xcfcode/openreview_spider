{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124470964, "tcdate": 1518457168015, "number": 172, "cdate": 1518457168015, "id": "BJ_UWIJwM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJ_UWIJwM", "signatures": ["~Shiyu_Liang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification", "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "paperhash": "liang|understanding_the_loss_surface_of_singlelayered_neural_networks_for_binary_classification", "_bibtex": "@misc{\n  liang2018understanding,\n  title={Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification},\n  author={Shiyu Liang and Ruoyu Sun and Yixuan Li and R.Srikant},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UWIJwM}\n}", "authorids": ["sliang26@illinois.edu", "ruoyus@illinois.edu", "yl2363@cornell.edu", "rsrikant@illnois.edu"], "authors": ["Shiyu Liang", "Ruoyu Sun", "Yixuan Li", "R.Srikant"], "keywords": [], "pdf": "/pdf/882207a245cc1f48ea5f76a1c06c9c75e40c5454.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582981767, "tcdate": 1519795544972, "number": 1, "cdate": 1519795544972, "id": "SJZPp2Q_M", "invitation": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer1"], "content": {"title": "Strong and Solid Theory Paper", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This is a very strong and solid theory paper on neural networks. Analyzing classification is often a more difficult problem than regression. This paper considers many aspects that affect the classification error, including: loss function, activation function and data distribution. The data distribution assumption is very interesting and may inspire further research. Further, this paper also provides counter examples which are useful for theoretical researchers.\nOverall I recommend to accept strongly.\n\n(Since proofs are not included, I cannot check the correctness.)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification", "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "paperhash": "liang|understanding_the_loss_surface_of_singlelayered_neural_networks_for_binary_classification", "_bibtex": "@misc{\n  liang2018understanding,\n  title={Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification},\n  author={Shiyu Liang and Ruoyu Sun and Yixuan Li and R.Srikant},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UWIJwM}\n}", "authorids": ["sliang26@illinois.edu", "ruoyus@illinois.edu", "yl2363@cornell.edu", "rsrikant@illnois.edu"], "authors": ["Shiyu Liang", "Ruoyu Sun", "Yixuan Li", "R.Srikant"], "keywords": [], "pdf": "/pdf/882207a245cc1f48ea5f76a1c06c9c75e40c5454.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582981574, "id": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper172/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer2"], "reply": {"forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582981574}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582959112, "tcdate": 1520141070113, "number": 2, "cdate": 1520141070113, "id": "rJIGQ-KdM", "invitation": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer3"], "content": {"title": "Interesting progress in the classification setting", "rating": "7: Good paper, accept", "review": "The paper has some nice results on the loss landscape of a single layer neural network when the activation function is strongly convex and when the loss function is a smooth version of the hinge loss then every local minimal is the same as the global minimum.\n\nThe paper is clearly written in that it explicitly lists out the four assumptions under which the conditions hold. The result is also significant in that many recent works focus on the regression setting.\n\nIt seems the work replies heavily on the smoothness and large curvature of the functions, for example, the activation functions need to be strictly convex and the smoothed hinge loss must satisfy p >= 6. It will be interesting to talk about briefly why such smoothness is needed. And what is the difficulty with proving ReLU and leaky ReLU?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification", "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "paperhash": "liang|understanding_the_loss_surface_of_singlelayered_neural_networks_for_binary_classification", "_bibtex": "@misc{\n  liang2018understanding,\n  title={Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification},\n  author={Shiyu Liang and Ruoyu Sun and Yixuan Li and R.Srikant},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UWIJwM}\n}", "authorids": ["sliang26@illinois.edu", "ruoyus@illinois.edu", "yl2363@cornell.edu", "rsrikant@illnois.edu"], "authors": ["Shiyu Liang", "Ruoyu Sun", "Yixuan Li", "R.Srikant"], "keywords": [], "pdf": "/pdf/882207a245cc1f48ea5f76a1c06c9c75e40c5454.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582981574, "id": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper172/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer2"], "reply": {"forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582981574}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582738357, "tcdate": 1520665368366, "number": 3, "cdate": 1520665368366, "id": "HkeQQZ-KM", "invitation": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer2"], "content": {"title": "Review for \"Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification\"", "rating": "4: Ok but not good enough - rejection", "review": "This paper shows that under certain conditions, all local minima of a single-layer neural network has zero training error for binary classification. The key assumption is on the distribution of input data. It assumes that the positive examples and the negative examples belong to two different low-dimensional linear subspaces, and these two subspaces don't overlap.\n\nComparing to existing work, the result of this paper is interesting in that it doesn't require the number of hidden nodes to be more than the number of training examples. In particular, when the subspaces of positive and negative examples are highly separable, the number of hidden nodes can be much smaller than the number of samples. Nevertheless, the conclusion only holds for certain smooth activation functions and loss.\n\nThe main limitation is the strong assumption on the data distribution, which never holds in practice. Indeed, under this paper's assumption, we can fit two PCA models, on the positive examples and the negative examples respectively, then for any data point, we can use the fitting error on these two models to predict its binary class (the correct class has zero fitting error, while the incorrect class has strictly positive fitting error). This simple approach can achieve zero training and testing error with much lower sample complexity, by training much less parameters. It means that the problem itself is so easy that neural network is a complete overkill. The theoretical result for such a problem is thus not interesting, especially when it only holds under restrictive assumptions.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification", "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "paperhash": "liang|understanding_the_loss_surface_of_singlelayered_neural_networks_for_binary_classification", "_bibtex": "@misc{\n  liang2018understanding,\n  title={Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification},\n  author={Shiyu Liang and Ruoyu Sun and Yixuan Li and R.Srikant},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UWIJwM}\n}", "authorids": ["sliang26@illinois.edu", "ruoyus@illinois.edu", "yl2363@cornell.edu", "rsrikant@illnois.edu"], "authors": ["Shiyu Liang", "Ruoyu Sun", "Yixuan Li", "R.Srikant"], "keywords": [], "pdf": "/pdf/882207a245cc1f48ea5f76a1c06c9c75e40c5454.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582981574, "id": "ICLR.cc/2018/Workshop/-/Paper172/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper172/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper172/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper172/AnonReviewer2"], "reply": {"forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper172/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582981574}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573556302, "tcdate": 1521573556302, "number": 58, "cdate": 1521573555950, "id": "B1hhCAAtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJ_UWIJwM", "replyto": "BJ_UWIJwM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification", "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "paperhash": "liang|understanding_the_loss_surface_of_singlelayered_neural_networks_for_binary_classification", "_bibtex": "@misc{\n  liang2018understanding,\n  title={Understanding the Loss Surface of Single-Layered Neural Networks for Binary Classification},\n  author={Shiyu Liang and Ruoyu Sun and Yixuan Li and R.Srikant},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UWIJwM}\n}", "authorids": ["sliang26@illinois.edu", "ruoyus@illinois.edu", "yl2363@cornell.edu", "rsrikant@illnois.edu"], "authors": ["Shiyu Liang", "Ruoyu Sun", "Yixuan Li", "R.Srikant"], "keywords": [], "pdf": "/pdf/882207a245cc1f48ea5f76a1c06c9c75e40c5454.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}