{"notes": [{"id": "r1eBeyHFDH", "original": "SkeO74o_wS", "number": 1504, "cdate": 1569439468853, "ddate": null, "tcdate": 1569439468853, "tmdate": 1594223809440, "tddate": null, "forum": "r1eBeyHFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Jx2yatJbi", "original": null, "number": 1, "cdate": 1576798725019, "ddate": null, "tcdate": 1576798725019, "tmdate": 1576800911492, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "All reviewers unanimously accept the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726704, "tmdate": 1576800278892, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Decision"}}}, {"id": "S1lklsZcor", "original": null, "number": 3, "cdate": 1573685991415, "ddate": null, "tcdate": 1573685991415, "tmdate": 1573685991415, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "Bkx9phEroS", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment", "content": {"title": "Thanks for the detailed answers", "comment": "Thank you for addressing all issues raised in a convincing and thorough manner and preparing a revised manuscript!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eBeyHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1504/Authors|ICLR.cc/2020/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155036, "tmdate": 1576860560356, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment"}}}, {"id": "Sket3aNBoB", "original": null, "number": 2, "cdate": 1573371313448, "ddate": null, "tcdate": 1573371313448, "tmdate": 1573371313448, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "r1eqrvhf5S", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment", "content": {"title": "Thank you for your review and suggestions", "comment": "Thank you for your review and suggestions\n\nQ: Suppose that Y is a scalar and X is a noisy estimate of Y. Suppose we restrict F to the family of Gaussian distributions. That is, with side information x, f[x](y)  = N(x, s). Without side information, f[empty](y) = N(u, s). The functions f are parameterized by u and s. \nIs this a \"predictive family\"? To make sure I understand it correctly, can you please walk me through the Eq 1 for this particular example? \n\nResponse: This is not a predictive family because Eq. 1 doesn\u2019t hold. We can break down Eq 1 for this example into two parts\n\n\u201cFor every f \\in F, P \\in range(f)\u201d translates to \u2014> for any Gaussian distribution N(c, s) where c is any real number\n\n\u201cThere exists f\u2019 \\in F, f\u2019[x] = P, f\u2019[empty] = P\u201d translates to \u2014> there is an f such that f[empty] = N(c, s), f[x] = N(c, s). In this example, such an f cannot always be found because we are \u201cforced\u201d to use x as the mean of the Gaussian (i.e., we cannot ignore it). \n\nThis will be an F information with a small modification: f[x] = N(ax+b, s), f[empty]=N(u, s) where a, b, u are parameters we can optimize. To check Eq 1 we can verify \n\n\u201cThere exists f\u2019 \\in F, f\u2019[x] = P, f\u2019[empty] = P\u201d -> this can be achieved by choosing a=0, b=c, u=c \n\nIn fact, this quantity is equal to the R^2 coefficient (Proposition 1.5) \u2014 a common measurement of dependence between two random variables. \n\nNote that many nice properties continue to hold, but without Eq. 1 F-information can be negative.\n \n\nQ: Reference Shannon and Weaver was published in 1963, not 1948.  In Page 5, \"maybe not expressive\" should be \"may not be expressive\". \n\nResponse: Thank you for the correction. We have fixed them. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eBeyHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1504/Authors|ICLR.cc/2020/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155036, "tmdate": 1576860560356, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment"}}}, {"id": "Bkx9phEroS", "original": null, "number": 1, "cdate": 1573371074301, "ddate": null, "tcdate": 1573371074301, "tmdate": 1573371074301, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "ryl1OrQ0KS", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment", "content": {"title": "Thank you for your review and suggestions", "comment": "Thank you for your review and suggestions. \n\nQ: Please add a short section of current shortcomings and caveats, especially with regard to applying the methods in practice. \n\nResponse: We have added a limitations section, reproduced below:\n\nF-information is empirically useful and has appealing theoretical properties. However, some elegant properties of Shannon information are lost. For example, Shannon information can be manipulated with algebra (e.g. H(X, Y) = H(X) + H(Y | X)), while F-Information cannot (for general F). Additionally, for F-Information to be useful in practice, the predictive family F should be easy to optimize over. Machine learning research has identified many such functions (e.g. linear functions, convex functions, ReLU neural networks) suitable for a variety of data types. Nevertheless one should be cautious when applying F-Information to functions and data types that are not well understood in the machine learning literature. In the finite data regime, overfitting is also an issue to consider, and standard techniques to prevent it (e.g., crossvalidation) should be applied.\n\nAn interesting direction for future work is to better integrate F-Information with other areas of machine learning. The production of usable information (representation learning), acquisition of usable information (active learning) and exploitation of usable information (classification and reinforcement learning) could potentially benefit from the F-information concept.\n\n\nQ: Please comment on solving the variational optimization problem (the infimum) which is part of the definition of (conditional) F-information. \n\nResponse: F-information estimation degrades gracefully with sub-optimal optimization. Let I be the true F-information, I\u2019 be its finite-data estimation with perfect optimization (the infimum is achieved), and I\u2019\u2019 be its estimation with imperfect optimization. Theorem 1 upper bounds | I - I\u2019 |, and we can immediately derive an upper bound on | I - I\u2019\u2019 | by triangle inequality | I  - I\u2019\u2019 | \\leq | I - I\u2019 | + | I\u2019 - I\u2019\u2019 |\n\nIn other words, the estimation error can only increase by | I\u2019 - I\u2019\u2019 |, which is the gap between perfect optimization and imperfect optimization.\n\nIn practice, machine learning research has identified many function families that are empirically easy to optimize (including modern deep neural networks) \u2014 which we use as our function family F. We used standard optimization algorithms (e.g. SGD for neural networks) and the wall clock time is identical to other estimators. \n\n\nQ: The name F-information might easily get confused with the use of f-divergences, perhaps there is a better, more informative name. \n\nResponse: Thank you for this suggestion. We are considering a name change to V-information as in variational information. \n\n\nQ: Have you had any thoughts on how F-information could be used in a rate-distortion / information-bottleneck type framework for a theory of \u201crelevant usable information\u201d?\n\nResponse: The application to information-bottleneck should be straight-forward. In fact, our fairness experiment can be thought of as the opposite of an information bottleneck:\n\nFairness:\t\t\t   minimize F information between the learned representation and target (sensitive attributes) and maximize F information w.r.t input. \n\nInformation bottleneck: maximize F information between the learned representation and target (labels) and minimize F information w.r.t input. \n\n\nQ: The paragraph above 3.3 almost sounds a bit like Shannon (and the data processing inequality) was wrong. I\u2019d rather phrase this as a \u201cno-free-lunch problem\u201d \n\nResponse: We have reworded a few sentences to highlight the no-free-lunch perspective. \n\n\nQ: When choosing function classes that allow for universal function approximation, would F-information degrade to Shannon information? \n\nResponse: Yes, this is an expected and desirable property as in Proposition 1. Roughly speaking, if F contains every function and every probability measure \u2014 there are no computational constraints \u2014 then all information is usable, which is exactly what Shannon information measures. The statistical and computational burden however, makes this a poor design choice for many machine learning problems.  \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eBeyHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1504/Authors|ICLR.cc/2020/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155036, "tmdate": 1576860560356, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1504/Authors", "ICLR.cc/2020/Conference/Paper1504/Reviewers", "ICLR.cc/2020/Conference/Paper1504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Official_Comment"}}}, {"id": "ryl1OrQ0KS", "original": null, "number": 1, "cdate": 1571857767210, "ddate": null, "tcdate": 1571857767210, "tmdate": 1572972459972, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThe paper introduces a framework for quantifying information about one random variable, given another random variable (\u201cside information\u201d) and, importantly, a function class of allowed transformations that can be applied to the latter. This matches the typical scenario in machine learning, where observations (playing the role of side information) can be transformed (with a restricted class of transformation-functions) such that they become maximally predictive about another random variable of interest (\u201clabels\u201d). Using this framework, the paper defines the notion of conditional F-entropy and F-entropy (by conditioning on an empty set). Interestingly, both entropic quantities are shown to have many desirable properties known from Shannon entropy - and when allowing the function class of transformations to include all possible models F-entropies are equivalent to Shannon entropies. The paper then further defines \u201cpredictive F-information\u201d which quantifies the increase in predictability about one random variable when given side information, under a restricted function-class of allowed transformations of the side information. Importantly, transformations of side information can increase predictive F-information (which is the basis for the notion of \u201cusable\u201d information), which is in contrast to the data processing inequality that applies to Shannon information and states that no transformation of a variable can increase predictability of another variable further than the un-transformed variable (information cannot be generated by transforming random variables). The paper highlights interesting properties of the F-quantities, most notably a PAC bound on F-information estimation from data, which gives reason to expect F-information estimation to be more data-efficient than estimating Shannon-information (particularly in the high-dimensional regime). This finding is confirmed by four types of interesting experiments, some of which make use of a modified version of a tree-structure learning algorithm proposed in the paper (using predictive F-information instead of Shannon mutual information).\n\nContributions\ni) Proposal of a framework for measuring and reasoning about information that transformed random variables have about other random variables, when the class of transformation functions is restricted. Interesting properties are highlighted and corresponding proofs are given. Important conclusions to Shannon-information measures are drawn.\n\nii) PAC guarantees for estimating F-information quantities from data. A nice result that justifies some optimism about the scalability of F-information estimation.\n\niii) Modification of a tree-structure learning algorithm, and application to four types of experiments with comparisons against methods for estimating Shannon(-mutual)-information. \n\nQuality, Clarity, Novelty, Impact\nThe paper is very well written, the motivation and main results are clear and connections to known measures for information in complex systems are drawn (which often appear as corner-cases, or unrestricted cases of F-information). I am not an expert on various information measures, thus I cannot fully judge the novelty of the framework (given that the central idea is fairly simple and quite elegant, the main work lies in the proofs and connections to other frameworks). However, I have not seen the framework being discussed in the machine learning literature before. I personally would rate the potential impact of the F-information framework as high because it addresses many problems that Shannon-(mutual-)information has (hard to estimate, generality means complete blindness against model-classes). The experiments in the paper already illustrate how F-information could be very useful for a range of ML problems that cannot be tackled by strong competitor methods based on Shannon-information estimation. My only criticism is that the paper does not clearly state current limitations and shortcomings and does not comment on the difficulties / potential problems with solving the variational problem that is part of the definition of (conditional) F-information. I currently vote and argue for accepting the paper, though my assessment is of medium confidence only, and I am happy to take issues raised by the other reviewers and the rebuttal into account. I have not checked the proofs in the appendix in great detail.\n\nImprovements\ni) Please add a short section of current shortcomings and caveats, especially with regard to applying the methods in practice. \n\nii) Please comment on solving the variational optimization problem (the infimum) which is part of the definition of (conditional) F-information. In particular, are there any theoretical statements / bounds / etc. to be made for the case where the infimum is not found exactly - does the measure degrade gracefully or can small errors in this optimization lead to wildly varying/divergent F-information? From a practical point-of-view: how was this optimization done in the experiments (particularly when involving a neural network model), how much computational overhead did this optimization add (and how does it compare against other methods, e.g. in terms of wall-clock time or other reasonable metrics, the more the better)?\n\niii) This is a minor one and feel free to completely ignore it. The name F-information might easily get confused with the use of f-divergences, perhaps there is a better, more informative name. Also, while I personally like the term \u201cusable\u201d in the title, I\u2019m not so sure about \u201ccomputational constraints\u201d - the latter somehow suggests that the method has small computational footprint, or can easily scale to different computational budget. Perhaps there is a way that more strongly indicates that this refers to restrictions on the model-/function-class (which the term \u201cusable\u201d does already to some degree admittedly).\n\n\nMinor Comments\na) Have you had any thoughts on how F-information could be used in a rate-distortion / information-bottleneck type framework for a theory of \u201crelevant usable information\u201d? This is probably beyond the scope of this paper, just out of curiosity.\n\nb) The paragraph above 3.3 almost sounds a bit like Shannon (and the data processing inequality) was wrong. I\u2019d rather phrase this as a \u201cno-free-lunch problem\u201d - while the DPI and Shannon (mutual) information is very elegant, it is necessary to make further assumptions/restrictions (the function class of allowed transformations) to make more fine-grained statements and define more precise (but less general) informational-quantities tailored to the specific function class.\n\nc) When choosing function classes that allow for universal function approximation, would F-information degrade to Shannon information? "}, "signatures": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633068368, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1504/Reviewers"], "noninvitees": [], "tcdate": 1570237736420, "tmdate": 1575633068381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Official_Review"}}}, {"id": "r1eqrvhf5S", "original": null, "number": 2, "cdate": 1572157249699, "ddate": null, "tcdate": 1572157249699, "tmdate": 1572972459928, "tddate": null, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "invitation": "ICLR.cc/2020/Conference/Paper1504/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a generalization of classical definitions of entropy and mutual information that can capture computational constraints. Intuitively, information theoretic results assume infinite computational resources, so they may not correspond to how we treat \"information\" in practice. One example is public-key encryption. An adversary that has infinite time will eventually break the code so the decrypted message conveys the same amount of information (in a classical sense) as the plaintext message. In practice, this depends on computational time. \n\nThe authors' approach is to first restrict the class of conditional probability distribution p(Y|X) to a restricted family F that satisfies certain conditions. Unfortunately, the main condition in Def 1 that the authors assume is not natural and is only added to ensure that mutual information remains positive. However, putting this aside, the subsequent definitions that general entropy, conditional entropy, and mutual information are well-motivated. \n\nThe authors, then, show that many measures of \"uncertainty\" can be viewed as \"entropies\" under this generalized definition including the Mean Absolute Deviation and the Coefficient of Determination. \n\nThe overall framework can justify practices that we commonly use in machine learning, which would be justifiable using classical information. One important example is Representation Learning, which is a post-processing of data to aid the prediction task. According to classical information theory, this post-processing shouldn't help because it cannot add more information about the label Y than what was original available in X. Under the formulation presented in this paper, postprocessing can help if we keep in mind information about Y in X are hard to extract to begin with. \n\nIn terms of practical applications, the main advantage of the new definition is that F-information can be estimated from a finite sample, simply because F is a restricted set. However, this restriction helps compared to using state-of-the-art estimators for Shannon mutual information as shown in the experiments. \n\nFinally, the literature review section is quite excellent. \n\nI find the overall approach to be quite interesting and definitely worth publishing. The only suggestion I have is that the authors include immediately after Definition 1 a concrete example that illustrates it. For example, suppose that Y is a scalar and X is a noisy estimate of Y. Suppose we restrict F to the family of Gaussian distributions. That is, with side information x, f[x](y)  = N(x, s). Without side information, f[empty](y) = N(u, s). The functions f are parameterized by u and s. \nIs this a \"predictive family\"? To make sure I understand it correctly, can you please walk me through the Eq 1 for this particular example? \n\nSome minor remarks:\n- Reference Shannon and Weaver was published in 1963, not 1948. \n- In Page 5, \"maybe not expressive\" should be \"may not be expressive\". \n \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1504/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xuyilun@pku.edu.cn", "sjzhao@stanford.edu", "tsong@cs.stanford.edu", "russell.sb.nebel@gmail.com", "ermon@cs.stanford.edu"], "title": "A Theory of Usable Information under Computational Constraints", "authors": ["Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon"], "pdf": "/pdf/8d57c8f0f8fa4453c6e1f25f4d364e1d23bf0d3e.pdf", "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon\u2019s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon\u2019s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information .", "keywords": [], "paperhash": "xu|a_theory_of_usable_information_under_computational_constraints", "code": "https://github.com/Newbeeer/V-information", "_bibtex": "@inproceedings{\nXu2020A,\ntitle={A Theory of Usable Information under Computational Constraints},\nauthor={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eBeyHFDH}\n}", "original_pdf": "/attachment/9693a2f717e07546262882056716dff16a4eff57.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eBeyHFDH", "replyto": "r1eBeyHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633068368, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1504/Reviewers"], "noninvitees": [], "tcdate": 1570237736420, "tmdate": 1575633068381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1504/-/Official_Review"}}}], "count": 7}