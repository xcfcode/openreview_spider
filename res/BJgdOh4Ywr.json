{"notes": [{"id": "BJgdOh4Ywr", "original": "Sye9k-3ULr", "number": 48, "cdate": 1569438832514, "ddate": null, "tcdate": 1569438832514, "tmdate": 1577168251903, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "913RlcGax", "original": null, "number": 1, "cdate": 1576798685927, "ddate": null, "tcdate": 1576798685927, "tmdate": 1576800949030, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Decision", "content": {"decision": "Reject", "comment": "The main concern raised by reviewers is limited novelty, poor presentation, and limited experiments. All the reviewers appreciate the difficulty and importance of the problem. The rebuttal helped clarify novelty, but the other concerns remain.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730570, "tmdate": 1576800283388, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper48/-/Decision"}}}, {"id": "Hkefiga9jB", "original": null, "number": 7, "cdate": 1573732505686, "ddate": null, "tcdate": 1573732505686, "tmdate": 1573732505686, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "BJxSVlxMsr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Visually rich environments", "comment": "Thank you for taking the time to address these comments.\n\nAs a final note, I believe one of the main concerns with this work is that the experimental domain (humanoid), while a challenging control problem, is perhaps not as visually challenging as other RL domains.  As the primary motivation for this work is to enable imitation learning from visual observations, it would strengthen the argument for the value of the proposed approach (relative to existing methods), if it were evaluated in domains with more complex visual representations.  A good example might be the CoinRun domain: https://github.com/openai/coinrun, which specifically tests the robustness of RL methods to different visual representations."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "rJeWINRKoH", "original": null, "number": 6, "cdate": 1573672008667, "ddate": null, "tcdate": 1573672008667, "tmdate": 1573672008667, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "Syl0ilrb9B", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Rebuttal Discussion", "comment": "Dear Reviewer, \n\nCould you let us know if our response has addressed the concerns raised in your review? We would be happy to provide further revisions or experiments to address any remaining issues and would appreciate a response from you on the points that we raised."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "B1eU44AYsr", "original": null, "number": 5, "cdate": 1573671981699, "ddate": null, "tcdate": 1573671981699, "tmdate": 1573671981699, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "rJeK8ITJ5r", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Rebuttal Discussion", "comment": "Dear Reviewer, \n\nCould you let us know if our response has addressed the concerns raised in your review? We would be happy to provide further revisions or experiments to address any remaining issues and would appreciate a response from you on the points that we raised."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "BJe0PQlfiS", "original": null, "number": 3, "cdate": 1573155686285, "ddate": null, "tcdate": 1573155686285, "tmdate": 1573155686285, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "rJeK8ITJ5r", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Discussion of novelty", "comment": "We appreciate the comments on the work.\n\nThe main concern raised in this review is the novelty of the work. While Siamese networks and learning to imitate from vision are not novel by themselves, there has not been work that combines these methods, especially with sequence-based methods. The reviewer points out we should \u201ccompare with the most updated state-of-the-art baselines\u201d. We examine a setting for which few state-of-the-art baselines exist and have only been applied to simpler tasks. We do compare to recent methods, TCN and GAIL, we find that our method outperforms these methods (Figure 4(a)). Also, while often training RL using image data slows down learning, we show in Figure 4(b,c) that our combination of spatial and temporal distances leads to a rich reward landscape that results in faster learning. While we do not apply our system directly on a robot, we do show that our method outperforms TCN, which was explicitly used on robots. The evaluations performed in the paper are over two separate simulated domains, shown in figures 3 and 10, and for one of these domains, we apply the method over multiple imitation tasks.\n\nAdditionally, the simulated environment used in this work was created and is the only simulation library available that provides visual data for expert demonstrations across multiple-tasks. We plan to add more tasks and release this simulation library to enable others to explore the problem of visual imitation learning further. We leave applying this method to a real robot as interesting future work."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "BJxSVlxMsr", "original": null, "number": 2, "cdate": 1573154861196, "ddate": null, "tcdate": 1573154861196, "tmdate": 1573154861196, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "HkgFxwxAFr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Thank you, will will make the writing more clear.", "comment": "Thank you for the feedback on the paper. There is a large amount of content necessary to fit into the paper. In the next version of the paper, we will move the negative sampling and auto-encoder losses before the combined loss. We will also update equation 4."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "HkgprklzjS", "original": null, "number": 1, "cdate": 1573154628775, "ddate": null, "tcdate": 1573154628775, "tmdate": 1573154648084, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "Syl0ilrb9B", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment", "content": {"title": "Oracle/manual rewards are not available in the real world, our contribution is to learn a reward function that can.", "comment": "Thank you for the comments on the work, we regret the confusion.\n\nThe main concern raised in the review is that the manual reward function results in higher performance than VIRL. The manual result in Figure 4(b) is meant to be illustrative of the performance of VIRL compared to a manual or oracle reward function. In this \u201cmanual\u201d result, the learning system has access to the noise-free true state of the agent and expert to compute rewards. However, we point out that the contribution of the work is a method that can learn how to imitate quickly when the agent, similar to a human, only has access to noisy images of the expert demonstration and agent. Figure 4(b) shows that our method that receives noisy image data of the demonstration can accelerate learning compared to the manual/oracle reward function that is not available in real-world settings.\n\nThe second concern in this review is the unsupervised data labelling process. This data labelling scheme is a natural extension of the process used in the \u201cTime Contrastive Networks\u201d and Dwibedi et al. (2018) papers with a few novel additions as a result of using sequences instead of states. We tried many different labelling schemes for this work. Some of these schemes introduced forms of bias, and we found that the minimal set described in the paper gave us the best results across all tasks we trained. We did not observe instability related to the data relabeling process. However, we will add further details about the labelling process and move this to the main part of the paper. To make the writing more precise, we will also move the equations from the appendix to earlier in the paper.\n\nLearning how to imitate from a single video demonstration of an articulated 3D humanoid is challenging and has not been demonstrated before. In this work, we show that we can train policies to imitate many different motions. In addition, we also show that using the method described in the paper, which uses a combination of spatial and temporal distance, increases learning speed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper48/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgdOh4Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper48/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper48/Authors|ICLR.cc/2020/Conference/Paper48/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177148, "tmdate": 1576860546590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper48/Authors", "ICLR.cc/2020/Conference/Paper48/Reviewers", "ICLR.cc/2020/Conference/Paper48/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Comment"}}}, {"id": "HkgFxwxAFr", "original": null, "number": 1, "cdate": 1571845873043, "ddate": null, "tcdate": 1571845873043, "tmdate": 1572972645372, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents visual imitation with reinforcement learning (VIRL), an algorithm for learning to imitate expert trajectories based solely on visual observations, and without access to the expert\u2019s actions.  The algorithm is similar in form to GAIL and its extensions, learning a reward function which captures the similarity between an observed behavior and the expert's demonstrations, while simultaneously using reinforcement learning to find a policy maximizing this reward, such that the learned policy will replicate the demonstrated behavior as well as possible.  A key feature of this method is that the learned reward function is defined by a learned distance metric, which evaluates the similarity between the agent's current trajectory, and the nearest demonstrated expert trajectory.\n\nThe network describing the distance metric is recurrent, such that the distance is defined between trajectories rather than individual states.  The distance function network is trained via a negative sampling approach, where expert trajectories are randomly reordered to produce examples that dissimilar to the expert trajectories.   The distance network also defines a variational autoencoder, and the reconstruction of the target trajectories is treated as an auxiliary task to help train better representations of the trajectory space.\n\nWhile previous work has considered the problem of visual imitation learning, the approach taken here is novel in its architecture and loss function, and significantly outperforms the baselines in terms of the similarity between the resulting behavior and the expert behavior.\n\nThe clarity of the technical presentation could be improved, however.  In particular, it would be helpful for the reader if the definitions of the negative sampling loss and the autoencoder losses were given before the combined loss, and if we saw the form of the loss for both positive and negative sequence pairs.  Equation 4 could also be made explicit, with the full summation term included."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575912136697, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper48/Reviewers"], "noninvitees": [], "tcdate": 1570237757906, "tmdate": 1575912136712, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Review"}}}, {"id": "rJeK8ITJ5r", "original": null, "number": 2, "cdate": 1571964496678, "ddate": null, "tcdate": 1571964496678, "tmdate": 1572972645328, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an imitation learning method that deploys previously well-studied techniques such as siamese networks, inverse RL, learning distance functions for IRL and tracking.\n\n+ the paper studies an important problem of IL using visual data.\n+ I found the ablation studies in the appendix quite useful in understanding the efficiency of the proposed method.\n\n-In terms of novelty, the proposed approach is a combination of several past works so the technical novelty is limited. Additionally, it is not clear how impactful the proposed method can be given that it is only tested on a synthetic domain which is the same as the train domain. So, from the current experimental results it is not clear if this approach would be effective to be applied in a real system (e.g. robots) on the practical side. \n\n-There are not enough evaluation done to compare with the most updated state-of-the-art baselines. The evaluations are done on just a single synthetic domain with a single character. Therefore, the train and test videos are very similar. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575912136697, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper48/Reviewers"], "noninvitees": [], "tcdate": 1570237757906, "tmdate": 1575912136712, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Review"}}}, {"id": "Syl0ilrb9B", "original": null, "number": 3, "cdate": 1572061350323, "ddate": null, "tcdate": 1572061350323, "tmdate": 1572972645281, "tddate": null, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "invitation": "ICLR.cc/2020/Conference/Paper48/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The idea of the paper is to learn a distance function between observed and the agent\u2019s behaviors. Once they have the distance function, they can learn the agent\u2019s policy efficiently given a single demonstration of each task. In their formulation, the distance function and the policy are jointly learned.\u00a0\n\nThe idea is reasonable and the performance outperforms baselines like GAIL and VAE. However, the paper is not-well written with many relevant equations defined in the supplementary material. The unsupervised data labeling part seems Adhoc with many details in the supplementary material. I wonder if the process stable or not. How many lower than the average performance of the proposed method as shown in F.g 4 are caused by unsupervised data labeling?\n\nIn Fig. 4b, the manual performance is very strong once converged.\u00a0Although the proposed method initially reaches high reward,\u00a0after twice many iterations the manual performance even outperforms the proposed method on average many times. Hence, I am not very convinced about the proposed method will be the best-picked method in practice.\n\nOverall, I think the idea is good. But the paper is poorly written and I concern the most about the stability of the unsupervised data labeling process. The experimental results are also not super convincing. Hence, I recommend for weak rejection."}, "signatures": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper48/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks", "authors": ["Glen Berseth", "Christopher Pal"], "authorids": ["gberseth@gmail.com", "christopher.pal@polymtl.ca"], "keywords": ["imitation learning", "reinforcement learning", "imitation from video"], "TL;DR": "Learning recurrent distance models for imitation from a single video clip using reinforcement learning.", "abstract": "It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration.  However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent\u2019s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent\u2019s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance.  Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency.  These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task \u2013 the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "pdf": "/pdf/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "paperhash": "berseth|visual_imitation_with_reinforcement_learning_using_recurrent_siamese_networks", "original_pdf": "/attachment/87b3aeaae15eff061a2a49170fcbab9dbc1511ae.pdf", "_bibtex": "@misc{\nberseth2020visual,\ntitle={Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks},\nauthor={Glen Berseth and Christopher Pal},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgdOh4Ywr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgdOh4Ywr", "replyto": "BJgdOh4Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575912136697, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper48/Reviewers"], "noninvitees": [], "tcdate": 1570237757906, "tmdate": 1575912136712, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper48/-/Official_Review"}}}], "count": 11}