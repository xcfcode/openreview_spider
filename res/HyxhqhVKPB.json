{"notes": [{"id": "HyxhqhVKPB", "original": "rJeCBcxlvr", "number": 129, "cdate": 1569438867745, "ddate": null, "tcdate": 1569438867745, "tmdate": 1577168218085, "tddate": null, "forum": "HyxhqhVKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ukaBTxuHmT", "original": null, "number": 1, "cdate": 1576798688252, "ddate": null, "tcdate": 1576798688252, "tmdate": 1576800946823, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Decision", "content": {"decision": "Reject", "comment": "This papers proposed an interesting idea for distributed decentralized training with quantized communication. The method is interesting and elegant. However, it is incremental, does not support arbitrary communication compression, and does not have a convincing explanation why modulo operation makes the algorithm better. The experiments are not convincing. Comparison is shown only for the beginning of the optimization where the algorithm does not achieve state of the art accuracy. Moreover, the modular hyperparameter is not easy to choose and seems cannot help achieve consensus.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714523, "tmdate": 1576800264248, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper129/-/Decision"}}}, {"id": "B1eLpFlt2r", "original": null, "number": 4, "cdate": 1574664638233, "ddate": null, "tcdate": 1574664638233, "tmdate": 1574665490981, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This papers proposed an interesting idea for distributed decentralized training with quantized communication. The authors show that naively compressing the exchanged model can fail to converge, and introduce to compress the model difference with modulo operation. The idea is further applied to decentralized data and asynchronous optimization settings. Convergence analyses are provided for non-convex problems. \n\nThough I found this paper interesting and novel, I have several concerns:\n\n1. There is no convincing explanation why modulo operation makes the algorithm better. In particular, the equation $((m_2)_j - (m_1)_j) \\text{mod} \\theta = (m_2)_j - (m_1)_j$ does not hold. For example, let $(m_2)_j = 5$, $(m_1)_j = 3$ and $\\theta=2$, then $((m_2)_j - (m_1)_j) \\text{mod} \\theta = (5 - 3) \\text{mod} 2 = 0$ and $(m_2)_j - (m_1)_j = 5- 3 = 2$.\n\n2. The proposed algorithm requires knowledge of $\\theta$, which depends on the upper bound of gradient. If $\\theta$ is underestimated, the algorithm will suffer from large errors. For instance, suppose that true $\\theta$ is $5$ while we use $4.5$, then an identity quantizer cannot even recover the original value, e.g., $(5/4.5) \\text{mod} 1*4.5 = 0.5 << 5$. On the other hand, if $\\theta$ is overestimated, the convergence is dramatically slowed down. I checked authors' response to Reviewer 3 regarding this issue. However, they are not efficient and can still provide wrong estimates.\n\n3. The upper bound of staleness is missing in the main text and convergence bound. I found that bounded delay is assumed in the supplementary. However, the constant for bounded delay is missing in Theorem 4.\n\n4. The experiments are not convincing. In particular, Figure 2 shows that all the quantization methods perform very bad with low bits format. The centralized method DoubleSqueeze uses error feedback and supports arbitrary compression. Based on my experience, even with the 1-bit quantizer introduced in (Karimireddy et al., 2019), DoubleSqueeze converges as fast as full precision SGD. Also, it seems that final test accuracy of D-PSGD on ResNet110 is just 80+, which is much lower than its official 93.6% test accuracy. This indicates that experiments are not appropriately done.\n\n5. CIFAR-10 is a very small scale dataset. For distributed training, it is more convincing to conduct experiments on larger datasets such as ImageNet.\n\nThey are a lot of typos. The authors need to double check.\n\nKarimireddy et al., Error Feedback Fixes SignSGD and other Gradient Compression Schemes. ICML 2019.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575115061929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper129/Reviewers"], "noninvitees": [], "tcdate": 1570237756644, "tmdate": 1575115061945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Review"}}}, {"id": "S1goKh_njB", "original": null, "number": 10, "cdate": 1573846147347, "ddate": null, "tcdate": 1573846147347, "tmdate": 1573846730686, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "SkgXxRvnir", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for sharing these concerns!", "comment": "Thank you for your reply!\n\n(i) You are right that we should highlight this in that section! Thank you for pointing this out and we will update this! However, we would like to point out that, in the results of ResNet18 in Figure 7, the accuracy gap you mentioned is from full-precision D-PSGD baseline (Moniqua doesn't suffer gap compared to D-PSGD). This \"gap\" is aligned with the result from D-PSGD paper [4], where they also get test accuracy below 90%. \n\nThe gap in D-PSGD can be caused by many reasons. Since the rebuttal time is limited and ResNet18 is a newly added experiment, we do omit batch norm in all of the baselines as well as Moniqua in training ResNet18, which could be a reason. (But for ResNet110 in figure 7 where we use batch norm, that converges to the s.t.o.a accuracy.)\n\nSince Moniqua is a work that aims at achieving same rate as full-precision D-PSGD, we think it's beyond our scope to improve baseline D-PSGD's accuracy on a particular task. \n\n[4] Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent.\nXiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang and Ji Liu.\n(https://arxiv.org/pdf/1705.09056.pdf)\n\n(ii) The step size scheme we use is from [5]. We actually did try step size larger than 1 in the experiments on ChocoSGD. However, we found that this aggressive step size causes severe instability of algorithm and is easy to diverge (large step size also contradicts theory shown in [1]). In practice, we haven't seen any algorithm using step size >1 (or >2). We understand this might be a new direction to explore and using large step size can be something significant in this domain. But since [1] does not specify any open source code, we felt hard to reproduce their results.\n\n[5] Deep Residual Learning for Image Recognition\nKaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun.\n(https://arxiv.org/pdf/1512.03385.pdf)\n\nThanks again for your concerns! Please let us know if there are other questions!"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "SkgXxRvnir", "original": null, "number": 9, "cdate": 1573842411081, "ddate": null, "tcdate": 1573842411081, "tmdate": 1573842411081, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "rJxNy8vioH", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for your reply, however, I don\u2019t agree with the points you made. I have the following questions: \n\n(i) I also notice that in your new experiments with ResNet110 and ResNet18 (Fig. 7). you still have 6-10% accuracy gap (which was not stated). For training ResNet18 on Cifar10 depending on the width, the accuracy it can achieve is between 91 and 95 %. This should be stated clearly in the paper with the accuracy gap value. \n\n(ii) [1] reaches s.t.o.a accuracy for their model. So it looks strange not including in your tuning range stepsizes larger than 1, where the baselines ([1]) provided evidence for using these stepsizes. As it stands now one would conclude that your tuning is done in an unfair way.\n\nSuggestion:\nAnother reasons for this large accuracy gap you have might be that you use an uncommon training procedure (decreasing the stepsize by factor of 10 every 30 epoch). Even d-psgd baseline could not achieve a good accuracy. I suggest you to try another training procedure. I think the training procedure of [1] is more standard. "}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "rJxNy8vioH", "original": null, "number": 8, "cdate": 1573774811683, "ddate": null, "tcdate": 1573774811683, "tmdate": 1573782754130, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "BylRkeLosr", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thanks for the quick reply!", "comment": "Thank you very much for such quick reply!\n\n1. Thanks for sharing this paper! (we refer it as [1]) \nFirst, we think it is an inspiring work! However, we find some of the results shown in [1] contradict several previous results in [2] and [3] ([1][2][3] are running exactly the same task, ResNet20 on CIFAR10 with ring 8 network).  Specifically, [2] shows DCD-PSGD cannot converge at 4 bits and ECD-PSGD can with unbiased quantizer while [1] shows exactly the opposite result. And [3] shows that [1] diverges with 2 bits while [1] shows they can achieve s.t.o.a accuracy with fewer than 1 bit. It's probably because [1] is adopting uncommon step size (1.60, as shown on page 19 where normally ResNet is tuned with step size <1), and that is beyond our tuning scope.\nWe notice that results in [1] is reported at 300 epoch while full-precision algorithm could achieve s.t.o.a accuracy much earlier (aound 230 epochs). It is not clear whether the more iterations will compensate the benefit from communication acceleration (since they do not include wall clock time comparison).\n \nSecond, Moniqua can achieve s.t.o.a accuracy, but not in these extreme cases (1-2 bits). We agree with you that ultimate accuracy is also important. However in practice, different from centralized SGD,  extreme low bits-budget such as 1 or 2 bit communication is rarely used in decentralized training since it could cause consensus problem and make workers fall into different local optima, especially with when data is locally gathered. Moniqua is able to achieve s.t.o.a accuracy with slightly more bits on ResNet110 (<6 bits) and VGG16 (<4bits) (as discussed in the rest of the rebuttal.)\n\nThird, as we point out in \"Response to Concern 2\", the number of bits needed largely depends on the model, dataset and task. Note that we are testing ResNet110, which has 6.3X parameters than ResNet20. And  we are using stochastic rounding, which is a much simpler quantizer compared to those used in [1]. (We will try our best to add more results using advanced quantizers from [1] before rebuttal ends, but we think that issue is orthogonal to Moniqua.)\n\nFourth, different from Moniqua with zero additional memory overhead, [1] needs at least 3X additional memory (in the worst case $n^2$ where n is number of workers). This is not very practical in many cases. In practice, unlike datacenter networks, decentralized SGD is often used in cases where workers have limited memory, computing resources and bandwidth. Applicable scenarios include ad hoc networks, mobile networks and Internet of Things (IoT). In these cases, the most serious issue is how to utilize limited resources to get close to the state-of-the-art accuracy as much as possible. Previous works on compressed communication for decentralized learning mitigate the bandwidth problem by trading it off for  more memory and computation [1][2][3], which does not fully solve the problem since memory and computation are also typically limited. Motivated by this, Moniqua is a additional-memory-and-computation-free solution that enables decentralized SGD to use less bandwidth, thus achieving acceleration in the limited-bandwidth setting with no extra overheads. \n\n[2]Communication Compression for Decentralized Training. \nHanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang and Ji Liu.(https://arxiv.org/pdf/1803.06443.pdf)\n\n[3] DeepSqueeze: Decentralized Meets Error-Compensated Compression.\nHanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang and Ji Liu.\n(https://arxiv.org/pdf/1907.07346.pdf)\n\n\n4&5: Sorry for the misunderstanding, we list the hyperparameters for baselines here: \nIn the \"Wall-clock Time Evaluation\": \nChocoSGD:(step size=0.05, $\\gamma=0.75$); \nDeepSqueeze: (step size=0.1, $\\eta=0.8$); \nDCD-PSGD: (step size=0.05); \nECD-PSGD: (step size=0.1)\n\nIn the \"Aggressive Quantization\": \nFor 2 bits experiment, (step size=0.01, $\\gamma=0.25$) for ChocoSGD and (step size=0.01, $\\eta=0.40$) for DeepSqueeze; (step size=0.01) for DCD-PSGD; (step size=0.01) for ECD-PSGD.\nFor 3 bits experiment: (step size=0.01, $\\gamma=0.35$) for ChocoSGD and (step size=0.05, $\\eta=0.60$) for DeepSqueeze; (step size=0.01) for DCD-PSGD; (step size=0.01) for ECD-PSGD.\n\nThanks again for the quick reply. Please let us know if you have more concerns!"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "BylRkeLosr", "original": null, "number": 7, "cdate": 1573769190355, "ddate": null, "tcdate": 1573769190355, "tmdate": 1573769190355, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "BkeOLbMsoS", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Some concerns", "comment": "I have checked only parts of the authors comments so far, and this is not my full response. However, as the rebuttal phase is ending soon, I put some of my concerns that the authors still have time to reply. \n\n1. \u201cIn these very extreme cases (2 or 3 bits), no algorithms can achieve state-of-the-art test accuracy\u201d -> I noticed that [1] shows that with sign compression (around 1 bit per coordinate) SOTA accuracy can still be achieved on the Cifar10 dataset (the same dataset as used here). Therefore, I still believe that the experimental comparison is not convincing enough. The goal of using compressed communications is to speed up the convergence while not compromising the accuracy too much (10-20% is a lot). If with compressed communications the algorithm cannot achieve SOTA accuracy, then in practice it makes sense to wait a bit longer and use full precision training instead.\n\n[1] Decentralized Deep Learning with Arbitrary Communication Compression\nA. Koloskova, T. Lin, S. U. Stich, M. Jaggi (https://arxiv.org/abs/1907.09356)\n \n4. I think there is a misunderstanding: by the averaging rate I meant the \\gamma parameter in Choco-SGD and \\eta in DeepSqueeze --- these are hyperparameters. Without stating hyperparameter tuning details (of averaging learning rate for the baselines / theta for the Moniqua; and of SGD learning rate), the results are not reproducible and also are questionable. \n \n5. when reporting hyperparameters, please report hyperparameters of the baselines too. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "rJeJhbGjsB", "original": null, "number": 5, "cdate": 1573753255119, "ddate": null, "tcdate": 1573753255119, "tmdate": 1573765944626, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "BJlhW4Q6FS", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for the detailed reviews and comments!", "comment": "Thank you very much for your reviews and comments! Below we address your each concern individually:\n\n==========Response to Concern 1.==========\nThanks for sharing this insightful concern! To clarify, all the performances are measured on the averaged model, which is a model that takes the average over all the model parameters on all the workers. We are measuring the averaged model since this is the standard output of decentralized SGD algorithms, including D-PSGD, D^2, AD-PSGD and Moniqua (Please refer to the output of Algorithm 1 on page 4). In the experiments, we use an independent process in the backend to compute the averaged model and measure its training and test accuracy/loss by a forward pass over the DNN model on the training and test set.\n\n==========Response to Concern 2.==========\nIn theory, we prove all the workers will reach consensus in Lemma 6 on page 23 for Moniqua (Lemma 12 and 19 for the two extensions.) \n\nIn validation, you are correct that results on the consensus error should also be included. We provide extra experimental results in Appendix C.5 of the revised paper. These experiments show that: 1) The average distance among workers are decreasing to zero even with aggressive quantization (2 and 3 bits) as the setting in the original paper, where constant step size is used. 2) With decreasing step size, the average distance among workers are also decreasing to zero. We validate this on both ResNet110 and ResNet18.\n\n==========Response to Concern 3.==========\nFirst, we\u2019d like to point out that $\\theta$ is proportional to the infinity norm of gradient, which is generally independent of the model size. And we have a theoretical result that guarantees consensus among workers (Lemma 6 on page 23 for Moniqua, Lemma 12 and 19 for the two extensions). \n\nSecond, in practice we could start choosing or tuning $\\theta$ from a small value, even if the value we end up choosing is smaller than the true $\\theta$, this also works. To reason about this, consider the toy example with two workers as shown in the \u201cIntroduction\u201d: with true $\\theta$, $m_1=m_1+\\frac{1}{2}(m_2-m_1)$. If $\\theta$ is smaller than the true $\\theta$, then the equation becomes $m_1=m_1+\\frac{\\gamma}{2}(m_2-m_1)$ with some factor  $\\gamma$. Note that this new expression is equivalent to previous work of using deteriorating factor or extrapolation  in communication (Reisizadeh et al., 2018;Koloskova et al., 2019), which also leads to convergence as long as  $\\gamma$ is not extremely small. From a theoretical perspective, this means we are using a communication matrix with slightly small spectral gap as defined in Assumption 2 on page 4.\n\nThere are also several effective ways of choosing $\\theta$ in practice,  please refer to \u201cResponse to Question 10\u201d to Reviewer 3.\n\nThanks again for your insightful suggestions and concerns! If you think our clarification and the extra results address your concerns, we would be appreciated if you could reconsider your score for this paper. On the other hand, if you believe there are still significant flaws or questions, please let us know and we would be happy to improve it!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "BkeOLbMsoS", "original": null, "number": 4, "cdate": 1573753168172, "ddate": null, "tcdate": 1573753168172, "tmdate": 1573765833862, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "B1gOjgMjsH", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for your detailed comments and suggestions! (Part 3)", "comment": "==========Response to minor comments==========\nThank you very much for sharing these detailed minor comments. Since some of the comments are regarding implementations, we provide response to them below:\n1. About the definition of modulo when x is a float number: In computing, the modulo operation returns the remainder of numerator/denominator. And the return type can be specified. For example, in C++11, such modulo operation on float number is defined as \u201cfmod\u201d.\n2. Intuition behind Moniqua: This is a toy example for better illustrating the idea of Moniqua. To simplify things, we consider two workers in the system and model is one dimensional. And we believe it is trivial to extend it to high dimensional since coordinates do not have dependency when applied Modulo operation.\n3. We provide assumption for the two extensions on page 27 and page 39-40. For asynchronous communication specifically, please refer to section E for some preliminary illustrations.\n4. Other algorithm refers to (Reisizadeh et al.), which does not converge in the domain of non-convex DNN problem.\n5. About floored outputs in 16-bit integers: Note that since all the workers are using the same delta, we compute the delta on the receiver side. For example, if a sender needs to send 0.98 and delta is 0.01, then it sends 98 and the receiver recovers it by times 98 with delta and get 0.98.\n6. Your other comments are regarding writing and organization of the paper. We sincerely thank you for detailed suggestions and we will fix them in the final version of the paper.\n\nThanks again for your detailed reviews and suggestions! If you think our clarification and the extra results address your concerns, we would be appreciated if you could reconsider your score for this paper. On the other hand, if you believe there are still significant flaws or questions, please let us know and we would be happy to improve it!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "ByesrlMjjr", "original": null, "number": 2, "cdate": 1573752899027, "ddate": null, "tcdate": 1573752899027, "tmdate": 1573765795787, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "Hyl8Hk66KS", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for your detailed comments and suggestions! (Part 1)", "comment": "Thank you very much for your suggestions and comments! Below we address your each concern individually:\n\n==========Response to Concern 1. ==========\n(i) About constant learning rate. \nIn theory, non-constant step size is still an open question in the domain of decentralized SGD. All the baselines as well as variants of decentralized SGD including D-PSGD, D^2 and AD-PSGD analyze theoretical convergence rates with constant step size (Lian et al., 2017a;Tang et al., 2018a;Lian et al., 2017b;Tang et al. 2018b;Koloskova et al. 2019). Since Moniqua is based on plain D-PSGD, in theory we follow the constant step size scheme of D-PSGD (and D^2 and AD-PSGD) while leaving the theory of non-constant step size as future work. We want to emphasize that the goal of Moniqua is to provide a technique for compressed communication that can be applied to a wide variety of decentralized learning algorithms while achieving the same theoretical convergence rate: modifying or advancing the full-precision baselines is beyond the scope of what we are trying to do.\n\nIn the spirit of being consistent with theory, we include experimental results based on constant step size. However, you are correct that empirical results of non-constant step size should also be included. We provide extra experimental results on decreasing step size in Appendix C.5 of the revised paper. This experimental results show that with decreasing step size, Moniqua is able to achieve state-of-the-art test accuracy and more robust to low-bits budget compared to baselines. \n\n(ii) About accuracy gap and s.t.o.a test accuracy.\nThanks for pointing this out! \nFirst, we want to clarify that in the section of \u201cAggressive Quantization\u201d where you observe the accuracy gap, we aim at showing the robustness of each algorithm, that is, their ability to converge under extreme cases. In these very extreme cases (2 or 3 bits), no algorithms can achieve state-of-the-art test accuracy. Of all the algorithms tested, Moniqua is the one that is most close to full-precision algorithm.\n\nSecond, you are absolutely right that investigating how many bits allows Monqua to achieve s.t.o.a. accuracy is also important. we discuss the number of bits needed by Moniqua to guarantee convergence in section \u201cEfficient Moniqua\u201d and section C.1 of the original paper as well as appended experimental results in section C.5 of the revised paper. The extra experimental results show that when training ResNet110 on CIFAR10, Moniqua needs 6bits to achieve state-of-the-art test accuracy (while baselines suffer from significant accuracy gap). On the other hand, when training ResNet18 on CIFAR10, Moniqua needs 4bits to achieve state-of-the-art test accuracy (while baselines suffer from accuracy gap). By comparison, baselines need more bits to achieve the same accuracy. We also discuss methods of using even fewer bits below 6 bits on ResNet110 in section \u201cEfficient Moniqua\u201d.\n\n==========Response to Concern 2. ==========\nThank you for sharing this concern! \nFirst, please note that the expression on page 5 is the upper bound for the number of bits needed. That being said, for the ring 8 case the number of bits required to converge is at most 5. \n\nSecond, in practice the number of bits needed largely depends on the model, dataset and task. Baselines are showing results based on smaller models like ResNet20 (Tang et al., 2018a;b) and logistic regression (Koloskova et al. 2019), and even with these simpler models, no baseline can close accuracy gap below 4 bits (according to their experimental results). In contrast, we are testing Resnet110, and we show in the extra experimental results (section C.5) and \u201cEfficient Moniqua\u201d that Moniqua needs 5-6 bits in average (ring network) for ResNet110 to achieve s.t.o.a accuracy, which is a lower bit-level than baselines. \nWe also found the number of bits required largely depends on the structure of model as well. For example, VGG-type models are more robust than ResNet-type models since they have more fully-connected layers (details in section C.1, this finding is supported by previous work (Grubic et al., 2018)). We found 3-4 bits (ring network) in average is sufficient for VGG16 to achieve s.t.o.a accuracy. However, these investigations are beyond the main gist of Moniqua, thus we don\u2019t discuss that in detail in the paper.\n\n==========Response to Concern 3.==========\nThank you for pointing this out. It is correct that for some graphs, the spectral gap is not a constant. However, the expression for spectral gap largely depends on the structure and size of the graph. In the presentation of asymptotic convergence rate, we maintain the consistency with related and previous works in decentralized SGD to treat it as a constant and point that out at the bottom of Theorem 2 (Lian et al., 2017a;Tang et al., 2018a;Lian et al., 2017b;Tang et al. 2018b;Koloskova et al. 2019). And for number of bits, we include spectral gap in expression on page 5."}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "B1gOjgMjsH", "original": null, "number": 3, "cdate": 1573752992315, "ddate": null, "tcdate": 1573752992315, "tmdate": 1573765771696, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "ByesrlMjjr", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for your detailed comments and suggestions! (Part 2)", "comment": "==========Response to Concern 4.==========\nPlease note that the averaging rate is not a hyperparameter. As stated in the pseudo code of each algorithm (including DeepSqueeze, ChocoSGD and Moniqua), average step is conducted once per iteration (this is also how baselines implement their algorithms). And none of baselines as well as Moniqua is listing it as a hyperparameter. On the other hand, we agree with you that communication frequency is also an interesting topic, which we will leave as future work.\nAnd for the $\\theta$, we include that in the last line of the first paragraph under \u201cWall-clock Time Evaluation\u201d. And $\\theta$ in other experiments are given in \u201cResponse to Concern 5\u201d.\n\n==========Response to Concern 5.==========\n-We list part of the hyperparameters in the paragraph \u201dConfiguration\u201d. Here we list more details: for \u201cWall-clock Time Evaluation\u201d, we use step size=0.05 and $\\theta$=3.0. In the \u201cAggressive Quantization\u201d, we use step size=0.01 and $\\theta$=1.0 for 2 bits and $\\theta$=1.5 for 3bits.\n-Asynchronous communication (details on page 39) is where worker communicate by randomly choose one neighbor to average model parameters. We do not model asynchrony but run the algorithm in an actual distributed system (Google Cloud). We believe it\u2019s only in the actual system that allows us to obtain more accurate performance.\n-Since this concern is overlapping with Question 10, please refer to Response to Question 10 for details.\n\n==========Response to Concern 6.==========\nThis assumption does NOT restrict x to be in [-1,1]. On the contrary, it says the quantizers we study has the following property: \n1). If it quantizes an $x \\in [-1,1]^d$, then we assume $\\|Q(x)-x\\|_\\infty \\leq \\delta$; \n2). if it quantizes an $x$ outside $[-1,1]^d$, no assumption is needed.\nThus we are only making assumptions on the quantizer with respect to a small region. Compared to previous work  (Koloskova et al., 2019; Tang et al., 2018c; 2019) where they assume quantizer is unbiased and has bounded error for all $x \\in \\mathbb{R}^d$, our assumption is much weaker. And this assumption works with large number of quantizers as we discussed in the \u201cQuantizer\u201d section.\n\n==========Response to Concern 7.==========\nThis is a good question! Note that in Theorem 1, $\\theta\\delta$ is proportional to the step size while the step size is proportional to $1/\\sqrt{K}$, where $K$ is the number of total iterations. That means, if we use a quantizer with assumption $\\|Q(x) - x\\| \\leq \\theta\\delta$, we will have to increase the precision to reduce $\\|Q(x) - x\\|$ as $K$ increases, thus at the cost of more bits. So can we use the fixed number of bits to do this? Yes and this is where Moniqua \u201ckicks in\u201d. By Modulo operation, we could achieve higher precision without using more bits. Part of the intuition is introduced in the \u201cIntroduction\u201d, here we provide another toy example: Consider using 2 bits to quantize a scalar in [0,3] (without the loss of generality we consider linear quantization), then we get quantized points 0,1,2,3 and the quantization error bound is 1. After using Modulo operation, we rescale the quantization space to a smaller region, e.g. [0,1], then with the same 2 bits quantizing we get 0, 0.333, 0.667, 1 and the quantization error bound becomes 0.333. The intuition part in the Introduction shows that with this rescale, we can still get the model difference as desired.\n\n==========Response to Concern 8.==========\nAs we point out in the asynchronous section, \u201can iteration represents a single gradient update on one randomly-chosen worker\u201d, that being said, $\\tau_k$ refers to the delay of one worker that\u2019s doing the k-th update. Note that this is a standard formulation in previous analysis of asynchronous algorithms (especially in the domain of parallel SGD) (Lian et al., 2017b).\n\n==========Response to Concern 9.==========\nNote that when data is decentralized, D-PSGD does not have the guarantee to converge (for detailed explanation and analysis, please refer to the previous work in (Tang et al., 2018a)). Intuitively in deep learning, when running D-PSGD on decentralized data, the worker could fall into different local optimas. Note that the results shown in the paper, where D-PSGD is not converging, is aligned with experimental results in previous work (Tang et al., 2018a).\n\n==========Response to Question 10.==========\nThis is a good question! In practice, we recommend two methods for computing $\\theta$: 1) we could first run D-PSGD for a few iterations or epochs, and checkpointed the largest value of the infinity norm of gradients, then use our expression shown in Theorem 2 to calculate $\\theta$. Since workers are getting close as training proceeds (proof in Lemma 6 on page 23), $\\theta$ in the first few iterations can be approximately used as a bound for Modulo operation. 2) another way is to treat $\\theta$ as a hyperparameter as step size and use tuning methods such as grid search or random search.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "ryxdBfMoiH", "original": null, "number": 6, "cdate": 1573753407966, "ddate": null, "tcdate": 1573753407966, "tmdate": 1573753407966, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "We thank all the reviewers for their time and suggestions!", "comment": "Dear Reviewers,\n\nThank you very much for all the concerns and reviews. \n\nWe submitted a revised version, where we fix some wording and provide some extra experimental results to address your concerns and questions. Here is a list of experimental results (details are in section C.5 of the revision paper, page 15-16):\n\n1. We provide results on Moniqua with decreasing step size on different models.\n2. We provide the final test accuracy on Moniqua and other baselines under different bit-level.\n3. We provide consensus error results when using Moniqua both with constant and decreasing step size.\n\nPlease let us know if you have any other questions. Thank you again for your reviews!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "HygIRkMoiB", "original": null, "number": 1, "cdate": 1573752782158, "ddate": null, "tcdate": 1573752782158, "tmdate": 1573752782158, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "Bke4EFvwcr", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment", "content": {"title": "Thank you very much for your reviews and comments!", "comment": "Thank you very much for your reviews and comments! Your comments on  our paper is a really good summary that captures the main gist and contributions of Moniqua. We will be excited to see how Moniqua could advance the domain of decentralized optimization!"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxhqhVKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper129/Authors|ICLR.cc/2020/Conference/Paper129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175981, "tmdate": 1576860559970, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper129/Authors", "ICLR.cc/2020/Conference/Paper129/Reviewers", "ICLR.cc/2020/Conference/Paper129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Comment"}}}, {"id": "BJlhW4Q6FS", "original": null, "number": 1, "cdate": 1571791875706, "ddate": null, "tcdate": 1571791875706, "tmdate": 1572972635099, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies an important problem in the decentralized optimization, i.e., communication compression. Unlike gradient compression, the model compression in decentralized optimization is more challenging because the model parameters will not vanish like gradient. To solve this problem, the authors proposed Moniqua, where only lower-order bits of model are communicated since their higher-order bits are getting close. A hyperparameter $\\theta$ is used as a \u201ccriterion\u201d to separate the higher-order bits and lower-order bits of the model parameter via a modular arithmetic. The authors also apply Moniqua on D^2 and AD-PSGD.\n\nThere are several major concerns:\n1.\tIt is not clear how the performance is measured in this paper. If the authors simply take the average of training loss over all nodes, this is not a good evaluation unless consensus is achieved. Unless clarifying this, it is hard to judge the performance. \n2.\tIt is better to provide evaluation on the consensus error which meansures the model consistency.\n3.\tThe modular hyperparameter $\\theta$ is not easy to choose and seems cannot help achieve consensus. On the one hand, the theory suggests its value to be proportional to the gradient magnitude bound, which could be very large in practice. But if $\\theta$ is large, we cannot achieve even approximate consensus because of the quantization error. On the other hand, a small $\\theta$ cannot ensure sufficient model average since higher-order bits are ignored by the modular arithmetic but actually they should be communicated. Either way, it doesn\u2019t seem to be good for achieving consensus. \n\nOverall, the idea of Moniqua is interesting and the authors provide useful extensions based on it. But the evaluation is not convincing and whether the proposed Moniqua can achieve consensus is unclear.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575115061929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper129/Reviewers"], "noninvitees": [], "tcdate": 1570237756644, "tmdate": 1575115061945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Review"}}}, {"id": "Hyl8Hk66KS", "original": null, "number": 2, "cdate": 1571831613940, "ddate": null, "tcdate": 1571831613940, "tmdate": 1572972635055, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers training of DL models in a decentralized setting. Previous work (Tang et al 2018b, Koloskova et al 2019, Tang et al 2019) introduced communication compression in order to reduce communication cost in decentralized SGD. This paper shows that the naive way to apply quantization to compress communication can fail (but, none of the previous works used this naive way). The paper proposes a novel decentralized SGD algorithm called MONIQUA which communicates only with compressed vectors. The paper theoretically proves that MONIQUA asymptotically converges with the same speed as D-PSGD (Decentralized SGD with full communications) for non-convex functions. \nThe main benefit of the proposed algorithm is that in contrast to the baselines, MONIQUA doesn\u2019t require any additional memory and computation overhead connected to that. \nThe paper also allow to combine MONIQUA with asynchronous communications (AD-PSGD) and decentralized data (D^2), which is novel, previous baseline didn\u2019t allow asynchronous communications. Paper experimentally validates MONIQUA and show that it converges faster than the baselines on early optimization stage, but omit comparison of final results.\n\nMy score is weak reject. The method is interesting and elegant, doesn\u2019t require additional memory, theoretically convergent with a good speed and allows asynchronous communication. However, it looks a bit incremental. In contrast to the baselines, MONIQUA does not support arbitrary communication compression. Experimental comparison is shown only for the beginning of the optimization where the algorithm doesn\u2019t achieve state of the art accuracy. \n\nMajor concerns: \n\n1. Superior experimental results are shown only for (i) the constant stepsize schedule and (ii) only in the beginning of the optimization. The algorithm is unable to achieve s.o.t.a. test accuracy and has severe accuracy drop of 10-20%. It is unclear if MONIQUA is able to close the accuracy gap. \nTo get good test accuracy the exponentially decaying learning rate schedules are usually used when training ResNet on Cifar10. However, that is unclear if this learning rate schedule could be supported by the algorithm. I see that theoretically the same result would be not possible. Once the learning rate dropped, ||x_i - x_j|| has to be smaller than the new theta, which is not true. \n\n2. In contrast to the baselines, MONIQUA doesn\u2019t support arbitrary quantization. The quantization level is fixed. For example, for the ring 8 case (which was used in experiments), the number of bits required to converge is at least 5 (I calculated it using expression for B on page 5). This also questions if MONIQUA will be able to converge to good accuracy in extreme bit-budget. \n\n\nOther concerns that should be addressed: \n\n3. The spectral gap is usually not a constant: for example for the ring topology it decreases as 1/n^2. That should be clarified in the convergence rate and the number of bits required to communicate. \n\n4. In the experiments, to ensure fair comparison, the tuning details of the averaging rate for DeepSqueeze and Choco baselines, and theta parameter for MONIQUA were not given.\n\n5. Reproducibility of the experiments: Many experimental details are omitted: \n   - What are the values of hyperparameters used in the plots?\n   - How did you model asynchrony in experiments?\n   - How did you choose the parameter \\theta in experiments?\n\n6. In (1), why x is \\in [-1, 1]^d? What if this assumption doesn\u2019t hold in (4)? The Theorem 1 is not clear then.\n\n7. Why taking mod \\theta is required? Cannot you assume that ||Q(x) - x|| <= theta delta? The whole analysis will stay true.\n\n8. In Asynchronous communication section, why \\tau_k are the same for all the workers? Cannot different workers have different delays? \n\n9. Why in experiments D-PSGD doesn\u2019t converge when the data are decentralized? This is not consistent with the theory, D-PSGD should converge just slower. \n\nQuestions: \n10. Is there a way to compute theta it in practice beforehand? \n\nMinor comments: \n- x mod y is not defined when x is a float number.\n- Intuition behind Moniqua: do you assume that worker 1 has only one neighbour (worker 2)? does m_1 is one dimensional or high dimensional? \n- Algorithm 1, the notation \\bar X_k might be confusing as in the proof big letters correspond to matrixes. Consider to change on \\bar x_k.\n- why some equations are numbered and some are not? \n- In Asynchronous communication section: what are the assumptions on W_k? \n- it would be helpful for the clarity to remind what is the stochastic rounding quantization in configuration of experiments section. \n- which are the other algorithms you talk about in footnote 6? \n- In wall-clock time evaluation section: how floored outputs could be represented as 16-bit integers? aren\u2019t the vectors which you quantize always smaller than one? \n- wall-clock time evaluation section: the statement that \u201calgorithms diverge\u201d might be confusing.\n- Aggressive quantization add  \u201d.\u201d\n- why the number of epochs differ in different experiments? Fig. 3(a) and Fig. 4(a) ?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575115061929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper129/Reviewers"], "noninvitees": [], "tcdate": 1570237756644, "tmdate": 1575115061945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Review"}}}, {"id": "Bke4EFvwcr", "original": null, "number": 3, "cdate": 1572464940477, "ddate": null, "tcdate": 1572464940477, "tmdate": 1572972635010, "tddate": null, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "invitation": "ICLR.cc/2020/Conference/Paper129/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work is about the use of quantized communication in decentralized stochastic gradient descent. The general advantage of using quantized communication is that is has the potential to reduce the amount of data exchanged, thereby leading to faster convergence -- and the main focus of this work is to include ideas that stem from quantized communication into decentralized training approaches. Competing methods of this kind suffer from potentially severe problems like memory overhead, limited applicability to convex problems, or the need for nonlinear quatizers.\n\nThe proposed Moniqua algorithm is an attempt to overcome these shortcomings. A central technical contribution is the analysis of direct quantization strategies in decentralized SGD in Theorem 1, which basically states that local models can fail to converge even for simple objective functions. The motivation for the Moniqua algorithm -- which is designed to solve this problem of local models -- is rather intuitive, and the algorithm can be implemented quite easily in practice. Some theoretic insight is provided, such as the asymptotic convergence rate (which is the same as in D-PSGD) and the loglog(n)-bound on the number of bits per parameter communicated as a function of the number of parallel workers.\n\nMoniqua is empirically evaluated on a large number of different network configurations, and it seems that it indeed converges faster than other related algorithms, while being highly robust to aggressive quantization and strict bit-budgets. In summary, this work addresses a highly relevant problem and it nicely combines formal asymptotic analysis with experimental validation. I think, this work has indeed a great potential to advance this field of research. "}, "signatures": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper129/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Moniqua: Modulo Quantized Communication in Decentralized SGD", "authors": ["Yucheng Lu", "Christopher De Sa"], "authorids": ["yl2967@cornell.edu", "cdesa@cs.cornell.edu"], "keywords": ["decentralized training", "quantization", "communicaiton", "stochastic gradient descent"], "TL;DR": "We propose a general method that allows decentralized SGD to use quantized communication.", "abstract": "Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "pdf": "/pdf/00510f225a8d2d34c4fd2b04037abc80726f51fb.pdf", "paperhash": "lu|moniqua_modulo_quantized_communication_in_decentralized_sgd", "original_pdf": "/attachment/8232672315846051a3880e2ec8a001f72fbf9f23.pdf", "_bibtex": "@misc{\nlu2020moniqua,\ntitle={Moniqua: Modulo Quantized Communication in Decentralized {\\{}SGD{\\}}},\nauthor={Yucheng Lu and Christopher De Sa},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxhqhVKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxhqhVKPB", "replyto": "HyxhqhVKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575115061929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper129/Reviewers"], "noninvitees": [], "tcdate": 1570237756644, "tmdate": 1575115061945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper129/-/Official_Review"}}}], "count": 16}