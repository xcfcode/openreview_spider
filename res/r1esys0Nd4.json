{"notes": [{"id": "r1esys0Nd4", "original": "B1x_ZzbQ_4", "number": 39, "cdate": 1553423075098, "ddate": null, "tcdate": 1553423075098, "tmdate": 1562082116983, "tddate": null, "forum": "r1esys0Nd4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Train Neural Network by Embedding Space Probabilistic Constraint ", "authors": ["Kaiyuan Chen", "Zhanyuan Yin"], "authorids": ["chenkaiyuan@ucla.edu", "yinzhanyuan1999@ucla.edu"], "keywords": ["probability", "constraint", "constraint learning", "weak supervision", "embedding", "deep neural network"], "TL;DR": "We introduce an embedding space approach to constrain neural network output probability distribution.", "abstract": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.  While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples.", "pdf": "/pdf/5a009d486acd0435496e04284706fec2eacd2412.pdf", "paperhash": "chen|train_neural_network_by_embedding_space_probabilistic_constraint"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "rklRNBltFE", "original": null, "number": 1, "cdate": 1554740533800, "ddate": null, "tcdate": 1554740533800, "tmdate": 1555511882968, "tddate": null, "forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "content": {"title": "The work is highly relevant to the workshop. Results are good. Writing is sufficiently clear. Maths and notation are not.", "review": "1/ Introduction\nThe first sentence is embarrassing as 'probability' has a mathematical definition.\nThe problem target precisely the workshop topic. \n\n2/ Related work\nOk.\n\n3/ Embedding...\nThere are inconsistencies in the notations and many typos.\nIs Q a 2d vector ? Why is it compared to P(k) later ?\n\n4/ Evaluation\nResults are competitive.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Train Neural Network by Embedding Space Probabilistic Constraint ", "authors": ["Kaiyuan Chen", "Zhanyuan Yin"], "authorids": ["chenkaiyuan@ucla.edu", "yinzhanyuan1999@ucla.edu"], "keywords": ["probability", "constraint", "constraint learning", "weak supervision", "embedding", "deep neural network"], "TL;DR": "We introduce an embedding space approach to constrain neural network output probability distribution.", "abstract": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.  While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples.", "pdf": "/pdf/5a009d486acd0435496e04284706fec2eacd2412.pdf", "paperhash": "chen|train_neural_network_by_embedding_space_probabilistic_constraint"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "cdate": 1553713415385, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415385, "tmdate": 1555511825624, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper39/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Skgk0KR9FV", "original": null, "number": 3, "cdate": 1554864582949, "ddate": null, "tcdate": 1554864582949, "tmdate": 1555511879498, "tddate": null, "forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "content": {"title": " Leveraging additional supervision to handle label distribution shift", "review": "\nThis paper tries to incorporate label distribution into model learning, when a limited number of training instances is available. \nIntuitively, the output label distribution could be wrongly biased, and the prior information like label distribution could be helpful. \nTo handle this problem, the authors propose two different techniques, the first regulate the output distribution and the second regularize the constructed representation. \nPerformance comparison demonstrated the effectiveness of the proposed method when only a limited number of instances are available. \n\nI think the studied problem is interesting and the proposed solution is novel and reasonable. \nMy main concern is about the assumption of the algorithm. \nThe proposed learning algorithm assumes that the algorithm can access a relative accurate label distribution, and the output distribution regularization depend on this term. \nBut for real world applications, it could be hard to get such knowledge, since in order to get the required annotation (as described in Appendix), the user needs to have a good understanding of the real distribution or annotate all instances in that batch. \nBesides, I noticed that in the experiment results, the proposed method sometimes achieves worse performance than baselines when all training data is available. \nThis phenomenon seems to me implies that the proposed method cannot fully leverage the additional information, as intuitively, with more information, it should perform better. \n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Train Neural Network by Embedding Space Probabilistic Constraint ", "authors": ["Kaiyuan Chen", "Zhanyuan Yin"], "authorids": ["chenkaiyuan@ucla.edu", "yinzhanyuan1999@ucla.edu"], "keywords": ["probability", "constraint", "constraint learning", "weak supervision", "embedding", "deep neural network"], "TL;DR": "We introduce an embedding space approach to constrain neural network output probability distribution.", "abstract": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.  While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples.", "pdf": "/pdf/5a009d486acd0435496e04284706fec2eacd2412.pdf", "paperhash": "chen|train_neural_network_by_embedding_space_probabilistic_constraint"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "cdate": 1553713415385, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415385, "tmdate": 1555511825624, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper39/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "ryxjFza5FE", "original": null, "number": 2, "cdate": 1554858626765, "ddate": null, "tcdate": 1554858626765, "tmdate": 1555511876941, "tddate": null, "forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "content": {"title": "Nice performance, but needs some work before publication", "review": "This paper introduces a method to perform semi-supervised learning with deep neural networks.  The model is trained on a few labeled examples and the expected label distribution to achieve relatively high accuracy, given the small training size.  The problem is motivated well and provides a clear introduction and background of related work. Included figures provide useful reference for the paper and the experiments demonstrate that the approach works well.  \n\nI found that the assumption that the probability distribution of true labels could be well known to not be very realistic.  I think it would be quite interesting to see how your approach performs when faced with different levels of error (including non-gaussian error) in the probability distribution. The assumption that out-of-distribution batches are identified through weak supervision seems like a large caveat to me. \n\nWhen discussing the Model Description in Section 4, you say: \u201cTo guarantee that our comparison focuses on output probability distribution instead of one single instance\u2019s label, we train our models with batch size 128.\u201d This statement seems overly specific to me. How does a batch size of 128 specifically guarantee that your comparison focuses on the output probability distribution? Would any sufficiently large batch size work? How do you find sufficiently large batches?\n\nOverall, the paper presents an architecturally simple solution that seems to work well to improve accuracy in a semi-supervised setting. \n\nThe paper has quite a few grammatical and formatting errors. It should be thoroughly read and edited before publication (I noted some of the grammatical issues below that I hope will be helpful). \n\nEditing Suggestions: \n- There appear to be several instances where the spacing between words and citations is incorrect. There are also a few times where parenthesis around citations were not properly closed. \n- missing closing parenthesis in \u201cconstraints (Stewart & Ermon (2016),\u201d\n- check spacing in citations - Ho et al. \n- It would be nice to see a reference to prior work which introduces \u201chigher level knowledge\u201d in the last sentence of the \u201cWeak Supervision\u201d paragraph. \n\nSmall notes on language that may make it more readable: \n- \u201cneural networks fails to learn the decision boundary correctly from limited number of examples\u201d -> \u201cneural networks fail to learn the decision boundary correctly from a limited number of examples\u201d\n- \u201cwe perform our probabilistic constraint on neural network\u2019s embedding space,\u201d -> \u201cwe perform our probabilistic constraint on the neural network\u2019s embedding space,\u201d\n- \u201clow dimensional space through autoencoder.\u201d -> \u201clow dimensional space through (an or the) autoencoder.\u201d\n- \u201cour model can converge to an high accuracy faster than\u201d -> \u201cour model can converge to a high accuracy faster than\u201d\n- \u201crequire large quantity and high-quality labels for training\u201d-> \u201crequire a large quantity of high-quality labels for training\u201d\n- \u201ccan reflect neural network\u2019s confidence towards certain label.\u201d -> \u201ccan reflect the neural network\u2019s confidence towards a certain label.\u201d\n- \u201cInstead of performing counting the arguments of the maxima for all labels, which is not inefficient,\u201d -> \u201cInstead of counting the arguments of the maxima for all labels, which is not inefficient,\u201d\n- \u201cWe add additional embedding layer with width 40,\u201d -> \u201cWe add an additional embedding layer with width 40,\u201d\n- \u201cWe experiment our model under different level of constraints. \u201c -> \u201cWe experiment under different level of constraints. \u201c\n- \u201cthat constraints output probability distribution in an embedding space.\u201d -> \u201cthat constrains the output probability distribution in an embedding space.\u201d\n- \u201cwe need far less high quality training examples to reach high accuracy\u201d -> \u201cwe need far fewer high quality training examples to reach high accuracy\u201d\n- \u201cThus, we conclude jointly optimizing\u201d -> \u201cThus, we conclude that jointly optimizing\u201d\n- \u201cIt can me written mathematically\u201d -> \u201cIt can be written mathematically\u201d\n- \u201cThus, we adopt the structure of decoder of autoencoder and \u201c -> \u201cThus, we adopt the structure of the decoder of an autoencoder and\u201c\n- \u201cOur proposed method uses unsupervised loss\u201d -> \u201cOur proposed method uses an unsupervised loss\u201d\n- \u201cand uses domain knowledge of output probability distribution to determine the actual decision boundaries.\u201d -> \u201cand uses domain knowledge of the output probability distribution to determine the actual decision boundaries.\u201d", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Train Neural Network by Embedding Space Probabilistic Constraint ", "authors": ["Kaiyuan Chen", "Zhanyuan Yin"], "authorids": ["chenkaiyuan@ucla.edu", "yinzhanyuan1999@ucla.edu"], "keywords": ["probability", "constraint", "constraint learning", "weak supervision", "embedding", "deep neural network"], "TL;DR": "We introduce an embedding space approach to constrain neural network output probability distribution.", "abstract": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.  While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples.", "pdf": "/pdf/5a009d486acd0435496e04284706fec2eacd2412.pdf", "paperhash": "chen|train_neural_network_by_embedding_space_probabilistic_constraint"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Official_Review", "cdate": 1553713415385, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper39/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415385, "tmdate": 1555511825624, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper39/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "BkgJSxpG9V", "original": null, "number": 1, "cdate": 1555382326610, "ddate": null, "tcdate": 1555382326610, "tmdate": 1555510974470, "tddate": null, "forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Train Neural Network by Embedding Space Probabilistic Constraint ", "authors": ["Kaiyuan Chen", "Zhanyuan Yin"], "authorids": ["chenkaiyuan@ucla.edu", "yinzhanyuan1999@ucla.edu"], "keywords": ["probability", "constraint", "constraint learning", "weak supervision", "embedding", "deep neural network"], "TL;DR": "We introduce an embedding space approach to constrain neural network output probability distribution.", "abstract": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.  While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples.", "pdf": "/pdf/5a009d486acd0435496e04284706fec2eacd2412.pdf", "paperhash": "chen|train_neural_network_by_embedding_space_probabilistic_constraint"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper39/Decision", "cdate": 1554736076861, "reply": {"forum": "r1esys0Nd4", "replyto": "r1esys0Nd4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736076861, "tmdate": 1555510962005, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 5}