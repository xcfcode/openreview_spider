{"notes": [{"id": "SkxgnnNFvH", "original": "rJeEsrvWvr", "number": 175, "cdate": 1569438887582, "ddate": null, "tcdate": 1569438887582, "tmdate": 1585176692873, "tddate": null, "forum": "SkxgnnNFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "jxi4e-JIVt", "original": null, "number": 1, "cdate": 1576798689404, "ddate": null, "tcdate": 1576798689404, "tmdate": 1576800945739, "tddate": null, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "invitation": "ICLR.cc/2020/Conference/Paper175/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper presents a new architecture that achieves the advantages of both Bi-encoder and Cross-encoder architectures. The proposed idea is reasonable and well-motivated, and the paper is clearly written. The experimental results on retrieval and dialog tasks are strong, achieving high accuracy while the computational efficiency is orders of magnitude smaller than Cross-encoder. All reviewers recommend acceptance of the paper and this AC concurs.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730270, "tmdate": 1576800283029, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper175/-/Decision"}}}, {"id": "ByeUgZT35r", "original": null, "number": 3, "cdate": 1572815086365, "ddate": null, "tcdate": 1572815086365, "tmdate": 1574227460155, "tddate": null, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "invitation": "ICLR.cc/2020/Conference/Paper175/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "Summary: This work proposes a new transformer architecture for tasks that involve a query sequence and multiple candidate sequences. The proposed architecture, called poly-encoder, strikes a balance between a dual encoder which independently encodes the query and candidate and combines representations at the top, and a more expressive architecture which does full joint attention over the concatenated query and candidate sequences. Experiments on utterance retrieval tasks for dialog and an information retrieval task show that poly-encoders strike a good trade-off between the inference speed of the dual encoder model and the performance of the full attention model. \n\nPros:\n- Strong results compared to baselines on multiple dialog and retrieval tasks. \n- Detailed discussion of hyperparameter choices and good ablations.\n- Paper is well written and easy to follow.\n\nCons:\n- Limited novelty of methods. Ideas similar to the model variants discussed in this work have been considered in other work (Eg: [1]). It is also known that in-domain pre-training (i.e, pre-training on data close to the downstream task\u2019s data distribution) helps (Eg: [2]). So this work can be considered as an application of existing ideas to dialog tasks. \n- In terms of impact, utterance retrieval has fairly limited applicability in dialog. The dialog tasks considered in this work have a maximum of 100 candidate utterances, whereas in practice, the space of possible responses is much larger. While retrieval models are useful, I am skeptical about the practical value of the improvements shown in the paper (especially the improvements over bi-encoder, which is already a decent model).\n\nSuggestions:\nOne way to get around the inefficiency of the cross-encoder architecture is to first use an inexpensive scoring mechanism such as TFIDF or bi-encoder to identify a small number of promising candidates from all the possible candidates. We can then use the cross-encoder to do more precise scoring of only the promising candidates. I am curious where a pipelined model such as this compares against the variants discussed in the paper in terms of speed and performance. \n\nWhile the paper presents strong results on several dialog utterance retrieval tasks, the methods presented have limited novelty and impact. I am hence leaning towards borderline. \n\nReferences\n\n[1] Logeswaran Lajanugen, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-Shot Entity Linking by Reading Entity Descriptions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n[2] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.\n\nEdit: I have read the author response. Based on the rebuttal, I am more convinced about the practical impact of the approach. I am raising my score and recommending accept. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666140637, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper175/Reviewers"], "noninvitees": [], "tcdate": 1570237755947, "tmdate": 1575666140649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper175/-/Official_Review"}}}, {"id": "BylSsUmijH", "original": null, "number": 3, "cdate": 1573758620935, "ddate": null, "tcdate": 1573758620935, "tmdate": 1573758620935, "tddate": null, "forum": "SkxgnnNFvH", "replyto": "ByeUgZT35r", "invitation": "ICLR.cc/2020/Conference/Paper175/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review.\n\nRe: \"utterance retrieval has fairly limited applicability in dialog\"\n\nFirstly, there are whole competitions run by dialogue researchers evaluating retrieval systems, e.g. DSTC7 Track 1 last year, and again this year in DSTC8, implying researchers in the field feel it is very important. Secondly, on a number of dialogue tasks, direct human evaluation comparing SOTA generative models with SOTA retrieval models ends up with retrieval models winning, see e.g. https://openreview.net/forum?id=r1l73iRqKm from last ICLR.\n\n\nRe: \"The dialog tasks considered in this work have a maximum of 100 candidate utterances, whereas in practice, the space of possible responses is much larger.\"\n\nThe tasks do really involve 100,000+ candidates (all utterances from the training set, see Table 1) but the evaluation metrics used in previous work involve using a subset of those per evaluated example, presumably because methods such as cross-encoder are too slow to evaluate otherwise. Poly-encoder can handle these sizes as evidenced in Table 5, which is of course the actual goal. Note the IR task evaluation did use 10,000 candidates also (Table 4), where the methods worked very well. This point seems to be more a criticism of standard evaluation practice, which we follow,  than our method.\n\n\nRe: \"I am skeptical about the practical value of the improvements shown in the paper (especially the improvements over bi-encoder, which is already a decent model).\"\n\nThe anonymity constraint of ICLR makes this response harder than it should be for us to reply to -- in fact, this approach has become our standard method going forward that we use in real situations, and hence we do emphasize it has strong practical value. The method is practical because it is elegant & simple, fast and gives great results, as evidenced by the evaluation metrics (Table 4) and inference speed (Table 5). For us, it\u2019s one of those papers where you do actually end up using the method, which definitely isn\u2019t every time!\n\n\nRe: \"One way to get around the inefficiency of the cross-encoder architecture is to first use an inexpensive scoring mechanism such as TFIDF or bi-encoder to identify a small number of promising candidates from all the possible candidates. We can then use the cross-encoder to do more precise scoring of only the promising candidates. I am curious where a pipelined model such as this compares against the variants discussed in the paper in terms of speed and performance.\"\n\nBuilding hybrids, pipelines and ensembles is often useful, but in this case looks tricky. For example, if we cut down the number of candidates from 100k to 1k with a bi-encoder, then switched to a cross-encoder, we would still need 20 seconds (on CPU) to rank with the cross-encoder (see Table 5). In this paper, we only compare single, related architectures against each other (bi, cross, poly). \n\n\nRe: \"Ideas similar to the model variants discussed in this work have been considered in other work (Eg: [1]).\"\n\nFirstly, our work actually predates that work (an earlier version of this submission was uploaded to a non-archival venue). In any case, their brief description of architectures is not completely clear to us in terms of overlap, but apparently what they tried did not work as they conclude \u201cThe significant gap between Full-Transformer and the other variants shows the importance of allowing fine-grained comparisons between the two inputs via the cross attention mechanism embedded in the Transformer\u201d. This is quite a different conclusion to ours, where we developed Poly-encoders which have almost the same performance as cross-encoders, but with huge speed-ups.\n\n\nRe: \"It is also known that in-domain pre-training (i.e, pre-training on data close to the downstream task\u2019s data distribution) helps (Eg: [2]). So this work can be considered as an application of existing ideas to dialog tasks.\" \n\nWe agree that it is long-known that multi-tasking similar tasks is more useful than dissimilar tasks, as cited in our paper, and indeed our work is another example of this. As we understand [2], which is also on a different topic as you say, doesn\u2019t actually compare two types of pre-training to show this helps though, it only compares \u201cusing no pre-training with pre-training on WikiText-103\u201d (and then fine-tunes on the data of interest).  We also note that WikiText 103 experiments are not on the same scale as ours -- our pre-training on Reddit is more than 100x larger and compares to modern BERT pre-training.  We believe our result is important because much recent work is ignoring related-domain pre-training and using BERT-based (and variant) models etc. and scaling to larger & larger data without considering this crucial point of related-domain pre-training.  Our work provides clear empirical results that this is important even at massive scale.  (We will however add this cite, thanks.)"}, "signatures": ["ICLR.cc/2020/Conference/Paper175/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper175/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxgnnNFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper175/Authors", "ICLR.cc/2020/Conference/Paper175/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper175/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper175/Reviewers", "ICLR.cc/2020/Conference/Paper175/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper175/Authors|ICLR.cc/2020/Conference/Paper175/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175243, "tmdate": 1576860551609, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper175/Authors", "ICLR.cc/2020/Conference/Paper175/Reviewers", "ICLR.cc/2020/Conference/Paper175/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper175/-/Official_Comment"}}}, {"id": "H1xLuTzbcr", "original": null, "number": 1, "cdate": 1572052333611, "ddate": null, "tcdate": 1572052333611, "tmdate": 1572972629384, "tddate": null, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "invitation": "ICLR.cc/2020/Conference/Paper175/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a new neural network architecture based on transformers called poly-encoders. These are compared against many state-of-the-art approaches including cross-encoders and bi-encoders on many large-scale datasets. \nBi-encoders > Poly-encoders > Cross-encoders in terms of speed and \nCross-encoders > Poly-encoders > Bi-encoders in terms of accuracy.\n\nI am not an expert in this area. However, to the best of my knowledge I don't see anything immediately wrong with this. The experiments are also comprehensive. Therefore I recommend acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666140637, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper175/Reviewers"], "noninvitees": [], "tcdate": 1570237755947, "tmdate": 1575666140649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper175/-/Official_Review"}}}, {"id": "HJeESBliqS", "original": null, "number": 2, "cdate": 1572697403815, "ddate": null, "tcdate": 1572697403815, "tmdate": 1572972629341, "tddate": null, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "invitation": "ICLR.cc/2020/Conference/Paper175/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes an approach for scoring sentences based on pre-trained transformers. The paper describes two main existing approaches for this task, namely bi-encoders and cross-encoders and then proposes a new formulation called poly-encoders which aims to sit between the existing approaches offering high accuracy -- similarly to cross-encoders -- and high efficiency -- similarly to bi-encoders. The paper is well written and although this is not related to my research I enjoying reading it. The approach proposed seems reasonable to me, and of sufficient novelty while the results presented are impressive. Moreover the paper seems a good fit for ICLR."}, "signatures": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper175/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring", "keywords": [], "pdf": "/pdf/7750aeca818ae9db25577bc1049cae42855a6614.pdf", "authors": ["Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston"], "authorids": ["samuelhumeau@fb.com", "kshuster@fb.com", "malachaux@fb.com", "jaseweston@gmail.com"], "paperhash": "humeau|polyencoders_architectures_and_pretraining_strategies_for_fast_and_accurate_multisentence_scoring", "_bibtex": "@inproceedings{\nHumeau2020Poly-encoders:,\ntitle={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},\nauthor={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxgnnNFvH}\n}", "original_pdf": "/attachment/3bd54f8cadd2192adcb1ef01ef018caa0054168e.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxgnnNFvH", "replyto": "SkxgnnNFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666140637, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper175/Reviewers"], "noninvitees": [], "tcdate": 1570237755947, "tmdate": 1575666140649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper175/-/Official_Review"}}}], "count": 6}