{"notes": [{"id": "jM76BCb6F9m", "original": "AkRyxe7G2dE", "number": 3083, "cdate": 1601308342008, "ddate": null, "tcdate": 1601308342008, "tmdate": 1615832132654, "tddate": null, "forum": "jM76BCb6F9m", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IFYC3PfxBqI", "original": null, "number": 1, "cdate": 1615606595380, "ddate": null, "tcdate": 1615606595380, "tmdate": 1615606595380, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Comment", "content": {"title": "Thanks for your work", "comment": "Hi, thanks for the wonderful work!\n\nI really like your paper's motivation: using learnable filterbanks initialized by the mel filterbanks.\nWe published a paper at ICASSP'20, **\"Data-driven Harmonic Filters for Audio Representation Learning\"** from the same motivation.\n\nWe did propose learnable filterbanks motivated by harmonicity of the audio domain and demonstrated the effectiveness on music-tagging, keyword spotting, and sound event detection benchmarks.\nCould you consider our work as one of the prior works of yours?\n\nAnyway, thanks for your nice paper!\n\n\nWon, Minz, et al. \"Data-driven harmonic filters for audio representation learning.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020."}, "signatures": ["~Sanghyuk_Chun1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Sanghyuk_Chun1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jM76BCb6F9m", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649465108, "tmdate": 1610649465108, "id": "ICLR.cc/2021/Conference/Paper3083/-/Comment"}}}, {"id": "Vo7DMfBMs7H", "original": null, "number": 1, "cdate": 1610040491740, "ddate": null, "tcdate": 1610040491740, "tmdate": 1610474097696, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All Reviewers agree that the paper has a clear and solid contribution. Furthermore, all of them highlight that the paper has improved significantly after revision. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.\n\nPros:\n- Comparison across network architectures.\n- Comparison across a broad range of different data sets.\n- Compactness of the representation (few parameters to learn).\n- Authors will share code.\n\nCons:\n- Role of L2 normalization could be further discussed."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491727, "tmdate": 1610474097681, "id": "ICLR.cc/2021/Conference/Paper3083/-/Decision"}}}, {"id": "NYlNefvv5X", "original": null, "number": 4, "cdate": 1604183389666, "ddate": null, "tcdate": 1604183389666, "tmdate": 1606789666031, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review", "content": {"title": "A simple but interesting frontend for audio classification tasks", "review": "The paper proposes a learnable frontend for classification tasks on audio signals. The proposed learnable audio frontend (LEAF) is a generalization of a mel filterbank, used commonly in machine audition.\nLEAF consists of a Gabor filterbank, magnitude-squared nonlinearity, Gaussian lowpass filter and previously-proposed per-channel energy normalization. Learnable parameters of LEAF are the center frequencies and bandwidths of Gabor filters, bandwith of lowpass Gaussian filters and smoothing coefficients of sPCEN. The proposed LEAF matches the performance of competing frontends in most of the cases and leads to some improvements in some cases.\nIn general, I like the paper and the proposed frontend is very sensible from perspective of audio-related applications.\nHowever, I believe there\u2019s some exaggeration in terms of impact of the proposed frontend based on the results in this paper. The proposed system is not actually challenging status quo, as many learnable frontends have been proposed in the literature in the past (as also listed in the references).\nThe paper is easy to read and authors communicate their contribution clearly.\nHowever, I believe that the title may be somewhat too general: LEAF is evaluated only on classification tasks, and IMHO that should be indicated in the title as well. There are other tasks, such as speech enhancement, where LEAF-like frontend may work well, but that is out of scope of this paper.\nExperimental results show that LEAF is performing well in the considered tasks. However, it would be interesting to understand the differences in performance when the encoder & head are changed and/or increased.\nMore specifically, the current results are obtained using a lightweight EfficientNet and linear heads. Is there any particular reason for this setup? Would the conclusions change with a different encoder/head?\nAlso, understanding of the influence of the filterbank setup (number of channels, window length, stride) would be beneficial.\n\nDetails:\n(1) Title should reflect the fact that LEAF has been evaluated only on classification tasks\n(2) Abstract: \u201cover a wide range of audio domains\u201d \u2014> It would be more appropriate to talk about a range of applications in audio domain.\n(3) Abstract: \u201cunprecedented\u201d -> I believe this is a bit exaggerated\n(4) Introduction: \u201cthis might not be the optimal approach for non-human sounds\u201d \u2014> This is a strange argument, and I believe the authors are confusing sound perception and sound production. Human sound perception works quite well for recognition on non-human sounds. The authors imply that a system which replicates a system mimicking human perception is suboptimal for non-human sounds. However, human auditory system is not designed for processing of only human-made sounds. Furthermore, optimality depends on the application, so stating that something is not optimal for a class of sounds makes no sense without the defined application, which in this case could be recognition of \u201cacoustic events or animal vocalizations\u201d\n(5) Introduction, last paragraph: \u201cwide and diverse range of tasks, including speech, music, audio events, and animal vocalizations\u201d \u2014> Signals, such as speech or music are not tasks. A task can be, e.g., speech recognition.\n(6) Conclusion: Stating there\u2019s a \u201chistorical statu quo of using hand-crafted mel-filterbanks\u201d with so many end-to-end systems giving the best performance in different applications is a bit too much.\n\n\n======= Review edit after authors' revisions ====== \n\nMost of my concerns have been resolved in the significantly-improved revised version of the paper.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082678, "tmdate": 1606915797916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3083/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review"}}}, {"id": "Q_5PI0NaIvZ", "original": null, "number": 2, "cdate": 1603905390621, "ddate": null, "tcdate": 1603905390621, "tmdate": 1606724503146, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review", "content": {"title": "Worthwhile investigation but lack of humility and truthfulness", "review": "This paper presents a new learnable representation fo audio signal classification and compares it to the classical mel-filterbanks representation and two other learnable representations on a broad range of audio classification tasks, from birdsongs to pitch, instrument, language or emotion recognition. The proposed representation combines several parameterized representation techniques from the recent litterature. It is reported to yield on par or better classification results than the other methods on several of these tasks using single- or multi-task learning.\n\nPros:\n- Learning an ultimate, universal, generic representation for all audio signals that renders the 80 years old mel-frequency scale obsolete is certainly an attractive goal\n- The proposed representation carefully and elegantly combines the best parts of several recently proposed parameterized representations and enjoys a nice interpretability while requiring few parameters to learn.\n- Comparing different audio representations on such a broad range of audio classification task is a welcome and unmatched effort, to the best of my knowledge.\n\nCons:\n- The paper lacks humility in its story-telling and its style. It employs formulations such as \"lived through the history of audio\", or \"challenging the historical statu quo\" when refereing to mel-frequency representations, although by the authors' own admition in the paper, a large amount of research effort has already been given in recent years towards learnable audio representations (the authors cite a dozen papers but there are more). Hence, this paper is not a first attempt. And despite the pompous use of \"universal\" in the title, I believe it is not a last attempt either. The authors claim that the proposed representation \"outperform mel-filterbanks over several tasks with a unique parametrization\" but this is far from clear when looking at the results carefully. In the majority of the tasks, the representation performs either slightly worse, equal, or about 0.5% better than Mel-filterbanks. It is not clear whether such improvement is significant, since no error bar or standard deviation is provided in the results (a sadly common habit in the audio litterature). The only tasks where a truly significant improvement is reported are language identification and emotion recognition, which are also the tasks where all the methods perform the poorest. It looks like any significant difference between the 4 compared approaches would vanish if these two tasks were omitted. The reason why the proposed representation performs well on these two very specific tasks is not clear and not discussed.\n- More generally, the paper would be much more valuable if it gave a sense of WHAT is actually learned by the proposed method. Is the final representation significantly different from a mel-filterbank? Given how close to mel most reported results are, this is doubtful. In fact, Fig. A.1. strongly suggests that LEAF just re-learned mel, but strangely this figure is never commented. Some comments on the learned compression-parameters would also be appreciated.\n- At least one important comparison point is clearly missing in the reported results: STFT + PCEN or mel-filterbanks + PCEN, e.g., Wang et al. (2017) or Schl\u00fcter & Lehner (2018) [note that the latter already uses sPCEN rather than PCEN, contrary to the authors' claim] . Omitting this from the comparisons prevents one from knowing whether the proposed parameterized Gabor filterbank brings any advantage over another time-frequency representation like STFT or mel-filterbanks. Less critically, another missing comparison point is LEAF + CNN14, in Table 4.\n- What the authors refer to as \"audio\" in the title and throughout the paper is in fact much more narrow, namely \"audio classification\". Learnable audio representations have been studied in a broader context in recent years, e.g., speech enhancement, source separation, dereverberation, sound localization or audio (re-)synthesis. In fact, one of the important breakthroughs recently brought by learnable audio frontends was in source separation with the paper TasNet (Luo et al. 2018) which is not cited by the authors. In the same context, (Ditter and Gerkmann 2020) presented a learnable gammatone-like filterbank and showed that fully-parameterized learned filterbanks tended to have logarithmic spread in frequencies. Moreover, the use of learnable analytical filterbanks/Hilbert pairs due to their envelop extraction/shift invariant properties was already discussed in depth in (Pariente et al. 2019) [cited in the paper]. \n\nOverall, while comparing different learnable audio representations on a broad range of audio classification tasks is a timely and worthwhile topic, and while the proposed representation elegantly combines several recent ideas in this area, the general presentation and angle of the paper strongly lacks humility. Instead of the proposed title, something like \"Benchmark of learnable audio representations on a broad range of classification tasks\" would be more truthful to the work. To make the investigation more worthwhile and insightful, additional comparison points (STFT + PCEN, mel-frequency + PCEN, Gabor + log, etc.) as well as an analysis of what the model has actually learned would be needed.\n\n======= Review edit after authors' revisions ======\nThe changes made by the authors in the title, abstract, introduction and conclusion to narrow the scope of the paper, better contextualize it, and make it more humble and truthful are very welcome. The extra experiments, figures, addition of error bars and new statistical tests are also a real plus. In doing so, the authors addressed all of my major concerns.\n\nFor these reasons, changed my evaluation score from 5 to 8.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082678, "tmdate": 1606915797916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3083/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review"}}}, {"id": "HpOvseLs8gH", "original": null, "number": 2, "cdate": 1605638177534, "ddate": null, "tcdate": 1605638177534, "tmdate": 1605692758772, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "NYlNefvv5X", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for their feedback.\n\n====\u201dThe proposed system is not actually challenging status quo, as many learnable frontends have been proposed in the literature in the past (as also listed in the references).\u201d\n\nWe acknowledge in the introduction that this problem has been addressed many times in the recent years. However, despite this huge corpus of work on replacing mel-filterbanks, we observe that state-of-the-art systems for audio classification, ASR or speaker recognition (reference added in the introduction) still rely on mel-filterbanks. This shows that learnable frontends have not been widely adopted for discriminative tasks, while they have become standard for tasks such as speech separation (references added in the introduction as well). The motivation for our work is both to benchmark previously proposed methods across many audio classification tasks, as well as proposing a convincing alternative, that we hope will contribute to the adoption of learnable frontends as an alternative to mel-filterbanks.\n\n====\u201dHowever, I believe that the title may be somewhat too general: LEAF is evaluated only on classification tasks, and IMHO that should be indicated in the title as well.\u201d\n\nWe revised the title, abstract and introduction of the paper to more precisely frame the scope of the paper, which focuses on audio classification as correctly pointed out by the reviewer.\n\n====\u201dHowever, it would be interesting to understand the differences in performance when the encoder & head are changed and/or increased.\u201d\n\nIn our terminology, a \u201chead\u201d is solely the last classification layer (which is different per-task in the multi-task setting). Any additional layer is part of the \u201cencoder\u201d. We acknowledge that in the multi-task setting one could increase the depth of the \u201cper-task\u201d network section from one layer to several, but to reduce the hyperparameter search to a reasonable space we relied on one head. However, we agree that showing results with different encoders is important as mel-filterbanks work well across a broad range of architectures. As a consequence, we added results with the state-of-the-art CNN14 on AudioSet in Table 4. Despite the significant change in architecture scale, we still observe improvements from using LEAF over mel-filterbanks and other learnable frontends.\n\n====\u201dAlso, understanding of the influence of the filterbank setup (number of channels, window length, stride) would be beneficial.\u201d\n\nIn our single and multi-task experiments we use the standard number of 40 filters, however when training on AudioSet we use 64 channels instead of 40 as Kong et al. found improvements from using 64 filters (added in the paper). To reduce the space of models to explore we stick to the standard 25ms window with 10ms hop size, the most commonly used parameters in the literature. However, as described in the conclusion, in future work we want to address the limitation of choosing a fixed window size and stride, shared among all filters regardless of their frequency bandwidth, which comes from casting the Gabor filtering as a 1-D convolution layer.\n\n===\u201dHuman sound perception works quite well for recognition on non-human sounds. The authors imply that a system which replicates a system mimicking human perception is suboptimal for non-human sounds. However, human auditory system is not designed for processing of only human-made sounds.\u201c\n\nThanks for this remark, we agree that our statement was imprecise. Indeed, the human auditory system is not designed for processing human-made sounds only, and the interaction between perception and production are out of the scope of this paper. We also agree that \u201coptimality\u201d depends on the task, and that for a single class of signal (e.g. speech) the best bank of filters will likely vary with the task (e.g. ASR, speaker identification or paralinguistic classification). We edited this part.\n\n====\"(2) Abstract: \u201cover a wide range of audio domains\u201d [...] (3) Abstract: \u201cunprecedented\u201d -> I believe this is a bit exaggerated\"\n\nWe reformulated the abstract accordingly.\n\n====\u201d(5) [...] Signals, such as speech or music are not tasks. A task can be, e.g., speech recognition.\u201d\n\nThis terminology was indeed imprecise, we corrected it by mentioning \u201cdiverse range of audio signals\u201d.\n\n====\u201dConclusion: Stating there\u2019s a \u201chistorical statu quo of using hand-crafted mel-filterbanks\u201d with so many end-to-end systems giving the best performance in different applications is a bit too much\u201d\n\nAs described in the introduction, most \u201cend-to-end\u201d systems for audio classification or ASR still use handcrafted features, and learnable frontends are yet to be integrated into state-of-the-art systems for these tasks. This is unlike tasks such as speech separation for which training end-to-end systems from the waveform has become the standard. We revised the title, scope and claims of the paper, as well as added references to show the successful application of learnable frontends to speech separation.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "V0A-RVtS77T", "original": null, "number": 7, "cdate": 1605638975169, "ddate": null, "tcdate": 1605638975169, "tmdate": 1605638975169, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to all reviewers", "comment": "We thank the reviewers for their feedback and suggestions. We found them very useful to improve the paper and support our claims more convincingly. We summarize here the main changes applied to the paper and replied to all individual comments below.\n \n* We revised the title, abstract, introduction and conclusion of the paper to more precisely frame its scope, which focuses on audio classification as correctly pointed out by AnonReviewer1 and AnonReviewer3.\n* We revised Table 1 and Table 3, reporting the values of the confidence intervals to reflect the uncertainty due to the limited sample size in each evaluation dataset, as pointed by AnonReviewer1 and AnonReviewer2. We also performed a statistical bootstrap analysis to measure uncertainty with respect to the choice of datasets. These analyses confirm the advantage of LEAF w.r.t. Mel-filterbanks, Time-Domain filterbanks and SincNet. \n* We added experiments on AudioSet with a state-of-the-art \u201cCNN14\u201d encoder, to address the concern of all reviewers regarding the use of a single encoder. This architecture allows us to match the current state-of-the-art on this dataset when using the LEAF frontend. We will include our reimplementation of CNN14 in the open-source release of LEAF, along with our implementations of Time-Domain Filterbanks and SincNet.\n* While the Mel+PCEN baseline was already included in our single task experiments over several datasets, we also included it in our experiments on AudioSet, for both encoders, thanks to the suggestion of AnonReviewer1. On this dataset, the performance is identical to Mel+Log.\n* To address the concern of AnonReviewer2 we added experiments on speech commands with additive Gaussian noise at varying Signal-to-Noise ratio. These experiments, reported in Section 4.5 and Figure A.6 show the robustness of LEAF w.r.t. mel-filterbanks, both when using a log-compression and PCEN.\n* We added an analysis of the learned filters, Gaussian lowpass and PCEN parameters in Section 4.4 and Figures A3-5, as suggested by AnonReviewer1.\n\nOverall, we did our best to address thoroughly all main concerns raised by the reviewers and hope these changes will be taken in consideration."}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "eKNe3ONfrEx", "original": null, "number": 6, "cdate": 1605638689260, "ddate": null, "tcdate": 1605638689260, "tmdate": 1605638689260, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "vgQYI2SlOS9", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for their feedback.\n\n====\u201dWhile the authors evaluate the proposed front-end on several tasks, the evaluation protocols are missing. Especially, the training partition and test partition and number of segments in each class.\u201d\n\nWe included additional details about the evaluation protocol in the revised manuscript. Table A.1 now reports the number of train/test examples and the number of distinct classes for each dataset. Note that we used the default train/test partitions that are specified by each dataset. \n\n====\u201dFor example, did they calculate accuracy using balanced classes?\u201d\n\nNo class rebalancing was performed, neither during training, nor during evaluation. A note has been inserted in the text.\n\n====\u201dWas classification done at the frame level or the utterance level?\u201d\n\nThis is already specified in the last paragraph of Section 4:\n\u201cTo address the variable length of the input sequences, we train on randomly sampled 1 second windows\u2026.During evaluation, we consider the full-length sequences, splitting them into consecutive non-overlapping 1 second windows and averaging the output logits over windows\u201d\n\n====\u201dThe authors evaluate LEAF on several tasks however the model architecture for classification models remains constant. Typically Mel FBs are utilized across architectures and one of the major shortcomings of the proposed work is the integration of LEAF with state-of-the-art models (for example x-vectors for SID). This would give more understanding about the generalization capability of the LEAF.\u201d\n\nWe repeated our experiments with different model architectures (EfficientNetB0 and CNN14) to demonstrate that the properties of the learnable frontend do not depend on the specific architecture used to process the computed audio features. Table 4 reports these new results. \nIn this paper we focused on audio classification tasks and we revised the title, introduction  and conclusion accordingly. At the same time, we acknowledge that LEAF can be potentially useful in other contexts (e.g., x-vector for SID) and we will definitely explore this interesting direction in our future work. Moreover, we will release an open-source implementation of LEAF, along with our implementation of CNN14, TD-fbanks and SincNet, to foster application of our model to other tasks.\n\n====\u201dAs pointed out by authors in the introduction, Mel FBs are invariant to deformation and noise-robust. All the audio problems addressed in this work encounter varying levels of noise in the real-world. It is extremely important to assess the noise robustness of the proposed front-end. I would recommend the authors test LEAF with different levels of noise.\u201d\n\nWe definitely acknowledge that the robustness to noise is an important aspect when deploying models in the real world. When evaluating LEAF, we were faced with two options: a) artificially add noise to clean input samples; b) use a wide variety of datasets, some of which also include different forms of noise. A shortcoming of option a) is that it requires to arbitrarily define the noise distribution along multiple axes (additive/convolutional, stationary/non-stationary, etc.). In our original submission we opted for b), including in our evaluation set datasets like Acoustic scenes, Speech commands, Birdsong detection and especially Audioset, which do contain noise sampled from a real distribution.\nIn addition, in the revised paper, we have now added a further experiment on the SpeechCommands dataset, with additive Gaussian noise. These experiments show that LEAF is at least as robust as mel-filterbanks when using PCEN, and significantly more robust when using logarithmic compression (see Figure A.1).\n\n====\u201dFor results in Table 1 the statistical significance should be computed. For the acoustic scene task, the difference in performance is very less and this dataset only has 810 samples overall for testing. Therefore, without confidence intervals, it is very hard to conclude the performance of LEAF.\u201d\n\nWe amended Table 1 and Table 3, reporting next to each accuracy value the confidence interval capturing the limited sample size. \nWhile for some of the datasets other frontends might outperform LEAF (e.g., TD-Fbanks for Acoustic Scenes or SincNet for SpeakerID), we are interested in the average performance across a diverse set of datasets. To evaluate the robustness of our results to the specific choice of the datasets, we applied the statistical bootstrap, resampling with replacement a set of K = 8 datasets, and computing the non-parametric distribution of DeltaAccuracy = Accuracy_LEAF - Accuracy_X (where X in {Mel, TD-FBanks, SincNet}). We tested the null hypothesis that the mean of DeltaAccuracy is zero, and measured a p-value equal to, respectively <1e-5, <1e-5 and 0.059, thus demonstrating the statistical significance of the LEAF outperformance. The corresponding bootstrap distribution is illustrated in Appendix."}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "QnyWGuG7dFe", "original": null, "number": 5, "cdate": 1605638525283, "ddate": null, "tcdate": 1605638525283, "tmdate": 1605638525283, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "S24VG38uxSN", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 2)", "comment": "====\u201dAt least one important comparison point is clearly missing in the reported results: STFT + PCEN or mel-filterbanks + PCEN, e.g., Wang et al. (2017) or Schl\u00fcter & Lehner (2018) [note that the latter already uses sPCEN rather than PCEN, contrary to the authors' claim] . Omitting this from the comparisons prevents one from knowing whether the proposed parameterized Gabor filterbank brings any advantage over another time-frequency representation like STFT or mel-filterbanks. Less critically, another missing comparison point is LEAF + CNN14, in Table 4.\u201d\n\nTable 2 already shows an ablation study of the compression function in the single task setting, including mel-filterbanks + PCEN. We furthermore added mel-filterbanks + PCEN in our AudioSet experiments (Table 4). Moreover, we reimplemented CNN14 and added the results to Table 4, showing that LEAF still maintains an advantage over learnable frontends and mel-filterbanks.\n\n====\u201d[note that the latter already uses sPCEN rather than PCEN, contrary to the authors' claim]\u201d\n\nThanks for pointing out this mistake, we corrected our reference to this work.\n\n====\u201dWhat the authors refer to as \"audio\" in the title and throughout the paper is in fact much more narrow, namely \"audio classification\". Learnable audio representations have been studied in a broader context in recent years, e.g., speech enhancement, source separation, dereverberation, sound localization or audio (re-)synthesis. In fact, one of the important breakthroughs recently brought by learnable audio frontends was in source separation with the paper TasNet (Luo et al. 2018) which is not cited by the authors. In the same context, (Ditter and Gerkmann 2020) presented a learnable gammatone-like filterbank and showed that fully-parameterized learned filterbanks tended to have logarithmic spread in frequencies. Moreover, the use of learnable analytical filterbanks/Hilbert pairs due to their envelop extraction/shift invariant properties was already discussed in depth in (Pariente et al. 2019) [cited in the paper].\u201d\n\nWe agree that our contribution is narrower than what the title suggests.  We acknowledge that the state-of-the-art in source separation has used neural networks on the raw waveform rather than spectrogram masking for several years (e.g. TasNet, DPRNN). We added these references in the introduction. In this work we are interested in alternatives to mel-filterbanks in tasks where they are still used in state-of-the-art systems, namely audio classification. We modified the title and the description of our contribution accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "S24VG38uxSN", "original": null, "number": 4, "cdate": 1605638442273, "ddate": null, "tcdate": 1605638442273, "tmdate": 1605638442273, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "Q_5PI0NaIvZ", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 1)", "comment": "We thank the reviewer for their feedback. We split our response in two comments due to the 5000 characters limitation.\n\n====\u201dThe paper embarassingly lacks humility in its story-telling and its style. It employs formulations such as \"lived through the history of audio\", or \"challenging the historical statu quo\" when refereing to mel-frequency representations, although by the authors' own admition in the paper, a large amount of research effort has already been given in recent years towards learnable audio representations (the authors cite a dozen papers but there are more). Hence, this paper is not a first attempt. And despite the pompous use of \"universal\" in the title, I believe it is not a last attempt either.\u201d\n\nOur work is motivated by the observation that despite the large corpus of work on learnable frontends, of which we cite a significant part, state-of-the-art classification models still rely on mel-filterbanks. We do acknowledge this vast amount of prior work in the introduction and related work, and we build on top of these previous approaches to propose an alternative. This won\u2019t indeed be a last attempt at replacing mel-filterbanks, however we believe that the broad range of tasks we consider, our focus on combining and improving the best components of previous work, as well as open-sourcing all our models makes this contribution valuable to the audio classification community. We revised the title, abstract, introduction and conclusion of the paper to more precisely frame the scope of the paper, which focuses on audio classification as correctly pointed out by the reviewer, as well as reformulated our claims.\n\n====\u201dThe authors claim that the proposed representation \"outperform mel-filterbanks over several tasks with a unique parametrization\" but this is far from clear when looking at the results carefully. In the majority of the tasks, the representation performs either slightly worse, equal, or about 0.5% better than Mel-filterbanks. It is not clear whether such improvement is significant, since no error bar or standard deviation is provided in the results (a sadly common habit in the audio litterature). The only tasks where a truly significant improvement is reported are language identification and emotion recognition, which are also the tasks where all the methods perform the poorest. It looks like any significant difference between the 4 compared approaches would vanish if these two tasks were omitted. The reason why the proposed representation performs well on these two very specific tasks is not clear and not discussed.\u201d\n\nWe revised Table 1 and Table 3, reporting the values of the confidence intervals to reflect the uncertainty due to the limited sample size in each evaluation dataset. \nWhile for some of the datasets other frontends might outperform LEAF (e.g., TD-Fbanks for Acoustic Scenes or SincNet for SpeakerID), we are interested in the average performance across a diverse set of datasets. To evaluate the robustness of our results to the specific choice of the datasets, we applied the statistical bootstrap, resampling with replacement a set of K = 8 datasets, and computing the non-parametric distribution of DeltaAccuracy = Accuracy_LEAF - Accuracy_X (where X in {Mel, TD-FBanks, SincNet}). We tested the null hypothesis that the mean of DeltaAccuracy is zero, and measured a p-value equal to, respectively <1e-5, <1e-5 and 0.056 for single-task classification and 0.048, <1e-5 and <1e-5 for multi-task classification, thus demonstrating the statistical significance of the LEAF outperformance.\n\n===\u201dMore generally, the paper would be much more valuable if it gave a sense of WHAT is actually learned by the proposed method. Is the final representation significantly different from a mel-filterbank? Given how close to mel most reported results are, this is doubtful. In fact, Fig. A.1. strongly suggests that LEAF just re-learned mel, but strangely this figure is never commented. Some comments on the learned compression-parameters would also be appreciated.\u201d\n\nWe added a Section 4.4 of analysis which comments this figure as well as adds visualizations and analysis of learned Gaussian lowpass filters and PCEN parameters, thanks to the additional space allowed at this stage. As correctly observed by the reviewer, at a high level, these filters do not deviate much from their mel-scale initialization. On the one hand, this indicates that the mel-scale is a strong initialization, a result consistent with previous work (references added in the paper). On the other hand, there are differences at both ends of the range, with LEAF covering a wider range of frequencies. For example, the lowest frequency filter is centered around 60Hz, as opposed to 100Hz for mel-filterbanks. We believe that is one of the reasons that explain the out-performance of LEAF on Audioset, as it focuses on a more appropriate frequency range to represent the underlying audio events.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "Pmp3tl3dcej", "original": null, "number": 3, "cdate": 1605638255290, "ddate": null, "tcdate": 1605638255290, "tmdate": 1605638255290, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "AR3JT8RiR71", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for their feedback.\n\n====\u201dIn page 4, the authors mentioned that the l2 normalization helps distinguishing the role of filtering and compression. I think this contribution is not trivial, so if the authors can add more experiments (or plots) to show the difference between models with and without l2 normalization, then it would be helpful.\u201d\n\nWhen the l2 normalization of the filters is disabled, we observed that the filters tend to have heterogeneous gains. This is problematic, especially when a non-linear compression operation is applied to the output of filtering (e.g., via PCEN), since the local slope and curvature of the nonlinearity depend on the scale of its input. Hence, we argue that normalization should always be applied. Also, note that all our results use Gabor 1D convolution kernels (as it worked consistently better in our early experiments), which are normalized by construction. The impact of the compression layer is illustrated in Table 2.\n\n====\u201dThe backbone model used in the paper is fixed, and showing that the proposed audio front-ends shows similar trends with multiple backend models can verify better generalization ability of the proposed approach. So, if the authors can add additional experiments with multiple backends, then it would be helpful.\u201d\n\nIn the revised version of our paper we experiment with different architectures to verify the generalization properties of LEAF. Namely, we use the CNN14 backbone network recently proposed in [Kong et al., 2019] as it achieves state-of-the-art results on Audioset. Our additional results reported in an updated Table 4 show that the trend is similar regardless of the backend encoder."}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jM76BCb6F9m", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3083/Authors|ICLR.cc/2021/Conference/Paper3083/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841332, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Comment"}}}, {"id": "vgQYI2SlOS9", "original": null, "number": 1, "cdate": 1603861289140, "ddate": null, "tcdate": 1603861289140, "tmdate": 1605024072071, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review", "content": {"title": "Review for A Universal Learnable Audio Frontend", "review": "In this work, the authors introduce a learnable front-end (LEAF) for audio. The paper evaluates it on several tasks in the audio domain such as acoustic event classification, speaker identification, keyword spotting, language identification, music classification etc. The study reports that proposed features outperform or perform similar to Mel filterbanks for many tasks. This paper is well written and easy to follow. The references are excellent as well. I have summarized my comments below which will help in improving the quality of this manuscript: \n\n1. While the authors evaluate the proposed front-end on several tasks, the evaluation protocols are missing. Especially, the training partition and test partition and number of segments in each class. For example, did they calculate accuracy using balanced classes? Was classification done at the frame level or the utterance level? I would highly recommend authors to add the experiment protocol to the manuscript as Table 1A does not provide enough information. \n\n2. The authors evaluate LEAF on several tasks however the model architecture for classification models remains constant. Typically Mel FBs are utilized across architectures and one of the major shortcomings of the proposed work is the integration of LEAF with state-of-the-art models (for example x-vectors for SID). This would give more understanding about the generalization capability of the LEAF.\n\n3. As pointed out by authors in the introduction, Mel FBs are invariant to deformation and noise-robust. All the audio problems addressed in this work encounter varying levels of noise in the real-world. It is extremely important to assess the noise robustness of the proposed front-end. I would recommend the authors test LEAF with different levels of noise.\n\n4. For results in Table 1 the statistical significance should be computed. For the acoustic scene task, the difference in performance is very less and this dataset only has 810 samples overall for testing. Therefore, without confidence intervals, it is very hard to conclude the performance of LEAF. \n\nMinor comments:\n1. Please add this missing reference: \nDavis, S. and Mermelstein, P., 1980. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE transactions on acoustics, speech, and signal processing, 28(4), pp.357-366.\n\n2. In Table 1, Mel and LEAF has the same accuracy for Music instrument and needs to be bolded. \n\n3. mel ---> Mel \n\n4. In Fig 2, you may improve the font size of x-axis labels. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082678, "tmdate": 1606915797916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3083/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review"}}}, {"id": "AR3JT8RiR71", "original": null, "number": 3, "cdate": 1603959448137, "ddate": null, "tcdate": 1603959448137, "tmdate": 1605024071872, "tddate": null, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "invitation": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review", "content": {"title": "The paper well connects the relationship between hand-crafted audio frontends (mel-spectrogram) with learnable frontends.", "review": "The paper shows a detailed interpretation on the relationship between each component of hand-crafted audio front-ends (such as mel-spectrograms) and learnable counterparts. To do that, they followed the narratives presented from the previous works such as SincNet and improved the model by changing the several components of it. The authors grouped the audio front-ends into mainly three parts which are filtering, pooling, and compression. And, the contributions were made at each stage. For filtering stage, instead of learning all the parameters of the convolution layer, they let the model to learn only center frequency and bandwidth of the filterbanks that are initially assigned with Gabor filters. For pooling stage, instead of using simple average or max poolings, they let the model to learn low pass filtering with small parameters. For compression, instead of using log based dynamic compression, they extended Per-Channel Energy Normalization by replacing a fixed smoothing factor to learnable parameters and named it to sPCEN.\n\nTo evaluate the proposed approach, they evaluated the models on 8 audio classification tasks which might have diverse audio and label characteristics (such as acoustic scene sound, animal sound, music, speech). The compared models are mainly mel-spectrogram and SincNet. The results shows that the proposed model outperforms the comparisons for most tasks. Then, they further ran a multi-task classification experiment to obtain universal audio front-ends. And, the results show the proposed learnable front-ends is showing some generalization ability on most tasks. Finally, they evaluated the proposed model on large-scale audio classification dataset (AudioSet) and verified that the proposed front-ends is also showing the good performance on it.\n\nIn page 4, the authors mentioned that the l2 normalization helps distinguishing the role of filtering and compression. I think this contribution is not trivial, so if the authors can add more experiments (or plots) to show the difference between models with and without l2 normalization, then it would be helpful.\n\nThe backbone model used in the paper is fixed, and showing that the proposed audio front-ends shows similar trends with multiple backend models can verify better generalization ability of the proposed approach. So, if the authors can add additional experiments with multiple backends, then it would be helpful.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3083/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3083/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEAF: A Learnable Frontend for Audio Classification", "authorids": ["~Neil_Zeghidour1", "~Olivier_Teboul2", "~F\u00e9lix_de_Chaumont_Quitry1", "~Marco_Tagliasacchi3"], "authors": ["Neil Zeghidour", "Olivier Teboul", "F\u00e9lix de Chaumont Quitry", "Marco Tagliasacchi"], "keywords": ["audio understanding", "frontend", "learnable", "mel-filterbanks", "time-frequency representations", "sound classification"], "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.", "one-sentence_summary": "We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zeghidour|leaf_a_learnable_frontend_for_audio_classification", "pdf": "/pdf/426d58043e09ff47db27ab72f40e8db575a46f7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzeghidour2021leaf,\ntitle={{\\{}LEAF{\\}}: A Learnable Frontend for Audio Classification},\nauthor={Neil Zeghidour and Olivier Teboul and F{\\'e}lix de Chaumont Quitry and Marco Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jM76BCb6F9m}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jM76BCb6F9m", "replyto": "jM76BCb6F9m", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082678, "tmdate": 1606915797916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3083/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3083/-/Official_Review"}}}], "count": 13}