{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396373007, "tcdate": 1486396373007, "number": 1, "id": "HkpAoGIdg", "invitation": "ICLR.cc/2017/conference/-/paper128/acceptance", "forum": "Sk2iistgg", "replyto": "Sk2iistgg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396373511, "id": "ICLR.cc/2017/conference/-/paper128/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sk2iistgg", "replyto": "Sk2iistgg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396373511}}}, {"tddate": null, "tmdate": 1483483096417, "tcdate": 1483482855076, "number": 3, "id": "B1yevitBg", "invitation": "ICLR.cc/2017/conference/-/paper128/official/review", "forum": "Sk2iistgg", "replyto": "Sk2iistgg", "signatures": ["ICLR.cc/2017/conference/paper128/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper128/AnonReviewer4"], "content": {"title": "Lacking in several aspects; limited novelty", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. \n\nThe clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. \n\nThe motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as $\\tau$ is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? \n\nThe proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1). \n\n\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483482855715, "id": "ICLR.cc/2017/conference/-/paper128/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper128/AnonReviewer3", "ICLR.cc/2017/conference/paper128/AnonReviewer1", "ICLR.cc/2017/conference/paper128/AnonReviewer4"], "reply": {"forum": "Sk2iistgg", "replyto": "Sk2iistgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483482855715}}}, {"tddate": null, "tmdate": 1483435612751, "tcdate": 1483419155921, "number": 2, "id": "SkhfCsuBg", "invitation": "ICLR.cc/2017/conference/-/paper128/public/comment", "forum": "Sk2iistgg", "replyto": "ryNp9bzre", "signatures": ["~Ravi_Garg1"], "readers": ["everyone"], "writers": ["~Ravi_Garg1"], "content": {"title": "reply to AnonR1", "comment": "Firstly, we want to clarify that  the aim of the paper is to solve an ill-posed inverse problem using low dimensionality of the inferred solution as a prior. We would like for the benefit of the reviewer provide STANDARD definition of: Inverse problem and causal factors while explaining the utility of dimensionality reduction in the context of solving an ill-posed inverse problem. \n\nAn inverse problem in science is the process of calculating from a set of observations the causal factors that produced them: for example in figure 1 and section 4.2 of the paper, estimating the 3D structure (S_i) at every time instance (i) of human performing an action, which when filmed with a camera, leads to a set of 2D projections (W_i) onto the image which we observe. This type of problems are named inverse problem because we start with the results (2D image measurements) and then calculate the causes (3D structure S_i and camera locations R_i at any given time).  \nThe inverse problem we are interested in are ill-posed in nature. I.e. the 2D projection of some 3D structure albit is a well defined physical process (forward problem), we lose depth information while projecting the 3D point on the image plane. Thus, in this case starting from the observations, it is impossible to recover the causal factor without any further assumption. I.e. f(W,S) =0 is ill-posed for estimating S.\nIn this work our assumption is that the causal factor (S_i) lie on a unknown nonlinear manifold expressed by a mercer kernel. Now we want to simultaneously: (i) solve the inverse problem to estimate the $S_i$\u2019s which explain $W_i$\u2019s and (ii) estimate a manifold $C$ on (or near) which all these causal factors lie. \n\nGiven the manifold is known, part (i) in the literature is called pre-image estimation. I.e. find the data point whose mapping in the feature space is known on a given subspace of the feature space.\nPart (ii) Estimate the manifold on which all $S_i$\u2019s lie. This part is non-linear dimensionality reduction for which KPCA /LVMs can be used if $S_i$\u2019s were known. \nWe have to solve both (i) and (ii) to achieve the goal we have set for ourselves. So it is no accident that we have to discuss KPCA, other non-linear dimensionality reduction (e.g. LVMs) and use of non-linear dimensionality reduction for solving ill-posed inference problems.\n\nEquipped with above explanation now we can answer the quarries of reviewer more precisely: \n\n\n\u201cwhy one is interested in step (iii) outlined on page 2\u201d\nAs out goal in NRSfM is to find out what are the 3D shapes which lead to 2D projections we have observed, merely finding the projection of $\\Phi(S_i)$ onto a manifold $C_i$ is almost useless for NRSfM (or for matrix completion or any other reconstruction task in general).  We have to find out the 3D shape (pre-image) which can be projected to the manifold and explains the observation $W_i$. This operation is not as straight forward in the case of non-linear manifolds as it is for linear subspace.\n\n\u201ca data point (pre-image) corresponding to each projection in the input space\u201d is not a standard step in KPCA. \u201c\nThis is not a disadvantage of the KPCA. It is the disadvantage of  a naive pre-image estimation framework which starts with an initial point ($S_i$) in the data space (which we do not have) , project its non linear transformation \\phi(S) onto the manifold $C^T$ and then try to reconstruct the \\bar(S) which can be projected at the estimated location in the feature space. The issue is that the mapping from data space to feature space is not one to one and onto.  \n\n\u201c$\\mathcal{X} \\times N$, $\\mathcal{Y} \\times N$, $\\mathcal{H} \\times N$\u201d\nSorry for the confusing notations, these are simply 3 matrices (of dim X by N/ Y by N etc..)  formed by stacking N tuples related to N observations.\n\n\u201cExperiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered \u201c\nThe goal of the Matrix completion experiment on oil flow dataset was to mainly compare the robust non-linear dimensionality reduction methods which are equipped to deal with missing data with our energy minimization approach minimizing the dimensionality of the manifold. SVT and OptSpace to our understanding targets matrix completion problem under the assumption that the columns of the completed matrix lie on a linear subspace of low-rank. As oil flow data can be modeled better with a low dimensional nonlinear manifold (it is inherently 2D), extensively evaluating all linear dimensionality reduction methods on this data was considered less important. As linear low rank assumption is suboptimal here. However will shortly include the numbers with optspace and SVT for reference.\n\n\u201cExperiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.\u201d  \nWe have included state of the art NRSfM method of Dai et.al. CVPR2012 /Garg et.al. CVPR2013 (with modifications to handle missing 2D measurement we call it TNH) as a very strong baseline for NRSfM. In the supplementary material, this method is compared favourably on full CMU Mocap against other NRSfM solutions for which the code is publically available. These additional comparisons were left in supplementary material because of the lack of space. \n     \n\n\"The authors do not address the out-of-sample problem\"/  \"KPCA is trained via a closed-form update, but there is still training (R3)\":\n\nThese two quarries stem from the misunderstanding of the problem 1 to be dimensionality reduction alone. In this paper we are NOT trying to learn a non-linear manifold using a representative training-set (noisy or clean) which can be used to infer/reconstruct an out-of-sample instance (to check the generalization capability of learned manifold). With the exception of experiment in appendix A where we validate our close form solution, all experiments aims to only infer the instances on/near a manifold (not know apriori) for which the observations are given. For example, we do not suggest that a manifold which explain the 3D shapes for a given set of 2D observations, say a human doing push-ups will explain the 3D structures of a human running or vice-versa. Each such instance of inverse problem in our framework is solved separately and the estimated manifolds do not generalize for out of sample problem. More importantly, we have NO 3D shapes available to train our manifold but the low-dimensionality constraints are used (and are critical) to infer 3D shapes purely from the 2D observations. \n\n \u201cThe problem reformulation of Kernel PCA uses somewhat standard tricks\u201d\nWe agree that assuming gaussian noise in the feature space and doing L1 approximation of L0 (trace norm relaxation of rank) are standard tricks. However, it is important to reiterate here that designing an unified energy minimization framework to penalize non-linear dimensionality, which can deal with implicitness of feature mapping (it is non-trivial to map from feature space to input space) and non-trackability of feature space (which is infinite dimensional for example with RBF kernel) while solving a highly ill-posed inverse problem is non-trivial and to our knowledge is not done before.\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716306, "id": "ICLR.cc/2017/conference/-/paper128/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2iistgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper128/reviewers", "ICLR.cc/2017/conference/paper128/areachairs"], "cdate": 1485287716306}}}, {"tddate": null, "tmdate": 1483434329168, "tcdate": 1483406889989, "number": 1, "id": "BJMN0OOBg", "invitation": "ICLR.cc/2017/conference/-/paper128/public/comment", "forum": "Sk2iistgg", "replyto": "rJXJeHYEg", "signatures": ["~Ravi_Garg1"], "readers": ["everyone"], "writers": ["~Ravi_Garg1"], "content": {"title": "response to AnonReviewer3", "comment": "\nWe thank the reviewer for his very valuable inputs.\n\nFirst, We would like to clarify a crucial aspect of the proposed framework which differs from KPCA and GP-LVMs leading to some misinterpretations in the review. As explained in the motivation and introduction the goal of this work is not to merely propose a robust KPCA framework (although it is a part of the work) but to develop a dimensionality regularizer for ill-posed inference. Our aim is to penalize the non-linear dimensionality of the causal factors associated with an ill-posed (under-constrained) inverse problem which we assume lie on a nonlinear manifold. \n\nThe main difference between our work to the counterpart which we call \"pre-training\" followed by constrained inference (with iterative pre-image estimation for example) on manifold is explained clearly in para 3 of the introduction. \n\nLet us explain with the example problem of non-rigid structure from motion again what we mean by a \"pre-training\". In the NRSfM, cost function f(W,Z,R,S) relates the causal factors i.e. 3D shapes (a 3d vector per-frame per-landmark on a human skeleton as shown in the images) to the 2D image observations W (2d vector per-frame, per-landmark). \nAssuming gaussian noise in the 2D measurement W, the least square framework for inferring S given R is highly under-constrained and can not be solved directly. It is important to note here that in NRSfM,  ONLY the 2D projections of the 3D landmarks on deformable object as observed from various viewpoints are known. Although it is reasonable to believe that a non-linear manifold of low-dimension must explain all the 3D shapes we want to infer, we do not know apriori what that manifold is for a given capture.\n\n\nTo KPCA/RKPCA/LVMs can only be applied to estimate a manifold of human poses if a representative 3D dataset was available. If we were presented with a large (maybe noisy) dataset of human skeletons where we know the 3D location of the joints as the humans perform different actions, we can indeed allpy LVM\u2019s or any robust form of KPCA to learn a manifold which then can be used to solve the problem of recovering the 3D shapes (near manifold) which leads to the observed 2D projections in a new sequence. This setup which uses a large set of 3D human skeleton  is what we call pre-training based method.\n\nA standard approach to solve this NRSfM with \"pre-training\" followed by iterative inference would be:\nStep 1-> Assume a noise model (say gaussian) and learn a manifold spanned by human skeletons performing different actions. Variants of KPCA like RKPCA or LVMs could be used for this step we call \u201cpre-training\u201d.  \nStep 2 ->  solve f(W,R,Z,S) = 0, which is highly under constrained and gives trivial solutions of large dimensionality/rank when solved without manifold/rank constraints.\nStep 3 -> project the solution to the pre-estimated non-linear manifold and refine the solution iteratively by estimating the pre-image of the projection.\n\nIn the proposed framework we solve the NRSfM problem without requiring a dataset of 3D human skeleton (noisy or noise free). Our solution can be simply considered as a nonlinear extension of the energy minimization frameworks which rely on nuclear norm regularization to minimize the rank of matrices while solving ill-posed problems. These frameworks have been used to solve various problems ranging from netflix recommendations to NRSfM (See references) without using PCA/RPCA or its variants as the first step. To the best of our knowledge no variant of GPLVM or KPCA is well established in the literature as non-linear counterpart to trace norm regularization. Proposing such regularizer which can be used in a general energy minimization framework is the main contribution of the paper we present.\n\nNow we would try to address some of the technical errors R3 suggests are in the manuscript:\n\n1)  \"paper proposes a closed form solution to robust KPCA\"- This is indeed correct. Let us clarify. Problem 1 as explained in the paper decomposes into two parts. The first part is solving the ill-posed problem (NRSfM for example) with the constraint that the inferred causal factors lie on (or near) a GIVEN manifold defined by mercer kernel (RBF) and nonlinear principal component denoted by matrix $C$ in section 3.1 are known. It is this pre-image estimation problem (inferring a data point on the given manifold to explain observation) which requires the LM iterations. In our knowledge other known solution to this problems are iterative as well. However this problem does not correspond to KPCA as this part assumes $C$ to be known.\n\nRobust KPCA is the second part (subproblem 7) of problem 1, where the noisy data (or noisy kernel matrix) is given and the goal is to estimate a non-linear manifold near which the data lie. We assume gaussian noise (in feature space) as correctly pointed out by the reviewer and estimate this non-linear manifold in closed form. i.e. you can replace KPCA/GPLVMs directly with our closed form solution given in algorithm 2 to estimate a robust manifold given noisy kernel matrix.\n\n2) \"proposed approach can be trivially generalized to incorporate other cost functions\" : \"cost function\" here correspond to the the function f(W,S) associated with the inverse problem. The decoupling of equation 5 into 2 subproblems (6,7) guarantees that the function f(W,S) does not feature in the subproblem (7). Thus the \"close form solution of the inner loop\" remain intact even after changing the function f. For example instead of using L2 loss for matrix completion on can use a robust huber loss or more interestingly, f can actually be defined by physical process of camera projection relating 3D structures with their image projections.\n\n3) \"novel energy minimization framework to solve problems of the general form of eq. (2)\" : The experiments with oil-flow dataset (in appendix and main body) along with NRSfM suggests that the relaxation proposed works in practice. We will reword the introduction accordingly and explicitly say that we solve a relaxed version of the problem 1. The relaxation proposed in this framework however are (i) trace norm approximation of rank and (ii) gaussian noise assumption in the feature space both of which are studied independently by other body of works (RPCA and GP-LVMs respectively) and used extensively.\n\n4) LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\": In the text we are referring to the use of the LVMs as the pre-training step for low dimensional inference by Prisacariu & Reid (2011) for segmentation; Dame et al. (2013) for 3D reconstruction and other works which indeed use clean training data. The crucial point however is that these approaches learn the latent space APRIORY with a training set which in our case does not exist. \n\nOn \"The experimental evaluation should demonstrate robustness to more complex noise and outliers\"\nIn a very loose sense, NRSfM can be seen as doing dimensionality reduction for severely corrupted data with a very structured pattern which arises from the loss in the information while doing image projections of 3D structures. In context of NRSfM every data sample we want to infer (3P vectors storing the 3D locations of P skeleton joints) is corrupted severely by removing the depth coordinate from each landmark point in every frame. This kind of structured corruption occurs naturally in practical inverse problems which is much severe to recover from than synthetic noise.\n\nFinally on the datasets: Oil flow dataset, we agree is old and simple. It is used here only to facilitate direct comparisons with a large body of work done on robust non-linear dimensionality reduction in presence of noise and missing data. It is important to note that our framework is more generic than robust and missing data KPCA/LVMs for matrix completion. Most of the baselines used for matrix completion on oil flow dataset are not trivially extensible to solve problem 1 (or its relaxations).\nHowever, a direct comparison (which show significant performance gain over baselines on simple but non saturated dataset) helps us answering questions like \"how solving for a local optima of our doubly relaxed problem helps in solving the original problem\".\n\nCMU mocap for NRSfM is a very challenging real dataset and is used to benchmark current state-of-the-art NRSfM solutions till date. We show benefits of non-linear regularizer over its linear (trace norm regularization) counterpart in solving a practical and challenging problem like NRSfM -- for which dimensionality reduction is essential component in most state of the art solutions. That being said, if the reviewers can point us to the newer non-linear dimensionality reduction frameworks which can solve an inverse problems without requiring a \"training stage\", we will be happy to include the relevant experiments.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716306, "id": "ICLR.cc/2017/conference/-/paper128/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2iistgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper128/reviewers", "ICLR.cc/2017/conference/paper128/areachairs"], "cdate": 1485287716306}}}, {"tddate": null, "tmdate": 1482984124207, "tcdate": 1482984124207, "number": 2, "id": "ryNp9bzre", "invitation": "ICLR.cc/2017/conference/-/paper128/official/review", "forum": "Sk2iistgg", "replyto": "Sk2iistgg", "signatures": ["ICLR.cc/2017/conference/paper128/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper128/AnonReviewer1"], "content": {"title": "Not clear", "rating": "3: Clear rejection", "review": "This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  \n\n- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.\n\n- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each\n\n- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding \u201ca data point (pre-image) corresponding to each projection in the input space\u201d is not a standard step in KPCA. \n\n- On page 3, you never define $\\mathcal{X} \\times N$, $\\mathcal{Y} \\times N$, $\\mathcal{H} \\times N$. Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. \n\n- On page 3, Section 2, $\\mathcal{X}$ and $\\mathcal{Y}$ are sets. What do you mean by $\\mathcal{Y} \\ll \\mathcal{X}$\n\n- On page 5, $\\mathcal{S}^n$ is never defined. \n\n- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered \n\n- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  \n\n- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don\u2019t see how the argument goes through at this point. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483482855715, "id": "ICLR.cc/2017/conference/-/paper128/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper128/AnonReviewer3", "ICLR.cc/2017/conference/paper128/AnonReviewer1", "ICLR.cc/2017/conference/paper128/AnonReviewer4"], "reply": {"forum": "Sk2iistgg", "replyto": "Sk2iistgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483482855715}}}, {"tddate": null, "tmdate": 1482407898771, "tcdate": 1482407898771, "number": 1, "id": "rJXJeHYEg", "invitation": "ICLR.cc/2017/conference/-/paper128/official/review", "forum": "Sk2iistgg", "replyto": "Sk2iistgg", "signatures": ["ICLR.cc/2017/conference/paper128/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper128/AnonReviewer3"], "content": {"title": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem.  The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483482855715, "id": "ICLR.cc/2017/conference/-/paper128/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper128/AnonReviewer3", "ICLR.cc/2017/conference/paper128/AnonReviewer1", "ICLR.cc/2017/conference/paper128/AnonReviewer4"], "reply": {"forum": "Sk2iistgg", "replyto": "Sk2iistgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483482855715}}}, {"tddate": null, "tmdate": 1481885460225, "tcdate": 1481885460225, "number": 1, "content": {"title": "N/A", "question": "N/A"}, "id": "SJnGDHbNl", "invitation": "ICLR.cc/2017/conference/-/paper128/pre-review/question", "forum": "Sk2iistgg", "replyto": "Sk2iistgg", "signatures": ["ICLR.cc/2017/conference/paper128/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper128/AnonReviewer3"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481885460682, "id": "ICLR.cc/2017/conference/-/paper128/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper128/AnonReviewer3"], "reply": {"forum": "Sk2iistgg", "replyto": "Sk2iistgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper128/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481885460682}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478241188372, "tcdate": 1478241188364, "number": 128, "id": "Sk2iistgg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sk2iistgg", "signatures": ["~Ravi_Garg1"], "readers": ["everyone"], "content": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "pdf": "/pdf/1d8b8cdde54fc6b0120c3ded795c1d1677e066d5.pdf", "TL;DR": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "paperhash": "garg|nonlinear_dimensionality_regularizer_for_solving_inverse_problems", "keywords": ["Computer vision", "Optimization", "Structured prediction"], "conflicts": ["adelaide.edu.au", "qut.edu.au", "qmul.ac.uk", "ox.ac.uk"], "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 8}