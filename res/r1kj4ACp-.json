{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730188873, "tcdate": 1508988054823, "number": 106, "cdate": 1518730188863, "id": "r1kj4ACp-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1kj4ACp-", "original": "Hy0qECA6b", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Understanding Deep Learning Generalization by Maximum Entropy", "abstract": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "pdf": "/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf", "TL;DR": "We prove that DNN is a recursively approximated solution to the maximum entropy principle.", "paperhash": "zheng|understanding_deep_learning_generalization_by_maximum_entropy", "_bibtex": "@misc{\nzheng2018understanding,\ntitle={Understanding Deep Learning Generalization by Maximum Entropy},\nauthor={Guanhua Zheng and Jitao Sang and Changsheng Xu},\nyear={2018},\nurl={https://openreview.net/forum?id=r1kj4ACp-},\n}", "keywords": ["generalization", "maximum entropy", "deep learning"], "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "authorids": ["zhenggh@mail.ustc.edu.cn", "jtsang@bjtu.edu.cn", "csxu@nlpr.ia.ac.cn"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260077089, "tcdate": 1517250180736, "number": 834, "cdate": 1517250180722, "id": "H16FLJarG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Deep Learning Generalization by Maximum Entropy", "abstract": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "pdf": "/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf", "TL;DR": "We prove that DNN is a recursively approximated solution to the maximum entropy principle.", "paperhash": "zheng|understanding_deep_learning_generalization_by_maximum_entropy", "_bibtex": "@misc{\nzheng2018understanding,\ntitle={Understanding Deep Learning Generalization by Maximum Entropy},\nauthor={Guanhua Zheng and Jitao Sang and Changsheng Xu},\nyear={2018},\nurl={https://openreview.net/forum?id=r1kj4ACp-},\n}", "keywords": ["generalization", "maximum entropy", "deep learning"], "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "authorids": ["zhenggh@mail.ustc.edu.cn", "jtsang@bjtu.edu.cn", "csxu@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642380830, "tcdate": 1511983949252, "number": 1, "cdate": 1511983949252, "id": "HkBIjt2xz", "invitation": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "signatures": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Review", "rating": "2: Strong rejection", "review": "Summary:\n\nThis paper presents a derivation which links a DNN to recursive application of\nmaximum entropy model fitting. The mathematical notation is unclear, and in\none cases the lemmas are circular (i.e. two lemmas each assume the other is\ncorrect for their proof). Additionally the main theorem requires complete\nindependence, but the second theorem provides pairwise independence, and the\ntwo are not the same.\n\nMajor comments:\n\n- The second condition of the maximum entropy equivalence theorem requires\n  that all T are conditionally independent of Y. This statement is unclear, as\nit could mean pairwise independence, or it could mean jointly independent\n(i.e. for all pairs of non-overlapping subsets A & B of T I(T_A;T_B|Y) = 0).\nThis is the same as saying the mapping X->T is making each dimension of T\northogonal, as otherwise it would introduce correlations. The proof of the\ntheorem assumes that pairwise independence induces joint independence and this\nis not correct.\n\n- Section 4.1 makes an analogy to EM, but gradient descent is not like this\n  process as all the parameters are updated at once, and only optimised by a\nsingle (noisy) step. The optimisation with respect to a single layer is\nconditional on all the other layers remaining fixed, but the gradient\ninformation is stale (as it knows about the previous step of the parameters in\nthe layer above). This means that gradient descent does all 1..L steps in\nparallel, and this is different to the definition given.\n\n- The proofs in Appendix C which are used for the statement I(T_i;T_j) >=\n  I(T_i;T_j|Y) are incomplete, and in generate this statement is not true, so\nrequires proof.\n\n- Lemma 1 appears to assume Lemma 2, and Lemma 2 appears to assume Lemma 1.\n  Either these lemmas are circular or the derivations of both of them are\nunclear.\n\n- In Lemma 3 what is the minimum taken over for the left hand side? Elsewhere\n  the minimum is taken over T, but T does not appear on the left hand side.\nExplicit minimums help the reader to follow the logic, and implicit ones\nshould only be used when it is obvious what the minimum is over.\n\n- In Lemma 5, what does \"T is only related to X\" mean? The proof states that\n  Y -> T -> X forms a Markov chain, but this implies that T is a function of\nY, not X.\n\nMinor comments:\n\n- I assume that the E_{P(X,Y)} notation is the expectation of that probability\n  distribution, but this notation is uncommon, and should be replaced with a\nmore explicit one.\n\n- Markov is usually romanized with a \"k\" not a \"c\".\n\n- The paper is missing numerous prepositions and articles, and contains\n  multiple spelling mistakes & typos.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Deep Learning Generalization by Maximum Entropy", "abstract": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "pdf": "/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf", "TL;DR": "We prove that DNN is a recursively approximated solution to the maximum entropy principle.", "paperhash": "zheng|understanding_deep_learning_generalization_by_maximum_entropy", "_bibtex": "@misc{\nzheng2018understanding,\ntitle={Understanding Deep Learning Generalization by Maximum Entropy},\nauthor={Guanhua Zheng and Jitao Sang and Changsheng Xu},\nyear={2018},\nurl={https://openreview.net/forum?id=r1kj4ACp-},\n}", "keywords": ["generalization", "maximum entropy", "deep learning"], "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "authorids": ["zhenggh@mail.ustc.edu.cn", "jtsang@bjtu.edu.cn", "csxu@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642380733, "id": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper106/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer1", "ICLR.cc/2018/Conference/Paper106/AnonReviewer3", "ICLR.cc/2018/Conference/Paper106/AnonReviewer2"], "reply": {"forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642380733}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642380788, "tcdate": 1512016447290, "number": 2, "cdate": 1512016447290, "id": "SyDSqb6gz", "invitation": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "signatures": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "extremely hard to follow, needs major revision", "rating": "3: Clear rejection", "review": "The paper aims to provide a view of deep learning from the perspective of maximum entropy principle.  I found the paper extremely hard to follow and seemingly incorrect in places.  Specifically:\na) In Section 2, the example given to illustrate underfitting and overfitting states that the 5-order polynomial obviously overfits the data.  However, without looking at the test data and ensuring the fact that it indeed was not generated by a 5-order polynomial, I don\u2019t see how such a claim can be made.\nb) In Section 2 the authors state \u201cImposing extra data hypothesis actually violates the ME principle and degrades the model to non-ME model.\u201d \u2026 Statements like this need to be made much clearer, since imposing feature expectation constraints (such as Eq. (3) in Berger et al. 1996) is a perfectly legitimate construct in ME principle.\nc) The opening paragraph of Section 3 is quite unclear; phrases like \u201chow to identify the equivalent feature constraints and simple models\u201d need to be made precise, it is not clear to me what authors mean by this.\nd) I\u2019m not able to really follow Definition 1, perhaps due to unclear notation.  It seems to state that we need to have P(X,Y) = P(X,\\hat{Y}), and if that\u2019s the case not clear what more can be accomplished by maximizing conditional entropy H(\\hat{Y}|X).  Also, there is a spurious w_i in Definition 1.\ne) Definition 2.  Not clear what is meant by notation E_{P(T,Y)}.\nf) Definition 3 uses t_i(x) without defining those, and I think those are different from t_i(x) defined in Definition 2.\n\nI think the paper needs to be substantially revised and clarified before it can be published at ICLR.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Deep Learning Generalization by Maximum Entropy", "abstract": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "pdf": "/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf", "TL;DR": "We prove that DNN is a recursively approximated solution to the maximum entropy principle.", "paperhash": "zheng|understanding_deep_learning_generalization_by_maximum_entropy", "_bibtex": "@misc{\nzheng2018understanding,\ntitle={Understanding Deep Learning Generalization by Maximum Entropy},\nauthor={Guanhua Zheng and Jitao Sang and Changsheng Xu},\nyear={2018},\nurl={https://openreview.net/forum?id=r1kj4ACp-},\n}", "keywords": ["generalization", "maximum entropy", "deep learning"], "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "authorids": ["zhenggh@mail.ustc.edu.cn", "jtsang@bjtu.edu.cn", "csxu@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642380733, "id": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper106/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer1", "ICLR.cc/2018/Conference/Paper106/AnonReviewer3", "ICLR.cc/2018/Conference/Paper106/AnonReviewer2"], "reply": {"forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642380733}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642380749, "tcdate": 1512107786962, "number": 3, "cdate": 1512107786962, "id": "Sy7fJuCxM", "invitation": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "signatures": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "This paper presented a theoretical result for the generalization of DNN using the maximum entropy principle.", "rating": "6: Marginally above acceptance threshold", "review": "The presentation of the paper is crisp and clear. The problem formulation is explained clearly and it is well motivated by theorems. It is a theoretical papers and there is no experimental section. This is the only drawback for the paper as the claims is not supported by any experimental section. The author could add some experiments to support the idea presented in the paper.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Deep Learning Generalization by Maximum Entropy", "abstract": "Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.", "pdf": "/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf", "TL;DR": "We prove that DNN is a recursively approximated solution to the maximum entropy principle.", "paperhash": "zheng|understanding_deep_learning_generalization_by_maximum_entropy", "_bibtex": "@misc{\nzheng2018understanding,\ntitle={Understanding Deep Learning Generalization by Maximum Entropy},\nauthor={Guanhua Zheng and Jitao Sang and Changsheng Xu},\nyear={2018},\nurl={https://openreview.net/forum?id=r1kj4ACp-},\n}", "keywords": ["generalization", "maximum entropy", "deep learning"], "authors": ["Guanhua Zheng", "Jitao Sang", "Changsheng Xu"], "authorids": ["zhenggh@mail.ustc.edu.cn", "jtsang@bjtu.edu.cn", "csxu@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642380733, "id": "ICLR.cc/2018/Conference/-/Paper106/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper106/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper106/AnonReviewer1", "ICLR.cc/2018/Conference/Paper106/AnonReviewer3", "ICLR.cc/2018/Conference/Paper106/AnonReviewer2"], "reply": {"forum": "r1kj4ACp-", "replyto": "r1kj4ACp-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642380733}}}], "count": 5}