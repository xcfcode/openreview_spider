{"notes": [{"id": "S0BA16bsUub", "original": null, "number": 5, "cdate": 1594112622615, "ddate": null, "tcdate": 1594112622615, "tmdate": 1594112622615, "tddate": null, "forum": "rJxycxHKDS", "replyto": "jymmAlDhyV", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment", "content": {"title": "Plasticity term and Initialization details", "comment": "Thanks for your interest in our work, and for giving us the chance to clarify on those issues.\n\n1. You are correct, there is a mismatch in the usage of the term \"plasticity\". As you point out, in Section 3.1 we call the plasticity p and in in Section 4.2 plasticity is 1 - p. What we wanted to achieve with the plasticity term is to modulate the learning rate of the gates, so a larger value of the plasticity should semantically mean 'more flexible gates'. So, a \"plasticity term that decays linearly as training progresses\" means that the gates become less and less flexible over time. Maybe a less unfortunate wording would then be \"the *parameter p* that controls the plasticity is initially set to a small value\", which is then consistent with \"plasticity (1-p) is decayed linearly as training progresses\". So, as p increases, 1-p decreases.\n\n2. For some experiments we initialize specific branches with parameters from other well-known networks such as ImageNet. We add a small random noise to avoid having the exact same parameters in different branches. Notice also that the gate parameters are part of each particular branch too, so differences in the gates will influence the training as well."}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxycxHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2456/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2456/Authors|ICLR.cc/2020/Conference/Paper2456/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141081, "tmdate": 1576860560414, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment"}}}, {"id": "jymmAlDhyV", "original": null, "number": 1, "cdate": 1593948419227, "ddate": null, "tcdate": 1593948419227, "tmdate": 1593948833759, "tddate": null, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Public_Comment", "content": {"title": "Queries regarding the paper", "comment": "1. In Section 3.1 (page 4) of the paper it's mentioned that the plasticity is initially set to a small value and then increased as the training continues.But in the Section 4.2(Implementation Details), it is mentioned that the plasticity is decayed linearly as training progresses and plasticity is defined as 1-p.\nPlease clarify the above points.\n2. Are all the branches initialized with same values? If yes then won't both the branches have the same parameters and behave like a single branch? If no, then how are they initialized?\nAnd could you please let us know when you plan on releasing the code?"}, "signatures": ["~Sumukh_Aithal_K1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sumukh_Aithal_K1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxycxHKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504180028, "tmdate": 1576860593435, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Public_Comment"}}}, {"id": "rJxycxHKDS", "original": "HJxhD0lFvS", "number": 2456, "cdate": 1569439878958, "ddate": null, "tcdate": 1569439878958, "tmdate": 1583912046894, "tddate": null, "forum": "rJxycxHKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "5CmnutmWy", "original": null, "number": 1, "cdate": 1576798749563, "ddate": null, "tcdate": 1576798749563, "tmdate": 1576800886333, "tddate": null, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Although some criticism remains for experiments, I suggest to accept this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718093, "tmdate": 1576800268518, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Decision"}}}, {"id": "HygvQy4otr", "original": null, "number": 2, "cdate": 1571663646648, "ddate": null, "tcdate": 1571663646648, "tmdate": 1574400272397, "tddate": null, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "After Discussion Period:\n\nI stick to my original score. My issues are largely resolved.\n\n----\nThe submission is using adaptive computation graphs for domain adaptation. Multi-flow network is the main architectural element proposed in the submission. And, it is composed of parallel blocks of computations which aggregated using weighted summation with learnable weights. The domain adaptation is performed by setting different weights for source and target dataset. The adaptive weights and network parameters are all learned jointly by minimizing the combination of classification loss and domain difference loss.\n\nAlthough the idea of adaptive computation is not novel and has been explored, their application to the domain adaptation problem is novel to the best of my knowledge. Moreover, the proposed method is sensible and technically sound.\n\nThe submission talks about different amount of computation needed per domain as an intuition behind the method. This is sensible and intuitive; however, it has not been experimented. The paper uses the same amount of layers for all domains making the amount of computation exactly same. It would be interesting to see the performance when different paths actually lead different computations. For example, parallel blocks can have different number of layers etc.\n \nThe submission only provides result for RPT and DANN. These are clearly not state-of-the-art domain adaptation methods. Proposed method does not necessarily need to have state-of-the-art adaptation results to be accepted, but not reporting what state-of-the-art performance is makes the experimental results incomplete.\n\nFigure 4 suggests that there is no real parameter sharing at the end of the training. And, all domains have different computations. Authors should try to explain this behaviour since it is quite counter-intuitive. \n\nIn summary, proposed method is somewhat novel, interesting and seems to be working well. Improved discussion on the experimental study is definitely needed.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574751795233, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Reviewers"], "noninvitees": [], "tcdate": 1570237722558, "tmdate": 1574751795245, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review"}}}, {"id": "BygqFzoYjr", "original": null, "number": 3, "cdate": 1573659266049, "ddate": null, "tcdate": 1573659266049, "tmdate": 1573659266049, "tddate": null, "forum": "rJxycxHKDS", "replyto": "H1lBZPAc_B", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment", "content": {"title": "Comments incorporated in revised version", "comment": "Thank you for your thorough review. Here are our responses:\n\n> Experimental issues:\n> -     Comparison with other state-of-the-art UDA methods (e.g., CDAN) is a must.\n> This paper improves UDA in terms of adaptive parameters sharing, which is\n> completely independent from most of the UDA contributions (including the DANN\n> you compared) which improve the distribution alignment between feature\n> representations. Therefore, it is imperative to compare that line of SOTA\n> methods, otherwise why should we consider adaptative parameters sharing instead\n> of distribution alignment? At best, the proposed multiflow network combined\n> with the SOTA feature alignment method (e.g., CDAN other than DANN) should be\n> considered and expected to beat CDAN itself.\n\nAs acknowledged by R3, the contribution of our method happens at the representation extraction level, and is agnostic to the distribution alignment itself.\nOur comparison to RPT was motivated by the fact that it is the closest approach to ours, and since RPT relies on DANN, we also used it. However, our approach can indeed be employed with other distribution alignment strategies, and after the submission deadline, we have experimented with the Maximum Classifier Discrepancy (MCD) term of Saito et al., 2018. Our new results evidence that this can further improve our performance and consistently outperforms the original MCD.\n\n> -     Many ablation studies or hyperparameter sensitivity analyses are missing.\n> o     How do you determine the number of parallel flows, i.e., K? Is it possible\n> that 3 or 4, more than 2 flows, are better even in the UDA between two domains?\n\nWe are including experiments studying this in the reviewed version of the paper. During the training process, we have observed that the networks quickly choose to ignore extra flows when K > D. This suggests that they did not contribute to the learning of our feature extraction. We did not find experimental evidence to support that K > D is beneficial.\n\n> o     Do you try any other possibilities of grouping a computational unit, and\n> how will different configurations influence the performance?\n\nThere are arbitrarily many ways to group layers into computational units, and in our experiments, we used the blocks naturally emerging from the original architecture, such as the convolutional blocks defined in ResNet-50.\nAn extensive evaluation would require much more time than that available for this rebuttal, and we believe that our current results already show the benefits of our approach.\n\n> o     Is there a possibility that none of the gates in the final layer is\n> activated? Do you need some constraints?\n\nIt is theoretically possible that none of the gates be activated.  In practice, we have found that both the classification signal (for supervised domains) and the feature alignment (for unsupervised ones) are enough to prevent this from happening.\n\n> -     Since the authors mentioned the potential of the multi-flow network in\n> adaptation between multiple domains, it is necessary to investigate\n> multi-source or multi-target domain adaptation. Only in this case may the\n> significance of different K values be demonstrated.\n\nWe have included experimental results using two and three source domains. The unsupervised multi-target domain problem is significantly more difficult and will require additional constraints. While we believe it to be an interesting problem, we consider it more suitable for future work.\n\n> -     The baseline results in Table 1 are not comparable to some reported papers,\n> and even lower than those reported in other UDA papers.\n\nWe have either used the results available on the respective references, when available, or used publicly available code to generate results under the suggested experimental conditions (number of training epochs, batch size, data preprocessing, optimization algorithm, learning rate). We have been careful not to introduce any biases from our method so as to make meaningful comparisons.  It is, however, possible that under different experimental conditions other papers arrive to different results.\n\nWe have updated the manuscript to reflect the above changes.\n\nThank you for your input."}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxycxHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2456/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2456/Authors|ICLR.cc/2020/Conference/Paper2456/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141081, "tmdate": 1576860560414, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment"}}}, {"id": "B1emJzstsS", "original": null, "number": 2, "cdate": 1573659099031, "ddate": null, "tcdate": 1573659099031, "tmdate": 1573659099031, "tddate": null, "forum": "rJxycxHKDS", "replyto": "HygvQy4otr", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment", "content": {"title": "Comments incorporated in revised version", "comment": "Thank you for your analysis. Here are our comments:\n\n> Although the idea of adaptive computation is not novel and has been explored,\n> their application to the domain adaptation problem is novel to the best of my\n> knowledge. Moreover, the proposed method is sensible and technically sound.\n\n> The submission talks about different amount of computation needed per domain\n> as an intuition behind the method. This is sensible and intuitive; however,\n> it has not been experimented. The paper uses the same amount of layers for\n> all domains making the amount of computation exactly same. It would be\n> interesting to see the performance when different paths actually lead\n> different computations. For example, parallel blocks can have different\n> number of layers etc.\n\nWe have included results showing the behavior under this setting in the revised version of the paper. In particular, we study both using different capacities for the flows and incorporating additional flows to have more flows than domains. The results are provided in Appendix C and show that, in the first case, the more complex domains tend to use flows with higher capacity, and in the second case, that learning tends to discard some flows when there are more than domains.\n\n> The submission only provides result for RPT and DANN. These are clearly not\n> state-of-the-art domain adaptation methods. Proposed method does not\n> necessarily need to have state-of-the-art adaptation results to be accepted,\n> but not reporting what state-of-the-art performance is makes the experimental\n> results incomplete.\n\nOur comparison to RPT was motivated by the fact that it is the closest approach to ours. Since RPT relies on the same domain classifier as DANN, DANN came as a natural baseline. However, we will further report the state-of-the-art performance.\n\nNote that most of the SOTA techniques can also benefit from our approach. To evidence this, we performed additional experiments by replacing the DANN domain classifier in our approach with the Maximum Classifier Discrepancy (MCD) term of Saito et al., 2018. This further improves our results and our approach consistently outperforms the original MCD.\n\n> Figure 4 suggests that there is no real parameter sharing at the end of the\n> training. And, all domains have different computations. Authors should try to\n> explain this behaviour since it is quite counter-intuitive.\n\nWe believe that there was some confusion. In Fig. 4, each row indicates how much each domain uses each flow. In each column, the same color indicates the same flow. As such, at the end of the training, Fig. 4 shows that, for example, all domains share the same flows in computational units conv2_x and conv5_x. We acknowledge that, in conv1, each domain uses its own private flow. This, we believe, confirms our intuition: Initially, the domains need to undergo different computations, because of their appearance differences, but can then share some of the following computations in later stages of the network. We hope that this clarifies the reviewer's concern.\n\n> In summary, proposed method is somewhat novel, interesting and seems to be\n> working well. Improved discussion on the experimental study is definitely\n> needed.\n\nWe have updated the manuscript to reflect the above changes.\n\nThank you for your input."}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxycxHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2456/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2456/Authors|ICLR.cc/2020/Conference/Paper2456/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141081, "tmdate": 1576860560414, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment"}}}, {"id": "SJl_mWjtiH", "original": null, "number": 1, "cdate": 1573658912143, "ddate": null, "tcdate": 1573658912143, "tmdate": 1573658912143, "tddate": null, "forum": "rJxycxHKDS", "replyto": "B1x4SgJRFS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment", "content": {"title": "Comments incorporated in revised version", "comment": "Thank you for your insights. Here are our responses:\n\n>- I would prefer to get the paper additionally linked to a few more transfer\n>learning techniques out of the deep learning domain which is important as well\n\nWe focused our literature review on deep learning because our work proposes an\napproach for deep architectures. For the sake of completeness, we will\nnonetheless include the following papers:\n\n- Boosting for Transfer Learning [Dai07]\n- Covariate Shift by Kernel Mean Matching [Gretton09]\n- Domain adaptation via transfer component analysis [Pan10]\n- Unsupervised Visual Domain Adaptation Using Subspace Alignment [Fernando13]\n- Deep CORAL: Correlation Alignment for Deep Domain Adaptation [Sun16]\n- A DIRT-T approach to unsupervised domain adaptation [Shu18]\n\n>- do you really need to call it (multi) flow network .... - a flow network is\n>a well established concept in algorithmics and refers to a graph problem ...\n>to avoid name clashes ...\n\nWe propose to rename our approach as Domain-Adaptive Multibranch Networks.\n\n>- in the references you have provided back links to the pages where the\n>references are used - this is handy but also confusing and a bit unusual - I\n>think it was not part of the standard template\n\nWe will abide by the template and remove the back links.\n\n>- please avoid using arxiv references but replace them by reviewed material.\n>In parts I am willing to accept such kind of gray literature provided by well\n>known authors but this should not become a standard habit\n\nWe have replaced the arxiv references with reviewed ones when available.\n\n>- I am happy to see that the code will be published - I hope this is really\n>done, because from the material it maybe hard to reconstruct the method\n\nOur code is currently stored in a private github repository, which will be set as public once the blind review period ends.\n\nWe have updated the manuscript to reflect the above changes.\n\nThank you for your input."}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxycxHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2456/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2456/Authors|ICLR.cc/2020/Conference/Paper2456/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141081, "tmdate": 1576860560414, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Authors", "ICLR.cc/2020/Conference/Paper2456/Reviewers", "ICLR.cc/2020/Conference/Paper2456/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Comment"}}}, {"id": "H1lBZPAc_B", "original": null, "number": 1, "cdate": 1570592508788, "ddate": null, "tcdate": 1570592508788, "tmdate": 1572972335797, "tddate": null, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors proposed to address the information asymmetry between domains in unsupervised domain adaptation. Innovatively, they resort to a multiflow network where each domain adaptatively selects its own pipeline. I quite appreciate the idea itself, while there are many essential issues to be addressed first. \n\nPros:\n-\tThe way tackling the information asymmetry, or untie weights, between domains is novel and interesting. \n-\tThe proposed network/framework can be easily extended to the multi-task setting, or multi-source/multi-target domain adaptation.\n-\tThe paper is well-written and easy to follow. \n\nCons:\n-\tThe most critical downside of this paper is its insufficient experiments to support the whole idea, where we will detail in the next.\n\nExperimental issues:\n-\tComparison with other state-of-the-art UDA methods (e.g., CDAN) is a must. This paper improves UDA in terms of adaptive parameters sharing, which is completely independent from most of the UDA contributions (including the DANN you compared) which improve the distribution alignment between feature representations. Therefore, it is imperative to compare that line of SOTA methods, otherwise why should we consider adaptative parameters sharing instead of distribution alignment? At best, the proposed multiflow network combined with the SOTA feature alignment method (e.g., CDAN other than DANN) should be considered and expected to beat CDAN itself. \n-\tMany ablation studies or hyperparameter sensitivity analyses are missing. \no\tHow do you determine the number of parallel flows, i.e., K? Is it possible that 3 or 4, more than 2 flows, are better even in the UDA between two domains? \no\tDo you try any other possibilities of grouping a computational unit, and how will different configurations influence the performance? \no\tIs there a possibility that none of the gates in the final layer is activated? Do you need some constraints?\n-\tSince the authors mentioned the potential of the multi-flow network in adaptation between multiple domains, it is necessary to investigate multi-source or multi-target domain adaptation. Only in this case may the significance of different K values be demonstrated. \n-\tThe baseline results in Table 1 are not comparable to some reported papers, and even lower than those reported in other UDA papers. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574751795233, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Reviewers"], "noninvitees": [], "tcdate": 1570237722558, "tmdate": 1574751795245, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review"}}}, {"id": "B1x4SgJRFS", "original": null, "number": 3, "cdate": 1571840060258, "ddate": null, "tcdate": 1571840060258, "tmdate": 1572972335720, "tddate": null, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "invitation": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper provides an unsupervised domain adaptation approach\nin the context of deep learning. The motivation is clear, related work\nsufficient and experimental settings and results convincing. \nI have only very minor comments:\n- I would prefer to get the paper additionally linked to a few more\n  transfer learning techniques out of the deep learning domain\n  which is important as well\n- do you really need to call it (multi) flow network .... - a flow network\n  is a well established concept in algorithmics and refers to a graph problem\n  ... to avoid name clashes ...\n- in the references you have provided back links to the pages where the references\n  are used - this is handy but also confusing and a bit unusual - I think it was not part \n  of the standard template\n- please avoid using arxiv references but replace them by reviewed material. In parts\n  I am willing to accept such kind of gray literature provided by well known authors but\n  this should not become a standard habit\n- I am happy to see that the code will be published - I hope this is really done, because\n  from the material it maybe hard to reconstruct the method"}, "signatures": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2456/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["roger.bermudez@epfl.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Domain Adaptive Multibranch Networks", "authors": ["R\u00f3ger Berm\u00fadez-Chac\u00f3n", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/2590950d8db05c10ce655a6b0f2a6e404611b61e.pdf", "TL;DR": "A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.", "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "keywords": ["Domain Adaptation", "Computer Vision"], "paperhash": "berm\u00fadezchac\u00f3n|domain_adaptive_multibranch_networks", "_bibtex": "@inproceedings{\nBerm\u00fadez-Chac\u00f3n2020Domain,\ntitle={Domain Adaptive Multibranch Networks},\nauthor={R\u00f3ger Berm\u00fadez-Chac\u00f3n and Mathieu Salzmann and Pascal Fua},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxycxHKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7a5cb9eb4fa9d872a9eb34be33bc3b4a08e85d12.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxycxHKDS", "replyto": "rJxycxHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2456/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574751795233, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2456/Reviewers"], "noninvitees": [], "tcdate": 1570237722558, "tmdate": 1574751795245, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2456/-/Official_Review"}}}], "count": 10}