{"notes": [{"id": "HkghV209tm", "original": "rJgogeCqKQ", "number": 1484, "cdate": 1538087987531, "ddate": null, "tcdate": 1538087987531, "tmdate": 1545355438324, "tddate": null, "forum": "HkghV209tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1xyzFyIg4", "original": null, "number": 1, "cdate": 1545103623361, "ddate": null, "tcdate": 1545103623361, "tmdate": 1545354478858, "tddate": null, "forum": "HkghV209tm", "replyto": "HkghV209tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Meta_Review", "content": {"metareview": "The reviewers expressed some interest in this paper, but overall were lukewarm about its contributions. R4 raises a fundamental issue with the presentation of the analysis (see the D_infty assumption). The AC thus goes for a \"revise and resubmit\".", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Borderline paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1484/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352822766, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": "HkghV209tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352822766}}}, {"id": "BkxG7ywE1E", "original": null, "number": 7, "cdate": 1543954201826, "ddate": null, "tcdate": 1543954201826, "tmdate": 1543954570391, "tddate": null, "forum": "HkghV209tm", "replyto": "rJxli_wcAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "On $D_{\\infty}$ assumption", "comment": "I think that assuming that $D_\\infty$ is bounded is a lack of rigor. It is not a property of the problem but a property of the algorithm you use.  The goal is to show the appealing properties of your algorithm.\n\nLet me give you an example. if we consider the simple sequence $x_{t+1} = x_t - \\eta x_t$ (which is basically gradient descent on the one dimensional objective $f(x) = x^2$. ) Then if we assume that $D_{\\infty} = sum_{t} \\|x_t\\|$ is bounded we have: \n\n$ \\eta x_{t} = x_t - x_{t+1} => \\frac{\\eta}{T}\u00a0\\sum_{t=0}^T x_t = (x_0 - x_{T+1})/T  \n                                          => \\eta \\|\\frac{1}{T} \\sum_{t=0}^T x_t \\| \\leq  2 D_\\infty/T$\n\nNow with $\\eta = T$  we get that the average of $x_t$  converge at a rate $O(1/T^2)$.\n$\\|\\frac{1}{T} \\sum_{t=0}^T x_t \\| \\leq  2 D_\\infty/T^2$\n\nThat is not true because for $\\eta > 1$ it is easy to show that this sequence actually diverge !\n\nThis example underlines that $D_{\\infty}$ may actually depend on $\\eta$. \n\nI consider that it is paradoxical to assume that a sequence is bounded in order to eventually show that it's average actually converge."}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "H1eizHCITm", "original": null, "number": 4, "cdate": 1542018323341, "ddate": null, "tcdate": 1542018323341, "tmdate": 1543954366158, "tddate": null, "forum": "HkghV209tm", "replyto": "HkghV209tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "content": {"title": "An interesting way to combine regularized approximate minimal polynomial extrapolation and optimistic online learning.", "review": "This paper provides an interesting way to combine regularized approximate minimal polynomial extrapolation and optimistic methods. I like the idea and the experimental results look promising. However, I have some concerns:\n    - I'm wondering if the comparison with the baseline is fair. Actually, one iteration of Optimistic-AMSGrad is more expensive than one iteration of AMSGrad since it requires to compute m_{t+1}. The authors should explain to what extend this computation is significantly cheaper that a backprop (if it actually is).\n    - The discussion after Theorem 1 is not clear. To me it is not clear whether or not Optimistic-AMSGrad has a better Regret that AMSGrad: you did not compare the *sum* of the two additional term with the term with a $\\log(T)$ (second line of (8) with second lien of (9)). Do you get a better regret that O(\\sqrt{T}), a better constant ? Moreover you did not justify why it is reasonable to assume that each $g_t[i]^2-h_t[i]^2/\\sqrt{v_{t-1}[i]}$ are bounded.\n    - I'm also concerned about the definition of $D_{\\infty}$. Did you prove that this constant is not infinite ? (Reddi et al. 2018) proposed a projected version of their algorithm and did the analysis assuming that the constraints set was bounded. In your Algorithm 2 would you project in Line 8 and 9 or only on line 9 ? I think that the easiest fix would be to provide a projected version of your algorithm and to do your analysis with the standard assumption that the constraints set is bounded.\n    - The description of Alg 2 is not clear. \"Notice that the gradient vector is computed at w_t instead of w_{t\u22121/2}\" why would it be $w_{t-1/2}$ ? in comparison to what ? \"Also, w_{t+ 1/2} is updated from {w_{t\u2212 1/2} instead of w_t.\" Same. The comments are made without any description of the algorithm, fact is, if the reader is not familiar with the algorithm (which is introduced in the following page) the whole paragraph is hard to catch.\n    - Actually, Page 6 you explain how the optimistic step of Daskalikis et al. (2018) is unclear but you can merge the updates Lines 8\nand 9 to $w_{t+1} = w_{t} - \\eta_{t+1} \\frac{4 h_{t+1}}{(1-\\beta_1) \\sqrt{v_t}} - \\eta_t \\\u00b1rac{\\theta_t}{\\sqrt{v_t}} + \\eta_t \\frac{4 h_{t}}{(1-\\beta_1) \\sqrt{v_{t-1}}}$ (Plug line 8 in line 9 and then plug Line 9 at time t) to get a very similar update. If you look more closely at Daskalakis et al. 2018 their guess $m_{t+1}$ is $g_t$. Finally you Theorem 2 is stated in a bit unfair way since you also require $\\beta_1 <\\sqrt{\\beta_2}$, moreover it seems that Theorem 2 is no longer true anymore if, as you says, you impose that the second moment of ADAM-DISZ is monotone adding the maximization step. \n    - About the experiments, I do not understand why there is the number of iterations in the left plots and the number of epochs on the right plots. It makes the plots hard to compare. \n    - You should compare your method to extragradient methods.\n\n\n\nTo sum up, this paper introduce interesting results. The combination of (Scieur et al. 2016) and optimistic online learning is really promising and solid theoretical results are claimed. However, some points should be clarified (see my comments above). Especially, I think that the authors focused too much (sometimes being unfair in their discussion and propositions) on showing how their algorithm is better than (Daskalakis et al. 2018) whereas as they mentioned it \"The goals are different.\" ADAM-DISZ is designed to optimize games and is similar to extragradient. It is know that extragradient methods are slower than gradient methods for single objective minimization because of the extrapolation step using a too conservative signal for single objective minimization.\n\n\nSome minor remarks: \n    - Page One \"NESTEROV'SMETHOD\"\n    -  \"which can be much smaller than \\sqrt{T} of FTRL if one has a good guess.\" You could refer to Section 3 or something else because otherwise this sentence remains mysterious. What is a good guess (OR maybe you could say that standard \"good\" guesses are either the previous gradient or the average of the previous gradients)\n    - \"It combines the idea of ADAGRAD (Duchi et al. (2011)), which has individual learning rate for different dimensions.\"  what is the other thing combined ?\n    - Beginning of Sec 3.1 $\\psi_t$ represent $diag(v_t)^{1/2}$.\n\n===== After Authors Response =====\nAs developed in my response \"On $D_{\\infty}$ assumption \" to the reviewers, I think that the assumption that $D_\\infty$ bounded is a critical issue.\nThat is why I am moving down my grade.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "cdate": 1542234220266, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkghV209tm", "replyto": "HkghV209tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335956138, "tmdate": 1552335956138, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlG728ZJE", "original": null, "number": 6, "cdate": 1543756825655, "ddate": null, "tcdate": 1543756825655, "tmdate": 1543756825655, "tddate": null, "forum": "HkghV209tm", "replyto": "S1eV7r_9Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "response", "comment": "Thanks for taking your time to respond. Unfortunately, I don't find the responses to be very satisfying.\n\nRe boundedness of weights:\nThis is actually a very strong assumption as it implicitly requires a certain kind of stability that ensures that the weights output by the algorithm never diverge. This is already not trivial to ensure when the objective is convex, and certainly much harder if it is non-convex. So, projections are absolutely necessary for this part of the analysis to work out.\n\nRe extrapolation method:\nI do understand that you want to approximate the gradient as the fixed point of (4), I am just having trouble understanding why this would be a reasonable approximation. Specifically, in what cases do you expect this approximation to be accurate? Is there a natural case where the gradients actually follow such a linear recursion? (Maybe linear regression?) What are cases where this method leads to a bad approximation?\n\nPlease consider discussing these issues in more detail in a future version."}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "HkgiB4shAX", "original": null, "number": 5, "cdate": 1543447619102, "ddate": null, "tcdate": 1543447619102, "tmdate": 1543447619102, "tddate": null, "forum": "HkghV209tm", "replyto": "rklDgYP907", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "Response to author response", "comment": "Thanks for the response. \n\nI am aware that the algorithm doesn't reduce to AMSGrad when m_t=0, and I think that this makes it more important to carefully highlight when the new algorithm will outperform existing methods (such as AMSGrad). It's probably not reasonable to do this theoretically on real-world problems (although it would be interesting to show how the terms in question behave in your experiments), but even demonstrating cases where the proposed algorithm is better on toy objectives would be helpful for the reader.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "S1eV7r_9Cm", "original": null, "number": 4, "cdate": 1543304475619, "ddate": null, "tcdate": 1543304475619, "tmdate": 1543304475619, "tddate": null, "forum": "HkghV209tm", "replyto": "HJe5D4oj27", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "Response to AnonReviewer 1", "comment": "Thank you for the valuable comments. We have fixed most of the issued you raised under \"Detailed Comments\". For now, we leave the small capital letter typesettings for now but we will be happy to remove the capitalization later. \n\n== Regarding to $D_{\\infty}$: \nWe assume that it is finite. If $w^*$ is finite, then $D_{\\infty}$ should be finite. We think it is a reasonable assumption. We will consider the constraint case later.\n\n== Regarding to the extrapolation method: \nWe want to use the past few gradient vectors to predict $g_{t}$. So, $x^*$ is the fixed point of the (4).\nIf $x_{t-1}= x^{*}$, then $x_{t} = x^{*}$. By using the extrapolation method, we basically assumes that the relation of gradient vectors satisfies (4). \nWe admit that it is not true in general but the method helps to get a faster convergence in the experiments. \n\n== Experiment == \nWe think that our results illustrate an obvious acceleration, especially in first few epochs. Since our work focuses on a \"optimistic\" modification to the AMSGrad algorithm, we choose to compare the improvement brought by different optimistic term based on tuned AMSGrad optimizer, which might be more convincing for the \"extra effect\".\n\nAgain, thanks for the detailed comments.\n\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "rklDgYP907", "original": null, "number": 3, "cdate": 1543301359076, "ddate": null, "tcdate": 1543301359076, "tmdate": 1543303331142, "tddate": null, "forum": "HkghV209tm", "replyto": "Hye3jHqf2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the valuable comments and identifying the typos. We have fixed them. Please find as follows our response. \nWe've adjusted the names of algorithms and updated a new version accordingly.\n\n== Regarding to Theorem 1:\nThe proposed algorithm does not reduce to AMSGrad when $m_{t}=0$. So, the regret bound is different.\nIf one remove line 9, or set $h_{t+1}=0$, then the last two terms of the regret bound would disappear,\nwhich becomes the bound of AMSGrad (namely, (9)).\n\nAs AnonReviewer 4 points out, we should also compare the sum of the last two terms in Theorem 1 (namely, (8)) with the last term on (9). We are not claiming that the regret bound of Theorem 1 is always better than that of AMSGrad (namely, (9)).\nOur original discussion actually means that if $m_{t}$ and $g_{t}$ is close, then the last term of (8) would dominate. We then try to upper-bound it in a very loose sense (i.e. (10)). We treat each $\\frac{(g_t[i]^2- h_t[i]^2)}{\\sqrt{\\hat{v}_{t-1}[i]}}$ as a constant and get a $O(\\sqrt{T})$ bound. However, in practice, $\\sqrt{\\hat{v}_{t-1}[i]}$ might grow over time,\nand the growth rate is different for different dimension $i$. If $\\sqrt{\\hat{v}_{t-1}[i]}$ grows like $O(\\sqrt{t})$ then the last term of (8) is just $O(\\log T)$, which might be better than that of the last term on (9). One can also get a data dependent bound like the last term on (9). We just wanted to say that the regret bound might be better than that of AMSGrad under certain conditions."}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "rJxli_wcAQ", "original": null, "number": 1, "cdate": 1543301271653, "ddate": null, "tcdate": 1543301271653, "tmdate": 1543303095728, "tddate": null, "forum": "HkghV209tm", "replyto": "H1eizHCITm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for the valuable comments. Please find as follows our response. \nWe've updated the new version accordingly.\n\n== Iteration Cost == \nYes, like optimistic algorithms in general, each iteration would be slightly more expensive.  Using fewer iterations means that it has fewer access to training samples than what required by AMSGrad. The sample efficiency might have some advantages on certain applications. One can also design some algorithms or update rules to reduce the time of predicting $m_t$. \n\n== Theorem 1 == \nYes, we should also compare the sum of the last two terms in Theorem 1 (namely, (8)) with the last term on (9). We are not claiming that the regret bound of Theorem 1 is always better than that of AMSGrad (namely, (9)). Our original discussion actually means that if $m_{t}$ and $g_{t}$ is close, then the last term of (8) would dominate. We then try to upper-bound it in a very loose sense (i.e. (10)). We treat each $\\frac{(g_t[i]^2- h_t[i]^2)}{\\sqrt{\\hat{v}_{t-1}[i]}}$ as a constant and get a $O(\\sqrt{T})$ bound. However, in practice, $\\sqrt{\\hat{v}_{t-1}[i]}$ might grow over time,\nand the growth rate is different for different dimension $i$. If $\\sqrt{\\hat{v}_{t-1}[i]}$ grows like $O(\\sqrt{t})$ then the last term of (8) is just $O(\\log T)$, which might be better than that of the last term on (9). One can also get a data dependent bound like the last term on (9). We just wanted to say that the regret bound might be better than that of AMSGrad under certain conditions.\n\n== $D_{\\infty}$ == \nWe assume that it is finite. If $w^*$ is finite, then $D_{\\infty}$ should be finite. We think it is a reasonable assumption. Thanks for your suggestion, we will consider the constraint case later.\n\n== Discussion/Description of Algorithm 2 == \nThanks for the suggestion. We updated it accordingly.\n\n== Comparison with the update of ADAM-DISZ on Page 6 == \nThanks for the suggestion. We updated the discussion accordingly.\nIn short, combining (8) and (9) in Algorithm 2, we get that\n$$ w_{t+1} = w_{t-1/2} - \\eta_t \\frac{\\theta_t}{\\sqrt{\\hat{v}_t}}\n- \\eta_{t+1} \\frac{4}{1-\\beta_1} \\frac{h_{t+1}}{\\sqrt{\\hat{v}_t}}\n$$ Notice that,  $w_{t+1}$ is updated from $w_{{t-1/2}}$ instead of $w_{{t}}$. So, ADAM-DISZ is not really similar to the proposed algorithm here.\n\n== Experiments == \nOn the left we want to show the whole training path, and we followed the choice of axis units as in previous related works (e.g Reddi et al. 2018). On the right we want to focus on the acceleration in relatively early stage, so we choose to plot against number of epochs to highlight this point. We have added some explanation in the caption of figure 1. Thanks for your suggestion."}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "SJg5TdwqAm", "original": null, "number": 2, "cdate": 1543301314126, "ddate": null, "tcdate": 1543301314126, "tmdate": 1543301314126, "tddate": null, "forum": "HkghV209tm", "replyto": "BJehLc1R3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for the comments and identifying the typos. We have fixed them. Please find as follows our response. \nWe've updated the new version accordingly.\n\n== The extrapolation algorithm == \nWe choose the particular algorithm by (Scieur et al. 2016) because it has good empirical performance\nin practice. (Scieur et al. 2016) shows that using the last few updates of an optimization algorithm,\nthe method can predict a point that is much closer to the optimum than the last update of the optimization algorithm.\n\n== Scaling the next direction ==\nThis may be a good idea. We leave it as a future work.\n\n== \"... the gradient vectors at a specific time span is assumed to be captured by (5).\" ==\nWe elaborate it in the new version accordingly.\nWe want to have a good prediction $m_{t}$ of $g_{t}$ by using the past few gradients.\nIf the past few gradients can be modeled by the equation approximately, then\nthe method should predict the gradient well."}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605715, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkghV209tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1484/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1484/Authors|ICLR.cc/2019/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers", "ICLR.cc/2019/Conference/Paper1484/Authors", "ICLR.cc/2019/Conference/Paper1484/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605715}}}, {"id": "BJehLc1R3m", "original": null, "number": 3, "cdate": 1541433939703, "ddate": null, "tcdate": 1541433939703, "tmdate": 1541533098618, "tddate": null, "forum": "HkghV209tm", "replyto": "HkghV209tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "content": {"title": "Simple but nice extension with existing ideas from the literature", "review": "In this manuscript, the authors borrow the idea of \"optimism\" from the online learning literature and apply it to two frequently used methods for neural network training (AMSGrad and ADAM). More or less, replicating the theory known in the literature, they give a regret analysis. The manuscript ends with a comparison of the optimistic methods against their plain counterparts on a set of test problems.\n\nThis is a well-written paper filling a gap in the literature. Through the contribution does not seem significant, the results do support that such extensions should be out there. In addition to a few typos, some clarification on several points could be quite useful:\n\n1) It is not clear why the authors use this particular extrapolation algorithm?\n\n2) If we have the past r+1 gradients, can we put them into use for scaling the next direction like in quasi-Newton methods?\n\n3) The following part of the sentence is not clear: \"... the gradient vectors at a specific time span is assumed to be captured by (5).\"\n\n4) \\nabla is missing at the end of the line right after equation (6).\n\n5) The second line after Lemma 2 should be \"... it does not matter how...\" (The word 'not' is missing.)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "cdate": 1542234220266, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkghV209tm", "replyto": "HkghV209tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335956138, "tmdate": 1552335956138, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJe5D4oj27", "original": null, "number": 2, "cdate": 1541284962491, "ddate": null, "tcdate": 1541284962491, "tmdate": 1541533098417, "tddate": null, "forum": "HkghV209tm", "replyto": "HkghV209tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "content": {"title": "reasonable algorithms, no surprises", "review": "The paper proposes new online optimization algorithms by adding the idea of optimistic updates to the already popular components of adaptive preconditioning and momentum (as used in AMSGrad and ADAM). Such optimistic schemes attempt to guess the yet-unseen gradients before each update, which can lead to better regret guarantees when the guesses are accurate in a certain sense. This in turn can lead to faster convergence when the resulting algorithm is used in an optimization framework. The specific contribution of the present paper is proving formally that optimistic updates can indeed be combined with advanced methods like ADAM and AMSGrad, also providing a regret analysis of the former algorithm. On the practical front, the authors also propose a method closely resembling Anderson acceleration for guessing the next gradient, and the eventual scheme is shown to work well empirically in training deep neural networks.\n\nThe idea of optimistic updates has been popular in recent years within the online-learning literature, and has been used with particularly great success for achieving improved convergence guarantees for learning equilibria in games. More recently, optimistic updates have also appeared in more \"practical\" settings such as training GANs, where they were shown to improve stability of training. The present paper argues that the idea of optimism can be useful for large-scale optimization as well, if the gradient guesses are chosen appropriately.\n\nI have lukewarm feelings about the paper. On the positive side, the proposed method is a natural and sensible combination of solid technical ideas, and its theoretical analysis appears to be correct. As the authors point out, their algorithm incorporates the idea of optimism in a much more natural way than the related optimistic ADAM algorithm previously proposed by Daskalakis et al. (2018) does. The experiments also indicate some advantage of optimism in the studied optimization problems.\n\nOn the other hand, the theoretical contribution is marginal: the algorithm and its analysis is a straightforward combination of previous ideas and the result itself doesn't strike me as surprising at all. Then again, perhaps this is more of a presentation issue, as it may be the case that the authors did not manage to highlight clearly enough the technical challenges they needed to overcome to prove their theoretical results. Furthermore, I find the method for guessing the gradients to be rather arbitrary and poorly explained---at least I'm not sure if anyone unfamiliar with the mentioned gradient extrapolation methods would find this approach to be sensible at all.\n\nI am not fully convinced by the experimental results either, since I have an impression that the gradient-guessing method only introduces yet another knob to turn when tuning the hyperparameters, and it's not clear at all that this new dimension would indeed unlock levels of performance that were not attainable before. Indeed, the authors seem to fix all hyperparameters across all experiments and only switch around the optimistic components, rather than finding the best tuning for each individual algorithm and comparing the respective results. Also, I don't really see any qualitative improvements in the learning curves due to the new components---but maybe I just can't read these graphs properly since I have more of a \"theorist\" background.\n\nThe writing is mostly OK, although there is room for improvement in terms of English use (especially on the front of articles which seem to be off in almost every sentence).\n\nOverall, I don't feel very comfortable about suggesting acceptance, mostly because I find the results to be rather unsurprising. I suggest that the authors try to convince me of the nontrivial challenges arising in the analysis, or about the definite practical advantage that optimism can buy for large-scale optimization.\n\nDetailed comments\n=================\n- pp.1, abstract: \"We consider new variants of optimization algorithms.\"---This sentence is rather vague and generic. I guess you wanted to refer to *convex* optimization algorithms, which is actually what you consider in the paper. No need to be embarrassed about assuming convexity...\n- pp.1: A general nuisance with the typesetting that already shows on the first page is that italic and small capital fonts are used excessively and without any clearly identifiable logic. Please simplify.\n- pp.1: \"AdaGrad [...] exploits the geometry of the data and performs informative update\"---this makes it sound like other algorithms make non-informative updates.\n- pp.1: Regret was not defined even informally in the introduction, yet already some regret bounds are compared, highlighting that one \"can be much smaller than O(\\sqrt{T})\". This is not very friendly for readers with no prior experience in online learning.\n- pp.1: \"Their regret analysis are the regret analysis in online learning.\"---What is this sentence trying to say?\n- pp.2: For this discussion of FTRL, it would be useful to remark that this algorithm really only makes sense if the loss function is convex. Also related to this discussion: you mention that the bound for optimistic FTRL can be much smaller than \\sqrt{T}, but never actually say that \\sqrt{T} is minimax optimal---without this piece of context, this statement has little value.\n- pp.3: \"ADAM [...] does not converge to some specific convex functions.\"---I guess I understand what this sentence is trying to say, but it certainly doesn't say it right. (Why would an *algorithm* converge to a *function*?)\n- pp.3, bottom: This description of \"extrapolation methods\" is utterly cryptic. What is x_t here? What is the \"fixed point x^*\"? Why is this scheme applicable at all here? (Why would one believe the errors to be near-linear in this case? Would this argument work at all for non-convex objectives?)\n- pp.5, Lemma 1: Why would one expect D_\\infty to be finite? In order to ensure this, one would need to project the iterates to a compact set.\n- pp.5, right after lemma 1: \"it does matter how g_t is generated\" -> \"it does *not* matter how g_t is generated\"\n- pp.6, top: \"we claimed that it is smaller than O(\\sqrt{T}) so that we are good here\"---where exactly did this claim appear, and in what sense \"are we good here\"? Also, the norms in this paragraph should be squared.\n- pp.6, Sec 3.2: While this section makes some interesting points, its tone feels a bit too apologetic. E.g., saying that \"[you] are aware of\" a previous algorithm that's similar to yours and doubling down on the claim that \"the goals are different\" makes the text feel like you're taking a defensive stance even though I can't see a clear reason for this. In my book, the approach you propose is clearly different and more natural for the purpose of your study.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "cdate": 1542234220266, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkghV209tm", "replyto": "HkghV209tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335956138, "tmdate": 1552335956138, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hye3jHqf2Q", "original": null, "number": 1, "cdate": 1540691364176, "ddate": null, "tcdate": 1540691364176, "tmdate": 1541533098171, "tddate": null, "forum": "HkghV209tm", "replyto": "HkghV209tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "content": {"title": "Main idea is not sufficiently novel and technical and empirical results are not convincing enough", "review": "This paper combines recent results in online learning and convex optimization, specifically adaptivity, momentum, and optimism. The authors add an optimistic gradient prediction step into the AMSGrad algorithm proposed by Reddi et al, 2018. Moreover, they propose using the RMPE algorithm of Scieur et al, 2016 to come up with the gradient prediction step. The new method that they introduce is called Optimistic AMSGrad, and the authors present both theoretical guarantees as well as numerical experiments justifying this new method.\n\nThe paper is relatively well-written, and the authors do a good job of explaining recent work on adaptivity, momentum, and optimism in online learning and convex optimization to motivate their algorithm. The algorithm is also presented clearly, and the fact that the method is accompanied by both a regret bound as well as numerical experiments is appreciated.\n\nAt the same time, I found the presentation of this work to be a little misleading. The idea of applying optimism to Adam was already presented in Daskalakis et al, 2018. The algorithm in that paper is, in fact, called \"Optimistic Adam\". I found it very strange that the authors chose to rename that algorithm in this paper. There are two main differences between Optimistic Adam in Daskalakis et al, 2018 and Optimistic AMSGrad. The first is the extension from Adam to AMSGrad, which involves an extra maximization step (line 7 in Algorithm 2) that is immediate. The second is the choice of gradient prediction method. Since Daskalakis et al, 2018 were concerned with equilibrium convergence, they opted to use the most recent gradient as the prediction. On the other hand, the authors in this work are concerned with general online optimization, so they use a linear combination of past gradients as the prediction, based on a method introduced by Scieur et al, 2016. On its own, I do not find this extensions to be sufficiently novel or significant to merit publication. \n\nThe fact that this paper includes theoretical guarantees for Optimistic AMSGrad that were missing in Daskalakis et al, 2018 for Optimistic Adam does make it a little more compelling. However, I found the bound in Theorem 1 to be a little strange in that\n(1) it doesn't reduce to the AMSGrad bound when the gradient predictions are 0 and (2) it doesn't seem better than the AMSGrad or optimistic FTRL bounds. The authors claim to justify (2) by saying that the extra g_t - h_t term is O(\\sqrt{T}), but the whole appeal of adaptive algorithms is that the \\sqrt{T} terms are data-dependent. The empirical results also do not include error bars, which makes it hard to judge their significance. \n\nThere were also many grammatical errors and typos in the paper. \n\nOther comments and questions:\n1) Page 1: \"Their theoretical analysis are the regret analysis in online learning.\" Grammatical error.\n2) Page 2: \"The concern is that how to get good m_t\". Grammatical error.\n3) Page 3: \"as the future work\". Grammatical error.\n4) Page 3: \"Nestrov's method\". Typo. \n5) Page 4: \"with input consists of\". Grammatical error\n6) Page 4: \"Call Algorithm 3 with...\" What is the computational cost of this step? One of the main benefits of algorithms like AMSGrad is that they run in O(d) time with very mild constants. \n7) Page 4: \"For this extrapolation method to well well..., the gradient vectors at a specific time span is assumed to be captured by (5). If the gradient does not change significantly, this will be a mild condition.\" If the gradient doesn't change significantly, then choosing m_t = g_t would also work well, wouldn't it? Can you come up with examples of objectives for which this method makes sense? Even toy ones would strengthen this paper.\n8) Page 5: Equation (8). As discussed above, this bound doesn't appear to reduce to the AMSGrad bound for m_t = 0, which makes it a little unsatisfying. The fact that there is an extra expression that isn't in terms of the \"gradient prediction error\" that one has for optimistic FTRL also makes the bound a little strange.\n9) Page 7: \"The conduct Optimistic-AMSGrad with different values of r and observe similar performance\". You should mention that you show the performance for some of these different values in the appendix.\n10) Page 7: \"multi-classification problems\". Typo.\n11) Page 7: Figure 1. Without error bars, it's impossible to tell whether these results are meaningful. Moreover, it's strange to evaluate algorithms with online convex optimization guarantees on off-line non-convex problems.\n12) Page 7: \"widely studied and playing\". Grammatical error.\n13) Page 8: \"A potential directions\". Grammatical error.\n\n\n\n\n\n\n\n   \n\n \n\n\n\n\n\n\n\n\n\n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1484/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic Acceleration for Optimization", "abstract": "We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by  exploiting the predictability of gradients.  The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.", "keywords": ["optimization", "Adam", "AMSGrad"], "authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "pingli98@gmail.com"], "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "TL;DR": "We consider new variants of optimization algorithms for training deep nets.", "pdf": "/pdf/06002122d11de65c0db50d9107bc5112a302d160.pdf", "paperhash": "wang|optimistic_acceleration_for_optimization", "_bibtex": "@misc{\nwang2019optimistic,\ntitle={Optimistic Acceleration for Optimization},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2019},\nurl={https://openreview.net/forum?id=HkghV209tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1484/Official_Review", "cdate": 1542234220266, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkghV209tm", "replyto": "HkghV209tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1484/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335956138, "tmdate": 1552335956138, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1484/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}