{"notes": [{"id": "B1xm3RVtwB", "original": "HylNWPKuwB", "number": 1350, "cdate": 1569439402868, "ddate": null, "tcdate": 1569439402868, "tmdate": 1583912021201, "tddate": null, "forum": "B1xm3RVtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "r3T5OLjge", "original": null, "number": 1, "cdate": 1576798721203, "ddate": null, "tcdate": 1576798721203, "tmdate": 1576800915396, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The method presented, the simplified action decoder, is a clever way of addressing the influence of exploratory actions in multi-agent RL. It's shown to enable state of the art performance in Hanabi, an interesting and relatively novel cooperative AI challenge. It seems, however, that the method has wider applicability than that.\n\nAll reviewers agree that this is good and interesting work. Reviewer 2 had some issues with the presentation of the results and certain assumptions, but the authors responded so as to alleviate any concerns.\n\nThis paper should definitely be accepted, if possible as oral.\n\n \n\n\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724192, "tmdate": 1576800275797, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Decision"}}}, {"id": "HJxoIwDojS", "original": null, "number": 9, "cdate": 1573775186672, "ddate": null, "tcdate": 1573775186672, "tmdate": 1573775186672, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "Hkxfch4ssr", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Re^2: Response", "comment": "Once again, many thanks for the detailed feedback and also for the fast response. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "Hkxfch4ssr", "original": null, "number": 8, "cdate": 1573764233919, "ddate": null, "tcdate": 1573764233919, "tmdate": 1573764233919, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "Hkge_T7WjB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Re: Response", "comment": "Thanks for the fruitful comments and explanations. The presentation of the results with mean and max of 13 seeds seems much more solid. The performance gain solely due to greedy output is now clearly demonstrated.  I have updated the score."}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "Bkl2ONldFr", "original": null, "number": 2, "cdate": 1571452019867, "ddate": null, "tcdate": 1571452019867, "tmdate": 1573763917880, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper examines the problem of epsilon-greedy exploration in cooperative multi-agent reinforcement learning. Such exploration makes actions less informative to other agents because of the noise added to greedy optimal actions. The suggested solution is to consider two actions: one action is epsilon-greedy and it is passed to the environment, while another is fully greedy according to the agent\u2019s strategy, and it is shown to another agents. At test time these two actions are the same. This idea is applied to Hanabi game; the self-play regime is examined. The suggested method is claimed to show state-of-art results in 2-5 players Hanabi. \n\nThe Methods Section provides mathematical grounds of using Bayesian reasoning in ToM. It is shown how epsilon-greedy exploration leads to the blurring of the posterior and consequently making beliefs of other agents less informative. Thus, additional fully greedy input is motivated. However, there are still some vague parts in the description of methods used.\n1.\tIn Section 4.3 it is not clear what auxiliary supervised task is added to training. The information from Appendix should be moved to Section 4.3. Furthermore, some details on how training data for this task is collected and model is trained should be added. Also, how does this model affect the whole training procedure? \n2.\tThe description of the Q-learning in the first paragraph of Section 3.2 is too vague. In the first equation for Q(s,u), there is no u_t, u_{t\u2019}, so it\u2019s unclear why they pop out next. In the Bellman equation for Q(s, u) parentheses are omitted.\n\nThe advantage of the paper is that apart from the suggested additional greedy actions, several ingredients of the state-of-art approach (suggested in previous works) are tested separately, such as learning a joint Q-function and auxiliary supervised task. However, considering the results, it seems there is no clear winner for different number of players in Hanabi. From the Table 2, it is not clear whether the additional greedy input or the auxiliary task are beneficial for the best seed results. One of the three competing methods is the method named VDN in Tables 1 and 2. Is it the contribution of the paper or a previous work? If it is the contribution, the difference from the previous work should be clearly stated. If not, then it\u2019s not fully fair to say that the new approach shows state-of-art results.\n\nAlso, is it fair to say that s.e.m. of the best seed is 0.01? Comparing the results from Figure 3 and Table 2, one can see that the order of the winners is changing significantly, so 3 seeds are not enough to reliably estimate the performance of the best seed and s.e.m. should be higher.\n\nAnother comment on the Experiment 6.1: as far as BAD results are mentioned in the text, maybe they could be put on the plot? At least final training score as horizontal line, if the whole training curve is not available.\n\nMinor comments\n1.\tPage 1: Simply observing other agents are doing -> Simply observing what other agents are doing.\n2.\tPage 3: While our method are general, we restrict ourselves to turn based settings \u2013 odd phrase.\n3.\tPage 4: A combination these techniques -> A combination of these techniques.\n4.\t\\eps-greedy is written with a hyphen in some places of the paper and somewhere without.\n5.\tSection 5.3: Compute Requirements -> Computation Requirements.\n\nUPD: score updated.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842814874, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Reviewers"], "noninvitees": [], "tcdate": 1570237738655, "tmdate": 1575842814886, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review"}}}, {"id": "B1lP5nVwor", "original": null, "number": 7, "cdate": 1573502095302, "ddate": null, "tcdate": 1573502095302, "tmdate": 1573502095302, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "Hkge_T7WjB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Further comments / questions? ", "comment": "@Reviewer #3: \nAs mentioned above we have addressed the comments and concerns in the new version of the paper (uploaded on the 5th of Nov). Please let us know if there are any further questions. Many thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "B1lfiqEvjS", "original": null, "number": 6, "cdate": 1573501593980, "ddate": null, "tcdate": 1573501593980, "tmdate": 1573501593980, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "SJxjUgaEsB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Thank you for the timely response to our revision.", "comment": "@\u201dIt seems to me that implicit beliefs are still affected by the same kind of recursive explosion. SAD avoids this by learning a representation that retains much of the useful information but does not necessarily correspond to any particular Bayesian belief (and thereby is not really an implicit Bayesian belief).\"\n\nRe: Yes, this is a good point. We will see if there is a good way to get this idea across and update the paper accordingly for the next version. There is also an interesting question for future work here, to investigate what kind of belief SAD actually learns. It might turn out to be something close to a Bayesian belief, with the caveats around recursive reasoning (though at this point we simply do not know). \n\n@\u201d@2.b) .. I had not considered population based training from that perspective. Accordingly, I think that including scores for both the mean and the best seed, as is done in the revised version, makes the most sense.\u201d\n\nYes - this aspect of PBT is not normally mentioned and it does make a fair comparison with other methods more challenging. \n\n@Overall, the revised version addresses the concerns I had in my original review. I have revised my score correspondingly.\n\nPerfect, thanks again for the extremely constructive comments and the fast turn around. This is a great example of the review process working well. I wish it was more common. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "SygklZ5DYr", "original": null, "number": 1, "cdate": 1571426535195, "ddate": null, "tcdate": 1571426535195, "tmdate": 1573339241992, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper introduces a novel exploitation of the centralized training for decentralized execution regime for Dec-POMDPs with publicly/commonly known actions.  In particular, the paper augments independent value-based reinforcement learning by allowing each agent to announce the action it would have taken, had it acted greedily. This relaxation is consistent with decentralized execution, at which time agents always act greedily and no counterfactual announcement is required. The paper demonstrates the utility of this trick in Hanabi, where it achieves strong performance when combined with distributed Q-learning and an auxiliary task. The paper is largely well-written.\n\nI think that the trick introduced in this paper is a valuable contribution to the community. As the paper discusses, SAD is simpler and easier to implement than BAD, the algorithm with the most comparable ambitions with respect to scalability. Also, unlike BAD, it is model-free. I think that investigating lightweight alternatives to common/public knowledge based approaches, such as SAD, is an important research direction.\n\nThat being said, I have some issues with the presentation of the content in the paper. \n\n1. Section 4 is problematic. The entirety of the section is based on the assumption that agent a\u2019 observes the Markov state. The paper claims that this assumption is made for simplicity: \u201cwhere for simplicity we have assumed that agent a\u2019 uses a feed-forward policy and observes the Markov state of the environment.\u201d \n    a. First, this assumption is unmet, with few exceptions. Other agents do \n        NOT in general observe the Markov state. This is a very significant \n        part of why Dec-POMDPs are so difficult. Justifying SAD in a setting \n        that so radically departs from its use case is not informative. \n    b. Second, the claim made by the paper (that the assumption is made \n        for simplicity) is misleading. The assumption is made out of \n        necessity. The cost of neglecting public/common knowledge, as SAD \n        does, is that principled Bayesian reasoning is not possible. It is \n        important that the paper acknowledges this to counterbalance its \n        (valid) criticisms that public/common knowledge based approaches \n        are difficult to scale.\n2. That the experiments are expensive is understandable and a fully complete ablation study is not expected. But the results of these experiments should be presented clearly. \n    a. In both Table 1 and Table 2, presenting the \u201cbest seed\u201d is \n        not appropriate. The paper should report the mean or the median. \n        Given that only three seeds could be afforded, it is understandable \n        that the results would have high variance. \n    b. In Table 2, (if I am understanding the paper correctly) it is claimed \n        that the s.e.m. is less than or equal to 0.01 over a set of three seeds \n        for all of the experiments. But looking at Figure 4, this is clearly not \n        the case, especially in the four and five player settings. The intended \n        meaning should be clarified.\n    c. In Table 1, the paper should report the results for the baseline, VDN, \n        VDN with greedy input, and SAD separately, as is done in Table 2. \n        These are the main experimental results and merit space in the main \n        body of the paper. Moreover, there are a number of results that \n        deserve written attention.\n        i. It looks the baseline is very competitive with BAD in two player \n           Hanabi. This in itself is a very interesting finding and is worth \n           discussing.\n        ii. Looking at Table 2, it appears that VDN with GreedyInput \n            outperforms SAD in three of the four settings and quite \n            significantly in the five player setting. If this is also the case when \n            results are aggregated over the median or mean, it should be \n            discussed.\n3. The greedy input approach is specific to Dec-POMDPs in which actions are publicly/commonly available. This should at least be mentioned somewhere.\n\nOverall, I think the ideas and experimental findings in the paper are very interesting. However, as outlined above, there are a number of issues. At a high level, I think that the paper is too concerned with 1) justifying greedy input with Bayesian reasoning and 2) promoting state of the art results. The ideas and experimental findings are more than sufficient for strong contribution without these things.\n\nIn its current state, I feel that this work is a rejection. However, its issues are relatively easily amendable:\n1. For each algorithm and each setting, separately report median or mean results over the seeds, with uncertainty.\n2. Reallocate space to section 6.2 for a scientific discussion of the experimental results.\n3. Remove or qualify the arguments made in section 4.\n4. Mention that the greedy input approach is specific to Dec-POMDPs with publicly/commonly available actions.\n\nWere these issues addressed, my opinion would change.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842814874, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Reviewers"], "noninvitees": [], "tcdate": 1570237738655, "tmdate": 1575842814886, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review"}}}, {"id": "SJxjUgaEsB", "original": null, "number": 5, "cdate": 1573339219268, "ddate": null, "tcdate": 1573339219268, "tmdate": 1573339219268, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "BkxXJxNWoH", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Re: Response", "comment": "Thanks for your thoughtful response. Some further comments are provided below.\n\n@1) I have a small gripe with the statement \"implicit Bayesian beliefs are not affected by the same kind of recursive explosion.\" It seems to me that implicit beliefs are still affected by the same kind of recursive explosion. SAD avoids this by learning a representation that retains much of the useful information but does not necessarily correspond to any particular Bayesian belief (and thereby is not really an implicit Bayesian belief). In any case, I see what you are saying and think that the revised version of section 4 motivates the method nicely.\n\n@2.b) That is a fair point. I had not considered population based training from that perspective. Accordingly, I think that including scores for both the mean and the best seed, as is done in the revised version, makes the most sense.\n\nOverall, the revised version addresses the concerns I had in my original review. I have revised my score correspondingly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "BkxXJxNWoH", "original": null, "number": 4, "cdate": 1573105626951, "ddate": null, "tcdate": 1573105626951, "tmdate": 1573105626951, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "SygklZ5DYr", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Response ", "comment": "First of all, many thanks for an extremely thorough and insightful review. Below we address each of the individual requests.\n\n@1)\nAs suggested by R2, we have redone the derivations in Section 4. We now account for partial observability throughout. We had originally chosen to use an agent with access to the Markov state as a simplified setting to motivate the method, but as pointed out by R2 this does hide interesting technical and conceptual challenges. \n\nWe also added a paragraph that explains the issues around explicit Bayesian beliefs and the rationale for using common knowledge beliefs instead. We believe that in the current formulation the motivation is appropriate, since implicit Bayesian beliefs are not affected by the same kind of recursive explosion. \n\n@2)\nTo make these numbers meaningful we carried out further experimentations resulting in 13 seeds for each of the settings and algorithms. While we do not believe that there is a meaningful way to report averages and uncertainties across the original 3 seeds, we now report average results and best results (incl. uncertainties) for each of the settings and algorithms in the updated Table 1 and updated Table 2. We have updated the captions of both tables to clarify how the numbers were computed.\n\nWe also include training curves showing the average performance of our methods (including s.e.m.) across the training process.\n\n@2.b) \nThe numbers in Table 2 are obtained by first select the best model among 13 runs of each algorithm and then evaluating that model over 100K games with different seeds. The original numbers in the Hanabi challenge and BAD used population based training, effectively taking a max over 32 runs. Therefore, we do believe it is important for reproducibility purposes to also report the results for our best models.\n\n@2.c)\nWe have also added a more in depth scientific discussion of the results in Section 6, as suggested by R2. \nIn particular we point out that the auxiliary task only helps for 2 players and VDN matches the performance of SAD for 3 players.\n\n@3) \nIndeed, SAD requires all agents to be able to observe the last actions. This was mentioned in the later part of Section 3.1 (\u201c. Since we are interested in ToM, in our setting the observation function includes the last action of the acting agent, which is observed by all other agents at the next time step. We note that actions are commonly observable not only in board games but also in some real world multi-agent settings, such as autonomous driving.\u201d). We now moved this towards the beginning of Section 3.1 to emphasize the assumption.\n\nWe also added a paragraph to Section 4.3 which explores how the greedy-action input could be extended to settings where the last action is not directly observed, but can in principle be decoded by all agents through the environment dynamics and observation function. \n\nOther questions:\n@\"VDN with GreedyInput outperforms SAD\":\nYes - the auxiliary task only helps for the 2 player setting. As mentioned in our \u2018general comments\u2019 we have now restructured the paper to clearly mark the auxiliary task as optional, rather than being part of the core method. We also believe that it is an interesting question for future work to investigate why the auxiliary task only helps for 2 players. \n\nWe added some discussion on this point to Section 6.2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "Hkge_T7WjB", "original": null, "number": 3, "cdate": 1573104999547, "ddate": null, "tcdate": 1573104999547, "tmdate": 1573105109351, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "Bkl2ONldFr", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for an insightful review. \n\n@Auxiliary task in Section 4.3:\nThe intention behind SAD is to learn an implicit representation over the sufficient statistics given the trajectory. We explore the impact of an auxiliary loss function during training that encourages the neural network to learn the appropriate representations. You can think of the auxiliary task as a supervised loss term that gets optimized on top of the RL loss. Auxiliary tasks are fairly standard in deep RL, but we have added further details to the background section of the paper. \n\n@\"The description of Q-learning.. too vague\": \nWe have clarified the notation in this section. However, we also believe that Q-learning is a relatively well established method and expect that most readers will be broadly familiar with it. Furthermore, understanding Q-learning is not strictly required for understanding the rest of the paper. It simply happens to be the underlying learning algorithm of choice in our paper.\n\n@\"VDN & other ingredients of the methods\":\nWe do not claim VDN as part of our contribution. Notably we introduce VDN in the background section where we cite the original paper and mention it under \u2018best practice\u2019 in other parts of the paper.\n\nTo further clarify, we have updated Table 1 and Table 2, where we compare the mean scores and best scores of different ablations. These tables now include a \u201cBaseline\u201d label under IQL and VND. These tables not only clearly illustrate the effectiveness of the key idea of SAD, but also provide some insights into the relative improvements due to VDN, SAD and the auxiliary task.\n\nWe have also added more discussion in Section 6.2.\n\n@\u201cs.e.m. of the best seed is 0.01\u201d:\nYes, this is the uncertainty over true average score of our best model of our best run when evaluated for 100K games not over 3 runs. Also, as suggested by Reviewer #2, we have added a new table (Table 1 in the updated version) to show the mean and s.e.m. of our method & ablations over 13 different training runs.\n\n@\u201dExperiment 6.1.. BAD results\u201d:\nWe have now added a dashed line to indicate the final mean performance of BAD and will include actual training curves in the CRC. Note that for clarity and simplicity our version of the matrix game uses a tabular representation rather than  deep NN. Using tabular learning, SAD achieves perfect scores and even IQL exceeds the performance of BAD from the original paper (which used function approximation)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "SJeSyT7WsB", "original": null, "number": 2, "cdate": 1573104861039, "ddate": null, "tcdate": 1573104861039, "tmdate": 1573104861039, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "H1g0gKQlqr", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Response", "comment": "Many thanks for the encouraging feedback. Indeed, we are focussed on the self-play part of the challenge. Ad-hoc teamwork is an exciting future direction which could potentially also benefit from this kind of method. \n\nFor example it is imaginable that a large population of interesting policies could be trained using SAD, which would be a great starting point for ad-hoc teamplay. \n\nAs mentioned in the paper, we plan to open source the code and agents of our paper which hopefully will kick-start the investigation of ad-hoc teamplay in Hanabi.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "r1x4m2mWir", "original": null, "number": 1, "cdate": 1573104668006, "ddate": null, "tcdate": 1573104668006, "tmdate": 1573104668006, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment", "content": {"title": "Overall response", "comment": "We would like to thank all reviewers for the insightful reviews and comments. \n\nOne fairly consistent concern amongst the reviewers was the presentation of the results and the lack of clarity regarding the key contribution of the method compared to existing best practice. \n\nWe have taken this to heart and improved the presentation of the results with clearer focus on the novel aspect of SAD, i.e. the observation of the greedy-action during training. As requested by R2, we now also present mean scores including s.e.m. along with our best-in-class results across an increased number of runs (13 seeds). These results show that on average SAD significantly outperforms our ablations for 2,4 and 5 players and matches the performance of VDN for 3 players. We have also added a more granular discussion of the results to section 6.2\n\nTo clarify that the auxiliary task is optional compared to the core idea, we now call this combination \u201cSAD & AuxTask\u201d across the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xm3RVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1350/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1350/Authors|ICLR.cc/2020/Conference/Paper1350/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157332, "tmdate": 1576860539262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Authors", "ICLR.cc/2020/Conference/Paper1350/Reviewers", "ICLR.cc/2020/Conference/Paper1350/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Comment"}}}, {"id": "H1g0gKQlqr", "original": null, "number": 3, "cdate": 1571989750462, "ddate": null, "tcdate": 1571989750462, "tmdate": 1572972480184, "tddate": null, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents SAD (Simplified Action Decoder), a new method to address an issue in Centralized Training / Decentralized Control regimes: that exploratory actions during training can make those actions less informative for the agents who observe and attempt to learn from it. The method addresses this issue by allowing the agent to select two actions: one (the possibly exploratory action) is applied to the environment and the other (the greedy action) is presented to other agents as part of their observations for the next turn.\n\nThe authors use a distributed recurrent DQN architecture and apply the resulting agent to a toy problem and to the Hanabi Learning Environment. The authors claim that the method offers improvement over the Bayesian Action Decoder (BAD), that has been similarly applied to the same environments, being simpler, more sample efficient  and achieving overall higher scores, which is confirmed by their results: the agent outperforms BAD in 2-player Hanabi (where BAD was previously state-of-the-art) and the best scores out of any learning agent (although not as high as some non-learning agents such as WTFWThat in the 3-5 player versions. \n\nThe paper does not directly address the ad-hoc cooperation aspect of Hanabi, and it is unclear wheter the method could be used as-is for that problem, due to its reliance on centralized training. Nevertheless, the paper represents a relevant improvement to the self-play aspect of the game, and the core insight that the method leverages could conceivably be applied to minimize the noise introduced by exploration in other cooperative CT/DC problems. For this reason, I recommend the paper to be accepted.\n\nTypos/language issues:\n\nIntroduction: \u201cspend vast amounts of time coordinate\u201d -> coordinating\nSection 3.1: \u201cWhile our method are general\u201d -> methods\n\n\"accomplish a state of the art in Hanabi\"\nI see what you mean but this is a strange phrasing.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1350/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hengyuan@fb.com", "jakobfoerster@gmail.com"], "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning", "authors": ["Hengyuan Hu", "Jakob N Foerster"], "pdf": "/pdf/d81a17f86e6d742e583fdde70b183e75617f89e6.pdf", "TL;DR": "We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.", "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.", "code": "https://bit.ly/2mBJLyk", "keywords": ["multi-agent RL", "theory of mind"], "paperhash": "hu|simplified_action_decoder_for_deep_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nHu2020Simplified,\ntitle={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},\nauthor={Hengyuan Hu and Jakob N Foerster},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xm3RVtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e1a35290055e961a3f3294723ae2ce4210c706c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xm3RVtwB", "replyto": "B1xm3RVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842814874, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1350/Reviewers"], "noninvitees": [], "tcdate": 1570237738655, "tmdate": 1575842814886, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1350/-/Official_Review"}}}], "count": 14}