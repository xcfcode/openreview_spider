{"notes": [{"id": "Ske5UANYDB", "original": "B1xEWXvdwH", "number": 1149, "cdate": 1569439314195, "ddate": null, "tcdate": 1569439314195, "tmdate": 1577168284823, "tddate": null, "forum": "Ske5UANYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "98KiOKgzxI", "original": null, "number": 1, "cdate": 1576798715791, "ddate": null, "tcdate": 1576798715791, "tmdate": 1576800920742, "tddate": null, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Decision", "content": {"decision": "Reject", "comment": "The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714981, "tmdate": 1576800264791, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Decision"}}}, {"id": "SygzwYFssr", "original": null, "number": 5, "cdate": 1573783897601, "ddate": null, "tcdate": 1573783897601, "tmdate": 1573784567610, "tddate": null, "forum": "Ske5UANYDB", "replyto": "rklTS2CAKS", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment", "content": {"title": "To Reviewer 1&3", "comment": "The goal of this work is not to pursue the rate of convergence of interpolated-NN (which has been done in Belkin (2018) and Xing (2019). ) Our main theorem provides the EXACT MSE and Regret, rather than rate result up to unknown multiplicative constants. Therefore, it is not a comparable result to Belkin (2018) and Xing (2019), but a sharper improvement. Traditional kNN, interpolated-NN and the OWNN (Samworth, 2012) are all rate optimal, but our result enable us to rigorously compare the three on the multiplicative constant level (as described in Section 3.4).\n \nThe study of multiplicative constant, beyond the rate of convergence, provides more subtle insights. For example, Samworth (2012) claimed that the OWNN is the optimal weight choice by proving its multiplicative constant is the smallest; Locally-weighted NN (Cannings, et al, 2019) calculated the exact Regret to prove that pointwise local weighting scheme is better than a uniform choice of k.  More reference of studies on multiplicative constant is provided in the reference list.\n\nOur work, beyond the results of Belkin (2018) and Xing (2019), characterizes how the performance of interpolated-NN changes with respect to the interpolated level (i.e., gamma) and reveals a \u201cdouble descent\u201d phenomenon in NN algorithm which echoes many recent studies for over-parametrized models. This new insight is the most important message we would like to deliver to the readers, rather than the convergence rate results. Another insight is that a proper interpolation may be viewed as a form of regularization (in terms of slightly reducing bias) that improves the predictive power of optimal kNN. \n \n Reference:\n[1] Cannings, T. I., Berrett, T. B. and Samworth, R. J. (2019) Local nearest neighbour classification with applications to semi-supervised learning. Ann. Statist., to appear.\n[2] Cannings, T. I., Fan, Y. and Samworth, R. J. (2019) Classification with imperfect training labels. Biometrika, to appear.\n[3] Samworth, R. J. (2012) Optimal weighted nearest neighbour classifiers. Ann. Statist., 40, 2733-2763. DOI: 10.1214/12-AOS1049.\n[4] Sun, Will Wei, Xingye Qiao, and Guang Cheng. \"Stabilized nearest neighbor classifier and its statistical properties.\" Journal of the American Statistical Association 111.515 (2016): 1254-1265.\n[5] Duan, Jiexin, Xingye Qiao, and Guang Cheng. \"Distributed Nearest Neighbor Classification.\" arXiv preprint arXiv:1812.05005 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske5UANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1149/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1149/Authors|ICLR.cc/2020/Conference/Paper1149/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160486, "tmdate": 1576860533527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment"}}}, {"id": "HJxqBtYijS", "original": null, "number": 4, "cdate": 1573783874495, "ddate": null, "tcdate": 1573783874495, "tmdate": 1573784558236, "tddate": null, "forum": "Ske5UANYDB", "replyto": "rJeZ9nU15S", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment", "content": {"title": "To Reviewer 1&3", "comment": "The goal of this work is not to pursue the rate of convergence of interpolated-NN (which has been done in Belkin (2018) and Xing (2019). ) Our main theorem provides the EXACT MSE and Regret, rather than rate result up to unknown multiplicative constants. Therefore, it is not a comparable result to Belkin (2018) and Xing (2019), but a sharper improvement. Traditional kNN, interpolated-NN and the OWNN (Samworth, 2012) are all rate optimal, but our result enable us to rigorously compare the three on the multiplicative constant level (as described in Section 3.4).\n \nThe study of multiplicative constant, beyond the rate of convergence, provides more subtle insights. For example, Samworth (2012) claimed that the OWNN is the optimal weight choice by proving its multiplicative constant is the smallest; Locally-weighted NN (Cannings, et al, 2019) calculated the exact Regret to prove that pointwise local weighting scheme is better than a uniform choice of k.  More reference of studies on multiplicative constant is provided in the reference list.\n\nOur work, beyond the results of Belkin (2018) and Xing (2019), characterizes how the performance of interpolated-NN changes with respect to the interpolated level (i.e., gamma) and reveals a \u201cdouble descent\u201d phenomenon in NN algorithm which echoes many recent studies for over-parametrized models. This new insight is the most important message we would like to deliver to the readers, rather than the convergence rate results. Another insight is that a proper interpolation may be viewed as a form of regularization (in terms of slightly reducing bias) that improves the predictive power of optimal kNN. \n \n Reference:\n[1] Cannings, T. I., Berrett, T. B. and Samworth, R. J. (2019) Local nearest neighbour classification with applications to semi-supervised learning. Ann. Statist., to appear.\n[2] Cannings, T. I., Fan, Y. and Samworth, R. J. (2019) Classification with imperfect training labels. Biometrika, to appear.\n[3] Samworth, R. J. (2012) Optimal weighted nearest neighbour classifiers. Ann. Statist., 40, 2733-2763. DOI: 10.1214/12-AOS1049.\n[4] Sun, Will Wei, Xingye Qiao, and Guang Cheng. \"Stabilized nearest neighbor classifier and its statistical properties.\" Journal of the American Statistical Association 111.515 (2016): 1254-1265.\n[5] Duan, Jiexin, Xingye Qiao, and Guang Cheng. \"Distributed Nearest Neighbor Classification.\" arXiv preprint arXiv:1812.05005 (2018)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske5UANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1149/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1149/Authors|ICLR.cc/2020/Conference/Paper1149/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160486, "tmdate": 1576860533527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment"}}}, {"id": "B1eGR_toiS", "original": null, "number": 2, "cdate": 1573783754153, "ddate": null, "tcdate": 1573783754153, "tmdate": 1573784141785, "tddate": null, "forum": "Ske5UANYDB", "replyto": "rJeZ9nU15S", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment", "content": {"title": "To Reviewer 2&3", "comment": "Thanks for pointing out our writing problem. \n\nThe theorem 1 in the first submission shall serve as an important intermediate (i.e lemma) result. The corollaries derived from theorem 1 is actually our main point. We adjusted the displays of main theorems in a revision submission to enhance the readability. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske5UANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1149/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1149/Authors|ICLR.cc/2020/Conference/Paper1149/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160486, "tmdate": 1576860533527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment"}}}, {"id": "rkx3udtsiS", "original": null, "number": 1, "cdate": 1573783667706, "ddate": null, "tcdate": 1573783667706, "tmdate": 1573784135356, "tddate": null, "forum": "Ske5UANYDB", "replyto": "rJl9DU5MoH", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment", "content": {"title": "To Reviewer 2&3", "comment": "Thanks for pointing out our writing problem. \n\nThe theorem 1 in the first submission shall serve as an important intermediate (i.e lemma) result. The corollaries derived from theorem 1 is actually our main point. We adjusted the displays of main theorems in a revision submission to enhance the readability. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske5UANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1149/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1149/Authors|ICLR.cc/2020/Conference/Paper1149/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160486, "tmdate": 1576860533527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment"}}}, {"id": "B1eNXtKjiH", "original": null, "number": 3, "cdate": 1573783836415, "ddate": null, "tcdate": 1573783836415, "tmdate": 1573783847025, "tddate": null, "forum": "Ske5UANYDB", "replyto": "rklTS2CAKS", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment", "content": {"title": "To Reviewer 1", "comment": "In Belkin (2018), they technically only obtains a suboptimal bound for classification. And from their theorem, even if it is an optimal rate, it is only sufficient to state that \"interpolated-NN is not hurt by interpolation\". Our work, by proving that interpolated NN yields a smaller multiplicative constant, asserts that \"interpolation helps improve the performance\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske5UANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1149/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1149/Authors|ICLR.cc/2020/Conference/Paper1149/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160486, "tmdate": 1576860533527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Authors", "ICLR.cc/2020/Conference/Paper1149/Reviewers", "ICLR.cc/2020/Conference/Paper1149/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Comment"}}}, {"id": "rJl9DU5MoH", "original": null, "number": 3, "cdate": 1573197410377, "ddate": null, "tcdate": 1573197410377, "tmdate": 1573197410377, "tddate": null, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper studies theoretical perspective of double descent phenomenon for the interpolated K-NN classifier.\n\nThe paper is works in several interesting directions and gives theoretical reasoning to how interpolated K-NN could exhibit the double descent phenomenon. They give theoretical justifications albeit with strong assumptions.\n\nI think the paper is a good paper. However, I have concerns with the presentation quality of the paper. It is very tough to get through the paper till the end. \n\nIn my view, it would have been an Accept if the paper was well written.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576395341495, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Reviewers"], "noninvitees": [], "tcdate": 1570237741638, "tmdate": 1576395341515, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review"}}}, {"id": "rklTS2CAKS", "original": null, "number": 1, "cdate": 1571904581367, "ddate": null, "tcdate": 1571904581367, "tmdate": 1572972506390, "tddate": null, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper is about interpolation schemes in the particular case of the\nNearest Neighbor algorithm. The authors investigate the bene\ft, mainly\ntheoretical, of the proposed interpolation. They study minimax rates of\nthe proposed interpolated-NN for both classi\fcation and regression. The\nstatistical stability of the Interpolated-NN is adressed.\nThe paper is easy to understand and correctly written. Nevertheless,\nIt is a particular case of ( \\Over\ftting or perfect \ftting? Risk bounds for\nclassi\fcation and regression rules that interpolate\", Belkin, M., Hsu, D.,\nand Mitra, P. (2018a)) with an explicit interpolation schemes given by the\neuclidien distance power gamma. It appears as an application of the above\npaper which brings only few theoretical advantages and not enough to justify,\nalthough intuitive, the choice of these weights. Only few discussions and\nno comparison to others bounds (as those in the above paper) of the main\ntheorem are given. The paper is too much incremental from the papers of\nBelkin et al. (2018) and Xing et al. (2018) and the bene\fts of the proposed\ninterpolation are limited. Furthermore, the empirical performance of the\ninterpolate-NN is clearly not convincing and show no signi\fcant practical\nadvantages of the proposed method. As the goal of the paper is clearly\ntheoretical, the 'real data analysis' part is not necessary in my opinion."}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576395341495, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Reviewers"], "noninvitees": [], "tcdate": 1570237741638, "tmdate": 1576395341515, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review"}}}, {"id": "rJeZ9nU15S", "original": null, "number": 2, "cdate": 1571937417257, "ddate": null, "tcdate": 1571937417257, "tmdate": 1572972506343, "tddate": null, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018).\n\nUnder various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding).\n\nOverall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted.\n\nIn particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1149/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"], "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "pdf": "/pdf/c3afedd4587bde1e38b7383e9cc8783e4795cf5b.pdf", "paperhash": "xing|benefit_of_interpolation_in_nearest_neighbor_algorithms", "original_pdf": "/attachment/1333b0b06325ae291864fe2008f684df5e6b659f.pdf", "_bibtex": "@misc{\nxing2020benefit,\ntitle={Benefit of Interpolation in Nearest Neighbor Algorithms},\nauthor={Yue Xing and Qifan Song and Guang Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske5UANYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske5UANYDB", "replyto": "Ske5UANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576395341495, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1149/Reviewers"], "noninvitees": [], "tcdate": 1570237741638, "tmdate": 1576395341515, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1149/-/Official_Review"}}}], "count": 10}