{"notes": [{"id": "BylBfnRqFm", "original": "S1eGYFpqtm", "number": 1260, "cdate": 1538087948717, "ddate": null, "tcdate": 1538087948717, "tmdate": 1545355381678, "tddate": null, "forum": "BylBfnRqFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJla2RJ-eN", "original": null, "number": 1, "cdate": 1544777397474, "ddate": null, "tcdate": 1544777397474, "tmdate": 1545354527913, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Meta_Review", "content": {"metareview": "This paper proposes a meta-learning algorithm that performs gradient-based adaptation (similar to MAML) on a lower dimensional embedding. The paper is generally well-written, and the reviewers generally agree that it has nice conceptual properties. The method also draws similarities to LEO. The main weakness of the paper is with regard to the strength of the experimental results. In a future version of the paper, we encourage the authors to improve the paper by introducing more complex domains or adding experiments that explicitly take advantage of the accessibility of the task embedding.\nWithout such experiments that are more convincing, I do not think the paper meets the bar for acceptance at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1260/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352902946, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352902946}}}, {"id": "HyeF5PNARQ", "original": null, "number": 10, "cdate": 1543550865435, "ddate": null, "tcdate": 1543550865435, "tmdate": 1543550865435, "tddate": null, "forum": "BylBfnRqFm", "replyto": "SyeqYsIyCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Thank you for clarifications, still have concerns", "comment": "Thank you for the detailed replies, particularly regarding how CAML relates to prior work.\n\nI still have concerns about novelty and strength of experiments. Rusu et al. learn an embedding that can also be interpreted as a task encoding, and it\u2019s not clear from the results whether the choice of parameter regression (LEO) versus feature fusion (CAML) matters much. While CAML is admirably simpler, the experiments don\u2019t convincingly make the case that this simple change gives significant benefits. \nExperiments on more complex few-shot learning problems might illuminate these benefits. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "Byl9ch4FR7", "original": null, "number": 9, "cdate": 1543224466479, "ddate": null, "tcdate": 1543224466479, "tmdate": 1543224475307, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Rebuttal", "comment": "\nWe thank the reviewers for their time to evaluate our paper, and their valuable feedback.\n\nWe uploaded a revision of the paper. Besides editorial changes, we added a section of practical tips to the Appendix as suggested by Reviewer 3. We updated our related work section to better reflect how CAML differs from existing work, in particular MT-Nets [1] and LEO [2], in response to Reviewer 4.\n\nWe summarise these differences here:\n\nMT-Nets [1] learn which parameters to update in MAML [3]. To this end, they learn an M-net, which is a mask (sampled from a learned probability distribution for each new task), and determines which parameters are updated in the inner loop. In the outer loop, all parameters are updated. Hence the task-specific and shared-task parameters are not disjoint as in CAML, and no task embedding emerges. Additionally, they learn a T-net, which learns the update direction and step size of the parameters, which makes MT-Nets robust to the inner loop learning rate. CAML adjusts the inner loop learning rate automatically via the magnitude of the gradient, and can handle a wider range of initial learning rates compared to MT-Nets. This is possible because the parameter sets are disjoint, and the context parameters are inputs to the model (i.e., gradients do not get backpropagated further).\n\nLEO [2] learns an embedding which generates the weights of the last layer (for classification) or the entire parameter vector of the network (in the general case). This embedding is computed via an embedding network and a relation network. At test time, the gradient steps are done in this embedding space (with additional fine-tuning of the generated network for the Mini-Imagenet experiments). In contrast, we learn an embedding that modulates a fixed network, and we do so via backpropagation through that same network. Hence our method has fewer hyperparameters / architecture choices that have to be made.\n\nSome reviewers raised concerns about the experimental evaluation of CAML. We deliberately chose to show that CAML works well on a broad range of problems, instead of focusing on a single setting.\n\nCAML can be scaled up to achieve better performance on the Mini-Imagenet benchmark, but we see this as an orthogonal problem and a question of compute and hyperparameter search. After the reviewers\u2019 feedback, we ran an additional experiment using a Resnet-18 model to test how feasible it is to scale CAML up. We tested the same hyperparameters that we used for the CNN-based experiments. The implementation is easy: we used the ResNet readily available in PyTorch and added the context parameters / FiLM layer in-between the second and third residual block, together with a few lines of extra functionality (like resetting the context parameters to zero). For MAML / MT-Nest, we would have to manually access all network parameters to set up the computation graph. We get 52.16% (+/- 0.32) and 66.33% (+/- 0.26) accuracy on the 1 and 5 shot problem respectively, which is higher than our best CNN-based results in the paper and outperforms MT-Nets. Again, these results are achieved by adjusting only 100 context parameters at test time. This indicates that CAML can indeed be scaled up further, and we leave it to future work to try larger resnets and do a full hyperparameter search. (Note that these numbers might not be directly comparable to the SOTA scores of LEO [2], who get their final score by training on the training and validation set.)\n\nOverall, we believe our paper is interesting to the ICLR community, since compared to the popular algorithm MAML [3] and several papers that build on it (like Meta-SGD [4] and MT-Net [1]s) we explicitly learn task embeddings, which are separated from the network that is shared across tasks. We interpret the inner loop of meta-learning as a task identification step, and show that we only need to adapt a few parameters at test time, instead of the entire network. Our paper is the first to show that this is possible for a wide range of problems using a simple backpropagation operation on contextual input parameters. Compared to MAML, our method has the advantage of being robust to overfitting. It is also easier to implement (we do not need to access all weights of the network manually), needs fewer memory writes, and can be useful for distributed machine learning systems.\n\n[1] \u201cGradient-based meta-learning with learned layerwise metric and subspace\u201d Lee et al. (2018)\n[2] \u201cMeta-Learning with Latent Embedding Optimisation\u201d Rusu et al. (2018)\n[3] \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\u201d Finn et al. (2017)\n[4] \u201cMeta-SGD: Learning to learn quickly for few shot learning\u201d Li et al. (2017)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "SyeqYsIyCQ", "original": null, "number": 8, "cdate": 1542577025832, "ddate": null, "tcdate": 1542577025832, "tmdate": 1542577025832, "tddate": null, "forum": "BylBfnRqFm", "replyto": "SJxzujUyCX", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Reply (Part 2)", "comment": "\u201cI am confused by the comparison between adapting input parameters versus subsets of nodes at each layer or entire layers for the sinusoid regression task. Adapting subsets of nodes at each layer roughly corresponds to Lee and Choi, yet the reported numbers are quite different?\u201d\n- Yes, adapting subsets of nodes at each layer corresponds roughly to Lee an Choi, except that we choose which subset of nodes are adapted for this ablation study of alternative partitioning schemes. The MSE reported by Lee and Choi is higher than ours, and also their MSE scores for MAML are higher than in the original paper. We assume that this is due to differences in the implementation.\n\n\u201cIn Table 3, which CAML is a fair comparison (in terms of network size and architecture) to MT-NET?\u201d\n- MT-Nets use the same architecture as MAML (32 filters), so the expressiveness during the forward pass is the same as our smallest (32 filter) architecture. MT-Net additionally learns parameters to generate T and M, which for this network are around 4,000 parameters. To outperform MT-Nets, we need to scale up the number of filters in our convolutions - trading off implementation complexity (higher in MT-Nets) for a larger network (necessary for CAML) and having a separate task embedding (given in CAML)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "SJxzujUyCX", "original": null, "number": 7, "cdate": 1542577002325, "ddate": null, "tcdate": 1542577002325, "tmdate": 1542577002325, "tddate": null, "forum": "BylBfnRqFm", "replyto": "S1eoNpUQaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Reply (Part 1)", "comment": "Thank you for the time to evaluate our paper, and the thorough review. We address your raised points below.\n\n\u201cRusu et al (LEO) optimize a context vector, which is used to generate model parameters. Reducing the generative model to a point estimate, how is this different from generating the FiLM parameters as a function of context as done in CAML?\u201d\n- The outputs of the FiLM layer can be seen as parameters of the network, but this differs from the approach in LEO as follows. The FiLM outputs scale and shift entire feature maps in convolutional layers (but have no influence on the FiLM layer parameters, or convolution parameters, themselves). LEO generates the weights of the last layer of the neural network (for classification) or the entire parameter vector of the network (in the general case). We view the context parameter in CAML as modulating the activations in a fixed network, whereas LEO generates the parameters themselves.\n\n\u201cLee and Choi (MT-nets) propose a general formulation for learning which model parameters to adapt. CAML is simpler in that the model parameters to adapt are chosen beforehand to be inputs.\u201d\n- Lee and Choi learn which parameters to adapt, but they do not consider having additional inputs that can be adapted. Additionally, MT-Nets do not partition the network parameters into disjoint sets (a new mask is drawn for each new task, from a learned probability distribution; all parameters are updated in the outer loop). We introduce context parameters and show that this is sufficient compared to the more complex approach of MT-Nets, and that by this we can learn a task embedding via backpropagation.\n\n\u201cSnell et al. / Oreshkin et al. are prototype-based methods infer context via a neural network rather than optimizing for it.\u201d\n- Both these methods are specific to few-shot classification, whereas CAML can also be applied to regression and reinforcement learning. Methods for few-shot classification often rely on learning class embeddings, whereas we directly learn an embedding for the current task (i.e., all classes) which modulates the classification network.\n\n\u201cCAML is robust to the adaptation learning rate, but isn\u2019t this true of any scheme that separates meta-learned and adapted parameters into disjoint sets? (e.g. also true of Lee and Choi?) \u201c\n- We don\u2019t think that\u2019s necessarily true. If the task-specific parameters depend on shared parameters in earlier layers of the network, regulating the learning rate via the magnitude of the gradient would have an influence on the outer loop update (since those gradients would be backpropagated further). This can be countered to some extent, but might still be less flexible than CAML, where the context parameters are leaves of the computation graph. \nMT-Nets (Lee and Choi) learn an M and a T net. The M-net is responsible for selecting which parameters to update, and is sampled for each new task (from a learned probability distribution). If we understand correctly, all parameters are updated in the outer loop - hence the parameter sets are not entirely disjoint. The T-net learns the update direction and step size of the parameters, which is why MT-Nets are robust to the inner loop learning rate. For the regression task we show that CAML is robust within a learning rate range of 10^(-7) to 100 (see updated Figure 5), whereas MT-Nets cannot successfully scale to a learning rate of 10 (see their paper).\n\n\u201cThe visualizations of the context parameters are nice, but interpreting much higher dimensional context vectors (which would be necessary for harder tasks) is more difficult, so I\u2019m not sure what to take away from this?\u201d\n- We show the visualisations in the 2-D context to confirm that we indeed learn a task embedding via backpropagation, which corresponds to (and is smooth with respect to) the true task differences. This illustrates that the inner loop of meta-learning algorithms can be seen as a task embedding step, and that we successfully do this via backpropagation. For higher dimensional context vectors, visualisation methods such as t-SNE could be used. Few-shot classification is a special case, since the embedding is not disentangled (with respect to the different classes). This makes visualisation for separate classes difficult. If a disentangled representation is of interest, it might be possible to train CAML with this in mind, e.g. by updating only a part of the context vector per class. \n\n[continued below]"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "HklgSqLJA7", "original": null, "number": 6, "cdate": 1542576695886, "ddate": null, "tcdate": 1542576695886, "tmdate": 1542576695886, "tddate": null, "forum": "BylBfnRqFm", "replyto": "B1l_Q1cgTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time and review of our paper. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "rkgcmqIkR7", "original": null, "number": 5, "cdate": 1542576673793, "ddate": null, "tcdate": 1542576673793, "tmdate": 1542576673793, "tddate": null, "forum": "BylBfnRqFm", "replyto": "B1xX4-Eq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time to read and evaluate our paper. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "ByeMWqLy0X", "original": null, "number": 4, "cdate": 1542576634070, "ddate": null, "tcdate": 1542576634070, "tmdate": 1542576634070, "tddate": null, "forum": "BylBfnRqFm", "replyto": "H1g5yNXKhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your review, and the time to assess our paper. \n\nWe added a section with practical tips for implementation and hyperparameter selection to the Appendix. In general, we think choosing the hyperparameters in CAML can be guided by domain knowledge: since we separate the task-specific and shared parameters, the choice of both is more intuitive for the human designer than in MAML. The context parameters of CAML can be added on top of any network architecture, and they are updated only via backpropagation (unlike, e.g., LEO [1] which requires a separate network to encode the training data). Additionally, our method is not sensitive to network architecture (not prone to overfit like MAML), the inner loop learning rate, and can handle overparameterisation of the context parameters (as shown in the regression experiment, see updated Table 1).\n\n[1] \u201cMeta-Learning with Latent Embedding Optimisation\u201d Rusu et al. (2018)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}, {"id": "B1xX4-Eq3m", "original": null, "number": 2, "cdate": 1541189930531, "ddate": null, "tcdate": 1541189930531, "tmdate": 1542437826709, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "content": {"title": "Good paper in general", "review": "They are proposing a meta-learning method inspired by previous method, MAML. Their idea is separating the parameters in to two groups of context and shared parameters. The context parameters are learned through back-propagation of inner-loop and represents embedding for individual task. Shared-parameters on the other hand are shared between all tasks, and are learned in the outer-loop. \n\nCompared to MAML, the pros of their method is as follows:\n- Less sensitive to learning rate: thus more robust to hyper parameters.\n- Does not prone to overfit as MAML does.\n- It is easier to implement, more efficient from memory view point.\n\nCons in general,\n-  In Mini-ImgeNet data set, although they are beating MAML, but they are not able to beat other competitors in 5-shot classification.\n- They could have explored applying their method to deep residual networks and compare their results.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "cdate": 1542234269001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335907993, "tmdate": 1552335907993, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eoNpUQaQ", "original": null, "number": 4, "cdate": 1541791026797, "ddate": null, "tcdate": 1541791026797, "tmdate": 1541791026797, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "content": {"title": "incremental idea, weak experimental evidence", "review": "Summary\nCAML is a gradient-based meta-learning method closely related to MAML. It divides model parameters into disjoint sets of task-specific parameters $\\phi$ which are adapted to each task and task-independent parameters $\\theta$ with are meta-learned across tasks. $\\phi$ are then interpreted as an embedding and fed as input to the model (parameterized by $\\theta$). Experiments demonstrate that this approach performs on par with MAML while adapting far fewer parameters. An additional benefit is that this approach is less sensitive to the adaptation learning rate and is easier to implement and faster to compute.\n\nStrengths\nWhile not really explained in the paper, this work connects gradient-based to embedding-based meta-learning approaches. Adaptation is via gradient descent, but the adapted parameters are then re-interpreted as an embedding.\nThe method has the potential to perform on par with MAML while being simpler and faster.\nThe paper is well-written.\n\nWeaknesses\nThe field of meta-learning variants is crowded, and this paper struggles to carve out its novelty. \nRusu et al (LEO) optimize a context vector, which is used to generate model parameters. Reducing the generative model to a point estimate, how is this different from generating the FiLM parameters as a function of context as done in CAML? \nLee and Choi (MT-nets) propose a general formulation for learning which model parameters to adapt. CAML is simpler in that the model parameters to adapt are chosen beforehand to be inputs. \nSnell et al. / Oreshkin et al. are prototype-based methods infer context via a neural network rather than optimizing for it.\n\nIn this context, CAML appears to be yet another point drawn from the convex hull of choices already explored in episodic meta-learning (these choices can be broadly grouped into task encoding and conditional inference). The paper must then rest on its experimental results, which are at present unconvincing.\n\nOn the whole, the experimental results seem weak and analysis results largely uninformative. The method is benchmarked on the toy tasks of sinusoid regression and a 2-D point mass, as well as mini-ImageNet few-shot classification. The sinusoid and point mass navigation are toy and compared only to MAML, so it is hard to draw conclusions from those experiments. For mini-ImageNet, while CAML outperforms MAML, it seems that the pertinent comparison is with MT-NET (which CAML does not outperform) and LEO (missing fair comparison?).\n\nQuestions regarding experiments\n - CAML is robust to the adaptation learning rate, but isn\u2019t this true of any scheme that separates meta-learned and adapted parameters into disjoint sets? (e.g. also true of Lee and Choi?) \n - The visualizations of the context parameters are nice, but interpreting much higher dimensional context vectors (which would be necessary for harder tasks) is more difficult, so I\u2019m not sure what to take away from this? It\u2019s very unsurprising that the 2-D context vector encodes x and y position in the point mass experiment, for example. \n - I am confused by the comparison between adapting input parameters versus subsets of nodes at each layer or entire layers for the sinusoid regression task. Adapting subsets of nodes at each layer roughly corresponds to Lee and Choi, yet the reported numbers are quite different? \n - In Table 3, which CAML is a fair comparison (in terms of network size and architecture) to MT-NET? \n\nEditorial Notes\nIntro paragraph 3: fine-tuning image classification features for a semantic segmentation task is not a good example of task independent parameters, since fine-tuning end-to-end gives significant improvements.\nRelated work paragraph 2: Initializing context parameters to zero is not the only difference with Rei et al (2015), and seems a strange thing to highlight?\nTables 1 and 2: state what the task is in the caption\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "cdate": 1542234269001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335907993, "tmdate": 1552335907993, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1l_Q1cgTX", "original": null, "number": 3, "cdate": 1541607200411, "ddate": null, "tcdate": 1541607200411, "tmdate": 1541607200411, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "content": {"title": "interesting idea, falling short on experimental evidence", "review": "The paper talks about Meta-Learning where some of the parameters of the models adapt to the new task (context parameters) and rest of the parameters are kept fixed (shared parameters). The authors propose a more general approach and show how CAML works for supervised learning and reinforcement learning paradigms. \n\nquality - The paper is written with good mathematical notation and in general is of high quality. The references to related work and motivation of the problem is good.\n\nclarity - While the paper is clear in many parts, it can be a lot better. Specifically it is unclear why authors chose regression, classification and RL to make their point without landing either one of them fully confidently.\n\noriginality - the idea is good and general enough to be applicable for many situations. While variants of this idea have been tried with fine-tuning for transferred learning I still think this work can classify as original and novel.\n\nsignificance of this work - The significance of meta learning is good but based on the experiments authors conducted I am worried it has little significance. \n\npros and cons - Overall, while I am supportive of a weak accept because of the idea and it's broad applicability I feel authors should maybe chose one of the tasks and show much more value in using the CAML framework. The three tasks they chose are all toy problems and do not instill confidence in the validity of CAML for either large scale experiments or in setups where distribution is changing but tasks remain same. It would be great to strengthen the paper with a more cleaner story on the experiments section and show CAML achieves SOA convincingly. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "cdate": 1542234269001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335907993, "tmdate": 1552335907993, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1g5yNXKhX", "original": null, "number": 1, "cdate": 1541120994342, "ddate": null, "tcdate": 1541120994342, "tmdate": 1541533287588, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "content": {"title": "An interesting meta-learning algorithm", "review": "CAML seems an interesting meta-learning algorithm. I like the idea that the context parameters are used to modulate the whole network during the inner loop of meta-learning, while the rest of the network parameters are adaped in the outer loopand shared across tasks. Also, it is good to see that CAML is competitive with on few shot CNNs.\n\nThe paper is very well presented. Experiments are reasonably solid.\n\nIf I understood correctly, although CAML has achieved better accuracy it seems CAML still requires a decent amount of parameter/network structure optimisation. Would be good if the paper has a section talking about practical tricks of how to find the best CAML hyperparameter quickly.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Review", "cdate": 1542234269001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335907993, "tmdate": 1552335907993, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Skxk5LPL27", "original": null, "number": 1, "cdate": 1540941446610, "ddate": null, "tcdate": 1540941446610, "tmdate": 1541266463551, "tddate": null, "forum": "BylBfnRqFm", "replyto": "BylBfnRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Public_Comment", "content": {"comment": "A very nice read, the work is very admirable.\n\nI was thinking if there was a way to split the context parameters used in learning the policy into two separate streams; a linear and nonlinear stream. Something like in nonlinear control theory, where the linear stream would stabilize the local dynamics, and the nonlinear stream would handle global control. \nIn a reinforcement learning setup, this would bring the benefits of both linear and nonlinear policies, which would, in turn, lead to greater generalization and more scalability.", "title": "Making the model more scalable"}, "signatures": ["~Ali_Janalizadeh_Choobbasti1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["~Ali_Janalizadeh_Choobbasti1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311640568, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BylBfnRqFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311640568}}}, {"id": "HyxEtN6Ohm", "original": null, "number": 1, "cdate": 1541096572242, "ddate": null, "tcdate": 1541096572242, "tmdate": 1541096572242, "tddate": null, "forum": "BylBfnRqFm", "replyto": "Skxk5LPL27", "invitation": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "content": {"title": "Two-Stream Architecture", "comment": "\nThank you for the kind feedback and your comment.\n\nYes, splitting up the context parameters and the network architecture up into separate streams like that is possible with CAML: given the two forward streams, there would also be two backward streams through which the gradient gets propagated, for the respective parts of the context parameters. It would be interesting to see the network can make use of the opportunity to propagate information through the separate streams to speed learning."}, "signatures": ["ICLR.cc/2019/Conference/Paper1260/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAML: Fast Context Adaptation via Meta-Learning", "abstract": "We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.", "keywords": [], "authorids": ["lmzintgraf@gmail.com", "kyriacos@latentlogic.com", "vitaly.kurin@eng.ox.ac.uk", "katja.hofmann@microsoft.com", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "pdf": "/pdf/9fcceb7731f65a45e495855fd647620779a85bae.pdf", "paperhash": "zintgraf|caml_fast_context_adaptation_via_metalearning", "_bibtex": "@misc{\nzintgraf2019caml,\ntitle={{CAML}: Fast Context Adaptation via Meta-Learning},\nauthor={Luisa M Zintgraf and Kyriacos Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBfnRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1260/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618194, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBfnRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1260/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1260/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1260/Authors|ICLR.cc/2019/Conference/Paper1260/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1260/Reviewers", "ICLR.cc/2019/Conference/Paper1260/Authors", "ICLR.cc/2019/Conference/Paper1260/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618194}}}], "count": 15}