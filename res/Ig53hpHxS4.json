{"notes": [{"id": "Ig53hpHxS4", "original": "WD2RPSXFvrR", "number": 1218, "cdate": 1601308136366, "ddate": null, "tcdate": 1601308136366, "tmdate": 1616045602097, "tddate": null, "forum": "Ig53hpHxS4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5TcSYg43cRc", "original": null, "number": 1, "cdate": 1610040392408, "ddate": null, "tcdate": 1610040392408, "tmdate": 1610473986888, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes an autoregressive flow-based network, Flowtron, for TTS with style transfer. It integrates the Tacotron architecture with the flow-based generative model.  Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles. All reviewers consider the work interesting.  There are concerns raised on technical details which mostly have been cleared by the authors' rebuttal. The exposition also has been greatly improved based on the reviewers' suggestions and questions.  Overall, this is an interesting paper and I would recommend acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040392394, "tmdate": 1610473986871, "id": "ICLR.cc/2021/Conference/Paper1218/-/Decision"}}}, {"id": "RHN50cYZFEb", "original": null, "number": 1, "cdate": 1603853286718, "ddate": null, "tcdate": 1603853286718, "tmdate": 1606212050418, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review", "content": {"title": "Accept. The idea is good and experiments are good. There are some concerns about the clarity of the paper but those can be worked on.", "review": "### Summary\n\nThis paper introduces a novel application of normalizing flows to speech synthesis, allowing direct optimization of spectrogram log-likelihoods which results in more natural variation at inference compared to L1/L2 losses that model the mean. This setup also allows more control over non-textual information and interpolation between samples and styles.\n\n\n### Recommendation\n\n**Accept**\nThe idea is good and experiments are good. There are some concerns about the clarity of the paper but those can be worked on.\n\n### Positives\n\n1. The paper introduces a novel architecture and demonstrates improved output variation and more controllability, which is an important current issue for TTS research.\n1. Many experiments investigating controllability are described, as well as some ablation studies for the model architecture in the appendix.\n\n\n### Negatives\n\n1. **Figure 1**: Please provide more informative captions. In the text, timbre and F0 are mentioned but it is not clear how that is relevant in the images. It is not clear what the colors mean in 1b, and if 1b is supposed to show a separation between male and female speakers the colors make it worse. Finally, are the points in 1b cluster centers or a single random sample per speaker?\n\n1. **Figure 2**: Would it be possible to make it clearer that there is an autoregressive dependency in the Attention and Decoder blocks due to the LSTM cell memory? The way the figure is currently drawn makes it seem as if each attention/decoder block can be computed in parallel from only the inputs from the previous flow iteration. \n\n1. In section 2.2 NN and f are described as acting on the latent variable frames $z_t$, but Figure 2 applies the NN and flow on the mel spectrogram frames x in order to produce z. Similarly, there is text saying \"we take the mel-spectrograms and pass them through the inverse steps of flow\" and \"Inference, [...], is simply a matter of sampling z values [...] and running them through the network\" but Figure 2 marks the block as \"Step of Flow\" rather than \"Inverse Step of Flow\". It would be good to smooth out the consistency.\n\n1. More discussion about the end of sequence prediction would be appreciated. As the entire sequence of z must be used for inference, I assume there is some constant max inference length z used to obtain a final x, and the end of sequence prediction only happens to the final x rather than at each step of the flow? How well does this model adapt to inference samples that are much longer than any of the inputs seen during training?\n\n1. **3.3.2 Interpolation Between Samples**: I don't understand why there was a need to sample z to find z_h and z_s. Is it not possible to take z_h and z_s from a random training example for the speaker? Or does that mean there is a correlation between the latent space z and the __content__ that is being spoken? If the latter, I would like to see more discussion on that.\n\n1. **Table 2**: I assume bold means closest to ground truth? It's really hard to know how to interpret this table and I don't think it supports saying that FTA Posterior is more effective. FTA did not capture the increase in pitch mean from expressive -> high pitch, nor the decrease in std from expressive -> surprised. A better visualization may be to express this table as a bar graph (3 separate groups of 3 bars each) and conclude FTA Posterior is able to produce much more variation than Tacotron 2 GST?\n\n### Misc\n\n#### Abstract\n\n1. \"varation\" -> \"variation\"\n1. spell out IAF\n1. \"We provide results on speech variation\" etc. sounds weak. eg. \"Flowtron produces output with far more natural variation compared with Tacotron 2 and enables interpolation over time between samples and style transfer between seen and unseen speakers in ways that are either impossible or inefficient to achieve with prior works.\"\n\n#### 1 Introduction\n\n1. \"Their assumption is that variable-length embeddings are not robust to text and speaker perturbations\" citation needed\n1. \"Flowtron learns an invertible function that maps a distribution over mel-spectrograms to a latent z-space parameterized by a spherical Gaussian.\" mention IAF and cite Kingma et al., 2016 here instead of the previous paragraph. \n1. \"Finally, although VAEs and GANs provide a latent embedding that can be manipulated, they may be difficult to train, are limited to approximate latent variable prediction\" While the IAF approach allows for direct optimization of log-likelihood, the latent variable encoding part is still approximate, just like in a VAE. The original Kingma paper even states that it is an approximate posterior. \n\n#### 2.2 Invertible Transformations\n\n1. $f^{-1}$ wouldn't be applied to $z_t$. Maybe $f^{-1}(x_t)$ or $f^{-1}\\left(f(z_t)\\right)$.\n\n#### 3.1. Training Setup\n\n1. \"progressively adding steps of flow on the last step of flow has learned to attend to text\" on->once?\n\n#### 3.4.2 Seen Speaker with Unseen Style\n\n1. \"Flowtron succeeds in transferring not only the somber timbre, the low F0 and the long pauses\nassociated with the narrative style\" -> \"not only the somber timbre, but also\"\n\n#### Appendix\n\n1. IAF is known to be quite inefficient. Is there any noticeable impacts on training or inference time? Or is this not a problem due to only using 2 layers of flow? It would be great if the ablation study in the appendix regarding layers of flow also covers this, but this is quite optional.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123844, "tmdate": 1606915770334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review"}}}, {"id": "JIuaxt0ybTV", "original": null, "number": 6, "cdate": 1606212033436, "ddate": null, "tcdate": 1606212033436, "tmdate": 1606212033436, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "xfPFkxP5mb4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment", "content": {"title": "Re: Response to AnonReviewer3 11/17/2020", "comment": "Thank you for the reply. The updated paper looks great, the only nitpick I have is about the whitespace on top of page 6 and whether there is a mismatch between caption and subgraph in Figure 5. As the concerns have been addressed, I am updating the rating to 9 as I believe this paper is top 15%."}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ig53hpHxS4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1218/Authors|ICLR.cc/2021/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862264, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment"}}}, {"id": "A1PuJbEXCCQ", "original": null, "number": 5, "cdate": 1605771407181, "ddate": null, "tcdate": 1605771407181, "tmdate": 1605771407181, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Cx0PlzMI9Ng", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 11/18/2020 ", "comment": "Thank you for the question you have raised regarding how a user can explicitly control speech features or attributes in order to generate speech with the desired non-textual information.  We have added a new subsection, 3.5 Interpolation between styles (Prior and Posterior), in which we describe how a user can control the speaking style at inference time by simply selecting a sample with the target style and then performing posterior inference to sample from the region in latent space correlated with the target style. In the paper we show figures that illustrate a gradual transition in spectral profile from one style to another. Please listen to our new samples as well to aurally perceive the control over speaking style. We hope this new section clarifies the question you raised. \n\nGenerally speaking, the main two points in our paper are representation learning (Normalizing Flows learn an invertible map from data space to a latent space) and Bayesian inference (posterior sampling).  The mapping from data to latent space is such that acoustic properties present in data space will be correlated with regions in the latent space. By sampling from these regions in the latent space we can produce speech with the acoustic characteristics correlated with it. \n\nAfter finding sample(s) with the speech characteristics of interest, we use Bayesian inference to find and sample from regions in the latent space that contain such speech characteristics. Lambda described in Equation 9 is the parameter we use to navigate between regions in the latent space.\n\nRegarding embedding the samples on the web page, we have updated the web page to include all samples mentioned in the paper and extra samples. We hope it is now easier for you and other readers to navigate and compare our samples."}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ig53hpHxS4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1218/Authors|ICLR.cc/2021/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862264, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment"}}}, {"id": "xfPFkxP5mb4", "original": null, "number": 4, "cdate": 1605664513492, "ddate": null, "tcdate": 1605664513492, "tmdate": 1605664513492, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "RHN50cYZFEb", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 11/17/2020 ", "comment": "Thank you for your kind words and thoughtful review. It brings us joy when a reviewer with high confidence asks us to change our text from \"We provide results on speech variation\" to \"Flowtron produces output with far more natural variation compared with Tacotron 2 and enables interpolation over time between samples and style transfer between seen and unseen speakers in ways that are either impossible or inefficient to achieve with prior works.\"\n\nWe have altered the paper to address the points you have brought in your review. Please take a look at the modifications in the paper and the clarifications below and let us know what remains to be addressed.\n\nRegarding Figure 1:\nWe altered Figure\u2019s 1 caption to clarify. Different colors represent different speakers, + is used for male speakers and dot is used for female speakers.\n\nRegarding Figure 2: \nThank you for the suggestion, we added the LSTM dependencies to Figure 2. \n\nRegarding step of flow and inverse step of flow nomenclature.\nThank you for your calling our attention to this. We consolidated the nomenclature for consistency.\n\nRegarding end of sequence prediction:\nThe step of flow closest to the latent variable has a gating mechanism that prunes extra frames from the z-value provided to the model during inference. The resulting z-length after pruning is then fixed for the subsequent steps of flow. Hence, not necessarily the entire sequence of z will be used during inference, e. g. imagine passing 10 second worth of random z-values to synthesize the phrase \u201cThis is good.\u201d We\u2019ve clarified this in the paper.\n\nRegarding adapting to samples that are longer than inputs during training:\nSimilar to Tacotron and other models with attention, the attention mechanism in Flowtron can fail on sentences that are longer than the inputs seen during training. With loss of generality, more robust attention mechanisms such as \u201cLocation-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis\u201d by Battenberg. et al. can be used to replace Flowtron's attention mechanism. Although not in the scope of this paper, in the near future we will add such modifications to our repo.\n\nRegarding interpolation between samples:\nTheoretically there should be no correlation between the conditions, i.e. text input, and the z-values. Empirically, our posterior inference samples show evidence that we can sample the posterior while using text different from the evidence used to compute the posterior. Hence, we should be able to do what you suggest. Simply put: we did not think about it... \n\nRegarding Table 2:\nBold means closes to the reference sample providing the style. Hence, the goal of samples generated with style transfer is to match pitch summary statistics from reference samples providing the style. We've edited the text and the caption for Table 2 to clarify. We included a couple of cases in which Flowtron does not perform as well as Tacotron 2 GST to show possible areas for improvement.\n\nRegarding Variable Length Embeddings: \nWe've added the citation to \"Towards End-to-End prosody\" by RJ Skerry-Ryan et al. and quote the paper here:  \u201cIn our experiments, variable-length prosody embeddings are able to generalize to very long utterances; however, compared to fixed-length embeddings, variable-length embeddings are not as robust to text and speaker perturbations likely because they encode a stronger timing signal. Therefore, this paper focuses on fixed-length embeddings.\u201d\n\nRegarding Inverse Autoregressive Flows and Approximate Posteriors:\nThe setup in the \u201cImproved Variational Inference with IAF\u201d uses an IAF to compute an approximate posterior q(z|x) for variational inference. Exact latent-variable inference and log-likelihood evaluation is a property of normalizing flows models in general, including IAFs. Also, we will clarify that even though our inspiration to write Flowtron came from IAFs and the Parallel Wavenet paper, Flowtron is an Autoregressive flow, also known as Masked Autoregressive Flow.\n\nRegarding the appendix and model performance:\nIn our setup there are temporal dependencies in time both in training and inference due to the LSTM states, like you pointed out. Flowtron with 2 steps of flow is 7 times faster than real-time while Tacotron 2 is 10 times faster than real-time."}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ig53hpHxS4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1218/Authors|ICLR.cc/2021/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862264, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment"}}}, {"id": "4Qe9vj1rUem", "original": null, "number": 3, "cdate": 1605663108820, "ddate": null, "tcdate": 1605663108820, "tmdate": 1605663108820, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "iiaBa54FPW_", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 11/17/2020", "comment": "Thank you for the questions you have raised regarding our paper. We appreciate it that you found our paper to be a good read and hope that our recent modifications based on your comments makes the paper more clear to readers. \nPlease take a look at the modifications in the paper and the clarifications below and let us know what remains to be addressed.\n\nRegarding assumptions made by models that use fixed-length embeddings:\nIt\u2019s stated in \"Towards End-to-End prosody\" by R. J. Skerry-Ryan et al. that in their experiments \u201cvariable-length prosody embeddings are able to generalize to very long utterances; however, compared to fixed-length embeddings, variable-length embeddings are not as robust to text and speaker perturbations likely because they encode a stronger timing signal. Therefore, this paper focuses on fixed-length embeddings.\u201d. \nIn addition, the same paper by R. J. Skerry-Ryan et al also states that \u201cThe use of a fixed-length prosody embedding poses an obvious scaling bottleneck, preventing the extension of this approach to longer utterances.\u201d\n\nRegarding dimensions in Figure 1: \nWe have updated the paper to clarify that these are TSNE Dimensions.\n\nRegarding understanding coupling layers:\nPlease take a look at \"NICE: Non-Linear Independent Component Estimation\" by Dinh. et al. for a thorough explanation of coupling layers.\n\nRegarding where does z_t come from and what is the 0-vector:\nNN(z_{1:t-1}, text, speaker) will provide scaling and bias terms to affine transform z_{2:t}. \nWe use a 0-vector to obtain scaling and bias terms to affine transform z_1. \nPlease look at the updated text in 2.2 and let us know if it is clearer now.\n\nRegarding definition of s and the complete log-likelihood criterion:\nIn Eq. 5 we define s as the scaling term that is produced by NN. In Sec. 2.1 we define the latent distributions used in our paper. \nWhen z ~ N(z; 0, I), the log-likelihood log p_{\\theta}(z) is equivalent to computing the likelihood of a spherical gaussian.\n\nRegarding \"stating at this point ... how posterior inference will be used to generate actual samples.\":\nIn the original paper this information was provided in Appendix A.2, wherein we provide details on how posterior inference is used to generate samples, while Algorithm 1 describes an algorithm for posterior inference.\n\nRegarding \"The inference method has not yet been explained.\"\nThe inference method is defined in \"Section 2.4 Inference\", right before 2.5. Can you please clarify?\n\nRegarding what happens if you use more Flows:\nPlease look at Appendix 1.4.1 where we evaluate the effect of using 2, 3 and 6 steps of flow. \nThrough internal evaluations we decided to only compute MOS on models with 2 steps of flow given that they sounded similar to models with more steps of flow while being considerably faster.\n\nRegarding metric for assessment of the naturalness:\nThe Mean Opinion Scores implicitly include variety and naturalness given that the Flowtron samples used for collecting MOS are computed with Flowtron using sigma = 0.5, which has more variance than Tacotron 2. Please look at Figure 4 and compare samples from Flowtron sigma = 1.0 with samples from Tacotron 2.\n\nRegarding quantitative measures for interpolation samples and seen speaker with unseen style:\nThese are qualitative results. Please listen and compare the samples from each model.\n\nRegarding Table 2 and interpretation:\nWe\u2019ve edited the text to emphasize that the goal is to match the pitch summary statistics from the reference providing the style as closely as possible. We've also described that the unit for pitch is MIDI number.\n\nRegarding Gaussian Mixture Visualizations\nVisualizations are provided in the Appendix. We added information to the text to clarify.\n\nRegarding references:\nWe updated the references. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ig53hpHxS4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1218/Authors|ICLR.cc/2021/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862264, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment"}}}, {"id": "u-_GnHVLyW9", "original": null, "number": 2, "cdate": 1605661940537, "ddate": null, "tcdate": 1605661940537, "tmdate": 1605661940537, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "xbEC1xYTsid", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 11/17/2020", "comment": "Thank you for the points you have raised in your review. Also thank you for your kind words regarding our work, experiments and samples. We modified our paper to address them. Please take a look at the modifications in the paper and the clarifications below and let us know what remains to be addressed.\n\nRegarding \"how precisely Tacotron 2 is modified to incorporate the flow network\":\nSpecifically, in addition to comparing Tacotron 2 and Flowtron in the body of the paper, we have provided images (Figure 2 for Flowtron and Figure 6 in the Appendix A.4.4 for Tacotron 2) that can be used to compare Flowtron's architecture with Tacotron\u2019s architecture. Finally we\u2019ve also provided model summaries for Flowtron and Tacotron 2 in Appendix A7. \nRegarding what is mean by \"it is also possible to reverse the ordering of the mel-spectrogram frames\"\nIn most TTS models, including Tacotron, mel-spectrogram generation is done from past frames to future frames. \nIn Flowtron, by reversing the ordering of mel-spectrogram frames on certain steps of flow, we allow the model to also generate mel-frames in the reverse order. We have observed, and described it in A.4.2 that bidirectional processing improves speech quality when compared to unidirectional processing.\n\nRegarding how Table 2 is calculated:\nTable 2 provides pitch summary statistics computed over 5 phrases and 10 takes each. The goal is to produce samples whose pitch summary statistics (mean and standard deviation) are closer to the reference. Please let us know if the new caption for Table 2 makes the point clearer.\n\nRegarding the purpose of synthesizing samples with Tacotron 2 while varying the pre-net dropout probabilities:\nIn the Tacotron 2 paper, the authors state that \u201cIn order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder\u201d. Increasing or decreasing the dropout probability during inference can be seen as a mechanism to increase or decrease variation in Tacotron 2 at inference time."}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ig53hpHxS4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1218/Authors|ICLR.cc/2021/Conference/Paper1218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862264, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Comment"}}}, {"id": "Cx0PlzMI9Ng", "original": null, "number": 2, "cdate": 1603894947292, "ddate": null, "tcdate": 1603894947292, "tmdate": 1605024499767, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review", "content": {"title": "Proposes a flow-based TTS with high expressivity and style variation.", "review": "This paper presents a text-to-speech synthesis system, called Flowtron which uses a normalizing flow to generate a sequence of mel-spectrogram frames. The difference between the proposed Flowtron and the previously prosed flow-based methods is, the authors argue as the main contributions, its ability to produce more diverse and expressive speech samples of specific speech attributes by sampling from the latent space. Evaluation is done using the two public datasets, and a number of experiments are performed to show that the proposed method not only achieves a good MOS score in general but also generates speech samples with variation, including the style transfer.\n\nOverall, the paper is fairly well organized with figures and tables in appropriate positions. However, it is sometimes difficult to follow, sections 3.3.2 - 3.5.2 in particular, because there are too many short subsections without supporting figures or tables. Furthermore, it is also hard to navigate through the folders and files provided in the supplementary material to listen to the speech samples.\n\nEvaluation is well done. The proposed method yields a comparable MOS score to a reference (Tacotron 2) in terms of speech quality. However, focus in experiments is on the many features that are not feasible in other approaches, which include variation control such as pitch or duration, interpolation between samples, and style transfer. But it still remains unclear that how a user can explicitly control these speech features or attributes in order to generate speech with the desired non-textual information, which requires much more than simple and random variation in pitch and/or duration.\n\nMinor comments: \n- There are several typos (even in the abstract) and grammatical errors throughout the text.\n- Again, it is quite difficult to find and compare the speech samples that correspond to features describe in the text. It would be better to embed the samples on the website for easier navigation.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123844, "tmdate": 1606915770334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review"}}}, {"id": "iiaBa54FPW_", "original": null, "number": 3, "cdate": 1603941570255, "ddate": null, "tcdate": 1603941570255, "tmdate": 1605024499700, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review", "content": {"title": "Very interesting paper that I found rather hard to follow.", "review": "This paper describes a specific approach to the hot topic of  auto-regressive Flow-based speech synthesis. The fundamentals of coupled flow layers are exposed pretty well, but I quickly got lost in the details of the work. I have a number of specific comments to address that. Nonetheless, I find the paper is still a good (if confusing) read addressing a very interesting area.\n\n\"Their assumption is that variable-length embeddings are not robust to text and speaker perturbations, which we show not to be the case\" -- in what specific aspects do those works you refer make that assumption?\n\nRe: Figure 1, \"Figure 1 shows that acoustic characteristics like timbre and F0 correlate with portions of the z-space of Flowtron models trained without speaker embeddings.\" Label the axes of Figure 1?\n\nEq. (7): I can guess, but can you confirm the meaning of the circle-with-dot operator?\n\n\"Normalizing flows are typically constructed using coupling layers\": I nearly, but don't completely, understand the concept of the coupling layers. AIUI, the trick is that they are forward layers with two components that are inverses of each other.  I guess that Eqs. 5-7 in fact define just that. However, how is z_t computed? f(z_t) and its inverse defined in terms of s_t and b_t, which are functions of z_{1:t-1}. So where does z_t come from?\n\n\"we define z1 as a prependded 0 vector\" : (only one \"d\" in prepended). For z1, what is the 0 vector component prepended to? Or is z1 itself a zero vector?\n\n\"To evaluate the likelihood, we take the mel-spectrograms and pass them through the inverse steps of\nflow conditioned on the text and optional speaker ids, adding the corresponding log |s| penalties, and\nevaluate the result based on the Gaussian likelihoods.\"\n\nFor clarity, I suggest:\n- Define s.\n- Define the complete log-likelihood criterion.\n\n\"Our text encoder modifies Tacotron\u2019s 2\": spelling.\n\n\"Figure 1 shows evidence that speech several characteristics present in mel-spectrograms\": check grammar.\n\n\"Section 2.5, Posterior Inference\": I suggest stating at this point how posterior inference will be used to generate actual samples.\n\nSection 3.2: The inference method has not yet been explained.\n\nTable 1: what happens if you use more Flows? Do the MOS scores improve?\n\n\"3.3.1 SPEECH VARIATION\": i agree that FlowTron seems to be able to generate more varied speech, but this is only a good thing if the samples are still natural. I don't see a metric or assessment of the naturalness of those more-varied samples (maybe i missed it).\n\nSection 3.3.2, \"Our different speaker interpolation samples show that Flowtron is able to gradually\nand smoothly morph one voice into another.\" Where is this measured/reported in the paper?\n\n\"3.4.2 SEEN SPEAKER WITH UNSEEN STYLE... We compare samples generated with Flowtron and Tacotron 2 GST to evaluate their ability to emulate a speaking style unseen during training of a speaker seen during training.\": where is this comparison shown in the paper?\n\n\nTable 2: what are the units?\n\n\"Table 2 shows that while the samples generated with Tacotron 2 GST are not able to emulate\nthe high-pitched style from RAVDESS, Flowtron is able to make Sally sound high-pitched as in the\n\u201csurprised\" style.\" I find it a bit hard to interpret Table 2. Is the goal to match the reference mean and standard deviation as closely possible?\n\n\"3.5 SAMPLING THE GAUSSIAN MIXTURE In this last section we provide visualizations ...\": where do you provide it?\n\nre: 3.5.1 VISUALIZING ASSIGNMENTS and 3.5.2 TRANSLATING DIMENSIONS, same comment as before, where are the results provided?\n\n\n\nReferences:\n\"Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo\nKang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-time neural\ntext-to-speech. arXiv preprint arXiv:1702.07825, 2017b.\" --> remove superfluous \"O\" for first author? Or add missing \"O\" in the previous reference?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123844, "tmdate": 1606915770334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review"}}}, {"id": "xbEC1xYTsid", "original": null, "number": 4, "cdate": 1604370365180, "ddate": null, "tcdate": 1604370365180, "tmdate": 1605024499625, "tddate": null, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "invitation": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review", "content": {"title": "FLOWTRON", "review": "FLOWTRON\n\n1.\nThe authors present Flowttron, an autoregressive text-to-speech network that is\na merging of the well-established Tacotron architecture along with flow-based\ngenerative models. The benefits to Flowtron are its simplicity to train and\ncomparable MOS to other state of the art models. Moreover, the model is able to\nproduce various styles of speech depending on how the latent variables are\nsampled, as well style transfer between seen and unseen speakers.\n\n2.\nThe introduction and discussion in the paper are strong and review the\nnecessary background information. The experiments are comprehensive and\nconvincing. Although there are artifacts in the audio samples (repeated\nphrases, distortion), the variation of speech is impressive.\n\nThe overall description of the model lacks technical thoroughness. The\ndescription of the flow networks is sufficient. However, how precisely\ntacotron2 is modified to incorporate the flow network is severely lacking.\nFigure 2 is confusing. I also don't understand what the authors mean by \"it is\nalso possible to reverse the ordering of the mel-spectrogram frames in time\nwithout loss of generality\"\n\nIt is not clear to me how the values in Table 2 are calculated.\n\nOn page 4, the second paragraph, what is the purpose of synthesizing samples\nwith Tacotron2 while varying the pre-net dropout probabilities.\n\n3/4. \nI would recommend an accept, given the authors can address the major technical\nproblems with the paper. Specifically, the description of the system needs to\nbe much more clear. The work is interesting, the experiments are sufficient,\nand the audio samples are convincing.\n\n\n6. There are many typos that should be fixed.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1218/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1218/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "authorids": ["~Rafael_Valle1", "~Kevin_J._Shih1", "rprenger@nvidia.com", "~Bryan_Catanzaro1"], "authors": ["Rafael Valle", "Kevin J. Shih", "Ryan Prenger", "Bryan Catanzaro"], "keywords": ["Text to speech synthesis", "normalizing flows", "deep learning"], "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.", "one-sentence_summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "valle|flowtron_an_autoregressive_flowbased_generative_network_for_texttospeech_synthesis", "supplementary_material": "/attachment/892261d3f4575982c87b250c4aafc1bff95871f9.zip", "pdf": "/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvalle2021flowtron,\ntitle={Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},\nauthor={Rafael Valle and Kevin J. Shih and Ryan Prenger and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ig53hpHxS4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ig53hpHxS4", "replyto": "Ig53hpHxS4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123844, "tmdate": 1606915770334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1218/-/Official_Review"}}}], "count": 11}