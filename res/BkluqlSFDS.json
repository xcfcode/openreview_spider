{"notes": [{"id": "BkluqlSFDS", "original": "BJlt0RgtwS", "number": 2476, "cdate": 1569439887649, "ddate": null, "tcdate": 1569439887649, "tmdate": 1583912027539, "tddate": null, "forum": "BkluqlSFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "3-joY3kfZ3", "original": null, "number": 10, "cdate": 1580015038136, "ddate": null, "tcdate": 1580015038136, "tmdate": 1580154437682, "tddate": null, "forum": "BkluqlSFDS", "replyto": "pGEb1YoxcR", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "Relation of OT fusion to prior work", "comment": "Hi Martin & Sidak,\n\nThank you for bringing your work to our attention. We will add OT fusion to the discussion in the camera ready version of our paper. We also wish to mention that there is a prior work [1] that proposed a similar \"align and average\" framework that we build on in FedMA. We would appreciate if you can add discussion of [1] and FedMA to your paper.\n\nBest regards,\nAuthors\n\n[1] M. Yurochkin et al, Bayesian Nonparametric Federated Learning of Neural Networks, ICML 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "pGEb1YoxcR", "original": null, "number": 3, "cdate": 1578370903872, "ddate": null, "tcdate": 1578370903872, "tmdate": 1578370903872, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment", "content": {"title": "relation to 'Model Fusion via Optimal Transport'", "comment": "dear authors\ncongrats on your nicely written paper on this cool application! \nwe have a similar&simultaneous approach in the NeurIPS 2019 optimal transport workshop https://arxiv.org/abs/1910.05653 , where we also considered federated learning as an application. we also added some additional baselines for the standalone merging/fusion operator.\nwould you mind adding it to the discussion for your camera ready version?\nthanks in advance!\nmartin & sidak"}, "signatures": ["~Martin_Jaggi1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Martin_Jaggi1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179738, "tmdate": 1576860565542, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment"}}}, {"id": "aVDw9i8E6I", "original": null, "number": 1, "cdate": 1576798749920, "ddate": null, "tcdate": 1576798749920, "tmdate": 1576800885945, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "The authors presented a Federate Learning algorithm which constructs the global model layer-wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms\n\nThis paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720718, "tmdate": 1576800271603, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Decision"}}}, {"id": "rkgJ17ZpFH", "original": null, "number": 1, "cdate": 1571783382953, "ddate": null, "tcdate": 1571783382953, "tmdate": 1574653042984, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Post Rebuttal Summary\n---------------------------------\nI have nudged my score up to an \"Accept\", based on my comments to the rebuttal below. I hope the authors continue to improve the readability of Sec. 2.1\n\nReview Summary\n--------------\nOverall I think this is almost above the bar to be accepted, and I could be persuaded with a strong rebuttal.  The strengths here are the extensive experiments and the easy-to-implement method. The primary weakness of this paper is that it is a \"straightfoward\" way to extend the BBP-MAP method to CNNs and RNNs, so the methodological novelty is weak relative to the BBP-MAP past work (Yurochkin et al. ICML 2019). Other technical weaknesses limit the ability to use this method on clients with diverse class distributions, which will be common in real deployments.\n\nPaper Summary\n-------------\nThis paper addresses the problem of federated learning, where J separate \"clients\" with disjoint datasets each train a neural network model for a supervised problem, and then try to aggregate all J individual client models into one \"global model\" in a coherent way. The natural problem is that due to hidden units being permutable within one network, naively taking parameter averages across two client models will lead to bad accuracy without first coming up with a consistent ordering of the units in each layer. \n\nPrevious work (Yurochkin et al. ICML 2019) has developed a Bayesian nonparametric model based on the Beta-Bernoulli Process (BBP) for the case of federated learning of multi-layer perceptrons. However, the extension to convolutional layers or recurrent layers has yet to be solved, which is the focus of this paper. \n\nThis paper's algorithm (Federated Matched Averaging (FedMA), see Alg 1), proceeds by iteratively stepping thru the CNN or RNN layer by layer greedily from input to output. At each layer, we first solve a BBP-MAP optimization (bipartite matching using a BBP maximum a-posteriori objective as cost function, a subprocedure taken direclty from Yurochkin et al.). This obtains a consistent low-cost permutation for each client model. Then, the global model weights for that layer is the average of the aligned client weights. After the current layer update, each client keeps training, keeping all layers up to the current frozen but revising later layers. This layer-by-layer training can be applied to both CNNs and RNNs.\n\nThe proposed approach is compared to FedAvg and FedProx on MNIST and CIFAR image classification tasks with CNNs, and Shakespeare text classification tasks with RNNs. Later experiments explore the effect of communication efficiency (MB transfered between client and master), effect of local training epochs, handling biased class distributions, and interpretabilty.\n\n\n\n\nNovelty & Significance\n-----------------------\nSolving federated learning problems is of increasing practical importance, and certainly trying to do so for CNNs and RNNs (more than just large MLPs) is important. So I like where the paper is going.\n\nAlthough the method is \"new\", it is more or less a straightforward extension of work by Yurochkin et al. (ICML 2019) to CNNs and RNNs. If you read the last few sentences of Yurochkin et al., you'll see \"Finally, it is of interest to extend our model-ing framework to other architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The permutation invariance necessitating matching inference also arises in CNNs since any permutation of the filters results in the same output, however additional bookkeeping is needed due to the pooling operations.\" I view this paper as a well-executed implementation of this \"bookkeeping\". Certainly not trivial, but to some readers perhaps not clearly \"above the bar\" for a top conference like ICLR.\n\n\nTechnical Concerns\n------------------\n\n## Concern 1: Client models will not always be alignable after permutation\n\nMy first concern is that there will not always be a one-to-one permutation of the neurons learned by two client models with different class distributions. Given fixed capacity at each layer, some clients may learn a filter for \"horse hooves\" (esp. if horse images are common to that client), while other clients may learn a filter for \"snake skin\" (if snakes are more common to that client). I wonder if we can quantify how well the aligned filters match in practice, and if there is any benefit to revising the alignment to allow some client-specific customizations (e.g. by having the global model can learn more units than the client model). \n\n## Concern 2: Use of the BBP-MAP subprocedure poorly motivated\n\nThe paper prioritizes a clean and easy-to-implement algorithm to resolve practical alignment issues between client CNN and RNN models. However, I was a bit underwhelmed that the BBP-MAP solution used by Yurochkin et al. was treated as a black-box subprocedure without much justification. I could see 2 preferable alternatives to the current use of BBP-MAP. Either a simpler approach using Eq. 2 with a squared error cost and the Munkres algorithm to solve bipartitite matching to obtain the permutation (which seems more in spirit of the rest of the paper). Or, a more sophisticated probabilistic approach (taking a Bayesian hierarchical model from Yurochkin et al. seriously and forming the estimated global weights from a weighted sums that includes both the clients (weighted by dataset size) and the assumed prior). As it is, I feel the BBP-MAP subprocedure in the current Algorithm 1 is poorly motivated for the task at hand.\n\n\n\nExperimental Evaluation\n-----------------------\n\nOverall the experiments were extensive and demonstrated several apparent advantages (reduced need to transfer large memory during communication, etc.). \n\n\nMinor Presentation Concerns\n---------------------\nBefore Eq. 2, you should introduce the \"\\theta\" notation\n\nI'm a bit confused about how \"FedMA\" differs from \"FedMA with communication\", even after reading Sec. 2.3. How exactly are communicate costs kept down? What are you sending from master to client at beginning of every \"round\" if not the full global model (all weights of the CNN)?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576079913902, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Reviewers"], "noninvitees": [], "tcdate": 1570237722294, "tmdate": 1576079913915, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review"}}}, {"id": "HJlE_doTKH", "original": null, "number": 2, "cdate": 1571825772363, "ddate": null, "tcdate": 1571825772363, "tmdate": 1574365654878, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Edit: Thanks for the thorough and responsive rebuttal! I'm particularly happy to see the additional background on BBP-MAP and the baselines you've added for handling data bias. You've comprehensively addressed my questions and I think this paper should be accepted.\n\nOriginal review:\n\nThe authors extend the recently proposed Probabilistic Federated Neural Matching (PFNM) algorithm of Yurochkin et al. ( 2019) to more kinds of neural networks, show that it isn't as effective for larger models as it is for LeNet-sized ones, and propose enhancements that lead to a state-of-the-art approach they call FedMA. I'm convinced that this represents a meaningful advance in federated learning, although the paper could use some tightening up, and the experiments are somewhat limited.\n\nSome feedback:\n\n- I'd like to see a little bit more description of BBP-MAP, as even though it's not one of the components of the algorithm you directly modify it's still the underlying mathematical primitive. How far is it from having the same effect that the \"best possible\" permutation would? How is it able to allow the number of neurons in the federated model to grow relative to the size of the client models?\n\n- Can you include the \"entire data\" baseline in more of the figures/plots (especially Figure 2)?\n\n- The models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application. Since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, on-device ML) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).\n\n- The section that demonstrates how your model addresses skewed data domains is fascinating! That's one area in which your experiments are directly relevant to federated learning in practice, and it's a rapidly growing area of research in itself (e.g. in its relationship to causal learning that Leon Bottou has recently been exploring). Exploring this further could make for a whole separate paper. In the mean time, though, is there some kind of equivalent of the \"entire data\" baseline that would represent e.g. the best known technique for taking into account skewed domains outside the federated context?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576079913902, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Reviewers"], "noninvitees": [], "tcdate": 1570237722294, "tmdate": 1576079913915, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review"}}}, {"id": "HJgiqtU9sr", "original": null, "number": 5, "cdate": 1573706131441, "ddate": null, "tcdate": 1573706131441, "tmdate": 1573706131441, "tddate": null, "forum": "BkluqlSFDS", "replyto": "rke7FaeciB", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "We thank Reviewer 1", "comment": "We really appreciate your careful review and encouraging response to our rebuttal. We will elaborate on the connection to optimal transport that we mentioned in our response in the final version of the paper. We will also more smoothly integrate the section on \u201cSolving matched averaging\u201d (including the presentation of BBP-MAP) into the flow of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "rke7FaeciB", "original": null, "number": 4, "cdate": 1573682554544, "ddate": null, "tcdate": 1573682554544, "tmdate": 1573682554544, "tddate": null, "forum": "BkluqlSFDS", "replyto": "B1xIeO3viS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "Thanks for rebuttal!", "comment": "I appreciate the careful reply and revisions of Sec 2.1 especially. I think the novelty is above-the-bar (esp if connections to optimal transport could be made more explicit). My technical concern #1 is resolved thanks to a clear response from authors that the method handles the concern. My concern #2 (about motivation for BBP-MAP) is mostly resolved but for some presentation issues (I think perhaps another few editing passes to make sure the presentation of BBP-MAP in Sec 2.1 is clear and well-motivated might be needed, because it still feels like the current simplicity-focused justification is a bit weak). \n\nI'm happy to accept the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "BJxZaDnwsH", "original": null, "number": 2, "cdate": 1573533625295, "ddate": null, "tcdate": 1573533625295, "tmdate": 1573533953353, "tddate": null, "forum": "BkluqlSFDS", "replyto": "HJlE_doTKH", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the feedback and provide answers to the raised concerns below.\n\n>>> Additional details on BBP-MAP and adaptive global model size\n\nWe extended Section 2.1 with a general procedure for matched averaging with adaptive global model size, where BBP-MAP can be seen as a specific way to carry out the optimization. The idea behind the adaptive global model size is to introduce additional columns in the cost matrix to avoid \"poor\" matches while penalizing the model size. Please see eq. (3) and the surrounding discussion.\nRegarding the \"best possible\" permutation, for two neural nets Hungarian algorithm will find the global optima, but when averaging multiple neural nets, the iterative procedure we describe is only guaranteed to find a local optima.\n\n>>> Can you include the \"entire data\" baseline in more of the figures/plots (especially Figure 2)?\n\nWe included the \u201centire data\u201d baseline in all figures (except Figure 3, where it is not applicable).\n\n>>> The models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application.\n\nWe agree that the scale of our current experiments is lagging behind the size of the real world federated learning applications. However, as federated learning is a relatively new problem in the literature, we believe it is also an issue in the majority of the prior results in this area (e.g. several papers studied federated learning simulated with CIFAR-10 as we did, but we are not aware of any paper with ImageNet experiments). We note that there in an optimism that FedMA will benefit from larger datasets: our \"Data efficieny\" experiment (Figure 5) shows that FedMA utilizes additional data more efficiently in comparison to other federated learning approaches. Further, the \"Effect  of local training epochs\" (Figure 3) experiment shows that FedMA is the only method truly benefiting from well-trained local models, which might be important as we move onto larger datasets.\n\nFor the future work, we are exploring potential large scale datasets representative of the practical federated learning and planning to consider federated learning experiments simulated from ImageNet.\n\n>>> Is there some kind of equivalent of the \"entire data\" baseline that would represent e.g. the best known technique for taking into account skewed domains outside the federated context?\n\nWe agree that this is a valuable and natural question. We added three additional baselines to Figure 4 in the updated manuscript and updated the corresponding \"Handling data bias\" paragraph in the draft.\n\ni) Vanilla VGG training over CIFAR-10. For this baseline (No Bias), we simply conduct normal model training over the entire CIFAR-10 dataset without any grayscaling. This baseline is not a realistic solution to the data bias and is simply added for the reference.\n\nii) One way to alleviate data bias is to selectively collect more data to debias the dataset. In the context of our experiment, this means getting more colored images for grayscale dominated classes and more grayscale images for color dominated classes. To simulate this scenario we simply do a full data training where each class in both train and test images has equal amount of grayscale and color images. This procedure, Color Balanced, performs well, but selective collection of new data in practice may be expensive or even not possible.\n\niii) Instead of collecting new data, one may consider oversampling from the available data to debias. In Oversampling, we sample the underrepresented domain (via sampling with replacement) to make the proportion of color and grayscale images to be equal for each class (oversampled images are also passed through the data augmentation pipeline, e.g. random flipping and cropping, to further enforce the data diversity). Such procedure may be prone to overfitting the oversampled images and we see that this approach only provides marginal improvement of the model accuracy compared to centralized training over the skewed dataset and performs noticeably worse than FedMA.\n\nTo conclude, we agree that learning with skewed data is an exciting research direction. Our preliminary experiment indicates that FedMA has the potential to mitigate the data skewness, but further work is needed to obtain solutions as good as the one corresponding to the balanced data training (and without expensive additional data collection)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "B1xIeO3viS", "original": null, "number": 3, "cdate": 1573533677925, "ddate": null, "tcdate": 1573533677925, "tmdate": 1573533677925, "tddate": null, "forum": "BkluqlSFDS", "replyto": "rkgJ17ZpFH", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the thorough review and feedback. We address the concerns raised below.\n\nWe extended paragraph \"Solving matched averaging\" in Section 2.1 clarifying how our approach can learn the size of the global model and the motivation for using BBP-MAP. We hope this addresses both of the technical concerns. To summarize, our approach does allow global model to learn more units than the client models and in your example with \"horse hooves\" and \"snake skin\" it will do the right thing, i.e. it will not match \"horse hooves\" to \"snake skin\" and instead increase the size of the global model keeping both. In our experiments, when simulating heterogeneous CIFAR-10 partitioning, we use Dirichlet distribution with a small concentration parameter to partition each of the class examples across clients - this results in very diverse class distributions across clients (some might even completely lack several classes). Please see the \"Experimental Setup\" paragraph in Section 3 for details. Our experiments demonstrate that FedMA performs well under the diverse class distributions scenario. The global neural net found by FedMA is bigger than local models, but only mildly - please see rows \"Model growth rate\" in Tables 1 and 2.\n\nIn our extended \"Solving matched averaging\" paragprah we also gave a general recipe for performing matched averaging with adaptive global model size. The idea is to consider an extended cost matrix and iteratively apply the Hungarian (Munkres) algorithm. Iterations are needed to handle multiple unknown permutation matrices, but if we only have two neural networks, then a single run of the Hungarian algorithm with our cost matrix is sufficient. We also clarified the motivation for using BBP-MAP: it simply gives us a way to pick cost matrix, matching threshold and model size penalty simultaneously, based on the model of Yurochkin et al. Otherwise, their algorithm is a special case of our framework.\n\n>>> Regarding the novelty\n\nWe believe our work has both practical and methodological contributions in comparison to Yurochkin et al. We would like to emphasize that LSTMs matching presented in this paper is special and differs from MLPs and CNNs. In eq. (6) we show that it leads to a quadratic assignment problem due to permutation applied on both sides of the hidden-to-hidden weights in the LSTM cell. Our solution is to use linear assignment corresponding to input-to-hidden weights to find the permutations, but account for the special permutation structure of the hidden-to-hidden weights when averaging them. For the CNNs, we showed that it is essentially same as the MLPs and we agree that in this case it is a relatively straightforward extension of Yurochkin et al. Overall, we think that combining and formalizing permutation invariance structure of all key architectures in the language of permutation matrices (instead of Bayesian modeling) is also a valuable contribution. For example, our formalism shows an interesting connection to Optimal Transport, i.e. eq. (2) is very similar to Wasserstein barycenter formulation, while the quadratic assignment arising in LSTMs is related to Gromov-Wasserstein barycenters. This connection may lead to better estimation of permutations for matched averaging, replacing BBP-MAP.\n\nAnother methodological contribution of our work is the combination of layer-wise matching and local re-training. In Figure 1 we experimentally showed that simply extending approach of Yurochkin et al. to CNNs (labeled One-Shot Matching on the plots) only works for basic architectures. FedMA enables efficient federated learning of modern architectures and demonstrates strong empirical performance on more challenging datasets in comparison to Yurochkin et al.\n\n>>> Minor Presentation Concerns\n\nWe have added theta definition before the equation.\n\nOne round of FedMA requires communication rounds equal to the number of layers in a network. At each of these communications, clients send weights of a single layer, then master matches and averages them and broadcasts back the resulting global weights for this one layer. To summarize, one round of FedMA requires \"number of layers\" communications, but the total message size is equal to one communication round of FedAvg, i.e. size of the full model. FedMA with communication is basically repeating rounds of FedMA with one important detail. Recall that FedMA learns the global model size, hence the global neural net is usually slightly bigger than the local models. To keep the size of the local models constant when proceeding to the next FedMA round, we re-set local models to the \"subsets\" of the global model that they were matched to. This has no significant communication overhead as permutation matrices needed to obtain those subsets can be easily stored and broadcasted as lists of integers when running a FedMA round, and each client naturally has a global model copy by the end of each FedMA round."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "Syg7FvhwiH", "original": null, "number": 1, "cdate": 1573533563027, "ddate": null, "tcdate": 1573533563027, "tmdate": 1573533563027, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all the reviewers for the thoughtful comments. We have followed the reviewers suggestions to revise and improve our manuscript, while providing extra experiments as requested. One notable addition is the extended paragraph \"Solving matched averaging\" in Section 2.1 describing a general recipe for performing the matched averaging of neural nets with adaptive size of the global neural net. BBP-MAP then can be seen as a specific way of carrying out the optimization procedure we described. We answer each reviewer\u2019s questions individually."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2476/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2476/Authors|ICLR.cc/2020/Conference/Paper2476/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140786, "tmdate": 1576860531908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Comment"}}}, {"id": "SkexqR4GoB", "original": null, "number": 2, "cdate": 1573174919657, "ddate": null, "tcdate": 1573174919657, "tmdate": 1573174919657, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BygfWezloB", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment", "content": {"title": "rude review", "comment": "I have never seen such a review. This is rude, How can you determine the quality of this paper based on \"important area\"."}, "signatures": ["~Nan_Jiang7"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nan_Jiang7", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179738, "tmdate": 1576860565542, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment"}}}, {"id": "SJlZzmfeiB", "original": null, "number": 1, "cdate": 1573032713151, "ddate": null, "tcdate": 1573032713151, "tmdate": 1573032713151, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BygfWezloB", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment", "content": {"title": "More details", "comment": "Hi, could you please provide more details for the review?"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179738, "tmdate": 1576860565542, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2476/Authors", "ICLR.cc/2020/Conference/Paper2476/Reviewers", "ICLR.cc/2020/Conference/Paper2476/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Public_Comment"}}}, {"id": "BygfWezloB", "original": null, "number": 3, "cdate": 1573031930244, "ddate": null, "tcdate": 1573031930244, "tmdate": 1573031930244, "tddate": null, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "invitation": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #3", "review": "This paper offers a beautiful and simple method for federated learning. Strong empirical results. \n\nImportant area. \n                                                                                                                                                                                                                                                                                                                                                                                                .", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2476/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hongyiwang@cs.wisc.edu", "mikhail.yurochkin@ibm.com", "yuekai@umich.edu", "dimitris@papail.io", "yasaman.khazaeni@us.ibm.com"], "title": "Federated Learning with Matched Averaging", "authors": ["Hongyi Wang", "Mikhail Yurochkin", "Yuekai Sun", "Dimitris Papailiopoulos", "Yasaman Khazaeni"], "pdf": "/pdf/6b9ef72b07bb3390dcf6145f41df02ceffbb916e.pdf", "TL;DR": "Communication efficient federated learning with layer-wise matching", "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.", "keywords": ["federated learning"], "paperhash": "wang|federated_learning_with_matched_averaging", "code": "https://github.com/IBM/FedMA", "_bibtex": "@inproceedings{\nWang2020Federated,\ntitle={Federated Learning with Matched Averaging},\nauthor={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkluqlSFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/91e4a38abd8596c8f85df8727ad9befa1d5198e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkluqlSFDS", "replyto": "BkluqlSFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2476/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576079913902, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2476/Reviewers"], "noninvitees": [], "tcdate": 1570237722294, "tmdate": 1576079913915, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2476/-/Official_Review"}}}], "count": 14}