{"notes": [{"id": "ryGfnoC5KQ", "original": "BJlypba9F7", "number": 691, "cdate": 1538087850066, "ddate": null, "tcdate": 1538087850066, "tmdate": 1550872759465, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJej2EhxeN", "original": null, "number": 1, "cdate": 1544762546923, "ddate": null, "tcdate": 1544762546923, "tmdate": 1545354519535, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Meta_Review", "content": {"metareview": "this submission follows on a line of work on online learning of a recurrent net, which is an important problem both in theory and in practice. it would have been better to see even more realistic experiments, but already with the set of experiments the authors have conducted the merit of the proposed approach shines. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper691/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353123115, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper691/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper691/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper691/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353123115}}}, {"id": "B1e45z9n2Q", "original": null, "number": 2, "cdate": 1541345931903, "ddate": null, "tcdate": 1541345931903, "tmdate": 1543849682867, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "content": {"title": "Interesting evidence that extreme approximations to BPTT can work", "review": "This paper proposes a simple method for performing temporal credit assignment in RNN training.  While it seems somewhat naive and unlikely to work (in my opinion), the experimental results surprisingly show reasonable performance on several reasonably challenging artificial tasks.\n\nThe core of the approach is based on equation 7, which approximates the Jacobian between different hidden states at different time-steps as a single adaptively-learned matrix times a decay factor that depends on the time gap.  While this seems like a very severe approximation to make the authors speculate that some kind of feedback alignment-like mechanism might be at play.\n\nThe presentation needs work in several areas, and the experimental results require more explanation, but otherwise this seems like a solid paper.  I would probably increase my rating if the authors could address my issues satisfactorily. \n\n\nSee below for more detailed comments:\n\nAbstract & Section 1: \n\nIs \"sensitivity tensor\" or \"credit assignment tensor\" common term?  Because I've never heard them before.  Consider defining them before you discuss it, and using consistent jargon.  Later in Section 2 you seem to call this the \"RTRL tensor\" (whose meaning I can infer).  \n\nSection 2:\n\nGradient vanishing isn't so much a problem in itself, but a symptom that the sensitivity of the network's output to the action of some neuron in the past is very low. Ths gradient is just relaying this information, so I don't really see vanishing gradients as the problem to overcome, but rather low sensitivity on past activations.\n\nSection 3: \n\nDid you mean to write (W^out h^t + b^out) instead of (W^out h + b^out)^t ?\n\n\"[equation] represents the gradient of the cost with respect to the current hidden state\".  The RHS of this equation makes no sense to me.  Not only does this not depend on the nonlinearity in any way, it doesn't include any consideration of future outputs on which the current h surely depends. \n\nIt would make the paper much more pleasant to read if you gave your derivation of the learning rule before you stated it in gory detail.  It feels almost completely arbitrary reading it first without any justification. This might be fine if it were compact and elegant, but it's not.\n\nConsider using exp(x) instead of e^x since the symbol e already means something else in your notation.\n\nSection 4:\n\nPlease define \"temporal variation\"\n\n\nSection 5:\n\nYou should elaborate on the experimental setup you used.  Especially for the Addition and MNIST problems.  For example, what consistutes a \"step\" in figure 2?  Does KeRL take \"one\" step per time-step?  Or does \"step\" mean a complete gradient computation from running from t = 1 to t= T?  Is the BPTT truncated?  Are you counting one step of BPTT to be one complete forwards and backwards pass?\n\nYou should include some basic description of what an IRNN is.\n\nWhen you say that for MNIST that KeRL \"does not converge to as good of an optimum\" this seems like unjustified inference.  You don't really know that it is converging to a minimum of the original objective at all.  It could be converging to the minimum of some other objective it is implicitly optimizing due to your approximations (if one even exists).  Or it could be simply cycling around and failing to converge.  The fact that the loss plateaus isn't direct evidence of convergence in any sense.  If you wanted to measure this more directly you could look at the (true) gradient magnitude.\n\n\"only requires a few tensor operations at each time step\" -> this is also true of UORO", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "cdate": 1542234401868, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335781603, "tmdate": 1552335781603, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxjeIlqCm", "original": null, "number": 4, "cdate": 1543271923501, "ddate": null, "tcdate": 1543271923501, "tmdate": 1543271923501, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "B1lQ16dO2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for summarizing the positives features of KeRL while offering helpful critiques.\n\nWe may add that KeRL might also work in places that BPTT wouldn\u2019t: Imagine a scenario where the network is tasked with processing large amounts of real-time data quickly, and there is a speed/accuracy tradeoff. An algorithm like KeRL, which sacrifices accuracy for speed, should outperform BPTT.\n\nWith respect to the specific critiques, we have fixed the tables and added an experiment on the time complexity of KeRL vs. BPTT. Finally, we have extensively edited sections 3-4 so for clarity and to emphasize the more important parts of the derivation. "}, "signatures": ["ICLR.cc/2019/Conference/Paper691/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608577, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGfnoC5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper691/Authors|ICLR.cc/2019/Conference/Paper691/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608577}}}, {"id": "BJeu0VlqAX", "original": null, "number": 3, "cdate": 1543271631783, "ddate": null, "tcdate": 1543271631783, "tmdate": 1543271803341, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "B1e45z9n2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for the detailed review comments and criticisms, which were extremely helpful in improving our paper. \n\nUnderstanding the Ansatz is indeed key to understanding KeRL, and indeed giving various implementational details at the beginning obscured where the rule comes from. We now begin by first more clearly stating the learning rule, which actually does have a very simple form (this simple form was previously obscured by poor notation; we have now simplified the notation and also explain why the rule is simple), and then immediately showing the Ansatz that leads from the gradient-descent chain rule computation to KeRL. We have extensively edited the presentation of the Ansatz for clarity, and moved details on how to train the feedback weights and inverse-timescales to a different section.\n\nAs for the detailed suggestions below, we have corrected the noted typos, clarified or added definitions where suggested, and replaced non-standard terminology. We think that our paper should be substantially clearer and the central idea easier to follow after implementing these suggestions.\u2028\u2028\n\nSee below for more detailed comments:\n\n'Abstract & Section 1:  Is \"sensitivity tensor\" or \"credit assignment tensor\" common term?  Because I've never heard them before.  Consider defining them before you discuss it, and using consistent jargon.  Later in Section 2 you seem to call this the \"RTRL tensor\" (whose meaning I can infer).'\n  \nReplaced \u201ccredit assignment tensor\u201d and \u201cRTRL tensor\u201d (both used interchangeably before) with the more clearly defined single term, \u201csensitivity tensor\u201d, which has some precedence in the literature (please see paper text). \n\n'Section 2: Gradient vanishing isn't so much a problem in itself, but a symptom that the sensitivity of the network's output to the action of some neuron in the past is very low. Ths gradient is just relaying this information, so I don't really see vanishing gradients as the problem to overcome, but rather low sensitivity on past activations.'\n\n\u2028Removed this comment about gradient vanishing. \n\n\u2028'Section 3: Did you mean to write (W^out h^t + b^out) instead of (W^out h + b^out)^t ?\u2028'\n\nCorrected typo.\n\n' \"[equation] represents the gradient of the cost with respect to the current hidden state\".  The RHS of this equation makes no sense to me.  Not only does this not depend on the nonlinearity in any way, it doesn't include any consideration of future outputs on which the current h surely depends.'\n\n \u2028Corrected. \n\n'It would make the paper much more pleasant to read if you gave your derivation of the learning rule before you stated it in gory detail.  It feels almost completely arbitrary reading it first without any justification. This might be fine if it were compact and elegant, but it's not.'\n\nPlease see response above, and fully edited presentation of rule in paper text. \n\n'\u2028\u2028Consider using exp(x) instead of e^x since the symbol e already means something else in your notation' \n\nDone\n\n'Section 4: Please define \"temporal variation\" '\n\nClarified. \u2028\u2028\n\n'Section 5: You should elaborate on the experimental setup you used.  Especially for the Addition and MNIST problems.  For example, what consistutes a \"step\" in figure 2?  Does KeRL take \"one\" step per time-step?  Or does \"step\" mean a complete gradient computation from running from t = 1 to t= T?  Is the BPTT truncated?  Are you counting one step of BPTT to be one complete forwards and backwards pass?\u2028'\n\nClarified and elaborated. Also, replaced \u201ctraining steps\u201d with  \u201cnumber of minibatches.\"\n \n'\u2028You should include some basic description of what an IRNN is.'\n\n\u2028Done\n\u2028\n'When you say that for MNIST that KeRL \"does not converge to as good of an optimum\" this seems like unjustified inference.  You don't really know that it is converging to a minimum of the original objective at all.  It could be converging to the minimum of some other objective it is implicitly optimizing due to your approximations (if one even exists).  Or it could be simply cycling around and failing to converge.  The fact that the loss plateaus isn't direct evidence of convergence in any sense.  If you wanted to measure this more directly you could look at the (true) gradient magnitude.\u2028'\n\nReplaced with \u201cdoes not reach as high an asymptotic performance\u201d\n\n\u2028' \"only requires a few tensor operations at each time step\" -> this is also true of UORO'\n\nClarified. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608577, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGfnoC5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper691/Authors|ICLR.cc/2019/Conference/Paper691/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608577}}}, {"id": "ByeT17xqCm", "original": null, "number": 2, "cdate": 1543271140723, "ddate": null, "tcdate": 1543271140723, "tmdate": 1543271140723, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "B1lH3_Ian7", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "\"The paper is reasonably well written, but somewhat dense and hard to follow.\" \n\nThank you for the valuable comments. We have extensively edited the exposition to both better motivate the rule and make its derivation clearer and easier to read. \n\n\"The contribution seems novel. The main issue is the empirical evaluation.  All of the tasks (masked addition, pixel-by-pixel MNIST, and the AnBn problem) are artificial. In addition, the results on some of the tasks are mixed if not in favor of BPTT. \"\n\nSince KeRL performs stochastic gradient descent (SGD) with an approximate gradient, we do not expect it to outperform untruncated BPTT, which performs SGD with an exact gradient. Rather, the way to think about KeRL results is like the results on feedback alignment: relaxing the symmetric return weights in feedback alignment allows for learning by an approximation to SGD. Recurrent learning is notoriously harder than learning in feedforward networks;  it's notable that KeRL, which makes a strong approximation to the sensitivity, is able to perform almost as well across a variety of difficult tasks. Furthermore, KeRL holds the advantage over BPTT that the computation of the gradient does not scale with the length of the graph.\n\n\"I am not convinced that these results are enough to showcase the practical advantages of KeRL.\u2028 I am willing to increase my score, if the authors address this issue.\" \n\nWe disagree that that the empirical evidence for KeRL is lacking. The adding problem was one of Schmidhuber\u2019s \u201cpathological\u201d tasks that was used to demonstrate the utility of the LSTM. Using KeRL, we were able to solve sequence length 400 with a regular RNN and a squashing tanh nonlinearity. In fact, KeRL outperformed BPTT with the tanh nonlinearity, as the long timescales in the Ansatz were able to regularize the network towards a solution with longer sensitivity timescales. The pixel-by-pixel MNIST test, an even more challenging long term memory task as it involves remembering over nearly 1000 timesteps, was also solved on an RNN with KeRL. \n\n\u2028\"Detailed comments:\u2028\u2028- The authors mention that BPTT is not biologically plausible.  Although reasonable, I don't get why this would be an argument against it.\"\n\nWe are concerned with biological plausibility for three reasons. 1) From a neuroscience perspective we want to understand how the brain's recurrent networks learn tasks without the machinery to do BPTT; 2) Given that biological brains still outperform AI/deep networks on a wide range of problems, it is important to understand how brains solve these problems to build better AI, not just as a biological curiosity. 3) Designing and evaluating biologically realistic learning rules can be seen as an application of machine learning to neuroscience, one of the relevant topics of ICLR.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608577, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGfnoC5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper691/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper691/Authors|ICLR.cc/2019/Conference/Paper691/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers", "ICLR.cc/2019/Conference/Paper691/Authors", "ICLR.cc/2019/Conference/Paper691/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608577}}}, {"id": "B1lH3_Ian7", "original": null, "number": 3, "cdate": 1541396652708, "ddate": null, "tcdate": 1541396652708, "tmdate": 1541533771530, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "content": {"title": "limited empirical evidence", "review": "The paper proposes an alternative to backprop through time for\ntraining RNN models.\n\nThe paper is reasonably well written, but somewhat dense and hard\nto follow.  The contribution seems novel.\n\nThe main issue is the empirical evaluation.  All of the tasks\n(masked addition, pixel-by-pixel MNIST, and the AnBn problem)\nare artificial.\n\nIn addition, the results on some of the tasks are mixed if not\nin favor of BPTT.  I am not convinced that these results are enough\nto showcase the practical advantages of KeRL.\n\nI am willing to increase my score, if the authors address this\nissue.\n\nDetailed comments:\n\n- The authors mention that BPTT is not biologically plausible.  Although\n  reasonable, I don't get why this would be an argument against it.", "rating": "5: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "cdate": 1542234401868, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335781603, "tmdate": 1552335781603, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lQ16dO2Q", "original": null, "number": 1, "cdate": 1541078235351, "ddate": null, "tcdate": 1541078235351, "tmdate": 1541533771077, "tddate": null, "forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "content": {"title": "An interesting idea of improving BPTT by kernel recurrent learning. Skip in backpropagation is proposed and illustrated.", "review": "The proposed kernel recurrent learning (KeRL) provides an alternative way to train recurrent neural network with backpropagation through time (BPTT) where the propagation of gradients can be skipped over different layers. The authors directly assume the sensitivity function between two layers with a distance of tau in a form of Eq. (7). The algorithm of BPTT is then approximated due to this assumption. The model parameters are changed to learn the network dynamics. The optimization problem turns out to estimate beta and gamma of the kernel function. The learned parameters are intuitive. There are a set of timescales to describe the memory of each neuron and a set of sensitivity weights to describe how strongly the neurons interact on average. The purpose of this study is to save the memory cost and to reduce the time complexity for online learning with comparable performance. \n\nPros:\n1. KeRL only needs to compute a few tensor operations at each time step, so online KeRL learns faster than online BPTT for the case with a reasonably long truncation length.\n2. Biologically plausible statements are addressed.\n3. A prior is imposed for the temporal sensitivity kernel. The issue of gradient vanishing is mitigated.\n4. Theoretical illustration for KeRL in Sections 3 and 4 is clear and interesting.\n\nCons:\n1. The proposed method is an approximation to BPTT training. Suppose the system performance is constrained. Some guesses are made. The system performance can be further improved.\n2. The experiment on time cost due to online learning is required so that the reduction of time complexity can be illustrated.\n3. The format of tables 1 and 2 can be improved. Caption is required in Table 1. Overlarge size of Table 2 can be fixed.\n4.  A number of assumptions in Sections 3 and 4 are assumed.  When addressing Section 3, some assumptions in Section 4 are used. The organization of Sections 3 and 4 can be improved.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper691/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Kernel RNN Learning (KeRNL)", "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. ", "keywords": ["RNNs", "Biologically plausible learning rules", "Algorithm", "Neural Networks", "Supervised Learning"], "authorids": ["christopher_roth@utexas.edu", "ingmar@openai.com", "fiete@mit.edu"], "authors": ["Christopher Roth", "Ingmar Kanitscheider", "Ila Fiete"], "TL;DR": "A biologically plausible learning rule for training recurrent neural networks", "pdf": "/pdf/4ea360f981a40f9874d4c4e517414d724678f4af.pdf", "paperhash": "roth|kernel_rnn_learning_kernl", "_bibtex": "@inproceedings{\nroth2018kernel,\ntitle={Kernel {RNN} Learning (Ke{RNL})},\nauthor={Christopher Roth and Ingmar Kanitscheider and Ila Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGfnoC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper691/Official_Review", "cdate": 1542234401868, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGfnoC5KQ", "replyto": "ryGfnoC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper691/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335781603, "tmdate": 1552335781603, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper691/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}