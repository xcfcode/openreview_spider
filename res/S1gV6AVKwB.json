{"notes": [{"id": "S1gV6AVKwB", "original": "S1x6MZ9uvS", "number": 1391, "cdate": 1569439420482, "ddate": null, "tcdate": 1569439420482, "tmdate": 1577168214559, "tddate": null, "forum": "S1gV6AVKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iBea-HH5K", "original": null, "number": 1, "cdate": 1576798722149, "ddate": null, "tcdate": 1576798722149, "tmdate": 1576800914442, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose a novel approach for imitation learning in settings where demonstrations are unaligned with the task (e.g., differ in terms of state and action space). The proposed approach consists of alignment and adaptation steps and theoretical insights are provided on whether given MDPs can be aligned. Reviewers were positive about the ideas presented in the paper, and several requests for clarification were well addressed by the authors during the rebuttal phase. Key evaluation issues remained unresolved. In particular, it was unclear to what degree performance differences were purely caused by issues in alignment, and reviewers did not see sufficient evidence to support claims about performance on the full cross domain learning setting.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723925, "tmdate": 1576800275487, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Decision"}}}, {"id": "HJeDkFvGiH", "original": null, "number": 3, "cdate": 1573185758892, "ddate": null, "tcdate": 1573185758892, "tmdate": 1573443393104, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "HJgFzgcaYS", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"title": "Reply to Reviewer #3", "comment": "\nThank you for your constructive feedback. Below we respond to your questions. We've uploaded a revised draft that addresses all of your suggestions for improvement. \n\n\nQ. In the discussion after Def.4, given an alignment task set D_{x,y}, how do we know whether a common reduction exists? \n\nA: Applying Theorem 1, if the training error of GAMA is 0 then the learned alignments are jointly align all MDP pairs in the alignment task set. Thus a practical test of whether the MDP pairs in the alignment task set D_{x, y} are jointly alignable would be whether or not the GAMA training loss can be minimized close to 0. \n\n\nQ. The domain dynamics can be difficult to learn for complicated environments, which may jeopardize the learning of f and g as a result because they depend on the accuracy of the learned dynamics. The experiment only uses the hidden representation for the image as input. Such lower-dimensional representation is not always available.\n\nA: Our hidden representations in the image experiments are learned from images using spatial autoencoders. This modular approach of learning hidden representations via self-supervised learning then learning dynamics or a control policies on top of the learned representations has been shown to be effective in both robotics [1] and video-game [2] domains. In general, we agree that learning a latent space for dynamics model learning in high dimensional spaces is indeed a difficult problem undergoing active research. Learning image-to-image statemaps is also certainly an interesting problem, yet is still challenging even in the non-sequential decision making setting. (e.g CycleGAN is notoriously hard to train [3]) An interesting future research direction would be to extend CDIL with image-to-image translation maps and visual dynamics models. \n\n\nQ. What happens for UMA in Fig.4 top-right?\n\nA: UMA outputs cartpole coordinates which are completely out of bounds, hence the cart pole cannot be seen. Let $X$ be the state space of cartpole domain, $Y$ be the state space of pendulum domain and $E$ be a common feature space. UMA targets to learn linear maps (projections) $F: X \\rightarrow E$ and $G: Y \\rightarrow E$ such that if two states $x, y$ have similar local geometry, then their embeddings are close, i.e $Fx \\approx Gy$ if $R_x$ is \u2018similar\u2019 to $R_y$ where $R_x, R_y$ are the local geometry matrices of state $x$ and state $y$, respectively. Approximately 50$\\%$ of the demonstration states for domain $Y$ (pendulum) are near one state $y^*$ (i.e. pendulum standing upright) and thus $R_{y^*}$ is close to a zero matrix. Based on the local geometry similarity calculation of UMA, the distance $d(x, y)$ between two states $x, y$ satisfies $d(x,y) <= 2 \\min \\{ ||R_x||_F, ||R_y||_F \\}$. Thus, all the states in domain $X$ are considered similar to state $y^* \\in Y$ since $||R_{y*}||_F \\approx 0$. In order to minimize the cost defined in the UMA algorithm, $F$ should map all the states in $X$ to one embedding $e^* \\in E$ corresponding to the embedding for $y^*$. The only way to achieve this is to let $F$ be a near-zero matrix, which will make $||F^{-1}||$ very large. Moreover, a smaller fraction of states in $X$ domain demonstrations are near one point. Thus $||G||$ is not small enough to neutralize the effect of $F^{-1}$ and the pendulum->cartpole mapping $F^{-1}G$ has a very large norm. This causes points mapped points to be out of bounds. We\u2019ve added a summary of why UMA fails to the end of section 6.1. \n\n\nQ. Table 1 only has the results of three alignment tasks. How about the rest?\n\nA: Table 1 shows quantitative evaluations of the learned statemaps when a simple ground truth reduction (e.g permuation) exists between the domains. It is difficult to quantitatively evaluate learned statemaps between domains that aren\u2019t perfectly alignable, e.g snake3<->snake4, since it is not clear what the ground truth statemap should be. We instead implicitly evaluate the quality of more complex alignments by assessing how useful the alignments are for CDIL in section 6.2, 6.3. \n\n\nQ. What are the error bars in Table 1 (and also Fig.5)? Based on how many runs?\n\nA: Error bars/regions in Table 1/ Fig. 5 are the standard deviations obtained from training the alignment maps with multiple seeds. The randomness comes from (1). Sampling the alignment task set. (2). Optimization of GAMA\u2019s training objective via SGD. All experiments were run with 5 seeds. We\u2019ve clarified this in the captions for Table 1 and Fig. 5 in the uploaded revision.  \n\n\n[1]. \u201cDeep Spatial Autoencoders for Visuomotor Learning\u201d, Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel, arXiv:1509.06113\n[2]. \u201cWorld Models\u201d, David Ha, Jurgen Schmidhuber, arXiv:1803.10122\n[3] \u201cFactors Affecting Accuracy in Image Translation based on Generative Adversarial Network\u201d, Fumiya Yamashita, Ryohei Orihara, Yuichi Sei, Yasuyuki Tahara and Akihiko Ohsuga, ICAART 2018 "}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}, {"id": "BklC9xwfir", "original": null, "number": 2, "cdate": 1573183638240, "ddate": null, "tcdate": 1573183638240, "tmdate": 1573443384683, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "Hyx8KLvE5S", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"title": "Reply to Reviewer #1 ", "comment": "\nThank you for your feedback. Below we respond to your questions. We've uploaded a revised draft that addresses all of your suggestions. \n\nQ. For imitation between reacher2 and reacher3, there are multiple correspondences between the two domains due to the redundancy in the 3-link robot (and in general with n-link robots). How does GAMA deal with these redundancies? For example, an n-link robot is able to perform the same task with different configurations (elbow up, elbow down, etc.,) and all these will correspond to one configuration of a 2-link robot. \n\nA: If there exists multiple MDP reductions from a lower dimensional robot to a high dimensional robot, any one of them will be a global optimum of our optimization objective by Theorem 1. Hence GAMA should learn to map the states of the lower dimensional robot to states of the high dimensional robot which together compose one (out of potentially many) sequence of configurations in which the high dimensional robot accomplishes the task. We observe this empirically: reacher3 has many different sequences of states that correspond to successful goal reaching (due to redundancy). The (learned) statemap from reacher2 to reacher3 converges to one of them. Last paragraph of section 3 is also relevant to this point. \n\n\nQ. In practise, can 'alignment complexity' and 'adaptation complexity' help identify if transfer with GAMA between two domains is not beneficial? Results in Figure 5 only shows cases for GAMA in which the two metrics are good, resulting in good transfer. However, I am interested in knowing if there could be cases in which the two complexity metrics tell beforehand that transfer will not be beneficial. \n\nA: Yes, we believe good adaptation complexity and good adaptation complexity implies good transferability since good zero-shot imitation performance will \u201ckick start\u201d the policy getting rid of the burn-in phase. This is especially useful if the target task reward is sparse. If the adaptation complexity and alignment complexity are bad, we indeed observed that transferability is worse. For example, in the W2C task if we train on a small alignment task set (which has worse adaptation/alignment performance), then the policy trains slower on the target task. We can include these results in the final submission. \n\n\nQ. In Figure 5's caption, Adaptation complexity is on Left and Alignment Complexity in the Middle. However, the text refers to Alignment complexity on the Left and Adaptation complexity in the Middle.\n\nA: Thank you for catching this. We have updated the submission. \n\n\nQ. A reference is missing in Appendix B.\n\nA: Thank you for pointing this out. We have updated the submission. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}, {"id": "rJer0YDfoH", "original": null, "number": 4, "cdate": 1573185997343, "ddate": null, "tcdate": 1573185997343, "tmdate": 1573195915743, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "H1g_go22tH", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"title": "Reply to Reviewer #2, part 1 ", "comment": "We thank the reviewer for the detailed comments. We respond to your comments/questions below. We have also uploaded a revised draft addressing all suggested improvements. We\u2019re happy to add more experiments if the reviewer has suggestions. \n\n\nRESPONSE TO REVIEWER COMMENTS/QUESTIONS\n\nQ: Where is the cartpole in Figure 4 UMA? Fair to compare in subsequent sections?\n\nA: UMA outputs cartpole coordinates which are completely out of bounds, hence the cart pole cannot be seen. Let $X$ be the state space of cartpole domain, $Y$ be the state space of pendulum domain and $E$ be a common feature space. UMA targets to learn linear maps (projections) $F: X \\rightarrow E$ and $G: Y \\rightarrow E$ such that if two states $x, y$ have similar local geometry, then their embeddings are close, i.e $Fx \\approx Gy$ if $R_x$ is \u2018similar\u2019 to $R_y$ where $R_x, R_y$ are the local geometry matrices of state $x$ and state $y$, respectively. Approximately 50$\\%$ of the demonstration states for domain $Y$ (pendulum) are near one state $y^*$ (i.e. pendulum standing upright) and thus $R_{y^*}$ is close to a zero matrix. Based on the local geometry similarity calculation of UMA, the distance $d(x, y)$ between two states $x, y$ satisfies $d(x,y) <= 2 \\min \\{ ||R_x||_F, ||R_y||_F \\}$. Thus, all the states in domain $X$ are considered similar to state $y^* \\in Y$ since $||R_{y*}||_F \\approx 0$. In order to minimize the cost defined in the UMA algorithm, $F$ should map all the states in $X$ to one embedding $e^* \\in E$ corresponding to the embedding for $y^*$. The only way to achieve this is to let $F$ be a near-zero matrix, which will make $||F^{-1}||$ very large. Moreover, a smaller fraction of states in $X$ domain demonstrations are near one point. Thus $||G||$ is not small enough to neutralize the effect of $F^{-1}$ and the pendulum->cartpole mapping $F^{-1}G$ has a very large norm. This causes points mapped points to be out of bounds. Comparisons to UMA in the subsequent sections show that obtaining proper state alignments is important for attaining good CDIL performance. We\u2019ve added a summary of why UMA fails to the end of section 6.1. \n\n\nQ: Why are the other methods not able to align well when there is no domain shift? \n\nA: In section 6.1 we stated the reason: \u201cThe key reason behind this performance gap is that most baselines (Gupta et al., 2017; Liu et al., 2018) obtain state maps from time-aligned demonstration data. However, the considered alignment task set contains unaligned demonstrations with diverse starting states, up to 2x differences in demonstration lengths, and varying task execution rates.\u201d In summary, even when there is no domain shift, such as in pen<->pen, the trajectory data is unpaired and unaligned, making prior methods fail due to the time-alignment assumption. \n\n\nQ: How is dynamic time warping actually used to calculate the state correspondences for training IfO and IF? \n\nA: For IF, Dynamic Time Warping (DTW) uses the (learned) feature space as a metric space to estimate the domain correspondences. For IfO DTW is applied to the state space. We follow the EM procedure outlined in the IF paper: https://arxiv.org/pdf/1703.02949.pdf. We will also be releasing code that includes the full implementation including dynamic time warping. We added these descriptions to Appendix C, \u201cobtaining state correspondences\u201d. \n\n\nQ: How are the policies trained for the transferability task? Which RL algorithm? \n\nA: For the baselines, we first pretrain the policy on the transfer task with the proxy reward induced by the \nstatemap obtained during the alignment phase. We then train the policy with the ground truth reward function. All RL steps are performed with DDPG. The transferability plots (Fig 5. Right) show the learning curve for the second phase where the pretrained policy is trained on the ground truth reward with DDPG. \nWe have added this description added to Appendix C, Transfer Learning. \n\n\nQ. Do the algorithms still fail if the true reward is used with the alignment initialization? \n\nA: Could the reviewer please clarify what they mean by \u201calignment initialization\u201d? The transferability plot is showing the learning curve on training with the true reward after pretraining on the proxy reward defined by the learned state map. This is in agreement with the \u201cnegative transfer\u201d results in prior literature [1], e.g pretraining on Atari Pong leads to worse asymptotic performance on Atari Breakout. Mainly, training on an unrelated reward function (in this case a \u201cwrong\u201d reward function) can lead to asymptotically worse performance on the transfer task. Moreover, it\u2019s not possible to directly use the learned alignments as policy initialization for the baselines since they do not learn an action map.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}, {"id": "rJgDBowGjH", "original": null, "number": 6, "cdate": 1573186366779, "ddate": null, "tcdate": 1573186366779, "tmdate": 1573195625803, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "H1g_go22tH", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"title": "Reply to Reviewer 2, part 3", "comment": "\nQ-b ) Is agent rewarded for slowing down too?  \n\nA: No, the opposite is true. The agent is rewarded for performing the task as fast as possible. We have clarified the wording in section 6.2, \u201cReacher2Write\u201d. \n\nQ-c ) Does the goal of the reacher and the state representation corresponding to the vector between end effector and goal get updated once it reached a particular point? \n\nA: The goal location in the state representation is updated to be the next vertex in the letter. However, the difference vector between end effector and goal is removed from the state to make the task more challenging. We\u2019ve added more details about the writing task in Appendix D. \n\nQ-d ) Does the agent have to write one particular letter? Why do the other baselines not work at all on this task?  \n\nA: Agents must write two letters \u201cA\u201d, \u201cI\u201d in sequence. This task is difficult due to the sparse reward (you only get positive reward upon hitting the goals unlike the original reacher environment which gives reward inversely proportional to the distance between the goal and end effector). Other baselines fail to learn good alignments between domains and as a result the proxy reward does more harm than good due to negative transfer [1]. We\u2019ve added more details about the writing task in Appendix D. We will also include a more elaborate video of the environments and learned alignments in the final submission. \n\n\nQ. Is it possible to run experiments on environments presented in Invariant Features paper so that the importance of CDIL can be assessed better.\n\nA: The code for the exact environments used in the IF paper are not publicly available, but we have contacted the authors of the paper to see if we can obtain the environment source code and run experiments. \n\n\nQ. \u201cHIGH-LEVEL COMPARISON TO BASELINES\" section is misleading. Should add an \u201caction accessibility\u201d column. \n\nA: The high level comparison table has a checkpoint if the attribute was demonstrated in the paper. We agree that prior works (TCN, IfO, TPIL) have potential to be applied to the embodiment mismatch problem and would love to see works that attempt to do so. (TCN did show that you can learn interesting mappings between human-robot, but did not demonstrate that humans can teach new tasks to the robot using the mapping) We have added clarifications of the check/x marks meanings, a column for action accessibility, and interpretation of Table 2 in Appendix A. \n\n\nQ. How were the alignment videos generated?\n\nA: We rollout the imitator\u2019s policy while applying the (learned) statemap to the visited internal states of the robot to obtain the corresponding expert domain states. These states are mapped to images by using the set_state() function in the mujoco environments. \n\n\nQ. This paper has the potential to be an important paper in this field. But at this point needs further empirical evaluation with stronger baselines and known benchmark environments.\n\nA: We believe we\u2019ve done a thorough comparison against state-of-the-art baselines in the field and our environments clearly demonstrated shortcomings of prior work while highlighting the advantage of our method: namely, learning from unpaired, unaligned data and zero-shot imitation of the target task. If the reviewer suggests any additional environments/baselines we are happy to run additional experiments. \n\n\nMinor Comments:\n1)  \"boltmzman machine reconstruction error\" - Boltzmann\n\nA: Thank you for pointing this out. It\u2019s been revised. \n\n\nQUESTIONS FOR THE REVIEWER: \n\nQ1. Your decision statement mentioned benchmark environments and stronger baselines. \n(1). Could you suggest what benchmark environments you\u2019re referring to? Are there any additional experimental settings you would like us to evaluate our approach on?  \n(2). What stronger baselines are you referring to? Are there any particular ones you would like us to add? \n\nQ2. Could you clarify what you meant by \u201calignment initialization\u201d? \n\n\nREFERENCES\n[1] \u201cCharacterizing and Avoiding Negative Transfer\u201d, Zirui Wang, Zihang Dai, Barnab\u00e1s P\u00f3czos, Jaime Carbonell, arXiv:1811.09751 \n[2] \u201cManifold Alignment without Correspondence\u201d, Chang Wang, Sridhar Mahadevan, IJCAI 2009.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}, {"id": "ryed3qvMoH", "original": null, "number": 5, "cdate": 1573186223947, "ddate": null, "tcdate": 1573186223947, "tmdate": 1573195578015, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "H1g_go22tH", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"title": "Reply to Reviewer #2, part 2", "comment": "\nQ. Did you try reward scaling? \n\nA: Yes. We\u2019ve also attempted standard RL optimization tricks (e.g observation normalization used in OpenAI baselines) and weight regularization make the baselines train to reach expert level performance and observed them failing due to negative transfer [1]. We can add the results of trying different optimization tricks to the final version. \n\n\nQ. The environments considered for domain transfer seem quite trivial for the alignment between them to be a big problem. \n\nA: The W2C task is on par with the difficulty of reaching tasks presented in the IF paper (https://arxiv.org/pdf/1703.02949.pdf), namely in that the reacher must mobilize to new locations to solve the target task. Our setting is more difficult in the sense that we seek to generalize to the target task without the ground truth reward or an additional RL step, while IF, UMA, CCA have all assumed access to the ground truth reward for the target task.  \nThe R2W task is similar to the reacher task used in the original TPIL paper (https://arxiv.org/abs/1703.01703), but with a significantly larger (10x larger) viewpoint mismatch making it more challenging. (albeit they don\u2019t require an alignment tasks set)\nBoth papers were published ICLR in the last 3 years. Furthermore the additional main challenge of our experimental settings is that data is given in the form of \u201cunpaired unaligned\u201d demonstrations. This was sufficient to demonstrate the limitations of the time-alignment assumptions made in prior work. \n\n\nQ. What is challenging about the Third Person environment? One of the states of reacher is vector between goal and state which should still be informative enough to solve the task. Presumably the rotation creates a different view for the image based experiments. There also the transformation is quite easy for the spatial autoencoder given that it has access to coordinates through the spatial softmax layer.  \n\nA: We clarify the challenges of the third person environment: \nFirst when using the robot\u2019s internal state space: a reacher joint configuration $(\\theta_1, \\theta_2)$ corresponds to $(\\theta_1 + \\alpha, \\theta_2 + \\alpha)$ in the third person domain with a rotation of $\\alpha$ degrees along the $z$-axis (the reacher lies on the $x,y$ plane) while the goal coordinates $(x, y)$ stay the same. The challenge during the alignment phase is to learn what the value of $\\alpha$ is. Learning this from unpaired, unaligned data is challenging as we see that the baselines fail, even with dynamic time warping. If one were to apply the same policy learned in the original domain to the third person domain it would completely fail. For example, let $(\\theta_1, \\theta_2)$ correspond to the original reacher states for which the end effector reaches goal $(x, y)$. Then the optimal policy $\\pi^*$ will output $\\pi^*(\\theta_1, \\theta_2) = (0, 0)$, i.e we add no torque to the joints and keep the reacher still at the goal. However, in the third person domain, $(\\theta_1, \\theta_2)$ corresponds to a state $\\alpha$ degrees away from target configuration. So the third person reacher would not be moving in a state that has not yet reached the goal. The difference vector between the end-effector and goal was removed from the state space to make the task more challenging, so it cannot be used. For these reasons we see that the pretrained baselines (train the policy on the alignment task set MDPs for domain $x$, and directly apply that policy to the target task in domain $x$) also fail in Figure 5 (Bottom-Left). \nIn the image space: as the reviewer pointed out, this creates a different view, e.g a reacher point down in the original domain is point up in the third person domain. Even if the spatial autoencoder is able to \u201cperfectly\u201d extract the reacher\u2019s joint coordinates and its velocities, challenges would persist as described above. In practice we see that the spatial autoencoder extracts coordinates that are noisy, which poses an additional challenge for learning alignments. We have added more details about the third person environment in Appendix D.\n\n\nQ.  It is unclear what the R2W task entails or how difficult it is. \nQ-a ) How is this task specified? \nA: Vertices of the letter are spawned in sequence and the state representation contains the next target_vertex coordinate $(v_x, v_y)$. Here\u2019s pseudocode for how the reward is calculated and how the next vertex location is updated:  \nLet $e(s)$ be the location of the end effector for robot state $s$. \nif $||e(s) - (v_x, v_y)|| < \\epsilon$: then $R(s) = 100$, update $(v_x, v_y) =$ next target vertex\nelse : $R(s) = -1$ \nIn words: agent gets a large reward for hitting the vertex of a letter and the next vertex is spawned. Each step in the environment has a negative reward to promote fast task completion. The write task reward is significantly more sparse than the original reacher task reward which is defined as the inverse of the distance between the end effector and goal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}, {"id": "H1g_go22tH", "original": null, "number": 1, "cdate": 1571764975842, "ddate": null, "tcdate": 1571764975842, "tmdate": 1572972474930, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThis paper proposes a method to perform alignment between demonstrations in different domains in order to adapt a policy from expert domain to the test domain. They also include a formalism that studies alignments of different MDPs.\n\nStrengths\n1) The paper presents a formal framework to study many recent methods in cross-domain/cross-view imitation learning with a common lens. \n2) The solution proposed by the authors might have a big practical advantage for cross-domain imitation learning settings. They claim that they do not need to learn the policy using RL in the new domain as they learn the action mapping between the two domains to retrieve the corresponding action in new domain based on the action in expert domain. This is a very interesting result.\n3) The solution is quite modular with 3 important parts: i) state mapping network, ii) action mapping network, iii) dynamics model. \n\nWeakness\nThe biggest weakness of the paper is in the evaluation section. \n1) Issues with baselines:\ni) There seems to be some problem with UMA. In Figure 4, it cannot figure out the alignment with self domain. Why is that the case? Where is the cartpole in UMA row for pen-cartpole alignment ? Given such poor performance during alignment is it fair to compare to UMA in the subsequent section?\nii) Why are the other methods not able to align well when there is no domain shift? \niii) How is dynamic time warping actually used to calculate the state correspondences for training IfO and IF? Is it calculated on top of the states?\niv) How are the policies trained for the transferability task? Which RL algorithm? \"Baselines fail to learn the writing task as an inaccurate proxy reward function harms performance.\" \n2 aspects are involved here:\n1) initializing from alignment network\n2) using rewards from the state representation to train the policy.\nDo the algorithms still fail if the true reward is used with the alignment initialization? \n\nReward scaling is known to be important for RL algorithms[1]. May be features from these algorithms have to be scaled to scale the rewards for the RL algorithms to work.\n\n2) Issues with environments:\ni) The environments considered for domain transfer seem quite trivial for the alignment between them to be a big problem. For example, the authors present the task R2W: reacher2-tp that has a \"third person\" state space with a 180 camera angle offset. I am not sure how this rotation changes the state for the policy that uses states as input. One of the states of reacher is vector between goal and state which should still be informative enough to solve the task. Presumably the rotation creates a different view for the image based experiments. There also the transformation is quite easy for the spatial autoencoder given that it has access to coordinates through the spatial softmax layer. \n\nii) It is unclear what the R2W task entails or how difficult it is. \n\n\"The transfer task is writing letters as fast as possible. The transfer task differs from the alignment tasks in two key aspects: the end effector must draw a straight line from a letter\u2019s vertex to vertex and minimally slow down at the vertices. \"\n\nIt seems there is a term that penalizes the policy from doing things quickly. Some questions regarding this task:\n1) How is this task specified? \n2) Is agent rewarded for slowing down too?  Does the goal of the reacher and the state representation corresponding to the vector between end effector and goal get updated once it reached a particular point? Does the agent have to write one particular letter? Why do the other baselines not work at all on this task?  \n\niii) Is it possible to run experiments on environments presented in Invariant Features paper so that the importance of CDIL can be assessed better.\n\n4) \"HIGH-LEVEL COMPARISON TO BASELINES\" section is misleading. While the different baselines (TCN, IfO, TPIL) were developed in the context of view mismatch that does not mean the algorithm cannot be applied for embodiment mismatch. For IF also the same idea can be applied for viewpoint mismatch. TCN also proposed single-view version which can be used to learn representations without paired data. Hence, it is not right to say TCN can't work with unpaired alignment data. It also needs to be stated in this section that CDIL requires access to actions while some baselines like TCN do not need actions that can be used for training.  \n\nQuestions\n1) How were the alignment videos generated?\n\nDecision\nThis paper has the potential to be an important paper in this field. But at this point needs further empirical evaluation with stronger baselines and known benchmark environments.\n\nMinor Comments:\n1)  \"boltmzman machine reconstruction error\" - Boltzmann\n \nReferences\n[1] \"Deep Reinforcement Learning That Matters.\" Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575475760431, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Reviewers"], "noninvitees": [], "tcdate": 1570237738071, "tmdate": 1575475760443, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review"}}}, {"id": "HJgFzgcaYS", "original": null, "number": 2, "cdate": 1571819536619, "ddate": null, "tcdate": 1571819536619, "tmdate": 1572972474881, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes Generative Adversarial MDP Alignment (GAMA) for imitation learning. Given a set of paired MDPs, GAMA learns a state mapping f and an action mapping g such that one MDP can be reduced to another. For a new test MDP pair (x,y) where expert demonstrations are available for y, GAMA can use f to map a state of x to a state of y, mimic the expert behavior, then use g to map the expert action back to an action in x. The reduction is theoretically motivated, the optimization is based on adversarial learning and finally, experiments on common Gym environments show that GAMA can achieve effective transfer.\n\nPros\n- The writing is great and easy to follow\n- The method is theoretically motivated\n- Experiments prove effective\n\nCons\n- The proposed method may not work well for complicated environments\n\n(1) In the discussion after Def.4, given an alignment task set D_{x,y}, how do we know whether a common (w.r.t. all MDP pairs) reduction exists? In other words, how do we know that the MDP pairs are from the same equivalent class (joint alignable) in practice?\n\n(2) The alignment needs to learn a lot of components: state mapping f, action mapping g, domain dynamics P^x. The domain dynamics can be difficult to learn for complicated environments, which may jeopardize the learning of f and g as a result because they depend on the accuracy of the learned dynamics. The experiment only uses the hidden representation for the image as input. Such lower-dimensional representation is not always available.\n\n(3) Experiment:\n- What happens for the UMA in Fig.4 top-right?\n- Table 1 only has the results of three alignment tasks. How about the rest?\n- What are the error bars in Table 1 (and also Fig.5)? Based on how many runs?\n\nMinors:\n- It is more common to use \"state-action pair\" instead of \"state, action pair\".\n- Sec.6.1, task task exemplify"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575475760431, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Reviewers"], "noninvitees": [], "tcdate": 1570237738071, "tmdate": 1575475760443, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review"}}}, {"id": "Hyx8KLvE5S", "original": null, "number": 3, "cdate": 1572267645766, "ddate": null, "tcdate": 1572267645766, "tmdate": 1572972474839, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a learning approach for zero-shot imitation learning in an RL setting across domains with different embodiments and viewpoint mismatch. The proposed approach involves two steps, alignment and adaptation. In contrast to previous work, the alignment between domains, represented as MDPs, is learned from unpaired, unaligned samples from both domains. The paper presents a theoretical formulation of the cross-domain imitation learning problem, and presents an algorithm for training alignment and adaptation from data. \n\nI think the paper is well written and theoretically well-founded. The authors provide experiments comparing to many previous works in cross-domain imitation learning and show that their approach outperforms previous approaches. \n\nI only have minor comments.\n\n1. For imitation between reacher2 and reacher3, there are multiple correspondences between the two domains due to the redundancy in the 3-link robot (and in general with n-link robots). How does GAMA deal with these redundancies? For example, an n-link robot is able to perform the same task with different configurations (elbow up, elbow down, etc.,) and all these will correspond to one configuration of a 2-link robot. \n\n2. In practise, can 'alignment complexity' and 'adaptation complexity' help identify if transfer with GAMA between two domains is not beneficial? Results in Figure 5 only shows cases for GAMA in which the two metrics are good, resulting in good transfer. However, I am interested in knowing if there could be cases in which the two complexity metrics tell beforehand that transfer will not be beneficial. \n\n3. In Figure 5's caption, Adaptation complexity is on Left and Alignment Complexity in the Middle. However, the text refers to Alignment complexity on the Left and Adaptation complexity in the Middle.\n\n4. A reference is missing in Appendix B."}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575475760431, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Reviewers"], "noninvitees": [], "tcdate": 1570237738071, "tmdate": 1575475760443, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Review"}}}, {"id": "HkglyI0oDH", "original": null, "number": 1, "cdate": 1569609175532, "ddate": null, "tcdate": 1569609175532, "tmdate": 1569610917327, "tddate": null, "forum": "S1gV6AVKwB", "replyto": "S1gV6AVKwB", "invitation": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment", "content": {"comment": "Video of the cross domain state alignments learned by Generative Adversarial MDP Alignment (GAMA). \nGAMA learns these statemaps from unpaired, unaligned demonstrations without any additional supervision. \n\nlink: https://youtu.be/l0tc1JCN_1M", "title": "Videos of cross domain state alignments by Generative Adversarial MDP Alignment (GAMA)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1391/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross Domain Imitation Learning", "authors": ["Kun Ho Kim", "Yihong Gu", "Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["khkim@cs.stanford.edu", "gyh15@mails.tsinghua.edu.cn", "jiaming.tsong@gmail.com", "sjzhao@stanford.edu", "ermon@cs.stanford.edu"], "keywords": ["Imitation Learning", "Domain Adaptation", "Reinforcement Learning", "Zeroshot Learning", "Machine Learning", "Artificial Intelligence"], "TL;DR": "Imitation learning across domains with discrepancies such as embodiment and viewpoint mismatch. ", "abstract": "We study the question of how to imitate tasks across domains with discrepancies such as embodiment and viewpoint mismatch. Many prior works require paired, aligned demonstrations and an additional RL procedure for the task. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Cross Domain Imitation Learning (CDIL) problem, which encompasses imitation learning in the presence of viewpoint and embodiment mismatch. Informally, CDIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to CDIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from unpaired, unaligned demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when CDIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in both embodiment and viewpoint mismatch scenarios where aligned demonstrations don\u2019t exist and show the effectiveness of our approach.", "pdf": "/pdf/4657de442dacd38e6329146451a01deb4f6c0a0b.pdf", "paperhash": "kim|cross_domain_imitation_learning", "original_pdf": "/attachment/bbf15448cbe402acd5fd5c014b58d6b55612c3b8.pdf", "_bibtex": "@misc{\nkim2020cross,\ntitle={Cross Domain Imitation Learning},\nauthor={Kun Ho Kim and Yihong Gu and Jiaming Song and Shengjia Zhao and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gV6AVKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gV6AVKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1391/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1391/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1391/Authors|ICLR.cc/2020/Conference/Paper1391/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156720, "tmdate": 1576860561299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1391/Authors", "ICLR.cc/2020/Conference/Paper1391/Reviewers", "ICLR.cc/2020/Conference/Paper1391/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1391/-/Official_Comment"}}}], "count": 11}