{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519319827791, "tcdate": 1509124398783, "number": 499, "cdate": 1518730175420, "id": "r1wEFyWCW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1wEFyWCW", "original": "ryLNK1WR-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260096525, "tcdate": 1517249400092, "number": 180, "cdate": 1517249400065, "id": "SJxYXkpHM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper incorporates attention in the PixelCNN model and shows how to use MAML to enable few-shot density estimation. The paper received mixed reviews (7,6,4). After rebuttal the first reviewer updated the score to accept. The AC shares the concern of novelty with the first reviewer. However, it is also not trivial to incorporate attention and MAML in PixelCNN, thus the AC decided to accept the paper. ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516969063760, "tcdate": 1511960749239, "number": 3, "cdate": 1511960749239, "id": "rkHhxN2lG", "invitation": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Official Reviewer 1", "rating": "6: Marginally above acceptance threshold", "review": "This paper focuses on the density estimation when the amount of data available for training is low. The main idea is that a meta-learning model must be learnt, which learns to generate novel density distributions by learn to adapt a basic model on few new samples. The paper presents two independent method.\n\nThe first method is effectively a PixelCNN combined with an attention module. Specifically, the support set is convolved to generate two sets of feature maps, the so called \"key\" and the \"value\" feature maps. The key feature map is used from the model to compute the attention in particular regions in the support images to generate the pixels for the new \"target\" image. The value feature maps are used to copmpute the local encoding, which is used to generate the respective pixels for the new target image, taking into account also the attention values. The second method is simpler, and very similar to fine-tuning the basis network on the few new samples provided during training. Despite some interesting elements, the paper has problems.\n\nFirst, the novelty is rather limited. The first method seems to be slightly more novel, although it is unclear whether the contribution by combining different models is significant. The second method is too similar to fine-tuning: although the authors claim that \\mathcal{L}_inner can be any function that minimizes the total loss \\mathcal{L}, in the end it is clear that the log-likelihood is used. How is this approach (much) different from standard fine-tuning, since the quantity P(x; \\theta') is anyways unknown and cannot be \"trained\" to be maximized.\n\nBesides the limited novelty, the submission leaves several parts unclear. First, why are the convolutional features of the support set in the first methods divided into \"key\" and \"value\" feature maps as in p_key=p[:, 0:P], p_value=p[:, P:2*P]? Is this division arbitrary, or is there a more basic reason? Also, is there any different between key and value? Why not use the same feature map for computing the attention and computing eq (7)?\n\nAlso, in the first model it is suggested that an additional feature can be having a 1-of-K channel for the supporting image label: the reason is that you might have multiple views of objects, and knowing which view contributes to the attention can help learning the density. However, this assumes that the views are ordered, namely that the recording stage has a very particular format. Isn't this a bit unrealistic, given the proposed setup anyways?\n\nRegarding the second method, it is not clear why leaving this room for flexibility (by allowing L_inner to be any function) to the model is a good idea. Isn't this effectively opening the doors to massive overfitting? Besides, isn't the statement that the function \\mathcal{L}_inner void? At the end of the day one can also claim the same for gradient descent: you don't need to have the true gradients of the true loss, as long as the objective function obtains gradually lower and lower values?\n\nLast, it is unclear what is the connection between the first and the second model. Are these two independent models that solve the same problem? Or are they connected?\n\nRegarding the evaluation of the models, the nature of the task makes the evaluation hard: for real data like images one cannot know the true distribution of particular support examples. Surrogate tasks are explored, first image flipping, then likelihood estimation of Omniglot characters, then image generation. Image flipping does not sound a very relevant task  to density estimation, given that the task is deterministic. Perhaps, what would make more sense would be to generate a new image given that the support set has images of a particular orientation, meaning that the model must learn how to learn densities from arbitrary rotations. Regarding Omniglot character generation, the surrogate task of computing likelihood of known samples gives a bit better, however, this is to be expected when combining a model without attention, with an attention module.\n\nAll in all, the paper has some interesting ideas. I encourage the authors to work more on their submission and think of a better evaluation and resubmit.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642457318, "id": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper499/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer2", "ICLR.cc/2018/Conference/Paper499/AnonReviewer3", "ICLR.cc/2018/Conference/Paper499/AnonReviewer1"], "reply": {"forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper499/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642457318}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642457411, "tcdate": 1511798016705, "number": 1, "cdate": 1511798016705, "id": "HyKWS3KxM", "invitation": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "A solid paper", "rating": "7: Good paper, accept", "review": "This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning. The application is an obvious target for research and some relevant citations are missing, e.g. \"Towards a Neural Statistician\" (Edwards et al., ICLR 2017). Nonetheless, I think the current paper seems interesting enough to merit publication.\n\nThe paper is well-produced, i.e. the overall writing, visuals, and narrative flow are good. It was easy to read the paper straight through while understanding both the technical details and more intuitive motivations.\n\nI have some concerns about the architectures and experiments presented in the paper. For architectures: the attention-based model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta PixelCNN model is a standard PixelCNN trained with the MAML approach by Finn et al. For experiments: the ImageNet flipping task is clearly tailored to the strengths of the attention-based model, and the presentation of the general Omniglot results could be improved. The image flipping experiment is neat, but the attention-based model's strong performance is unsurprising. I think the results in Tables 1/2 should be merged into a single table. It would make it clear that the MAML-based and attention-based models achieve similar performance on this task.\n\nOverall, I think the paper makes a nice contribution. The paper could be improved significantly, e.g., by showing how to scale the attention-based architecture to problems with more data or by designing an architecture specifically for use with MAML-based inference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642457318, "id": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper499/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer2", "ICLR.cc/2018/Conference/Paper499/AnonReviewer3", "ICLR.cc/2018/Conference/Paper499/AnonReviewer1"], "reply": {"forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper499/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642457318}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642457375, "tcdate": 1511842659926, "number": 2, "cdate": 1511842659926, "id": "rJ3vXv5xf", "invitation": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Few shot learning with autoregressive density estimation", "rating": "6: Marginally above acceptance threshold", "review": "This paper focuses on few shot learning with autoregressive density estimation. Specifically, the paper improves PixelCNN with  1) neural attention, 2) meta learning techniques, and shows that the model achieve STOA few showt density estimation on the Omniglot dataset and demonstrate the few showt image generation on the Stanford Online Products dataset. \n\nThe model is interesting, however, several details are not clear, which  makes it harder to repeat the model and the experimental results. For example, what is the reason to use the (key, value) pair to encode these support images, what does the \"key\" means and what is the difference between \"keys\" and \"values\"? In the experiments, the author did not explain the meaning of \"nats/dim\" and how to compute it. Another question is about the repetition of the experimental results. We know that PixelCNN is already a quite complicated model, it would be even harder to implement the proposed model. I wonder whether the author will release the official code to public to help the community?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642457318, "id": "ICLR.cc/2018/Conference/-/Paper499/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper499/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper499/AnonReviewer2", "ICLR.cc/2018/Conference/Paper499/AnonReviewer3", "ICLR.cc/2018/Conference/Paper499/AnonReviewer1"], "reply": {"forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper499/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642457318}}}, {"tddate": null, "ddate": null, "tmdate": 1514318488842, "tcdate": 1514318121697, "number": 1, "cdate": 1514318121697, "id": "ryfVtml7f", "invitation": "ICLR.cc/2018/Conference/-/Paper499/Official_Comment", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["ICLR.cc/2018/Conference/Paper499/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper499/Authors"], "content": {"title": "Clarifications about the method, and corrections of two important factual errors in a review.", "comment": "We sincerely thank all reviewers for their thoughtful feedback. We note that AR2 and AR3 both recommend accepting the paper. AR1 recommends reject, although we believe there are a few critical factual errors in that review that, once corrected, should be reflected in a higher score.\n\nBelow we respond to each review:\n\nAR1:\n\nWe think there are a few important factual errors in this review regarding our method, which should have a substantial effect on the review score. We address these below, and will attempt to improve our writing in the paper to make these points clearer.\n\nFine-tuning: Meta PixelCNN inference is in fact different than standard fine-tuning by gradient descent. With traditional fine-tuning, the procedure is ad-hoc (e.g. how many fine-tuning gradient steps, what learning rate, what batch size) and needs to be carefully designed to avoid under- or over-fitting. With Meta PixelCNN (and model-agnostic meta learning approaches in general), the critical difference is that the fine-tuning process itself is learned. The key reference that will further clarify this point is https://arxiv.org/abs/1703.03400. https://arxiv.org/abs/1710.11622 provides further theoretical justification.\n\nInner loss: In fact L_{inner} is learned; we do not use likelihood as the inner loss. So we indeed learn to maximize likelihood without computing likelihoods at test time, as claimed in the paper.\n\nBelow we respond to the rest of the review feedback.\n\nClarity regarding contribution of different model aspects: For the first method (Attention PixelCNN), we demonstrate a clear quantitative benefit of adding attention to the baseline PixelCNN. Although (Attention + Autoregressive Image Model) is a natural idea, we prove that it does indeed work and show a simple and effective implementation, which will be valuable to the research community.\n\nWhy use separate key and value? As you suggest it is possible to use the same vector as both key and value. However, separating them may give the network greater flexibility. An ablation here where key and value are the same could be a good experiment, which we are happy to add to the paper.\n\nAssumption of ordered support set: The order can be randomly chosen (and in fact is in our experiments), so the use of a channel for support image identifier should not limit the generality of the method.\n\nWhy flexibility of L_{inner} is useful: There are several reasons that we might want L_{inner} to be flexible. For example, a learned L_{inner} may be more efficient to compute than alternatives, as in this paper, or L_{inner} may require less supervision, for example see https://arxiv.org/abs/1709.04905.\n\nConnection between first and second model: The only connection is that they are autoregressive models based on PixelCNN. They are independent models.\n\nAR2:\n\nPresentation: Thank you for the suggestions on how to improve the presentation; indeed combining the tables seems like a good idea.\n\nScalability of attention: Indeed, this is one of the major challenges in scaling to high-resolution images. Potentially the memory would need to become hierarchical, or we would need to delve more into multiscale variations of the few-shot learning model, which is an interesting area of future research.\n\nAR3:\n\nMeaning of keys/values: The pairs of (query, key) vectors are used to compute the attention scores. Then, the \u201cread\u201d output of memory is the sum of all value vectors in memory each weighted by the normalized attention scores.\n\nLog-likelihood results units: \u201cBits/dim\u201d results are interpretable as the number of bits that a compression scheme based on the PixelCNN model would need to compress every RGB color value (see e.g. https://arxiv.org/pdf/1601.06759.pdf page 6 for discussion). Nats/dim is the same but multiplied by ln(2). Concretely, in TensorFlow we can compute this value using tf.softmax_cross_entropy_with_logits or tf.sigmoid_cross_entropy_with_logits and then dividing by the total number of dimensions in the image.\n\nPublic PixelCNN replication: A great resource for this is https://github.com/openai/pixel-cnn, which is state of the art, and straightforward to modify. Furthermore, we are happy to help guide researchers replicate our experiments, especially on Omniglot which is now a common benchmark."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732609, "id": "ICLR.cc/2018/Conference/-/Paper499/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1wEFyWCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper499/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper499/Authors|ICLR.cc/2018/Conference/Paper499/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper499/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper499/Authors|ICLR.cc/2018/Conference/Paper499/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper499/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper499/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper499/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper499/Reviewers", "ICLR.cc/2018/Conference/Paper499/Authors", "ICLR.cc/2018/Conference/Paper499/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732609}}}, {"tddate": null, "ddate": null, "tmdate": 1510257290313, "tcdate": 1510257290313, "number": 1, "cdate": 1510257290313, "id": "ByMcMEGJG", "invitation": "ICLR.cc/2018/Conference/-/Paper499/Public_Comment", "forum": "r1wEFyWCW", "replyto": "r1wEFyWCW", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Previous work", "comment": "Hi I think it would be worth adding https://arxiv.org/abs/1612.02192 to your related work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.", "pdf": "/pdf/e28ae6e6977da3b13d1c15287710bc92ace11d99.pdf", "TL;DR": "Few-shot learning PixelCNN", "paperhash": "reed|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions", "_bibtex": "@inproceedings{\nreed2018fewshot,\ntitle={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\nauthor={Scott Reed and Yutian Chen and Thomas Paine and A\u00e4ron van den Oord and S. M. Ali Eslami and Danilo Rezende and Oriol Vinyals and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1wEFyWCW},\n}", "keywords": ["few-shot learning", "density models", "meta learning"], "authors": ["Scott Reed", "Yutian Chen", "Thomas Paine", "A\u00e4ron van den Oord", "S. M. Ali Eslami", "Danilo Rezende", "Oriol Vinyals", "Nando de Freitas"], "authorids": ["reedscot@google.com", "yutianc@google.com", "tpaine@google.com", "avdnoord@google.com", "aeslami@google.com", "danilor@google.com", "vinyals@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791685285, "id": "ICLR.cc/2018/Conference/-/Paper499/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "r1wEFyWCW", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper499/Authors", "ICLR.cc/2018/Conference/Paper499/Reviewers", "ICLR.cc/2018/Conference/Paper499/Area_Chair"], "cdate": 1512791685285}}}], "count": 7}