{"notes": [{"id": "3JI45wPuReY", "original": "qJ832i6O6P0", "number": 783, "cdate": 1601308091184, "ddate": null, "tcdate": 1601308091184, "tmdate": 1614985673640, "tddate": null, "forum": "3JI45wPuReY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oW4FaW6ip4", "original": null, "number": 1, "cdate": 1610040482524, "ddate": null, "tcdate": 1610040482524, "tmdate": 1610474087639, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes a framework to search for the topology of an artificial neural network jointly with the network training, via a genetic algorithm that can decide structural actions, such as addition or removal of neurons and layers. An extra heuristic based on Bayesian information criterion helps the optimization process decide on its decisions about the topology. They demonstrate improvements over baseline fully-connected networks on SVHN and (augmented) CIFAR-10.\n\nReviewers and myself agree that this is an interesting idea, and that the paper is easy to follow. While I may not agree that we need to achieve SOTA on these datasets, or see large scale ImageNet-type experiments for novel ideas, I agree with the reviewers, esp R1's point that the current experiments are not satisfactory to meet the bar for acceptance at ICLR.\n\nCIFAR-10 and SVHN are well-established tasks, and showing baseline accuracy of 75%/48% on them respectively doesn't seem to do them justice, especially when most methods (even with low compute requirements) can get > 95% on both, for the past few years. For this work to be of interest to the broader community, it needs to be improved to incorporate at least respectable baselines on these small datasets, and perhaps be improved to work beyond fully connected networks.\n\nAt this stage, we need to see a revision of the method and see improvements before an acceptance decision can be made."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040482512, "tmdate": 1610474087624, "id": "ICLR.cc/2021/Conference/Paper783/-/Decision"}}}, {"id": "R2246-NIOSf", "original": null, "number": 5, "cdate": 1606183482621, "ddate": null, "tcdate": 1606183482621, "tmdate": 1606183617786, "tddate": null, "forum": "3JI45wPuReY", "replyto": "0v6kpi4hV1d", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Comment", "content": {"title": "Comments", "comment": "Answering the authors' question, Regarding mainstream NAS algorithms, there have been many including DARTS, ProxylessNAS. I believe if this paper can be interweaved with these works, it would be interesting. In fact, presenting ImageNet results in relation to such direction of work could be a better application of the technique presented in the paper.\n\nHaving reviewed the revised manuscript and the authors' comments, as the work still lacks the results for ImageNet and yet hard to see how it can be applied to the current mainstream NAS, I will stay with the same score."}, "signatures": ["ICLR.cc/2021/Conference/Paper783/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3JI45wPuReY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper783/Authors|ICLR.cc/2021/Conference/Paper783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867227, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Comment"}}}, {"id": "bQptJQO1kVT", "original": null, "number": 3, "cdate": 1605717867025, "ddate": null, "tcdate": 1605717867025, "tmdate": 1605717867025, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Comment", "content": {"title": "To all reviewers", "comment": "We want to thank all the reviewers for the time and effort they dedicated to provide feedback for our manuscript, and are greatful for the insightful comments and valuable suggestions for improvements to our paper.\nThere have been concerns about the extendability and comparison to other state of the art results on larger datasets such as ImageNet.\nWe will address all concerns individually, but in summary want to clarify that this publication is intended as a proof of concept and as such does not aim to compete with state of the art accuracy levels.\n\nNovelty of results:\nWe see the novelty of our work in the combination of several techniques. There are a number of examples in the literature making use of either SVD or net2net techniques, but to the best of our knowledge these have not been applied in combination as a genetic algorithm, that does not rely on a black-box decision module.\nWe have included an additional paragraph at the end of section 2 (Related Work) to stress this point more clearly.\n\nConvergence to optimum:\nWe regard a proof of convergence as outside of the scope of this publication, but of course recognize its importance.\n\nRepresentation of network evolution:\nWe tried to depict network evolution in figures 2 and 3, where we present the topology before and after applying the surgeon (fig 2), as well as the evolution of the number of network parameters over time (fig 3). We are aware that neither figure provides a comprehensive overview of the evolution over time.\nWe have included a new figure (4) similar to figure 3 (right), where we hope that some of the adaptations made by the surgeon are highlighted in a more tangible way.\n\nComparability with state of the art:\nWe are aware that the results we present fall short of any state of the art accuracy levels. This is in part due to the fact that current state of the art is reached by much more complex networks/methods, which often rely on huge amounts of training time and quite large network structures. \nThe results we present were all achieved with fixed hyperparameter settings and no additional tuning was performed, to avoid meta-overfitting. \n\nImageNet:\nWe agree that additional experiments on larger scale datasets such as ImageNet can allow for better comparability with state of the art results.\nHowever we also feel that results derived from this dataset will still be somewhat limited by our current restriction to only fully connected networks.\nIntegrating more sophisticated structures such as convolutional layers goes beyond the scope of this publication, but will be included in a follow up publication.\n\nImproving mainstream neural architecture search algorithms:\nThis is a very interesting idea. Do you have any specific algorithms in mind, that in your opinion might particularly benefit or are well suited?\n\nWe hope that we have sufficiently addressed all concerns and welcome further discussion.\nBased on your recommendations we added another paragraph to section 2 (Related Work), as well as an additional figure (Fig. 4) to help understand the evolutionary steps performed by the Surgeon."}, "signatures": ["ICLR.cc/2021/Conference/Paper783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3JI45wPuReY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper783/Authors|ICLR.cc/2021/Conference/Paper783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867227, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Comment"}}}, {"id": "5knkbiyADxo", "original": null, "number": 1, "cdate": 1603718832622, "ddate": null, "tcdate": 1603718832622, "tmdate": 1605024606315, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Review", "content": {"title": "suggest a rejection", "review": "Summary:\nThis study aims to search for topology jointly with the network training. The topology is optimized by a heuristic search in structural modifications including adding/removing neurons/layers. Experiments on SVHN and CIFAR-10 are conducted to show the effectiveness. \n\nPros:\nThe paper is well organized. The proposed method enjoys good interpretability because the decision of modification is made based on the weight matrix properties by SVD and projecting onto its lower-rank subspaces. \n\nCons:\n1.\tThe major problem of this study is its extendibility. The method is only tested on fully-connected networks with very limited network depths and layer sizes. When the network is deep with much more non-linear activations, is the proposed method still valid? Besides, the authors only claim that the method can be generalized to convolutional network. But there is no corresponding description about how to generalize to convolutional network and its experimental results. \n2.\tThe current experiments are not sufficient. The network and datasets used to test are too simple and the performance is far from being acceptable. It is weird to use fully-connected network on real-image datasets, CIFAR-10 and SVHN. Besides, the training time, computational and memory cost should also be reported and compared with the baseline method. \n3.\tThe novelties are limited. The methods proposed in this paper are mainly heuristic implementations. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper783/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135100, "tmdate": 1606915795453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Review"}}}, {"id": "0v6kpi4hV1d", "original": null, "number": 2, "cdate": 1603844824833, "ddate": null, "tcdate": 1603844824833, "tmdate": 1605024606249, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "The paper seems to devise a set of modification modules and develops an overall genetic algorithm that utilizes the recommendation module to identify optimized topologies for networks.\n\nThe paper is a combination of genetic algorithm with recommendation rule which builds on some tools like BIC. While these tools are organized in a reasonable manner, the overall idea seems rather simplistic and the contributions seem rather humble.\n\nImportantly, the main concerns about the paper revolves around the evaluation. For example, the surgeon is tested on very small networks. As such, the evaluation seems very primitive. Without the results of networks for large datasets like ImageNet, it is hard to give credit to their evaluation nor gain confidence about the performance of this method in real scenarios.\n\nAlso, it would be very interesting to see the evolution of the topology. Maybe some figures may help the readers visualize the overall procedure. While this may be excessive for small networks like the ones authors rely on, I believe this may lead to new and interesting observations.\n\nFurthermore, it would be very inspiring if the paper presents some prospect of the work in improving the mainstream neural architecture search algorithms. I believe such evaluation with some empirical results would make this a much more relevant paper for people working on improving neural architecture search.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper783/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135100, "tmdate": 1606915795453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Review"}}}, {"id": "cLPFQSLgaM0", "original": null, "number": 3, "cdate": 1604383583434, "ddate": null, "tcdate": 1604383583434, "tmdate": 1605024606181, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Review", "content": {"title": "Good framework, weak theoretical ground and experiments ", "review": "This paper presents a neural network training framework based on a genetic algorithm called Surgeon. The idea consists of two modules. First, a series of network structure changes that aims to minimize the network input and output. Second, a heuristic (with regard to the objective function) for ranking and selecting the potentially good network modifications. The algorithm works iteratively between the former propose candidate network changes, and the later decides which candidate to accept and evolve into.\n\nThe paper is well written and easy to follow. As the authors stated, the paper is the first of the series of work. The framework presented is rigorous and flexible. I am fairly optimistic that the idea itself is pointing to a promising direction for exploring the neural network structure. However, I think the theoretical basis and experiments are a bit weak in their current form.\n\nI have two main concerns. First, the heuristic used by Surgeon, either by weights or by BIC, does not establish direct association to the objective function that the network is optimizing. Therefore there is no any form of guarantee that the evolution will converge to some optima. Second, the experiments are initiated from rather simple baseline, without comparing against more sophisticated work or discussing why other state-of-the-art results are not relevant in this context.\n\nOverall I like this paper, but for the two reasons listed above I think it may fall short to be accepted.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper783/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135100, "tmdate": 1606915795453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Review"}}}, {"id": "DgjgMZvsBke", "original": null, "number": 4, "cdate": 1604889391997, "ddate": null, "tcdate": 1604889391997, "tmdate": 1605024606115, "tddate": null, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "invitation": "ICLR.cc/2021/Conference/Paper783/-/Official_Review", "content": {"title": "novelty & experiment", "review": "this paper presents the Surgeon, a ANN/evolutionary algorithm hybrid optimization de- signed for neural architecture search. On SVHN and CIFAR-10, the network generated by the Surgeon is able to outperform the baseline in case of suboptimal topologies, or reach comparable accuracies while pruning the underlying network structure to less resource-intensive topologies. My major concern is the novelty, as the SVD technique and net2net techniques have all be proposed before. The experiment is not solid. The accuracy on Cifar is far from the state of the art. There is no large scale datasets. There's no performance comparison with related work, making the paper less convincing. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper783/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper783/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Network Surgery: Combining Training with Topology Optimization", "authorids": ["~Elisabeth_Schiessler1", "~Roland_Aydin1", "kevin.linka@tuhh.de", "christian.cyron@hzg.de"], "authors": ["Elisabeth Schiessler", "Roland Aydin", "Kevin Linka", "Christian Cyron"], "keywords": ["Neural Architecture Search", "Genetic Algorithm", "SVD"], "abstract": "With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for incurred change in input-output behaviour. Our ansatz is tested on both the SVHN and (augmented) CIFAR-10 datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy.", "one-sentence_summary": "We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.", "pdf": "/pdf/f610120725facb9cb89b7d61686b983bdb2863d0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schiessler|neural_network_surgery_combining_training_with_topology_optimization", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zNwa91zLVi", "_bibtex": "@misc{\nschiessler2021neural,\ntitle={Neural Network Surgery: Combining Training with Topology Optimization},\nauthor={Elisabeth Schiessler and Roland Aydin and Kevin Linka and Christian Cyron},\nyear={2021},\nurl={https://openreview.net/forum?id=3JI45wPuReY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3JI45wPuReY", "replyto": "3JI45wPuReY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135100, "tmdate": 1606915795453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper783/-/Official_Review"}}}], "count": 8}