{"notes": [{"id": "Nc3TJqbcl3", "original": "veu_evhUVo8", "number": 1844, "cdate": 1601308203279, "ddate": null, "tcdate": 1601308203279, "tmdate": 1615999867951, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CCyhzctR39S", "original": null, "number": 1, "cdate": 1610040391472, "ddate": null, "tcdate": 1610040391472, "tmdate": 1610473985829, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes a method to solve high-dimensional, continuous robotic tasks offering a trajectory optimization and a distill policy. The paper is well-written and the work is promising. It is very relevant for the robotics and RL communities."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040391456, "tmdate": 1610473985812, "id": "ICLR.cc/2021/Conference/Paper1844/-/Decision"}}}, {"id": "YYd38NRx7KL", "original": null, "number": 9, "cdate": 1606204060046, "ddate": null, "tcdate": 1606204060046, "tmdate": 1606204060046, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "ppTYm17KzDS", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Response to Reponse", "comment": "Thank you for your response and for checking our paper again. \nYou are right, we have changed to formulation to phrase it as future work and give reasons why this can work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "ppTYm17KzDS", "original": null, "number": 8, "cdate": 1606177995041, "ddate": null, "tcdate": 1606177995041, "tmdate": 1606177995041, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "MDMNh8L_ReG", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your response. The paper looks better now, and I agree the paper has solid contributions. However, I don't think \"believing\" the approach could be combined with a learned model/other IL methods is enough. I would suggest you mention them as future works instead of conclusions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "D6-qb5BKMwv", "original": null, "number": 5, "cdate": 1605950934858, "ddate": null, "tcdate": 1605950934858, "tmdate": 1605952082025, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "iwFxhDd6IJ", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Answer to Reviewer 2", "comment": "Thank you for the thorough review and that you like our paper. Please also check our general answer above.\nRegarding your comments listed under Cons:\n\n1) We see our novelty also in the integration, namely that the optimizer can improve with the policy, that policy learning is stable, etc. Regarding the adaptation of lambda: it helps to avoid premature convergence, as you correctly remarked, which is supported by several experiments (Fig. 3, 4, and 6). \nWe believe that avoiding policy collapse is an important improvement over the baselines. \n\nWhy our formulation and were does it help? \nIn addition to the effect above, our formulation allows specifying a more intuitive hyperparameter: the relative impact of auxiliary costs. If the main cost changes during the task over one order of magnitude (very common in sparse reward settings) a fixed lambda value is hard to define. We updated the text to motivate this better.\n\nAs regards the lambda evolution, we find it a great idea so we added a plot showing how it changes during a typical rollout at the beginning and later in learning, see Appendix D.\n\n2) Thank you for pointing that out, we changed them. Now Fig. 4 is split into two and some details are in the appendix. Fig 5 uses a more visible color scheme now.\n\n3) At the time of submission our runs were not completed, we are sorry for that. We updated it (now Fig 5 and Fig 8) which now shows that the policy can match the performance of the original iCEM expert and improve upon it in some cases. In cases where we cannot catch up, we think it is mostly due to iCEM finding brittle behaviors that are hard to imitate. \n\n4) and 5)  The reason which motivated this paper was to understand why it is so difficult to extract a policy from a stochastic optimizer like CEM. This was also shown in POPLIN [1], where only for simple environments, training the policy with Behavioral Cloning was successful. Their Cheetah performance was already not able to keep up with the expert\u2019s one (below 500 for BC training - around 1600 for GAN training). For more complicated environments their policy performance \u201cis almost random\u201d. Note that for those same environments the expert reached very high performance (with learned model), but the policy still lagged behind by several orders of magnitude. Since we decided to focus only on matching the expert\u2019s performance, we did this in isolation to the model learning problem.\n\n[1] Exploring model-based planning with policy networks, Tingwu Wang \\& Jimmy Ba, ICRL 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "s6h_rLJV1ap", "original": null, "number": 4, "cdate": 1605950342848, "ddate": null, "tcdate": 1605950342848, "tmdate": 1605952049465, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "HG1uoZVFanq", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Answer to Reviewer 3", "comment": "Thank you for your comments that we used to improve our paper. Please also check the general answer above. We would like to clarify your doubts:\n\n1) Apart from combining GPS and DAgger (similar to PLATO) our contribution is to integrate it with zero-order optimizers like CEM (iCEM): This required to address the problem of stochastic multimodal optimizer, something that might be of interest in a wider context.\nIn addition, our integration allows the optimizers to improve through the policy, also a novelty.\nWe agree that this was not sufficiently explained in the paper, despite being very important, so we added an explanation at the end Sec. 3.4 about the teacher improvement to better address your concerns.\n\n2) We cannot explain precisely why APEX without policy samples is initially performing better. One reason might be that initially policy actions are \"winning\" in the iCEM optimization and are not providing new learning signals.\nImportantly, however, the expert is not improving over iCEM significantly if policy samples are not added. We added figure 7 to show this effect. The updated plots better show asymptotic behavior.\n\n3) In general, our purpose was to extract strong policies from the trajectories produced by a population-based optimizer, rather than assessing the policy\u2019s performance on learned models. In other words, to close the gap between the performances of experts and policy. In POPLIN, in fact, the learned policy performance is still, in the best case, one order of magnitude lower than CEM. \nNevertheless, with our new insights and our method, we believe that our method will transfer. Learned models can actually be beneficial for a simple reason: CEM will not be able to exploit the simulator, producing overfitted and brittle solutions. As a result, the expert CEM actions will be easier to imitate by a policy. \n\nWe added a more detailed discussion about learned-models and how our method can be used in the conclusion section. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "v8atqwJcS9", "original": null, "number": 7, "cdate": 1605951979210, "ddate": null, "tcdate": 1605951979210, "tmdate": 1605951979210, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "4PmdEIolPdO", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Answer to Reviewer 1", "comment": "Thank you for your review and your positive evaluation of our work. We have used the comments to improve the paper. See also our general answer above.\n\n1) We agree that the plots were difficult to understand. We unpacked them now into 2 figures and deferred some detail to the appendix. Please check the updated paper.\n\n2) We cleared up what we meant with that sentence by adding Fig.7. It is shown that the expert in APEX is only noticeably improving upon iCEM if policy samples are added. That is why the APEX policy can improve upon \"APEX without adding policy samples\". We are currently running longer training to show this effect clearer. As additional evidence, in Fig 8 (Appendix) you can see how for a low budget the APEX policy improves significantly above the iCEM-expert performance.\n\n3) All solid curves show pure policy performance. Dashed lines show expert performances (with or without policy warm-starting etc). We made that clearer in the caption.\n\n4) Thank you for the suggestion. We could not run extensive experiments with noisy dynamics now, but preliminary results show that it plays more in our favor. We added a discussion into the conclusion section on this aspect. \nA perfect model can also play against policy extraction because the iCEM expert is extremely good at exploiting the simulator and finding highly specialized and overfitted solutions. We can provide two primary examples: \n\n- in the HalfCheetah environment, the optimizer (even plain CEM) finds an unphysical rolling/flipping motion by exploiting the coarse time discretization of the system \n(that eventually leads to ever-increasing speeds until the simulation crashes when using the optimizers). This flipping motion is very hard to learn by a policy via imitation learning. It is also found by a model-free optimizer like SAC but not by model-based baselines like PETS-CEM. We think that the rationale is that CEM couldn't converge to that kind of solution when the model is inaccurate and takes into account uncertainty.\n\n- in the Fetch Pick and Place task: Sometimes the task is solved not by fetching, picking, and placing the box to the goal, but by snapping it to the ground with one finger and let it fly to the goal. Of course, that can only happen if the agent has perfect information about the dynamics. Trying to learn a policy from such a demonstration is extremely hard and we argue that an imperfect (or rather, stochastic) model can help generate more robust solutions.\n\nThe multimodal and stochastic nature of the optimizer will not go away with learned models so we believe our method will transfer well. However, as there are also challenges with learning good models, isolating problems seems reasonable to us.\n\n5) We agree, the HalfCheetah is not a hard robotic task but it was one of the environments to fail in the POPLIN paper, where they tried for the first time to extract a policy from CEM. We thought it was important to report the results also on this task. We changed the wording."}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "MDMNh8L_ReG", "original": null, "number": 6, "cdate": 1605951617257, "ddate": null, "tcdate": 1605951617257, "tmdate": 1605951617257, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "QvokfzMxhw6", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "Answer to Reviewer 4", "comment": "Thank you very much for your helpful review that we used to improve our paper.\nSee also our general answer above.\n\nAnswer to \"weak points\":\n\n1) When we tried the first time to extract a policy from iCEM we realized that the simple combination of methods was not enough. Namely, using GPS and DAgger, like done in PLATO, was producing very poor performing policies. The reasons are stochasticity and multimodality of CEM. GPS partially fixes the first problem, but not the second. \nWith this paper, we primarily want to shed some light on why it is *not* straightforward and eventually propose a pipeline that works, even where model-free algorithms fail. \n\n2) Yes, as you said perfect dynamics was necessary for an unbiased analysis. But, as we already said to Reviewer 1 (answer 4) and Reviewer 3 (answer 3), having a ground truth model is *not necessary* for APEX to work. We only used them to have fewer moving parts. We have seen that having a perfect model and a nearly-optimal optimizer can produce very performing but also very brittle solutions, which are much harder to learn by a policy. The problems we identified with policy extraction are identical in the learned-model case. We updated the Conclusions.\n\n3) The sparse/semi-sparse reward setting is the largely preferred way for a user to actually specify the task. Dense reward versions are hand-engineered guesses on how good guidance looks like. In many cases, they cause unwanted side-effects and have to be tuned carefully. \nThus the sparse rewards are important, also in the model-based case, and remain a big challenge. Sampling-based optimizers are a good choice when the user does not want to design a cost by hand. We used semi-sparse rewards (e.g. object to goal distance, but not hand to object cost) in the experiments to make a comparison with SAC, and to show that model-based planning can rival model-free methods.\nHowever, we agree that providing suitable proxy costs can only help iCEM and APEX.\n\n4) In POPLIN [1] they show different approaches to policy extraction. In some cases, GAN training performs much better than BC. We actually do not aim at comparing with imitation learning (IL) baselines: our method can be applied to whatever IL framework.\nBut as you proposed, it could have been interesting to show that APEX can potentially benefit from other forms of IL like GAN training. Nevertheless, this was outside the scope of our contribution and we encourage other researchers to use APEX inside of their pipelines. \n\nAnswer to minor issues:\n\n1) We have no value function. We clarified the statement.\n\n2) It means that lines 10 and 11 of Alg. 2 (APEX) are not executed. The policy is trained only with off-policy behavioral cloning, while policy samples are still used by the expert. We explain the ablations in detail in the newly added Appendix B. \n\n3) The higher the budget the larger the performance, however, it is most interesting to get strong performance with relatively cheap (low sample number) iCEM. Currently, the computational budget is dominated by ground-truth simulations. Their speed depends on the system complexity. As shown in the iCEM paper, with learned models the speed can be real-time for the expert. We report wall-clock times in the appendix now.\n\n4) Reported SAC performance is at convergence, so it is just used for reference, as stated in the caption. The iteration number is not referring to SAC, that is why it is a horizontal dotted line. \n\n5) Thank you for bringing this up. We introduce this in a better way now.\n\n[1] Exploring model-based planning with policy networks, Tingwu Wang \\& Jimmy Ba, ICRL 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "Clqdl_mUuwV", "original": null, "number": 3, "cdate": 1605949106206, "ddate": null, "tcdate": 1605949106206, "tmdate": 1605949106206, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment", "content": {"title": "General Answer", "comment": "We thank the reviewers for their positive reviews and the constructive criticism which we used to improve and refine the paper. In particular:\n\n- we made the plots easier to parse, both visually and conceptually, respectively with additional figures and further explanations (added Fig. 5, better color code in Fig. 6, added Fig. 7, added, added Fig. 8 and section C, added Fig. 9 and section D)\n\n- we updated the rest of the figures with longer training curves (some runs are not yet finished and will be updated of course)\n\n- we clarified the problem of transferring to learned models by adding a paragraph in the conclusions, thanks to reviewer 1, 3 and 4\n\n- we added a section in the appendix on the adaptive lambda and its evolution in time, which was an interesting suggestion by reviewer 2\n\n- we added a section in the appendix explaining the ablations in details \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Nc3TJqbcl3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1844/Authors|ICLR.cc/2021/Conference/Paper1844/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855138, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Comment"}}}, {"id": "4PmdEIolPdO", "original": null, "number": 1, "cdate": 1603524887271, "ddate": null, "tcdate": 1603524887271, "tmdate": 1605024345392, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review", "content": {"title": "review1", "review": "This paper presents a method to combine zero order optimizer with imitation learning. Several components are essential for good performance, including policy guided initialization, DAgger and adaptive auxiliary loss.\n\nOverall the paper provides a clear description of the essential components of the algorithms and the result is also quite strong.\n\nSeveral comments below:\n\n(1)The right most figures in Figure 4 is really hard to unpack with so many information and almost similar colors. Honestly I don't understand these figures, it would be nice if the authors make it clearer, either in the caption or make cleaner figures.\n\n(2) For the first figure in Figure 5, the text said \" it performs\ninitially better than with adding the samples, but the experts do not improve with the policy in this\ncase such that the asymptotic performance is lower\". But the figure shows it has the same final performance as Apex, am I missing something here? Also more distinguished color coding can make the figures easier to digest.\n\n(3) Maybe related to the previous question, what does the learning curve shows? Is it performance of the policy or the performance of the ICEM warm starting with the policy?\n\n(4) How important is an accurate dynamics? All the experiments are done with perfect dynamics information, which I also believe is deterministic, which is not possible in real world scenario. Some experiments with dynamics noise added or with learned model in the CEM rollouts can make the paper's claim stronger.\n\n(5) half cheetah is not a difficult task as claimed in the introduction of the paper. The paper itself shoes SAC can already perform very well. Actually this task is a very strange choice compared to other tasks considered. I would recommend something that is more like the others.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109536, "tmdate": 1606915770055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review"}}}, {"id": "iwFxhDd6IJ", "original": null, "number": 3, "cdate": 1603903616173, "ddate": null, "tcdate": 1603903616173, "tmdate": 1605024345322, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review", "content": {"title": "Distillation of a planner expert into a policy; More analysis and baselines needed.", "review": "This paper presents an approach to distill a model-based planning expert into a policy to enable real-time execution on robotic systems. An improved version of the Cross-Entropy Method, iCEM is used to generate trajectories via model-based optimisation where the forward model is the ground-truth simulator dynamics. The expert is further improved by warm-starting using the learned policy. The policy is learned via imitation learning of trajectories from the expert. Several approaches for this step are presented, including the vanilla approach of behaviour cloning (BC), dataset aggregation (DAGGER) to reduce covariate shift and Guided Policy Search (GPS) where the planner is encouraged to keep close to the policy distribution via a trust region KL loss. The paper discusses the merits of each approach and proposes Adaptive Policy Extraction (APEX), an approach that learns the policy from the expert through a combination of DAGGER and GPS where the tradeoff between the cost terms on planner exploration and policy trust region KL is set adaptively. This approach is tested on four continuous control tasks and achieves good performance compared to baselines \u2014 the learned policy gets significant improvements compared to the baselines. Additionally, a study that ablates different parts of the APEX algorithm is also presented.\n\nPros:\n1. This paper tackles the problem of distilling model-based planner trajectories into a policy. This is a key problem in model-based RL where planner + policy proposals can often exceed the performance of the policy but the former can be too computationally expensive to run on the real system. The proposed approach takes some steps towards this by combining ideas from prior work (GPS, DAGGER, iCEM) to improve policy learning on challenging continuous control tasks.\n2. The paper is well written and motivated. The experiments are well structured and provide insights into the struggles of BC and other imitation learning methods when combined with stochastic optimisers like CEM. Different ideas such as DAGGER and GPS are introduced and the combination of these together with adaptive weighting is clearly motivated.\n3. The proposed approach leads to significant improvements compared to baseline model-free methods and other imitation learning variants on the presented tasks. Additional experiments including ablation studies also establish further intuition on different parts of the proposed method.\n\nCons:\n1. The proposed approach primarily combines ideas from prior work including GPS and DAGGER. One piece of novelty is the adaptive setting of the \\lambda parameter \u2014 this is set to be a fixed scalar times the ratio between the range of the main task cost and the range of the auxiliary costs. When all trajectories get the same or similar main task reward (as can happen in sparse reward tasks) this goes to zero, thereby avoiding collapsing the planner to the policy and leading to premature convergence. As evidenced in the experiments, it is not clear if this adaptation helps anywhere else (and it also comes across as a bit hacky). If this is the case, why not use a simpler adaptation scheme that just sets the lambda to zero when the task reward is uniform and a fixed lambda otherwise? In general though, I find the adaptive lambda not well motivated. It would be useful if there is more analysis on the behaviour of this adaptation throughout learning \u2014 maybe a plot of \\lambda throughout learning can shed more light.\n2. The plots in Fig 4 and Fig 5 are quite hard to parse \u2014 there are a few places where the colors of the lines are flipped (see eg. Door in both figures, the APEX line is miscoloured). Additionally, there are a few lines that are missing/appear extra in some plots (see eg. Bottom middle plot in Fig 4 where the dashed line is not relevant & bottom right where some lines are missing). It would be great if these can be fixed.\n3. Looking at the results in Fig 4 it looks like the expert (iCEM + Policy proposal) is still ~2x better than the learned policy even with the proposed approach. What is the reason for this? Can this gap be shortened further? It is a bit unsatisfying that the policy cannot match the performance of the planner.\n4. There are no strong model-based baselines provided apart from different variants of imitation learning coupled with iCEM (which are essentially ablations of the proposed method APEX). It would be great if a prior method which shows results on the Cheetah (Wang et al, 2020 from the paper) can be compared with the proposed approach. This should give a better understanding of the relative strengths of the proposed method compared to the state of the art.\n5. As the paper mentions, one key limitation of the approach is the use of a GT dynamics model. A next step could be to extend the approach to train models jointly with the policy. It would be useful to have a longer discussion on this in the paper.\n\nOverall, the paper presents some interesting ideas and the results are promising. More analysis on the adaptation scheme and better baselines are needed to significantly strengthen the paper. I would suggest a borderline accept.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109536, "tmdate": 1606915770055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review"}}}, {"id": "HG1uoZVFanq", "original": null, "number": 4, "cdate": 1604248091901, "ddate": null, "tcdate": 1604248091901, "tmdate": 1605024345258, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review", "content": {"title": "Review", "review": "The paper focusses on how to extract good policies from experts in imitation learning scenarios. The paper proposes a series of trick for policy extraction from Cross-Entropy-Method-based (CEM/iCEM) trajectory optimizers. The algorithm integrates iCEM with DAgger and Guided Policy Search. \nThe extracted policies show good performance versus a model-free baseline (Soft-Actor Critic) and various other extraction baselines (Behavior Cloning, DAgger, Guided Policy Search).\n\nStrengths:\n1. The motivation of the paper is clear. In particular, a pretty thorough analysis of the state of the art and its limitations motivates the current approach\n2. Experiments are conducted on various standard benchmarks (Humanoid Standup, Half Cheetah, DOOR, Fetch Pick& Place) including sparse reward domains\n3. Results are convincing\n4. An ablation study shows how different parts lof the proposed method do interact with different environments\n\n\nWeakness/Comments:\n1. It seems that this is primarily a combination of existing methods. What exactly is the new contribution apart from recombination. \n2. Why does not adding policy samples have a counter-intuitive effect in HALFCHEETAH RUNNING?\n3. Why do you think this will transfer to model-based RL with learned models\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109536, "tmdate": 1606915770055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review"}}}, {"id": "QvokfzMxhw6", "original": null, "number": 2, "cdate": 1603864815997, "ddate": null, "tcdate": 1603864815997, "tmdate": 1605024345189, "tddate": null, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "invitation": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review", "content": {"title": "An interesting paper that combines GPS and DAGGER for imitating CEM's policy", "review": "In summary, this paper combines GPS and DAgger to learn a policy network by imitating a model-based controller, which uses iCEM as the optimizer. Their approach uses both the iCEM controller and the learned policy with DAgger-like relabeling to collect data and then train the network with behavior cloning. An auxiliary loss inspired GPS, which encourages the consistency between the expert controller and the learned policy, together with an adaptive weighting method, is added to reduce the multi-modal issue. The experiment results show promising results.\n\nStrong points:\n1. Distilling/imitating CEM policy seems to be a promising approach for solving some control problems, especially when the model is known, or we can learn a good model. Given the deterministic model, CEM can find solutions much faster than an RL algorithm, using CEM to search demonstrations and distilling them into a policy network sounds like a very promising approach in the future.\n2. The authors discuss the issue of stochastic teachers and the multi-modal training data and solve them by combing GPS and DAgger. Their approach is simple yet effective.\n3. The adaptive auxiliary cost weighting is novel to me.\n\nWeak points:\n1. The whole pipeline is not very novel to me. The paper simply adds GPS into DAgger by replacing the LGQ with CEM, which is straightforward if one wants to combine optimal control with neural networks.\n2. However, unlike GPS or other model-based DRL papers, this paper requires the ground truth model, which largely limits its application. I agree it's still important to find a way to first solve the problem in the simulation, but I hope the author can give some words about this assumption.\n3. I don't think that the sparse reward and the flat regions matter in the model-based RL setting, where the cost is usually dense and designed by the user. Given the limited search horizon, for example, in PickAndPlace, if the agent can only receive rewards when the object is close to the goal, and the planning horizon is short, it's unlikely for a CEM controller to find a solution. Why not simply changing the reward to avoid those flat regions?\n4. This work studies how to do imitation learning when the expert comes from a stochastic teacher. However, no imitation learning baseline except the simplest BC is considered. Approaches like GAIL are not evaluated. I don't think  Wang & Ba (2020) can prove that GAN is not suitable in this case. As far as I know, they use the learned model instead of the ground truth model, which is different from the setting in this paper. The authors should at least compare with some STOA imitation learning methods.\n\nBased on the previous weakness, like the loss of the novelty, the missing imitation learning baseline, and the limited applications, I wouldn't recommend acceptance given the current version.\n\nMinor issues:\n1. Section 3. \"Optimal control is obtained if f is the trajectory cost until step h plus the cost-to-go under the optimal policy for an infinite-horizon problem, evaluated at the last state in the finite horizon trajectory. \" I believe some words are missing here. Do you have a value network like POLO in your optimization?\n2. Ablation study. APEX without DAgger? What's that? Does this mean no relabeling in Algorithm 2? What are the differences between that and the one without policy samples? How can you train the policy by sampling the trajectories with the policy network while not relabeling it?\n3. What's the time complexity? In Table 1. of the appendix, some results are blank. Does this mean the computation complexity is still too high to finish them in time?\n4. What do the iterations mean for SAC? Is this comparison fair? I think with the same number of environment episodes, SAC has much fewer environment steps. CEM needs extra environment steps to search for a solution.\n5. Presentation is not very clear. For example, what does \"expert rollout\" means in section 3.3? It's hard for me to guess its meaning without reading the Alg 2. in sec 3.5. There are also some grammar mistakes.\n\nPossible ways to improve:\n1. Maybe the authors can combine their approaches with a learned model like PETS-CEM. It would be great if their approach could also benefit in the model-based RL setting.\n2. Provide more imitation learning comparisons.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1844/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1844/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Nc3TJqbcl3", "replyto": "Nc3TJqbcl3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1844/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109536, "tmdate": 1606915770055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1844/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1844/-/Official_Review"}}}], "count": 13}