{"notes": [{"id": "1TIrbngpW0x", "original": "5LJbmLtwsWf", "number": 2385, "cdate": 1601308263098, "ddate": null, "tcdate": 1601308263098, "tmdate": 1614985760468, "tddate": null, "forum": "1TIrbngpW0x", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "erHYQ9Z-ACF", "original": null, "number": 1, "cdate": 1610040373371, "ddate": null, "tcdate": 1610040373371, "tmdate": 1610473965171, "tddate": null, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "invitation": "ICLR.cc/2021/Conference/Paper2385/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. Improvements are seen on only some datasets and the comparisons are not exact.\nA reject."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "tags": [], "invitation": {"reply": {"forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040373356, "tmdate": 1610473965152, "id": "ICLR.cc/2021/Conference/Paper2385/-/Decision"}}}, {"id": "R1vdH15I01f", "original": null, "number": 2, "cdate": 1603865258793, "ddate": null, "tcdate": 1603865258793, "tmdate": 1606414948813, "tddate": null, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "invitation": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review", "content": {"title": "Interesting idea but the results are not convincing", "review": "This paper proposes an independent mechanism that divides hidden representations and parameters into multiple independent mechanisms. The authors claim that the mechanism benefits the computation of sparse tensors; it does learn better inductive biases than a sizeable monolithic model. This idea is particularly similar to Recurrent Independent Mechanisms (RIM) [1], mentioned in the paper. The main contribution of this work is introducing competition between independent mechanisms. The authors evaluate their models on the image transformer model, speech enhancement, and NLP tasks.\n\nThe main thing that has been missing in this paper is the fine-details of the approaches. \n\nI think the paper has exciting ideas on reconstructing the architecture of the Transformer. However, the proposed models only give marginal improvements; thus it is tough to find the reasons for using this model. \n\nTo have a stronger claim, I suggest adding a comparison to RIM on Sequential MNIST Resolution Task to show that independent mechanisms can benefit the generalization performance.\n\nStrengths:\n- The authors introduce a novel transformer architecture to split the parameters into independent sections for a better inductive bias\n\nWeaknesses:\n- The performance improvement is not consistent in all experiments. The competition mechanism benefits only some tasks.\n- The evaluation results are not convincing\n\nSeveral issues and questions for the authors:\n1. Can you show the dynamics of the competition between mechanisms over time?\n2. In terms of evaluation, I didn't see any baseline for the Image Transformer model. It isn't easy to understand the significance of the proposed method.\n3. Please consider citing the relevant papers [2] [3]\n\n***Post-Rebuttal***\n\n> I want to thank the author for addressing my concerns. However, the authors did not address the issues of the evaluation. I think the paper can be further improved by providing more convincing results and analysis. For me, the significance of the proposed method is minimal. Thus, I will not change my score. \n\nReferences\n1. Recurrent Independent Mechanisms https://arxiv.org/pdf/1909.10893.pdf\n2. LeCun, Y., Cortes, C., & Burges, C. J. (2010). MNIST handwritten digit database.\n3. Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2385/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2385/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097559, "tmdate": 1606915765024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2385/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review"}}}, {"id": "XLt205dbsES", "original": null, "number": 1, "cdate": 1603826543752, "ddate": null, "tcdate": 1603826543752, "tmdate": 1605024223512, "tddate": null, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "invitation": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review", "content": {"title": "Interesting problem statement but experimental setup is not convincing or rigorous", "review": "Authors tackle the problem of making vanilla Transformer better by handling different positions (in data) via independent and competing mechanisms. Formulation is interesting but experimental setup has various deficiencies:\n1. It is quite heuristical to propose to use first two and last layers of Transformer to be vanilla. This suggests that either (1) proposed solution has something deeper going on (uninvestigated), or (2) it is an experimental thing that works for datasets used, or (3) \"competition\" should not happen in initial and later layers of network.\n2. To provide proof-of-concept for independent mechanisms, more complex problems should be picked like image segmentation and speech source separation (more than 2 sources), and they should be investigated with more than 2 mechanisms.\n3. Results should also be provided for TIM without matching its number of parameters to vanilla Transformer. This is to see if TIM still gives good performance or not. Also, in my opinion, increasing number of heads of TIM to match parameters with vanilla is not fair. It usually gives performance improvement. IMO only number of hidden layer units may be increased for matching #parameters.\n4. DNS performance is good but best is achieved without competition mechanism, which shows that independence and competitiveness are not complementary?\n5. Improvement on VoiceBank is decent but baseline chosen in very far away from SOTA. Check: https://arxiv.org/abs/2006.12847", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2385/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2385/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097559, "tmdate": 1606915765024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2385/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review"}}}, {"id": "Ip8881DKBou", "original": null, "number": 3, "cdate": 1603889087953, "ddate": null, "tcdate": 1603889087953, "tmdate": 1605024223453, "tddate": null, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "invitation": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review", "content": {"title": "Simple and efficient Transformer architecture tweak to allow for non-uniform data modelling", "review": "This paper discusses the potential limitation of Transformers on modelling data with clear non-uniform structure. Authors propose to integrate a useful structural inductive bias which helps to separate the information in hidden states of the Transformer (instead of using a single monolithic hidden representation). \n\nThe experiments on a toy image Transformer and speech enhancement demonstrate the effectiveness of the proposed approach. The experiments on BERT pre-training and fine-tuning do not bring much to the table, as the text data is relatively uniform and does not benefit much from the introduced structural inductive bias. As a result, the accuracy of the proposed model on GLUE benchmark is close to the accuracy of BERT baseline.\n\nOne obvious limitation of the proposed approach is the necessity to know the number of \"independent\" components in the data. It would be interesting to see whether it can be generalized to overcome this limitation, e.g., by choosing the number of independent mechanisms automatically.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2385/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2385/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097559, "tmdate": 1606915765024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2385/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review"}}}, {"id": "Ap7AKp8yKlP", "original": null, "number": 4, "cdate": 1604583180142, "ddate": null, "tcdate": 1604583180142, "tmdate": 1605024223329, "tddate": null, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "invitation": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review", "content": {"title": "An interesting idea but a rush paper ", "review": "# Summary of the paper\nIt is motivated from a perspective that a model should be decomposed into independent modules/mechanisms, such that the model is robust to data distribution shift. A TIM layer is proposed to replace the Transformer encoder layer, where a mechanism competition is introduced to suppress all but one group. Experiments are conducted on different Transformer architectures with a replacement of encoder layers by TIM layers in a) autoregressive image generation with Image Transformer, b) speech enhancement with a customized Transformer and c) BERT on language modeling. \n\n# Pros\nThe idea that grouping the hidden activations by multiple mechanisms is a promising inductive bias towards out-of-distribution generalization. It is an interesting attempt to integrate this idea to Transformers. \n\n# Cons\n1. The presentation is terrible: the method description is basically throwing the code (Algorithm 1) to the audience without much explanation. It is hard to understand the TIM layer if neither diagrams nor equations are supplied. \n2. I didn't get what does \"mechanism\" mean in Algorithm 1. The competition will for sure suppress some activations but how can you guarantee that different groups will learn different behaviors or achieve disentanglement? \n3. Experiments only demonstrate n_s = 2 mechanisms. Is n_s > 2 even working? All results suggest that TIM can make some neurons focusing on the foreground, which isn't that surprising as this is most attention based models aiming at. Do you have results showing that TIM can learn diverse functionalities from data which can be interpreted as mechanisms?  \n4. Figure 1 is confusing: how can we know the difference between \"head-wise self-attention\" and \"Mechanism-wise Self-Attention and Inter-mechanism Attention\", which are all represented by a blackbox. What is the competition patterns? How are they computed? \n5. Algorithm 1: functions such as GroupLinear(), Attention() are used before defining. \n6. Figure 2 (middle): what do you mean \"TIM learn to specialize over the two sides of the image\"? Also, what are these figures showing? ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2385/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2385/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformers with Competitive Ensembles of Independent Mechanisms", "authorids": ["~Alex_Lamb1", "~Di_He1", "~Anirudh_Goyal1", "~Guolin_Ke3", "~Chien-Feng_Liao1", "~Mirco_Ravanelli1", "~Yoshua_Bengio1"], "authors": ["Alex Lamb", "Di He", "Anirudh Goyal", "Guolin Ke", "Chien-Feng Liao", "Mirco Ravanelli", "Yoshua Bengio"], "keywords": ["transformer", "mechanism", "modularity", "modules", "independence"], "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ", "one-sentence_summary": "Transformers with Independent Mechanisms which have separate parameters, share information through attention, and specialize over positions.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|transformers_with_competitive_ensembles_of_independent_mechanisms", "supplementary_material": "/attachment/e7d59851e0e114f54e3cbef14665cc6907f94fa0.zip", "pdf": "/pdf/844949cababddd36ef6d17be742269e46f7dd7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VJzsCzb16o", "_bibtex": "@misc{\nlamb2021transformers,\ntitle={Transformers with Competitive Ensembles of Independent Mechanisms},\nauthor={Alex Lamb and Di He and Anirudh Goyal and Guolin Ke and Chien-Feng Liao and Mirco Ravanelli and Yoshua Bengio},\nyear={2021},\nurl={https://openreview.net/forum?id=1TIrbngpW0x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "1TIrbngpW0x", "replyto": "1TIrbngpW0x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097559, "tmdate": 1606915765024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2385/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2385/-/Official_Review"}}}], "count": 6}