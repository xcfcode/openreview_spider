{"notes": [{"id": "S1glGANtDr", "original": "rylKE9NdvH", "number": 990, "cdate": 1569439240170, "ddate": null, "tcdate": 1569439240170, "tmdate": 1583912020707, "tddate": null, "forum": "S1glGANtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "AVnirYQLEA", "original": null, "number": 1, "cdate": 1576798711619, "ddate": null, "tcdate": 1576798711619, "tmdate": 1576800924768, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias.\nThe reviewers unanimously recommend acceptance of this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716433, "tmdate": 1576800266576, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper990/-/Decision"}}}, {"id": "rylbBYHcjS", "original": null, "number": 8, "cdate": 1573701944624, "ddate": null, "tcdate": 1573701944624, "tmdate": 1573763689220, "tddate": null, "forum": "S1glGANtDr", "replyto": "BklNSppdjS", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Response to the Follow-up Questions", "comment": "We thank Reviewer #3 for the follow-up comments. Below are responses to your specific questions:\n\n- 1.Variance analysis\n\nIn general, if $R_{\\mathrm{res}}$ and $R_{\\mathrm{VAL}}$ are negatively correlated for the joint distribution of $\\mu_0$ and $d_{\\pi_0}$, we can reduce the variance. We leave it as future work to study the precise conditions under which it holds.\n\nOn the other hand, we do not claim DR has a lower variance than VAL, which does not seem to be true in general.  This can be seen in the special case of contextual bandits.  Our discussion following Proposition 3.1 is mainly to compare DR and SIS.\n\n\n- 2.Related work\n\nThanks for pointing out this interesting paper which gives a variant of our estimator (they replace V value function with Q action-value function). The paper is mostly theoretical, and it's interesting to have an experimental comparison in future work. We have cited the paper in the related work, see our newest version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "SyxNOqS5oH", "original": null, "number": 10, "cdate": 1573702251661, "ddate": null, "tcdate": 1573702251661, "tmdate": 1573702251661, "tddate": null, "forum": "S1glGANtDr", "replyto": "Hye5X3aYoB", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for raising the score! We will make the paper more clear in the final version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "HkxbciuDjH", "original": null, "number": 2, "cdate": 1573518216898, "ddate": null, "tcdate": 1573518216898, "tmdate": 1573702169761, "tddate": null, "forum": "S1glGANtDr", "replyto": "Syga485aKS", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank Reviewer #1 for the valuable comments and suggestions. We have updated a version based on your feedback, where the more important changes are highlighted in red.\nBelow are responses to your specific questions:\n\n- 1. Equivalent between equation (15) and equation (11) .\n\nThe main difference of equation (15) and equation (11) is that equation (15) uses a density function $\\rho(s)$ which is equal to $d_{\\pi_0}(s)w(s)$ in previous section for the density ratio function $w(s)$. We rewrote equation (11) in an explicit form that makes the connection clearer.\n\n\n-  2.  Generalization of DR to more complicated tasks for OPE.\n\nAlthough this paper focuses on theoretical and algorithmic contributions, we have evaluated the proposed estimator on benchmarks that are popularly used in the recent literature of off-policy evaluation.  That said, we appreciate the suggestion, and are indeed interested in a more systematic empirical evaluation across more tasks with varying complexity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "ryxVg9B9oH", "original": null, "number": 9, "cdate": 1573702124500, "ddate": null, "tcdate": 1573702124500, "tmdate": 1573702124500, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1lZXXAdsS", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Thank you for the suggestion!", "comment": "Thank you for the great suggestion. We will release the code and related benchmarks shortly after the acceptance of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "Hkl5PWypYH", "original": null, "number": 1, "cdate": 1571774818121, "ddate": null, "tcdate": 1571774818121, "tmdate": 1573669956349, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "  *Synopsis*:\n  This paper provides a new doubly robust estimator for off-policy policy evaluation, based on the new infinite horizon technique (i.e. using an estimate of the state density ratio as opposed to long products of action importance weights). They show the doubly robust estimator's bias is dependent on a product of the error of the value function estimate and stationary distribution ratio estimate, which provides improvements over the initial infinite horizon estimator. They also provide some nice discussion of the relation of their method and Lagrangian duality, which was quite interesting and insightful. Finally, the paper shows the usual empirical comparisons.\n\n  Main Contributions:\n  - Doubly robust estimator for infinite horizon off-policy policy evaluation\n\n  *Review*:\n  Overall, I quite like this paper and think the quality is at a high level. The proposed doubly robust estimator is well supported theoretically and empirically and improves on the prior art. I am recommending a weak accept for this paper as it will be a nice contribution for the community, but I have some clarifications/updates I would like to see to improve the readability of the paper (specifically C4).\n\n  I also have a few questions, and clarifications that I would like the authors to address during the rebuttal period.\n\n  *Clarifications*:\n  C1: In the section \"Off-policy state visitation importance sampling\", doesn't equation 4 involve an expectation over the target policy? Or am I missing something here?\n\n  C2: The bias of the estimator decreases as our estimate of the density ratio and value function improves. It might be useful to more clearly compare this bias to the original estimator proposed by Liu.\n\n  C3: Proof of theorem 3.1: I would like you to clearly finish the proof. I think clarity and completeness in appendices is importance over conciseness as there is usually no limit on pages.\n\n  C4: The proof of theorem 3.2 is not obviously linked to theorem A.3. The proof for theorem 3.2 should be clearly stated and included in the appendix, without an unstated implicit link to A.3. This will make your theoretical analysis more clearly understandable, and expendable for future work. I will happily increase my score if this is clarified in a future version. I also will decrease my score if this is not clarified.\n\n  *Competitors*\n\n  - You may want to include a model based approach, just for completeness (as in the infinite horizon paper from Liu).\n\n  *Questions*\n  \n   Q1: I'm curious about the difference between using an ANN for estimating the density ratio (as opposed to a kernel method). Have you run experiments with the kernel method proposed in Liu's paper? While I don't think it is necessarily needed for the paper to be complete, I think the difference would be interesting to see what factors contribute to the algorithm's better performance.\n\n\n  Minor typos not taken into account for the review:\n  - Section 3.1 \"it is useful to exam the\": exam -> examine\n  - Section 4: \"hence yielding an true expected reward\": an -> a\n  - Section 4: \"it is natural to intuitize\": intuitize -> intuit?\n  - The sentence right before section 4 could use a rewrite.\n  - For readability it would be useful to include theorem statements ahead of proofs in the appendix.\n  - Theorem A.3: do you mean with \\hat{R} defined in 3.2 (not the variance as this is not defined in 3.2).\n\nEdit:\n\n- Due to the author rebuttal and updates to the paper I've increased my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877901866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper990/Reviewers"], "noninvitees": [], "tcdate": 1570237743998, "tmdate": 1575877901878, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Review"}}}, {"id": "Hye5X3aYoB", "original": null, "number": 7, "cdate": 1573669921800, "ddate": null, "tcdate": 1573669921800, "tmdate": 1573669921800, "tddate": null, "forum": "S1glGANtDr", "replyto": "ByloMnuDoB", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Thank you for the response", "comment": "Thanks for the response, and the updates to the paper. You have clarified all the questions and concerns I mentioned well, and I think the paper is much clearer. I also appreciate the addition of the model based comparison as a benchmark. \n\nI'm quite happy with the updates and will increase my score."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "S1lZXXAdsS", "original": null, "number": 6, "cdate": 1573606169312, "ddate": null, "tcdate": 1573606169312, "tmdate": 1573606169312, "tddate": null, "forum": "S1glGANtDr", "replyto": "HkxbciuDjH", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Thank you; Open-Source Implementation would make the paper extremely useful!", "comment": "Thank you for your response. \n\nAs for (2) on generalization of DR to more complicated tasks - I do agree, and understand that this may perhaps be beyond the scope of the paper. It would be useful to add more empirical analysis, but certainly not needed, given evaluation is being already made on more of the standard OPE tasks. \n\nOne more comment : There are several recent progress in OPE and correcting for the state distributions in OPE. It would be very useful if you release the code of your paper, and the benchmarks used - as the recent progress in OPE methods will certainly benefit the community.\n\nThere are several benchmark evaluations available for many RL algorithms, but the open-source implementations on OPE benchmarks is certainly lacking to my knowledge. Open-sourcing the code base for the experiments would help the community a lot! :) "}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "BklNSppdjS", "original": null, "number": 5, "cdate": 1573604668035, "ddate": null, "tcdate": 1573604668035, "tmdate": 1573604668035, "tddate": null, "forum": "S1glGANtDr", "replyto": "SJlAGjuDjS", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Thanks the author for the response", "comment": "Thanks the author for the response. \n\n1. One more follow-up question about theorem 3.2. According to your response, it seems that theorem 3.2 assume \\pi and \\mu are *independent*, because it's easy to analysis, but make the variance looks large. In practice, due to \\pi and \\mu are *not independent*, the variance can be lower than the theorem. It's interesting to learn this. In the dependent case, does the variance of DR smaller than the variance of value function? Could you provide some intuition on why the dependent case seems to be a better case for DR? I understand it's could be technically hard to provide formal proof.\n\n2. For the related work, sorry it's my mistake. I think the same authors (Kallus & Uehara) have another work with a similar title (https://arxiv.org/abs/1909.05850), which is using the stationary ratio. I actually meant this one."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "ByloMnuDoB", "original": null, "number": 3, "cdate": 1573518354874, "ddate": null, "tcdate": 1573518354874, "tmdate": 1573527628727, "tddate": null, "forum": "S1glGANtDr", "replyto": "Hkl5PWypYH", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank Reviewer #2 for the valuable comments and suggestions. We have updated a version based on your feedback, where the more important changes are highlighted in red.\nBelow are responses to your specific questions:\n\n- 1. Equation 4 requires an on-policy expectation\n\nEquation 4 is in fact in the subsection \u201cEstimation via State Density Function\u201d.  It gives an alternative yet equivalent expression for the policy value, and is the basis for the next subsection \u201cOff-Policy State Visitation Importance Sampling\u201d (e.g., compare to equation 5).\n\n\n-2. Comparison of the Bias of the work proposed by Liu. et al.\n\nWe did compare the bias of the work proposed by Liu. et al. See the discussion for the paragraph after Theorem 3.1, where we list the result for $R^\\pi_{\\text{SIS}}[\\hat{w}] - R^\\pi$ can be written as the error of $\\mathbb{E}_{d_{\\pi_0}}[ \\hat{w}(s) r^\\pi(s) ]$. We have updated the proof for Theorem 3.1 in appendix which includes the bias analysis also for $R^\\pi_{\\text{SIS}}$ and $R^\\pi_{\\text{VAL}}$.\n\n\n-3. Completeness of Theorem 3.1.\n\nThe revised version now expanded the original proof (still in Appendix A.2).  The new proof includes more details and should make the proof clearer.  We also repeated the theorem statement in the appendix for the reader\u2019s convenience.\n\n\n-4. Clarification of theorem 3.2 (now Proposition 3.1).\n\nThank you for the great suggestion.  The new version gives a complete proof for Proposition 3.1 (Appendix A.3), and also connects it to Theorem A.3 explicitly.\n\ni) The proof of Theorem 3.2 is simply to break the variance into a sum of variances of two independent terms, $\\hat{R}_{\\text{VAL}}^\\pi$ and $\\hat{R}_{\\text{res}}^\\pi$.  Since the proof is simple, we change the theorem into a proposition.\nii) Link for Theorems 3.2 and A.3: The proof of Theorem 3.2 decomposes the variance of DR into two terms (see (i) above), while Theorem A.3 focuses on the second term, $\\rm{Var}[\\hat{R}_{\\text{res}}^\\pi[\\hat{V}, \\hat{w}]]$ by further analyzing the different sources of randomness.\n\n\n- 5. Comparing with model based approach \n\nWe added the additional results of the taxi environment with a model-based baseline in Figure 4 (see appendix D). General model-based estimators suffer from data with a small sample size where they don\u2019t have enough samples for each transition. The observation is similar to the result in Liu et al.\n\n\n-6. ANN vs kernel methods in Optimization.\n\nWe would like to clarify that for these two methods, we use a neural network to parameterize the density ratio function. The difference between these two methods is the choice of function class for the test function $f$: the method introduced in the appendix uses neural networks, while Liu et. al choose a RKHS.  Both methods can be viewed as alternatives for solving the minimax saddle-point problem (equation (15) in Liu et. al), with different choices of function classes for the dual variables in the minimax problem.\n\nThese two methods can be both efficient to learn the density ratio if we carefully design the function class and tune the hyperparameters. For the kernel method mentioned in Liu et.al, we can easily get rid of the inner maximization but it requires us to choose a proper ISPD kernel (e.g., Gaussian kernel with a proper bandwidth). For the method we mentioned in the appendix, it requires less hyperparameter tuning while it needs to perform additional inner maximization on an additional auxiliary neural network, which can be unstable in some scenarios. However, for empirical experiments in our paper, we did not observe a significant performance gap between these two methods when we carefully tune the hyperparameters.  \n\nFinally, we would like to emphasize that it is not our main contribution to propose new algorithms for optimizing density ratio or value functions. Instead, our focus is on the new DR estimator and its double robustness property. We will include a thorough discussion on related optimization techniques in a future version.\n\n\n-7. Typo\nThanks for pointing out the typos, which have been fixed in the revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "SJlKt2OPsS", "original": null, "number": 4, "cdate": 1573518465022, "ddate": null, "tcdate": 1573518465022, "tmdate": 1573521603709, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "General Reponse to All Reviewers", "comment": "We thank all the reviewers for the valuable comments and suggestions. We have updated a version of our paper in the following revisions (with important changes highlighted in red):\n\n1. We rewrote the variance analysis part based on the suggestions by reviewers #2 and #3.\n2. We added additional related works, including the one mentioned by reviewer #2.\n3. We improved the proofs in the appendix by restating the theorems to make it self-contained. And we added more discussion for bias and variance part.\n4. We changed equation (11) into an explicit form based on a suggestion of reviewer #1.\n5. We added a model-based comparison experiment in appendix D.\n6. We fixed the typos reviewers mentioned.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "SJlAGjuDjS", "original": null, "number": 1, "cdate": 1573518102060, "ddate": null, "tcdate": 1573518102060, "tmdate": 1573518131903, "tddate": null, "forum": "S1glGANtDr", "replyto": "rkelhjsaYB", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank Reviewer #3 for the valuable comments and suggestions. We have updated a version based on your feedback, where the more important changes are highlighted in red. \nBelow are responses to your specific questions:\n\n- 1. Clarification on Theorem 3.2 (Proposition 3.1 in the revised version).\n\nAs we stated in the paper, the variance of the doubly robust estimator is not guaranteed to be lower than decrease compared to previous estimators. The main difficulty of the analysis is that we have two different sources of samples, $\\mu_0$ and $d_{\\pi_0}$, which may not be independent in general.  Theorem 3.2 is for the simplified situation where $\\mu_0$ and $d_{\\pi_0}$ are assumed to be independent.  In practice, however, the actual variance of DR can be lower than what the theorem states, as discussed in the paragraph following Proposition 3.1.\n\n\n- 2. If the DR estimator has an oracle of density ratio, is the DR unbiased?\n\nYes.  This is a direct consequence of Thm 3.1: when the correct density ratio is used, $\\varepsilon_{\\hat{w}} = 0$, implying the bias of DR (i.e., RHS of (12)) is 0.  We discussed it in the paragraph right after Thm 3.1, and have made it more explicit.\n\n\n- 3. Related work on Doubly Robust off-policy evaluation on infinite RL\n\nThanks for suggesting the related and interesting paper, https://arxiv.org/abs/1908.08526 .  The authors considered *marginalized* importance sampling, where an importance ratio function is estimated for each step in an episode.  In contrast, we consider importance sampling in the *stationary/invariant* distribution, where a single importance ratio function is shared by all steps in an infinite-length trajectory.  We have updated the related work section accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1glGANtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper990/Authors|ICLR.cc/2020/Conference/Paper990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162985, "tmdate": 1576860560629, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper990/Authors", "ICLR.cc/2020/Conference/Paper990/Reviewers", "ICLR.cc/2020/Conference/Paper990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Comment"}}}, {"id": "Syga485aKS", "original": null, "number": 2, "cdate": 1571821108602, "ddate": null, "tcdate": 1571821108602, "tmdate": 1572972526419, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Comments : \n\nThis paper provides an approach for reducing bias in long horizon off-policy evaluation (OPE) problems, extending recent work from Liu et al., 2018 that estimates the ratio of the stationary state distributions in off-policy evaluation for reducing variance. The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function. The key idea of the paper is to provide low variance, low bias OPE since their approach relies on accurately estimating the state distribution ratio and also the estimation of the value function. \n\nThe paper provides a follow up on Liu et al., 2018 and other recent works in off-policy learning that tries to estimate the state distribution ratio directly, but can introduce high inaccuracy if the obtained ratio estimates are inaccurate. In line of that, this approach seems useful as it tries to reduce the error from inaccurate ratio estimates by incorporating prior works with an additional value function estimation. \n\nFurthermore, the paper introduces a new perspective to doubly robust estimation that tries to reduce bias, instead of variance in previously known OPE literature (Jiang et al., 2016; Thomas et al., 2016). It is interesting to see how doubly robust can be related to primal-dual relations and the connections between these approaches, which is a novel contribution to the best of my knowledge. \n\n- The fundamental connection comes from observing OPE withdoubly robust estimator that estimates \\hat{V} and equation 6 that incorporates the ratio of the state density. This is clearly written in equations 7 and 8, that are two ways of evaluating the value function with off-policy samples known in the literature.\n\n- It uses the property of the Bellman recursive expression for the estimated value function of doubly robust OPE estimator and uses it in the OPE with state density ratio, leading to equation 9, and obtaining the bridge estimator that now relies on both estimates of value function \\hat{V} and the state density ratio. Although initially this does not seem useful, but the authors clarify this and how a bias reducting method can be achieved as in equation 11. \n\n- It is a nice and elegant trick, exploiting the connections between OPE estimators leading to a bias reduction method that seems quite interesting. \n\n- The most interesting part of the paper comes from section 4 that exploits the connection between doubly robust approaches with Lagrangian duality, and that their approach is equivalent to a primal dual formulation of policy evaluation. This stands out in itself as a novel contribution of the paper. Equations 14 and 15 best writes down the connections, as to how policy evaluation can be formualted as a Lagrangian function for optimization. \n\n- Although the authors point out how equation 15 is equivalent to equation 11 - this does not seem straight forward at first unless carefully looked at. I would encourate the authors to perhaps add more clarity that exploits this connection, to make the paper more self-contained. \n\n- In terms of experiments, the paper compares the doubly robust approach with previous works that estimates the density ratio, along with other baseline comparisons such as weighted DR. In both the two evaluated problems, their approach seems useful in terms of obtaining better accuracy (lower MSE). \n\n- However, I am not sure to what extent this approach can be scaled to more complicated tasks for OPE? Are there any example domains where the proposed method fails, or is difficult to scale up to more complicated tasks? \n\n- The current set of experiment results seems adequate, given the theoretical contribution and that most OPE papers evaluate on such domains. But overall, it might be useful to analyse the significance of this approach when scaling to more complicated domains. \u000b\n\n\nOverall, I think the paper has a neat and elegant theoretical contribution, exploiting the connection of OPE with primal-dual frameworks that seems quite novel to me. Experimental set of results are properly presented too showing significance of the approach compared to previously known baselines. I think such papers exploting connections with other literature is useful for the community in general, and this paper has significantly novel theoretical contribution. Hence, I would recommend for acceptance of this paper. \n\n\u000b"}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877901866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper990/Reviewers"], "noninvitees": [], "tcdate": 1570237743998, "tmdate": 1575877901878, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Review"}}}, {"id": "rkelhjsaYB", "original": null, "number": 3, "cdate": 1571826600345, "ddate": null, "tcdate": 1571826600345, "tmdate": 1572972526379, "tddate": null, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "invitation": "ICLR.cc/2020/Conference/Paper990/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new algorithm for the off-policy evaluation problem in reinforcement learning. It combines the value function learning method and the stationary distribution ratio estimators. The proposed method achieves double robustness, which means the proposed estimator is consistent as long as the value function or ratio estimator is consistent. Empirical results on some control domains are presented to verify the effectiveness of the algorithm.\n\nI think this paper has some nice contribution to the area, by introducing a doubly robust estimator based on the density ratio, and also a new idea to achieve double robustness. I will vote for accept, but I think there is room for improvement of this paper.\n\nDetailed comments:\n - The proposed estimator is not using control variate but using dual structure between value function and stationary distribution ratio, which is a novel idea comparing with similar doubly robust estimators. \n - Theorem 3.2, or at least the way it is presented, is less intuitive and makes me confused. If the variance of the DR estimator is always larger than the variance of value function, why I should use this estimator instead of value function. If the argument is the MSE of value function is potentially larger due to the bias. Then an effective analysis would be about MSE instead of variance.\n - If I have an oracle of density ratio, is the doubly robust estimator still unbiased, which is generally true for DR using control variate? This would be an important point to compare this work with control variate methods.\n - Very recent work https://arxiv.org/abs/1908.08526 also proposes a doubly robust estimator in similar settings. It's worth to mention it in the related work."}, "signatures": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper990/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ztang@cs.utexas.edu", "yihao@cs.utexas.edu", "lihongli.cs@gmail.com", "dennyzhou@google.com", "lqiang@cs.utexas.edu"], "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "authors": ["Ziyang Tang*", "Yihao Feng*", "Lihong Li", "Dengyong Zhou", "Qiang Liu"], "pdf": "/pdf/72489238aa9fe4af7ee0da1419c73e07adc99343.pdf", "TL;DR": "We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.", "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.  In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.", "keywords": ["off-policy evaluation", "infinite horizon", "doubly robust", "reinforcement learning"], "paperhash": "tang|doubly_robust_bias_reduction_in_infinite_horizon_offpolicy_estimation", "_bibtex": "@inproceedings{\nTang*2020Doubly,\ntitle={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\nauthor={Ziyang Tang* and Yihao Feng* and Lihong Li and Dengyong Zhou and Qiang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1glGANtDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2512437add9ba0d428c686932f997cd9abc583a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1glGANtDr", "replyto": "S1glGANtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877901866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper990/Reviewers"], "noninvitees": [], "tcdate": 1570237743998, "tmdate": 1575877901878, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper990/-/Official_Review"}}}], "count": 15}