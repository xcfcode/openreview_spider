{"notes": [{"id": "B1g_BT4FvS", "original": "SJx85wIvvr", "number": 528, "cdate": 1569439039856, "ddate": null, "tcdate": 1569439039856, "tmdate": 1577168216672, "tddate": null, "forum": "B1g_BT4FvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xG7X-klkPm", "original": null, "number": 1, "cdate": 1576798698964, "ddate": null, "tcdate": 1576798698964, "tmdate": 1576800936862, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Decision", "content": {"decision": "Reject", "comment": "The authors aim to improve policy gradient methods by denoising the gradient estimate. They propose to filter the transitions used to form the gradient update based on a variance explains criterion. They evaluate their method in combination with PPO and A2C, and demonstrate improvements over the baseline methods.\n\nInitially, reviewers were concerned about the motivation and explanation of the method. The authors revised the paper by clarifying the motivation and providing a justification based on the options framework. Furthermore, the authors included additional experiments investigating the impact of their approach on the gradient estimator, showing that with their filtering, the gradient estimator had larger magnitude.\n\nReviewers found the justification via the options framework to be a stretch, and I agree. The authors should explain how the options framework leads to dropping gradient terms. At the moment, the paper describes an algorithm using the options framework, however, they don't connect the policy gradients of that algorithm to their method. Furthermore, the authors should more clearly verify the claims about reducing noise in the gradient estimate. While the additional experiments on the norm are nice, the authors should go further. For example, if the claim is that the variance of the gradient estimator is reduced, then that should be verified. Finally, there are many approaches for reducing the variance of the policy gradient (Grathwohl et al. 2018, Wu et al 2018, Liu et al. 2018) and no comparisons are made to these approaches.\n\nGiven the remaining issues, I recommend rejection for this paper at this time, however, I encourage the authors to address these issues and submit to a future venue.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705765, "tmdate": 1576800253621, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper528/-/Decision"}}}, {"id": "BylQuBY6tB", "original": null, "number": 2, "cdate": 1571816811280, "ddate": null, "tcdate": 1571816811280, "tmdate": 1574474862868, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes a novel way to denoise the policy gradient by filtering the samples to add by a criterion \"variance explained\". The variance explained basically measures how well the learn value function could predict the average return, and the filter will keep samples with a high or low variance explained and drop the middle samples. This new mechanism is then added on top of PPO to get their algorithm SAUNA. Empirical results show that it is better than PPO, on a set of MuJoCo tasks and Roboschool.\n\nFrom my understanding, this paper does not show a significant contribution to the related research area. The main reason I tend to reject this paper is that the motivation of their proposed algorithm is very unclear, lack of theoretical justification and the empirical justification is restricted on PPO -- one policy gradient method.\n\n1) It's unclear to me how it goes to the final algorithm, and what is the intuition behind it. Second 3.1 is easy to follow but the following part seems less motivated. In section 3.2 it's unclear to me why we need to fit a parametric function of Vex. In section 3.2, it's unclear to me why the filter condition is defined as Eq (7). The interpretation is a superficial explanation of what Eq 7 means but does not explain why I should throw out some of my samples, why high and low score means samples are helpful for learning and score in between does not?\n\n2) This paper argues the filter condition improves PG algorithms by denoising the policy gradient. This argument is not justified at all except a gradient noise plot in one experimental domain in figure 5b. That's not enough to support the argument that what this process is really doing. Some theoretical understanding of what the dropped/left samples will do is helpful.\n\n3) The method of denoising the policy gradient is expected to help policy gradient methods in general. It's important to show at least one more PG algorithm (DDPG/REINFORCE/A2C) where the proposed method can help, for verifying the generalizability of algorithm.\n\nIn general, I feel that the content after section 3.1 could be presented in a much more principled way. It should provide not just an algorithm and some numbers in experiments, but also why we need the algorithm, what's the key insight of designing this algorithm, what the algorithm really did by the algorithm mechanism itself instead of just empirical numbers.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575673597363, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper528/Reviewers"], "noninvitees": [], "tcdate": 1570237750837, "tmdate": 1575673597381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Review"}}}, {"id": "r1giq6Bk9H", "original": null, "number": 3, "cdate": 1571933587240, "ddate": null, "tcdate": 1571933587240, "tmdate": 1574439189390, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "In their post-review revision, the authors have added a much clearer motivation for SAUNA, along with extra experiments that validate and clarify the approach. \n\nThe revision is a significantly better paper than the original, and I am updating my score accordingly.\n\n-------------\n\nThis paper introduces 'the fraction of variance explained' V^{ex} as a measure of the ability of the value function to fit the true reward. Given the mean per-timestep reward as a baseline, V^{ex} measures the proportion of the variance in reward that is captured by the value function.\n\nIn this paper, the authors introduce a filtering condition aimed to ensure that no single transition excessively reduces the fraction of variance explained. This filtering condition relies on a prediction of the variance explained, which comes from an extra head added to the standard PPO architectures, and its parameters are updated along with all other model parameters at the end of each trajectory.\n\nIntuitively, I can believe that learning will be more stable under the condition that no single transition leads to too great a divergence between the predicted reward and true reward. However, I do not understand the authors' assertion that this filtering procedure removes noisy samples (how can we characterize these samples as noise?). I'm also insufficiently familiar with the related literature to properly gauge the theoretical implications of this modification (see low confidence below).\n\nThe paper compares learning accuracy over time of PPO with and without the variance explained filtering. It seems that the filtering does improve learning for the MuJoCo environments, as well as low resource models for a harder task from the Roboschool environment but it is hard to state definitively that the new method is better. I commend the authors on their discussion of non-positive Atari results in the appendix and I agree that it contributes significantly to the paper.\n\nWith the caveat that this paper is quite far from the realm of my expertise. I think that the approach is intriguing but, in the absence of any theoretical justification for the approach, I'm not sure that the empirical results are sufficiently convincing for ICLR. I also believe that the paper would be easier to understand with a more thorough investigation of the effect of the filtering on the learning procedure.\n\n\nQuestions for the authors:\n\n- what proportion of samples are rejected\n\n- how does this proportion change over the course of learning\n\n- how was the V^{ex} threshold of 0.3 chosen", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575673597363, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper528/Reviewers"], "noninvitees": [], "tcdate": 1570237750837, "tmdate": 1575673597381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Review"}}}, {"id": "Skgbaav2sS", "original": null, "number": 4, "cdate": 1573842360981, "ddate": null, "tcdate": 1573842360981, "tmdate": 1573842428219, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment", "content": {"title": "General response to official reviews", "comment": "We thank the reviewers for their time and thoughtful feedback.\n\nWe have updated the submission. We have clarified the motivation in the introduction and restructured section 3, where the method is now more carefully unveiled. We think that this will help to highlight better the relevance of the method. We have clarified the rationale for rejecting some of the samples to accelerate learning, added experiments on another policy gradient method to strengthen the paper, included a more theoretical investigation with temporal abstraction on the effect of sample dropout during the learning procedure, and we have resolved ambiguities regarding the denoising impact of SAUNA on the gradients.\n\nWe answer specific questions raised in the reviews by separately replying to each of them."}, "signatures": ["ICLR.cc/2020/Conference/Paper528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1g_BT4FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper528/Authors|ICLR.cc/2020/Conference/Paper528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170101, "tmdate": 1576860560492, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment"}}}, {"id": "SJxTQq82sB", "original": null, "number": 3, "cdate": 1573837349190, "ddate": null, "tcdate": 1573837349190, "tmdate": 1573837349190, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "r1giq6Bk9H", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment", "content": {"title": "Response to official review", "comment": "Thank you for the clear and encouraging review. We have addressed your key points below and incorporated the discussion into the revised article.\n\n\n> However, I do not understand the authors' assertion that this filtering procedure\n> removes noisy samples (how can we characterize these samples as noise?).\n\nThank you for the great comment (comment shared with Reviewer 1 and Reviewer 3). We have updated the manuscript to support this claim and included in Section 4.2.2 an additional experiment exposing the L1-norm of the gradients throughout the learning for PPO and for SAUNA applied to PPO. The important difference in norm suggests that the gradients are enriched with more informative samples allowing the algorithm to do bigger steps towards a better policy.\n\n\n> The paper compares learning accuracy over time of PPO with and without the variance\n> explained filtering. It seems that the filtering does improve learning for the MuJoCo environments,\n> as well as low resource models for a harder task from the Roboschool environment but it is hard to\n> state definitively that the new method is better.\n\nThank you for this observation. In order to verify the generalizability of our method and strengthen the paper, we have added a comparison with another policy gradient method, A2C. The results have been added in Section 4.1. We have also clarified and better emphasized the positive implications of such a change in the sampling procedure in Section 3 and 4.\n\n\n> I think that the approach is intriguing but, in the absence of any theoretical justification for the approach,\n> I'm not sure that the empirical results are sufficiently convincing for ICLR. I also believe that the paper would\n> be easier to understand with a more thorough investigation of the effect of the filtering on the learning procedure.\n\nWe have reformulated the theoretical grounding of the transition dropout introduced by our method in Section 3.2, which allows for reframing the sample dropout as a dynamic temporal abstraction, which we know from [1] that one way to improve on existing agents is to leverage abstraction. We have also included in Section 4.2.2 an additional experiment we referred to in the first point.\n\n[1] Smith, Hoof, and Pineau. An inference-based policy gradient method for learning options. ICML. 2018.\n\n\nThank you for the detailed questions. The answers are reflected in the updated article.\n\n> What proportion of samples are rejected and how does this proportion\n> change over the course of learning?\n\nOn average, SAUNA rejects 5-10% of samples at the beginning of training which reduces to 2-6% at the end. We have included this information in the manuscript in Section 4.2.1.\n\n> How was the V^{ex} threshold of 0.3 chosen?\n\nWe have tuned the hyperparameters of our method by performing a grid search and selecting the best combinations by considering those with the largest consensus. We have also included this information in the manuscript in Appendix E."}, "signatures": ["ICLR.cc/2020/Conference/Paper528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1g_BT4FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper528/Authors|ICLR.cc/2020/Conference/Paper528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170101, "tmdate": 1576860560492, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment"}}}, {"id": "Sylx6tUnoH", "original": null, "number": 2, "cdate": 1573837240388, "ddate": null, "tcdate": 1573837240388, "tmdate": 1573837240388, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "BylQuBY6tB", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment", "content": {"title": "Response to official review", "comment": "Thank you for the thoughtful review. These suggestions and questions are reflected in the updated article.\n\n> The main reason I tend to reject this paper is that the motivation of their\n> proposed algorithm is very unclear, lack of theoretical justification and\n> the empirical justification is restricted on PPO -- one policy gradient method.\n\nWe have restructured Section 2 and Section 3 in order to clarify the motivation for the method, and we now clearly articulate the building blocks of the method as well as the theoretical grounding for the transition dropout. We have enriched the paper with additional experiments in Section 4.1 - with experiments on an additional policy gradient method - and Section 4.2 - with a study of the impact of our method on the gradients.\n\n\nWe have addressed your key point below and incorporated the discussion into the revised article.\n\n1. Thank you for your great comments and question. We have restructured Section 3. In Section 3.1, we clarify that Vex should be fit by a parametric function because we need to estimate the value of V^ex(s_{t}) for each state s_{t} belonging to a trajectory \\tau. We also clarify the rationale behind Eq. 7. We also give in Section 2.1 a more thorough justification for the rejection of samples for which the state value function and the returns are not correlated.\n\n2. Thank you for this important observation. In order to study more thoroughly the implication of such a change in the sampling algorithm, we have added in Section 4.2.2 an experiment exposing the L1-norm of the gradients for PPO and when SAUNA is applied to PPO. Those graphs, together with the performance results, suggest that the gradients contain more useful information from each of the transitions that passed SAUNA sampling. This also infers that the temporal abstraction introduced in the policy gradient enriches the gradients with more qualitative information and that they have been partially denoised.\n\n3. We have compared our method with an additional policy gradient algorithm, A2C, and we report the (positive) results in Section 4.1 and Appendix B.1.\n\n\n> In general, I feel that the content after section 3.1 could be presented in a much more principled way.\n\nBased on your suggestion, in addition to restructuring Section 3, we have enriched and presented Section 4 in a more diligent way."}, "signatures": ["ICLR.cc/2020/Conference/Paper528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1g_BT4FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper528/Authors|ICLR.cc/2020/Conference/Paper528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170101, "tmdate": 1576860560492, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment"}}}, {"id": "r1xrEt83jr", "original": null, "number": 1, "cdate": 1573837100885, "ddate": null, "tcdate": 1573837100885, "tmdate": 1573837100885, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "HkgfXyihYS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment", "content": {"title": "Response to official review", "comment": "Thank you for the thorough review. We have updated the paper based on your suggestions.\n\n\n> I think showing why this method is relevant could be better executed. There are some gains\n> compared to a PPO baseline, but the gains are somewhat incremental and may only apply to\n> PPO, as other PG methods aren't tested.\n\nThank you for the great suggestions. We have restructured and reformulated both the motivation in Section 1 and Section 3, where we now clearly articulate the building blocks of the method. We have also, based on your comments, did additional experiments: we have included a comparison with another policy gradient method in Section 4.1 as well as an experiment in Section 4.2.2, exposing the gradients norm when using PPO or using SAUNA with PPO.\n\n\nWe have addressed your detailed comments below and incorporated the discussion into the revised article.\n\n> Is this method always relevant? Are there environments where it makes more sense to use?\n> (in terms of reward density/variance, exploration difficulty, etc.) Policy gradient algorithms\n> for which it makes more sense to add?\n\nIn order to strengthen the study of the relevance of our method, we have included a comparison with A2C in the revised version of the manuscript. Our experiments with Roboschool suggest that when the task is difficult and the model resources are low for the agent to learn, rejecting some samples based on whether they are correlated with the returns can help. PG algorithms with trust regions (such as TRPO or PPO) could, in practice, benefit more from a sample dropout than e.g., A2C. One of the reasons we think of is that, intuitively, algorithms constraining the new policy to stay within a small interval from the old one need richer and more qualitative gradients in order to shift the agent parameters with an amplitude similar to other methods. Although, in practice, we did not observe such results that support this.\n\n\n> \"applicable to any policy gradient method\", technically yes, but it remains untested\n\nIndeed, we meant \u201capplicable\u201d in the technical sense here, sorry for the confusion. We clarified this in the revised manuscript. In response to this very legitimate comment, we tested the method on an additional policy gradient method, which we think strengthens the paper. The experiments are included in Section 4.1 and Appendix B.1.\n\n\n> \"SAUNA removes noise\", also remains to be seen. A graph showing this would\n> add a lot to this paper\n\nThank you for the great comment, we have reformulated the claim. In addition, we have clarified the implications of using V^ex as a way to reject samples uncorrelated with the returns in Section 2.1, and included a justification for this claim by looking at the modifications SAUNA brings to the gradients in Section 4.2.2.\n\n\n> \"We [..] studied the impact [..] on both the exploitation [..] and on the exploration\u201d,\n> Figure 5 is a single data point, where one run for one environment got out of a \"well-known\"\n> local minima\n\nThank you for pointing out this. We have removed this claim in the conclusion of the revised manuscript and clarified the interpretation of the qualitative study in Section 4.2.3.\n\n\n> This method is very much related to prioritized experience replay and others, as observed in 2.2,\n> yet no comparison is made (PER can be implemented online using two exponential moving averages\n> to estimate the distribution of TD error, used to do rejection sampling). Simpler baselines could also\n> have been tested against, e.g. simply rejecting samples with high TD error.\n\nThank you for your suggestion. Because of time constraints, we were required instead to do more experiments on an additional policy gradient method, as the suggestion was unanimous among the Reviewers, and this would improve a lot the paper. Although, for the sake of thoroughness, we also experimented with randomly dropping out samples, and we include the results in Appendix D.\n\n\n> It's not clear how the threshold of 0.3 was chosen, nor what its effect is empirically. \n\nWe have tuned the hyperparameters of our method by performing a grid search and selecting the best combinations by considering those with the largest consensus. We have included this information in the manuscript in Appendix E. We have clarified the effects of thresholding in Section 3.2.\n\n\n> Estimating V^ex vs using empirical V^ex seems to make a big difference. It's not obvious\n> why we should be estimating V^ex at all, and I think this deserves more analysis.\n\nThank you for the suggestion. Vex should be fit by a parametric function because we need to estimate the value of Vex(s_{t}) for each state s_{t} belonging to a trajectory \\tau. Indeed, V^ex is defined for a trajectory. We clarify this in Section 3.1.\n\n\n> I've seen Roboschool cited, you can use @misc{klimov2017roboschool, title={Roboschool},\n> author={Klimov, Oleg and Schulman, J}, year={2017} }\n\nThank you, we have corrected this in revised manuscript."}, "signatures": ["ICLR.cc/2020/Conference/Paper528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1g_BT4FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper528/Authors|ICLR.cc/2020/Conference/Paper528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170101, "tmdate": 1576860560492, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper528/Authors", "ICLR.cc/2020/Conference/Paper528/Reviewers", "ICLR.cc/2020/Conference/Paper528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Comment"}}}, {"id": "HkgfXyihYS", "original": null, "number": 1, "cdate": 1571757850285, "ddate": null, "tcdate": 1571757850285, "tmdate": 1572972584179, "tddate": null, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "invitation": "ICLR.cc/2020/Conference/Paper528/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a simple modification to policy gradient methods that relies on the variance explained to filter samples. The paper contains experiments showing empirical gains to the method, as well as some evidence that filtering rather than simply predicting more quantities is making a difference. \n\nI put a \"weak accept\" score. This method is novel, afaict, and is based on interesting statistical hypotheses. I think showing why this method is relevant could be better executed. There are some gains compared to a PPO baseline, but the gains are somewhat incremental and may only apply to PPO, as other PG methods aren't tested.\n\nDetailed comments:\n- is this method always relevant? Are there environments where it makes more sense to use? (in terms of reward density/variance, exploration difficulty, etc.) Policy gradient algorithms for which it makes more sense to add?\n- The conclusion in particular claims much more than what is in the paper:\n>> \"applicable to any policy gradient method\", technically yes, but it remains untested\n>> \"SAUNA removes noise\", also remains to be seen. A graph showing this would add a lot to this paper\n>> \"We [..] studied the impact [..] on both the exploitation [..] and on the exploration\", Figure 5 is a single data point, where one run for one environment got out of a \"well-known\" local minima. To really convince readers of this you would need to test your method on multiple environments with actual exploration and sparse rewards (Mujoco locomotion tasks do not satisfy those requirements, even though they has local minima, this is far from exploration as commonly understood in RL)\n- This method is very much related to prioritized experience replay and others, as observed in 2.2, yet no comparison is made (PER can be implemented online using two exponential moving averages to estimate the distribution of TD error, used to do rejection sampling). Simpler baselines could also have been tested against, e.g. simply rejecting samples with high TD error.\n- It's not clear how the threshold of 0.3 was chosen, nor what its effect is empirically. \n- Estimating V^ex vs using empirical V^ex seems to make a big difference. It's not obvious why we should be estimating V^ex at all, and I think this deserves more analysis.\n- I've seen Roboschool cited, you can use @misc{klimov2017roboschool, title={Roboschool}, author={Klimov, Oleg and Schulman, J}, year={2017} }"}, "signatures": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper528/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Samples Are Useful? Not Always: denoising policy gradient updates using variance explained", "authors": ["Yannis Flet-Berliac", "Philippe Preux"], "authorids": ["yannis.flet-berliac@inria.fr", "philippe.preux@inria.fr"], "keywords": ["reinforcement learning", "policy gradient", "sampling"], "TL;DR": "SAUNA uses the fraction of variance explained (Vex) as a metric to filter the transitions used for policy gradient updates: such filtering improves the sampling prior for a better exploration of the environment and yields a better performance.", "abstract": "Policy gradient algorithms in reinforcement learning optimize the policy directly and rely on efficiently sampling an environment. However, while most sampling procedures are based solely on sampling the agent's policy, other measures directly accessible through these algorithms could be used to improve sampling before each policy update. Following this line of thoughts, we propose the use of SAUNA, a method where transitions are rejected from the gradient updates if they do not meet a particular criterion, and kept otherwise. This criterion, the fraction of variance explained Vex, is a measure of the discrepancy between a model and actual samples. In this work, Vex is used to evaluate the impact each transition will have on learning: this criterion refines sampling and improves the policy gradient algorithm. In this paper: (a) We introduce and explore Vex, the criterion used for denoising policy gradient updates. (b) We conduct experiments across a variety of benchmark environments, including standard continuous control problems. Our results show better performance with SAUNA. (c) We investigate why Vex provides a reliable assessment for the selection of samples that will positively impact learning. (d) We show how this criterion can work as a dynamic tool to adjust the ratio between exploration and exploitation.", "pdf": "/pdf/764b91f7c0e329b47f359f22fed554131b8c31a0.pdf", "code": "https://github.com/iclr2020-submission/denoising-gradient-updates", "paperhash": "fletberliac|samples_are_useful_not_always_denoising_policy_gradient_updates_using_variance_explained", "original_pdf": "/attachment/d447e0acd56ee54410fa66ae8ec82dc113a0007d.pdf", "_bibtex": "@misc{\nflet-berliac2020samples,\ntitle={Samples Are Useful? Not Always: denoising policy gradient updates using variance explained},\nauthor={Yannis Flet-Berliac and Philippe Preux},\nyear={2020},\nurl={https://openreview.net/forum?id=B1g_BT4FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1g_BT4FvS", "replyto": "B1g_BT4FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575673597363, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper528/Reviewers"], "noninvitees": [], "tcdate": 1570237750837, "tmdate": 1575673597381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper528/-/Official_Review"}}}], "count": 9}