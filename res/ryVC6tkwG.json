{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124442100, "tcdate": 1518472651847, "number": 331, "cdate": 1518472651847, "id": "ryVC6tkwG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "ryVC6tkwG", "signatures": ["~Thomas_George1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error.", "paperhash": "laurent|an_evaluation_of_fisher_approximations_beyond_kronecker_factorization", "keywords": ["convolutional neural networks", "optimization", "natural gradient", "kronecker factorization"], "_bibtex": "@misc{\n  laurent2018an,\n  title={An Evaluation of Fisher Approximations Beyond Kronecker Factorization},\n  author={C\u00e9sar Laurent and Thomas George and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},\n  year={2018},\n  url={https://openreview.net/forum?id=ryVC6tkwG}\n}", "authorids": ["cesarlaurent77@gmail.com", "georgeth@iro.umontreal.ca", "xavier.bouthillier@umontreal.ca", "ballas.n@gmail.com", "pascal.vincent@umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Thomas George", "Xavier Bouthillier", "Nicolas Ballas", "Pascal Vincent"], "TL;DR": "We study two coarser approximations on top of a Kronecker factorization of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks", "pdf": "/pdf/0b23c90c897d8a567b3b2021376aa43710265fca.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582988677, "tcdate": 1519582522373, "number": 1, "cdate": 1519582522373, "id": "SJGr6dxuG", "invitation": "ICLR.cc/2018/Workshop/-/Paper331/Official_Review", "forum": "ryVC6tkwG", "replyto": "ryVC6tkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer2"], "content": {"title": "Good starting point", "rating": "7: Good paper, accept", "review": "- A brief summary of the paper's contributions, in the context of prior work.\nThe paper aims to speed up 2nd order methods by using 2 approximations to the Fisher information for CNNs. \n\n- An assessment of novelty, clarity, significance, and quality.\nThe approximations (spatial uncorrelated activations, channel grouping) are reasonable, and lead to training speedups for 2nd order methods. As authors mention, the first approximation was proposed before, but not evaluated for 2nd order methods. \n\n- A list of pros and cons (reasons to accept/reject).\nPro: simple idea, seems to work well.\nCon: (not really cons, but suggestions for potential full paper) \n- is this relevant to other architectures as well? \n- how many channel groups can one use? \n- are there connections with theory? ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error.", "paperhash": "laurent|an_evaluation_of_fisher_approximations_beyond_kronecker_factorization", "keywords": ["convolutional neural networks", "optimization", "natural gradient", "kronecker factorization"], "_bibtex": "@misc{\n  laurent2018an,\n  title={An Evaluation of Fisher Approximations Beyond Kronecker Factorization},\n  author={C\u00e9sar Laurent and Thomas George and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},\n  year={2018},\n  url={https://openreview.net/forum?id=ryVC6tkwG}\n}", "authorids": ["cesarlaurent77@gmail.com", "georgeth@iro.umontreal.ca", "xavier.bouthillier@umontreal.ca", "ballas.n@gmail.com", "pascal.vincent@umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Thomas George", "Xavier Bouthillier", "Nicolas Ballas", "Pascal Vincent"], "TL;DR": "We study two coarser approximations on top of a Kronecker factorization of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks", "pdf": "/pdf/0b23c90c897d8a567b3b2021376aa43710265fca.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582988484, "id": "ICLR.cc/2018/Workshop/-/Paper331/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper331/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper331/AnonReviewer3"], "reply": {"forum": "ryVC6tkwG", "replyto": "ryVC6tkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper331/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582988484}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582897174, "tcdate": 1520501782519, "number": 2, "cdate": 1520501782519, "id": "Hk0zNtAuM", "invitation": "ICLR.cc/2018/Workshop/-/Paper331/Official_Review", "forum": "ryVC6tkwG", "replyto": "ryVC6tkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer3"], "content": {"title": "Interesting and important speedup, with one missing aspect", "rating": "6: Marginally above acceptance threshold", "review": "The paper describes an approach to approximating second-order gradient information for convolutional neural network optimization in less time and computation.  This approach hinges on two assumptions that allow for the uncorrelation of many elements of the information matrix, allowing for a faster block-wise inversion.  Especially given available space, the approach is extremely clearly explained.  Experiments are presented showing competative neural net performance with less computation compared with using a fully correlated information matrix.\n\nHowever, no discussion or argument is given to approximation quality.  It isn't immediately clear to me that the assumptions leveraged are fair, and that all the uncorrelations are reasonable.  How much possibly-important information is lost in this approximation?  Can I expect good results in general, or were the given experiments lucky?  Without this information, it is difficult to be fully enthusiastic about the given approach.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error.", "paperhash": "laurent|an_evaluation_of_fisher_approximations_beyond_kronecker_factorization", "keywords": ["convolutional neural networks", "optimization", "natural gradient", "kronecker factorization"], "_bibtex": "@misc{\n  laurent2018an,\n  title={An Evaluation of Fisher Approximations Beyond Kronecker Factorization},\n  author={C\u00e9sar Laurent and Thomas George and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},\n  year={2018},\n  url={https://openreview.net/forum?id=ryVC6tkwG}\n}", "authorids": ["cesarlaurent77@gmail.com", "georgeth@iro.umontreal.ca", "xavier.bouthillier@umontreal.ca", "ballas.n@gmail.com", "pascal.vincent@umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Thomas George", "Xavier Bouthillier", "Nicolas Ballas", "Pascal Vincent"], "TL;DR": "We study two coarser approximations on top of a Kronecker factorization of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks", "pdf": "/pdf/0b23c90c897d8a567b3b2021376aa43710265fca.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582988484, "id": "ICLR.cc/2018/Workshop/-/Paper331/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper331/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper331/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper331/AnonReviewer3"], "reply": {"forum": "ryVC6tkwG", "replyto": "ryVC6tkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper331/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582988484}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573561574, "tcdate": 1521573561574, "number": 82, "cdate": 1521573561235, "id": "HkG6AC0Kf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "ryVC6tkwG", "replyto": "ryVC6tkwG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error.", "paperhash": "laurent|an_evaluation_of_fisher_approximations_beyond_kronecker_factorization", "keywords": ["convolutional neural networks", "optimization", "natural gradient", "kronecker factorization"], "_bibtex": "@misc{\n  laurent2018an,\n  title={An Evaluation of Fisher Approximations Beyond Kronecker Factorization},\n  author={C\u00e9sar Laurent and Thomas George and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},\n  year={2018},\n  url={https://openreview.net/forum?id=ryVC6tkwG}\n}", "authorids": ["cesarlaurent77@gmail.com", "georgeth@iro.umontreal.ca", "xavier.bouthillier@umontreal.ca", "ballas.n@gmail.com", "pascal.vincent@umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Thomas George", "Xavier Bouthillier", "Nicolas Ballas", "Pascal Vincent"], "TL;DR": "We study two coarser approximations on top of a Kronecker factorization of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks", "pdf": "/pdf/0b23c90c897d8a567b3b2021376aa43710265fca.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}