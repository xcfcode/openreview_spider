{"notes": [{"id": "HyVbhi0cYX", "original": "BkxranO5Ym", "number": 687, "cdate": 1538087849354, "ddate": null, "tcdate": 1538087849354, "tmdate": 1545355406427, "tddate": null, "forum": "HyVbhi0cYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Complexity of Training ReLU Neural Networks", "abstract": "In this paper, we explore some basic questions on complexity of training Neural networks with ReLU activation function. We show that it is NP-hard to train a two-hidden layer feedforward ReLU neural network. If dimension d of the data is fixed then we show that there exists a polynomial time algorithm for the same training problem. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data.", "keywords": ["NP-hardness", "ReLU activation", "Two hidden layer networks"], "authorids": ["digvijaybb40@gatech.edu", "santanu.dey@isye.gatech.edu", "george.lan@isye.gatech.edu"], "authors": ["Digvijay Boob", "Santanu S. Dey", "Guanghui Lan"], "pdf": "/pdf/afb9678e8ebcb0eb37747b1fb9c4b7a722013705.pdf", "paperhash": "boob|complexity_of_training_relu_neural_networks", "_bibtex": "@misc{\nboob2019complexity,\ntitle={Complexity of Training Re{LU} Neural Networks},\nauthor={Digvijay Boob and Santanu S. Dey and Guanghui Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=HyVbhi0cYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJeZO-6yeV", "original": null, "number": 1, "cdate": 1544700264968, "ddate": null, "tcdate": 1544700264968, "tmdate": 1545354507208, "tddate": null, "forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "invitation": "ICLR.cc/2019/Conference/-/Paper687/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers agreed that, while the problem considered was of interest, the theoretical result presented in this work was of too limited scope to be of interest for the ICLR audience.\n\nBased on their comments, you might want to consider a more theoretically-oriented venue for such a submission.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Restricted theoretical result"}, "signatures": ["ICLR.cc/2019/Conference/Paper687/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper687/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Training ReLU Neural Networks", "abstract": "In this paper, we explore some basic questions on complexity of training Neural networks with ReLU activation function. We show that it is NP-hard to train a two-hidden layer feedforward ReLU neural network. If dimension d of the data is fixed then we show that there exists a polynomial time algorithm for the same training problem. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data.", "keywords": ["NP-hardness", "ReLU activation", "Two hidden layer networks"], "authorids": ["digvijaybb40@gatech.edu", "santanu.dey@isye.gatech.edu", "george.lan@isye.gatech.edu"], "authors": ["Digvijay Boob", "Santanu S. Dey", "Guanghui Lan"], "pdf": "/pdf/afb9678e8ebcb0eb37747b1fb9c4b7a722013705.pdf", "paperhash": "boob|complexity_of_training_relu_neural_networks", "_bibtex": "@misc{\nboob2019complexity,\ntitle={Complexity of Training Re{LU} Neural Networks},\nauthor={Digvijay Boob and Santanu S. Dey and Guanghui Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=HyVbhi0cYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper687/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353124721, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper687/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper687/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper687/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353124721}}}, {"id": "SkgrWz6n2m", "original": null, "number": 3, "cdate": 1541358076900, "ddate": null, "tcdate": 1541358076900, "tmdate": 1541533774505, "tddate": null, "forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "invitation": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "content": {"title": "Complexity of training ReLU Neural Networks", "review": "This paper claims results showing ReLU networks (or a particular architecture for that) are NP-hard to learn. The authors claim that results that essentially show this (such as those by Livni et al.) are unsatisfactory as they only show this for ReLU networks that are fully connected. However, the authors fail to criticize their own paper for only showing this result for a network with 3 gates. For the same reason that the Livni et al. results don't imply anything for fully connected networks, these results don't imply anything for larger networks. Conceivably certain gadgets could be created to ensure that the larger networks are essentially forced to ignore the rest of the gates. This line of research isn't terribly interesting and furthermore the paper is not particularly well written. \n\nFor learning ReLUs, it is already known (assuming conjectures based on hardness of improper PAC learning) that functions that can be represented as a single hidden layer ReLU network cannot be learned even using a much larger network in polynomial time (see for instance the Livni et al. paper, etc.). Proving NP-hardness results for proper isn't as useful as they usually are very restricted in terms of architectures the learning algorithm is allowed to use. However, if they do want to show such results, I think the NP-hardness of learning 2-term DNF formulas will be a much easier starting point. \n\nAlso, I think there is a flaw in the proof of Lemma 4.1. The function f *cannot* be represented by the networks the authors claim to use. In particular the 1/\\eta outside the max(0, x) term is not acceptable.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper687/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Training ReLU Neural Networks", "abstract": "In this paper, we explore some basic questions on complexity of training Neural networks with ReLU activation function. We show that it is NP-hard to train a two-hidden layer feedforward ReLU neural network. If dimension d of the data is fixed then we show that there exists a polynomial time algorithm for the same training problem. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data.", "keywords": ["NP-hardness", "ReLU activation", "Two hidden layer networks"], "authorids": ["digvijaybb40@gatech.edu", "santanu.dey@isye.gatech.edu", "george.lan@isye.gatech.edu"], "authors": ["Digvijay Boob", "Santanu S. Dey", "Guanghui Lan"], "pdf": "/pdf/afb9678e8ebcb0eb37747b1fb9c4b7a722013705.pdf", "paperhash": "boob|complexity_of_training_relu_neural_networks", "_bibtex": "@misc{\nboob2019complexity,\ntitle={Complexity of Training Re{LU} Neural Networks},\nauthor={Digvijay Boob and Santanu S. Dey and Guanghui Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=HyVbhi0cYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "cdate": 1542234402890, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper687/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780690, "tmdate": 1552335780690, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper687/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylXTxh92X", "original": null, "number": 2, "cdate": 1541222587331, "ddate": null, "tcdate": 1541222587331, "tmdate": 1541533774304, "tddate": null, "forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "invitation": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "content": {"title": "moderately interesting complexity result but perhaps the wrong venue", "review": "The main result of this work is to prove that a two-layer neural network with 2 hidden neurons in the first layer and 1 hidden neuron in the second layer is NP-hard to train when all activation functions are ReLU. Similar results with the hard-thresholding activation function were proved by Blum & Rivest in 1988. The main proof consists of a reduction from the 2-hyperplane separability problem which was known to be equivalent to NNs with hard-thresholding activation functions.\n\nQuality: Moderate. Good solid formal results but nothing really surprising. \n\nClarity: The manuscript is mostly well-written, and the authors gave proper credit to prior works. The only minor issue is perhaps the abuse of the variable w in introduction and the same variable w (with an entirely different meaning) in the rest of the paper. It would make more sense to change the w's in introduction to d's.\n\nOriginality: This work is largely inspired by Blum & Rivest's work, and builds heavily on some previous work including Megiddo and Edelsbrunner et al. While there is certainly some novelty in extending prior work to the ReLU activation function, it is perhaps fair to say the originality is moderate.\n\nSignificance: While the technical construction seems plausible and correct, the real impact of the obtained results is perhaps rather limited. This is one of those papers that it is certainly nice to have all details worked out but none of the obtained results is really surprising or unexpected. While I do agree there is value in formally documenting the authors' results, this conference is perhaps not the right venue. \n\nOther comments: The discussion on page 2 (related literature) seems odd. Wouldn't the results of Livni et al and Dasgupta et al already imply the NP-hardness of fully connected ReLU networks, in a way similar to how one obtains Corollary 3.2? If this is correct, then the contribution of this work is basically a refined complexity analysis where the ReLU network is shrunken to 2 layers with 3 nuerons?\n\nI really wish the authors had tried to make their result more general, which in my opinion would make this paper more interesting and novel: can you extend the proof to a family of activation functions? It is certainly daunting to write separate papers to prove such a complexity result for every activation function... The authors also conveniently made the realizability assumption. What about the more interesting non-realizable case?\n\nThe construction to prove Theorem 3.4 bears some similarity to a related result in Zhang et al. The hard sorting part appears to be different.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper687/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Training ReLU Neural Networks", "abstract": "In this paper, we explore some basic questions on complexity of training Neural networks with ReLU activation function. We show that it is NP-hard to train a two-hidden layer feedforward ReLU neural network. If dimension d of the data is fixed then we show that there exists a polynomial time algorithm for the same training problem. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data.", "keywords": ["NP-hardness", "ReLU activation", "Two hidden layer networks"], "authorids": ["digvijaybb40@gatech.edu", "santanu.dey@isye.gatech.edu", "george.lan@isye.gatech.edu"], "authors": ["Digvijay Boob", "Santanu S. Dey", "Guanghui Lan"], "pdf": "/pdf/afb9678e8ebcb0eb37747b1fb9c4b7a722013705.pdf", "paperhash": "boob|complexity_of_training_relu_neural_networks", "_bibtex": "@misc{\nboob2019complexity,\ntitle={Complexity of Training Re{LU} Neural Networks},\nauthor={Digvijay Boob and Santanu S. Dey and Guanghui Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=HyVbhi0cYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "cdate": 1542234402890, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper687/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780690, "tmdate": 1552335780690, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper687/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkleFFb9nX", "original": null, "number": 1, "cdate": 1541179767766, "ddate": null, "tcdate": 1541179767766, "tmdate": 1541533774045, "tddate": null, "forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "invitation": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "content": {"title": "Review", "review": "This paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node\nis NP-complete. This is an extension of the result of Blum and Rivest'88. The original theorem was proved for \nthreshold activation units and the current paper proves the same result for ReLU activations. The authors do this\nby reducing the 2-affine separability problem to that of fitting a neural network to data. The reduction is well \nwritten and is clever. This is a reasonable contribution although it does not add significantly to the current state of the art. \n  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper687/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Training ReLU Neural Networks", "abstract": "In this paper, we explore some basic questions on complexity of training Neural networks with ReLU activation function. We show that it is NP-hard to train a two-hidden layer feedforward ReLU neural network. If dimension d of the data is fixed then we show that there exists a polynomial time algorithm for the same training problem. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data.", "keywords": ["NP-hardness", "ReLU activation", "Two hidden layer networks"], "authorids": ["digvijaybb40@gatech.edu", "santanu.dey@isye.gatech.edu", "george.lan@isye.gatech.edu"], "authors": ["Digvijay Boob", "Santanu S. Dey", "Guanghui Lan"], "pdf": "/pdf/afb9678e8ebcb0eb37747b1fb9c4b7a722013705.pdf", "paperhash": "boob|complexity_of_training_relu_neural_networks", "_bibtex": "@misc{\nboob2019complexity,\ntitle={Complexity of Training Re{LU} Neural Networks},\nauthor={Digvijay Boob and Santanu S. Dey and Guanghui Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=HyVbhi0cYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper687/Official_Review", "cdate": 1542234402890, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyVbhi0cYX", "replyto": "HyVbhi0cYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper687/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780690, "tmdate": 1552335780690, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper687/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}