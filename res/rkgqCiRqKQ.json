{"notes": [{"id": "rkgqCiRqKQ", "original": "SklJfA69tm", "number": 921, "cdate": 1538087890147, "ddate": null, "tcdate": 1538087890147, "tmdate": 1545355400289, "tddate": null, "forum": "rkgqCiRqKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Inferring Reward Functions from Demonstrators with Unknown Biases", "abstract": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.", "keywords": ["Inverse reinforcement learning", "differentiable planning"], "authorids": ["rohinmshah@berkeley.edu", "noah.gundotra@berkeley.edu", "pabbeel@cs.berkeley.edu", "anca@berkeley.edu"], "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"], "TL;DR": "When we infer preferences from behavior, we can try to improve accuracy by jointly learning a bias model and preferences, though this requires new assumptions to make progress.", "pdf": "/pdf/e6576fa947d3846775e07d7a27b9bc4293a52da8.pdf", "paperhash": "shah|inferring_reward_functions_from_demonstrators_with_unknown_biases", "_bibtex": "@misc{\nshah2019inferring,\ntitle={Inferring Reward Functions from Demonstrators with Unknown Biases},\nauthor={Rohin Shah and Noah Gundotra and Pieter Abbeel and Anca Dragan},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgqCiRqKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkegL_4egV", "original": null, "number": 1, "cdate": 1544730696420, "ddate": null, "tcdate": 1544730696420, "tmdate": 1545354512223, "tddate": null, "forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper921/Meta_Review", "content": {"metareview": "The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.  To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting idea but paper needs more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper921/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper921/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inferring Reward Functions from Demonstrators with Unknown Biases", "abstract": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.", "keywords": ["Inverse reinforcement learning", "differentiable planning"], "authorids": ["rohinmshah@berkeley.edu", "noah.gundotra@berkeley.edu", "pabbeel@cs.berkeley.edu", "anca@berkeley.edu"], "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"], "TL;DR": "When we infer preferences from behavior, we can try to improve accuracy by jointly learning a bias model and preferences, though this requires new assumptions to make progress.", "pdf": "/pdf/e6576fa947d3846775e07d7a27b9bc4293a52da8.pdf", "paperhash": "shah|inferring_reward_functions_from_demonstrators_with_unknown_biases", "_bibtex": "@misc{\nshah2019inferring,\ntitle={Inferring Reward Functions from Demonstrators with Unknown Biases},\nauthor={Rohin Shah and Noah Gundotra and Pieter Abbeel and Anca Dragan},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgqCiRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper921/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353033958, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper921/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper921/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper921/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353033958}}}, {"id": "rkl48xtZ6m", "original": null, "number": 3, "cdate": 1541668939933, "ddate": null, "tcdate": 1541668939933, "tmdate": 1541668939933, "tddate": null, "forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "content": {"title": "Interesting topic and approach, needs work and careful evaluation", "review": "Not all examples in the introduction are necessarily biases but can be modeled with reward functions, where reward is given to specific states other than finishing work by the deadline. It would be helpful for the reader to get examples that  correspond to the investigated biases. \n\nIt would be good if the authors could at least mention that \u201cBoltzmann rational\u201d is a specific model of \u201csystematic\u201d bias for which much experimental support eith humans and animals exists. \n\nThe authors are strongly encouraged to review the literature on IRL, which includes other examples of modeling explicitly suboptimal agents, e.g.:\n- Rothkopf, C. A., & Dimitrakakis, C. (2011). Preference elicitation and inverse reinforcement learning. ECML.\nSimilarly, the idea to learn an agent\u2019s reward functions across multiple tasks has also appeared in the literature before, e.g.:\n- Dimitrakakis, C., & Rothkopf, C. A. (2011). Bayesian multitask inverse reinforcement learning. EWRL.\n- Choi, J., & Kim, K. E. (2012). Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. NIPS\n\nThe authors state:\n\u201cThe key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the model\u2019s \"understanding\" using backpropagation to infer the reward from actions.\u201d\nIt would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agent\u2019s planning given the rewards and the transition function are learned, including Ziebart et al. and Dimitrakakis et al. This is also related to \n- Neu, G., & Szepesv\u00e1ri, C. (2007). Apprenticeship learning using inverse reinforcement learning and gradient methods. UAI.\n\nIt would be great if the authors could also discuss how assumption 3 is a necessary for accurately inferring reward functions and biases and how deviations from this assumption interfere with the goal of this inference. This seems to be a central and important point for the viability of the approach the authors take here.\n\nCurrently, the evaluation of the proposed method is in terms of the loss incurred by a planner between the inferred reward function and the true reward function, figure 3. It would be important for the evaluation of the current manuscript to know what the inferred biases are. That using a wrong model of how actions are generated given values, e.g. myopic vs. Boltzmann-rational, results in wrong inferences, should not be too surprising. Therefore, the main question is: does the proposed algorithm recover the actual biases?\n\nMinor points:\n\u201clike they naive and sophisticated hyperbolic discounters\u201d\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper921/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inferring Reward Functions from Demonstrators with Unknown Biases", "abstract": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.", "keywords": ["Inverse reinforcement learning", "differentiable planning"], "authorids": ["rohinmshah@berkeley.edu", "noah.gundotra@berkeley.edu", "pabbeel@cs.berkeley.edu", "anca@berkeley.edu"], "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"], "TL;DR": "When we infer preferences from behavior, we can try to improve accuracy by jointly learning a bias model and preferences, though this requires new assumptions to make progress.", "pdf": "/pdf/e6576fa947d3846775e07d7a27b9bc4293a52da8.pdf", "paperhash": "shah|inferring_reward_functions_from_demonstrators_with_unknown_biases", "_bibtex": "@misc{\nshah2019inferring,\ntitle={Inferring Reward Functions from Demonstrators with Unknown Biases},\nauthor={Rohin Shah and Noah Gundotra and Pieter Abbeel and Anca Dragan},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgqCiRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "cdate": 1542234346387, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper921/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833499, "tmdate": 1552335833499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper921/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeDZjQ62m", "original": null, "number": 2, "cdate": 1541384959030, "ddate": null, "tcdate": 1541384959030, "tmdate": 1541533577286, "tddate": null, "forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "content": {"title": "Paper studies a relevant and interesting problem but needs extended empirical evaluation", "review": "This paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments.\n\nThe studied problem is relevant as many/most demonstrators have unknown biases and we still need methods to effectively learn from those.\n\nAs far as I am aware of the related literature, the problem has not been studied in that explicit form although there is related work which targets the problem of learning from suboptimal demonstrators or demonstrators that can fail, e.g. [1] (I suggest to discuss this and other relevant papers in a related work section).\n\nThe main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation:\n* For instance, the formalization of \u201eAssumption 1\u201c is unclear. In which sense does this cover similarity in planing? As far as I understand, the function D could still map any combination of world model and reward function to any arbitrary policy. What does it mean that the planning algorithm D is \u201efixed and independent\u201c?\n* A crucial point requiring more investigation in my opinion is Assumption 3 (well-suited inductive bias). Empirically the chosen experimental setup yields expected results. However, to better understand the problem of learning with unknown biases it would be important to see how results change if the model for the planner changes. A small step in that direction would have been to provide results for value iteration networks with different number of iterations and number neurons, etc. \n* If you use the differentiable planner instead of the VIN, how many iterations do you unroll?\n* Is there any evidence that the proposed approach can work effectively in larger scale domains with more difficult biases? Also in the case in which the biases are inconsistent among demonstrations?\n\nFurther suggestions:\n* Test how algorithm 1 performs if first initialized on simulated optimal demonstrations.\n* Improve notation for the planning algorithm D by using brackets.\n\n[1] Shiarlis, K., Messias, J., & Whiteson, S. (2016, May). Inverse reinforcement learning from failure. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems (pp. 1060-1068). International Foundation for Autonomous Agents and Multiagent Systems.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper921/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inferring Reward Functions from Demonstrators with Unknown Biases", "abstract": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.", "keywords": ["Inverse reinforcement learning", "differentiable planning"], "authorids": ["rohinmshah@berkeley.edu", "noah.gundotra@berkeley.edu", "pabbeel@cs.berkeley.edu", "anca@berkeley.edu"], "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"], "TL;DR": "When we infer preferences from behavior, we can try to improve accuracy by jointly learning a bias model and preferences, though this requires new assumptions to make progress.", "pdf": "/pdf/e6576fa947d3846775e07d7a27b9bc4293a52da8.pdf", "paperhash": "shah|inferring_reward_functions_from_demonstrators_with_unknown_biases", "_bibtex": "@misc{\nshah2019inferring,\ntitle={Inferring Reward Functions from Demonstrators with Unknown Biases},\nauthor={Rohin Shah and Noah Gundotra and Pieter Abbeel and Anca Dragan},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgqCiRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "cdate": 1542234346387, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper921/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833499, "tmdate": 1552335833499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper921/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxgiuKq2X", "original": null, "number": 1, "cdate": 1541212312197, "ddate": null, "tcdate": 1541212312197, "tmdate": 1541533577079, "tddate": null, "forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "content": {"title": "Excellent motivation of work but lacks technical merit; results not convincing", "review": "This paper has proposed algorithms for inferring reward functions from demonstrations with unknown biases. To achieve this, the authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions.\n\nThis paper has provided an excellent motivation of their work in Sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions. The paper is well-written, up till Section 4. \n\nOn the flip side, there does not seem to be any significant technical challenges, perhaps due to some of the assumptions that they have made. Like the authors have mentioned, I do find assumption 3 to be overly strong and restrictive, as empirically demonstrated in Section 5.2. Arguably, is it really weaker than that of noisy rationality? At this moment, it is difficult to overlook this, even though the authors have argued that it may not be as restrictive in the future when more sophisticated differentiable planners are developed.\n\nThe experimental results are not as convincing as I would have liked. In particular, Algorithm 2 (learning a demonstrator's model) does not seem to outperform that of assuming an optimal demonstrator for the noiseless case and a Boltzmann demonstrator for the noisy case (Fig. 3). This was also highlighted by the authors as well: \"The learning methods tend to perform on par with the best of two choices.\" It begs the question whether  accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not.\n\n\n\nOther detailed comments are provided below:\n\nI would have preferred that the authors present their technical formulations in Section 4 using the demonstrator's trajectories instead of policies.\n\nThe authors say that \"In some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and Boltzmann assumptions.\" But, Fig. 3 shows that Algorithm 2 does not perform better than either that of the optimal or Boltzmann demonstrator.\n\nIn Section 5.2, the authors have empirically demonstrated the poor approximate planning performance of VIN, as compared to an exact model the demonstrator. What then would its implications be on the adaptivity of Algorithms 1 and 2 to biases?\n\nThe following references on IRL with noisy demonstration trajectories would be relevant:\n\nBenjamin Burchfiel, Carlo Tomasi, and Ronald Parr. Distance Minimization for Reward Learning from Scored Trajectories. In Proc. AAAI, 2016.\n\nJ. Zheng, S. Liu, and L. M. Ni. Robust Bayesian inverse reinforcement learning with sparse behavior noise. In Proc. AAAI, 2014.\n\n\n\nMinor issues:\nOn page 4, the expression D : W \u00d7 R -> S -> A -> [0, 1] can be more easily understood with the use of parentheses.\n\nFor Assumption 2b, you can italicize \"some\".\n\nIn the first paragraph of section 4.1, what are you summing over?\n\nLine 3 of Algorithm 1: PI_W?\n\nPage 7: For the learning the bias setting?\n\nPage 7: figure 3 shows?\n\nPage 7: they naive?\n\nPage 7: so as long as?\n\nPage 8: adaption?\n\nPage 8: predicated?\n\nPage 8: figure 4 shows?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper921/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inferring Reward Functions from Demonstrators with Unknown Biases", "abstract": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.", "keywords": ["Inverse reinforcement learning", "differentiable planning"], "authorids": ["rohinmshah@berkeley.edu", "noah.gundotra@berkeley.edu", "pabbeel@cs.berkeley.edu", "anca@berkeley.edu"], "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"], "TL;DR": "When we infer preferences from behavior, we can try to improve accuracy by jointly learning a bias model and preferences, though this requires new assumptions to make progress.", "pdf": "/pdf/e6576fa947d3846775e07d7a27b9bc4293a52da8.pdf", "paperhash": "shah|inferring_reward_functions_from_demonstrators_with_unknown_biases", "_bibtex": "@misc{\nshah2019inferring,\ntitle={Inferring Reward Functions from Demonstrators with Unknown Biases},\nauthor={Rohin Shah and Noah Gundotra and Pieter Abbeel and Anca Dragan},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgqCiRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper921/Official_Review", "cdate": 1542234346387, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgqCiRqKQ", "replyto": "rkgqCiRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper921/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833499, "tmdate": 1552335833499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper921/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}