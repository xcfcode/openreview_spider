{"notes": [{"id": "SJz6MnC5YQ", "original": "HylebAT5Ym", "number": 1309, "cdate": 1538087957108, "ddate": null, "tcdate": 1538087957108, "tmdate": 1545355392192, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJgRcajgl4", "original": null, "number": 1, "cdate": 1544760725772, "ddate": null, "tcdate": 1544760725772, "tmdate": 1545354518898, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Meta_Review", "content": {"metareview": "Although one reviewer recommended accepting this paper, they were not willing to champion it during the discussion phase and did not seem to truly believe it is currently ready for publication. Thus I am recommending rejecting this submission.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "No reviewer was willing to champion this work"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1309/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352884774, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352884774}}}, {"id": "r1ek3L5VkV", "original": null, "number": 17, "cdate": 1543968422538, "ddate": null, "tcdate": 1543968422538, "tmdate": 1543968422538, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SyeBGNHD6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Wow, that's a lot of replies", "comment": "Thanks for the comments. I still think the work is interesting and the comments and improvements to the paper help. I'm unfortunately not convinced that it's yet good enough to go up to the next category."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "Bkez3clqRX", "original": null, "number": 16, "cdate": 1543273130076, "ddate": null, "tcdate": 1543273130076, "tmdate": 1543273130076, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SkemSsMfpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Recent modifications of Paper1309", "comment": "Dear Reviewer,\n\nThank you very much for your new and previous comments. We have revised our paper again in order to address all of them in the paper. The modifications are listed as followings:\n\n1. For graph deconvolution, we have modified and reorganized the content. The Section 3.2.2 on \u201cGraph Deconvolution\u201d has been reorganized to two subsections \u201cnode-to-edge deconvolution\u201d and \u201cedge-to-edge deconvolution\u201d. We also extended them to make the description on deconvolution operations clearer and more comprehensive.\n\n2. For graph deconvolution, we have also added a new figure and refined the equations\u2019 descriptions. Figure 3 is added to describe the mechanism of our proposed deconvolution operators as well as their correlation to the convolution operations. Equation 6, Equation 7, and their descriptions have also been revised to make them clearer and concrete. Specifically, Figure 3 describes how the node representation and edge representation are respectively decoded by our deconvolution layers, while Equation 6 and Equation 7 describe how to aggregate the decoded information into the final weighted adjacent matrix.\n\n3. We have referred to all the figures in the body of text. \n\n4. We have added statements to describe how to introduce random noises in the whole architecture, see in the 2nd paragraph of Section 3.1 in Page 4.\n\n5. We have added statements of describing the reason to use L1 loss and how L1 loss is applied, please see in the paragraph before Equation 2 in Page 4. Additionally, we also added the statements of how L1 norm and GAN loss function jointly, see in the paragraph after Equation 2.\n\n6. We have added the statements why the metrics are chosen to evaluate the scale-free dataset, please see in the 2nd Paragraph of Section 4.2.2.\n\nAdditionally, to improve the reproducibility of the proposed methodologies and experiments, we have already released our code in https://github.com/anonymous1025/Deep-Graph-Translation-.  More architecture parameters are also provided in Appendix E.\n\nThank you very much again for the comments and please let us know if there are any other issues.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "ByliURT_Rm", "original": null, "number": 14, "cdate": 1543196242620, "ddate": null, "tcdate": 1543196242620, "tmdate": 1543196242620, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "HJeWPj6eA7", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Re: good feedback", "comment": "Thank you!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "HJeWPj6eA7", "original": null, "number": 12, "cdate": 1542671192913, "ddate": null, "tcdate": 1542671192913, "tmdate": 1542671192913, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJlERxAm67", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "good feedback", "comment": "Dear Authors thank you for your extensive feedback ~\n\nI am able to better understand your paper ~ and I believe it would be beneficial to have it at conference.\n\nI am thus changing my rating:\n\n  Marginally below acceptance threshold  ==> Marginally ABOVE acceptance threshold\n\nThank you!\n\n<AnonReviewer3>"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SyeX3DBwpm", "original": null, "number": 11, "cdate": 1542047658518, "ddate": null, "tcdate": 1542047658518, "tmdate": 1542047658518, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "S1l4P2HuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Some statement explanations and modifications: Part III", "comment": "----------------------\nQ: Hard to parse \u201cDifferent and more difficult than graph generation designed only for learning the distribution of graph representations, graph translation one needs to learn not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously. \u201c\nA: We have modified it as: \u201cDifferent and more difficult than graph generation designed only for learning the distribution of graph representations, graph translation learns not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously. \u201c\n \n----------------------\nQ: Hard to parse \u201cgraph translation requires to learn\u201d\nA: We have modified it as: \u201cgraph translation focus on learning\u2026.\u201d\n \n----------------------\nQ: Hard to parse \u201cin most of them the input signal is given over node with a static set of edge and their weights fixed for all samples\u201d\nA: We have modified it as: \u201cmost of them try to embed the graphs based on the signals assigned on the nodes with the fixed graph topology\u201d\n \n----------------------\nQ: \u201cwe propose an graph\u201d -> \u201cwe propose a graph\u201d\nA: Thanks for correctness. We have modified in revised paper.\n \n----------------------\nQ: Hard to parse \u201cThe two components of the formula refers to direction filters as talked above\u201d\nA: We have modified it as: \u201cThe two components refers to out-direction and in-direction features as talked above\u201d\n \n----------------------\nQ: Hard to parse \u201cNext, graph translator requires to\u201d\nA: We have modified it as: \u201cNext, decoder part aims to\u201d\n \n----------------------\nQ: \u201cas shown in Equations equation 7 and Equations equation 6,\u201d -> \u201cas shown in Equation 6 and Equation 7\u201d\nA: Thanks for correctness. We have modified in revised paper.\n \n----------------------\nQ: Hard to parse \u201cThe challenge is that we need not only to learn the\u201d\nA: We have modified it as: \u201cThe challenge is not only learning the\u201d\n \n----------------------\nQ: Hard to parse \u201cwe randomly add another kjEj edges on it to form the target graph\u201d\nA: We have modified it as: \u201cwe randomly add kjEj edges on it to form the target graph\u201d\n\nWe hope we were able to explain everything clearly to your satisfaction, please let us know if there are any more open points.\n\nThank you once again!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "ryly9wHvTQ", "original": null, "number": 10, "cdate": 1542047623235, "ddate": null, "tcdate": 1542047623235, "tmdate": 1542047623235, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "S1l4P2HuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Some statement explanations and modifications: Part II", "comment": "\n----------------------\nQ: \u201cThe goal is to forecast and synthesize the future potential malicious authentication graphs of the users without any historical malicious behaviors, by the graph translator from normal to malicious graph trained based on the users with historical malicious-behavior records.\u201d - This isn\u2019t entirely clear. Are you trying to create new malicious graphs or show that a current graph will eventually go malicious?\n\nA: (1) What we do: For a user who has not been attacked, we want to synthesize what would be the attack activities like if he was attacked. To do this, we train a translator from the other user's historical data and use this translator to generate attacked activities for this user.\n(2) Why we do: If we can generate the attacked graphs of a user, we can build a prediction model (classifier) for this user before he is really attacked.\n\n \n----------------------\nQ: \u201cAll the comparison methods are directly trained by the malicious graphs without the conditions of input graphs as they can only do graph generation instead of translation.\u201d - not clear. For the synthetic data sets how did you choose which ones were malicious?\n\nA: Sorry for this statement error, it should be modified as: \u201cAll the comparison methods are directly trained by the target graphs without the conditions of input graphs as they can only do graph generation instead of translation.\u201d\n \n----------------------\nQ: \u201cGraphRNN is tested with graph size within 150. GraphGMG, GraphVAE is tested within size 10 and RandomVAE is tested on graphs within size 150.\u201d -> \u201cGraphRNN and RandomVAE are tested with graph up to size 150. GraphGMG, GraphVAE is tested with graphs up to size 10.\u201d\nA: We have modified it as: \u201cRandomVAE is tested on graphs within size 50.\u201d\n \n----------------------\nQ: \u201cHere, beyond label imbalance, we are interested in \u201clabel missing\u201d which is more challenging.\u201d - \u201cmissing labels\u201d?\nA: Sorry for the confusion. We take an example for Missing label in the authentication attack data. In a company, only some groups of users have been attacked and thus these users can easily build a prediction model based on both positive and negative data. In contrast, some other users who have not been attacked will not have any positive examples, which we called the \u201clabel missing\u201d problem.\n \n ----------------------\nQ: \u201cIn addition, we have also trained a \u201cgold standard\u201d classifier based on input graphs and real target graphs.\u201d - need to say more about this.\nA: Yes, we have explained the gold standard in above answers and more detailed explanation can be found in 1st and 2nd paragraphs of 4.2.3.\n \n----------------------\nQ: Hard to parse: \u201cwhich barely can be available for the accounts worth being monitored.\u201d\nA: We have modified it as: \u201cwhich barely can be available since these accounts always being monitored well\u201d.\n \n----------------------\nQ: Hard to parse \u201cModern deep learning techniques operating on graphs is a new trending topic in recent years.\u201d\nA: We have modified it as: \u201cDeep learning techniques dealing with graphs is a new trending topic in recent years.\u201d\n \n----------------------\nQ: Hard to parse \u201cHowever, these methods are highly tailored to only address the graph generation in a specific type of applications such as molecules generation\u201d\nA: We have modified it as: \u201cHowever, these methods are not general for being widely applied\u201d\n \n----------------------\nQ: Hard to parse \u201cExisting works are basically all proposed in the most recent year,\u201d\nA: We have modified it as: \u201cExisting works are almost proposed in the most recent year,\u201d\n \n----------------------\nQ: \u201cTypically, we focus on learning the translation from one topological pattern to the other one\u201d -> \u201cTypically we focus on learning the translation from one topological pattern to the other\u201d\nA: Thanks for correctness. We have modified in revised paper.\n \n----------------------\nQ: It\u2019s not clear in equation 1 how you represent G_X. Only much later is it mentioned about adjacency matrix.\nA: Yes, the graphs here are represented as the weighted adjacent matrix of a graph."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "Hkex58HP6Q", "original": null, "number": 9, "cdate": 1542047368327, "ddate": null, "tcdate": 1542047368327, "tmdate": 1542047368327, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "S1l4P2HuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Some statement explanations and modifications: Part I", "comment": "Next, we would like to reply to more specific comments.\n\n----------------------\nQ: \"The tremendous success of deep generative models on generating continuous data like image and audio\u201d - it is not clear what this continuous data is.\n\nA\uff1aThe continuous data here means that the elements (pixel or signal) in an image or audio is continuous in Euclidean structure and their features are fully related to spatial positions of elements.\n \n----------------------\nQ: \u201cThis requires us to learn the generic distribution of theft behaviors from historical attacks and synthesize the possible malicious authentication graphs for the other accounts conditioning on their current computer networks\u201d - given that these historical attacks are (hopefully) rare, is there enough data here to construct a model?\n\nA: (1) Yes, we have enough data here to train a good model since we used the historical attack data from all users, instead of constructing a model for each individual user. There are 315 training samples for the user authentication dataset, which is sufficient to get the good performance as shown in Tables 4 in Section 4.2.3  and Table 6 in Appendix C.\n(2) One attacker\u2019s rule is supposed to be shared among all the users. Thus, once parts of a company are attacked by one attacker, we can train a translator model by the data from the attacked users and generate synthetic attack activities for the un-attacked users.\n \n----------------------\nQ: Please define GCNN\n\nA: Sorry for this error, we have added the definition in the revised version. GCNN is short for Graph Convolution Neural Network, which is a general term referring to the encoder model in our framework.\n \n----------------------\nQ: \u201cOur GT-GAN is highly extensible where underlying building blocks, GCNN and distance measure in discriminator, can be replaced by other techniques such as (Kipf & Welling, 2017; Arjovsky et al., 2017) or their extensions.\u201d - this sounds more like a feature of what you have contributed rather than a contribution.\n\nA: we would like to state that that DGT is more like a general graph translation framework than a special model. The encoder and decoder model we use here can be regarded as a special case and it can also be replaced by others with different distance measurements.\n \n----------------------\nQ: In the context of synthetic data, what is ground-truth?\n\nA: (1) In the synthetic dataset, there are also input graphs and target graphs, just like the real-world dataset.\n(2) Target graphs are generated based on the input graphs by following some predefined rules. We use the input graphs and target graphs to train the translator. Thus, to evaluate the generated graphs, the target graphs are the ground-truth. The detail of the generation rules of target graphs is given in 4.1.1 Datasets.\n \n----------------------\nQ: Figure 2 would seem to need more explanation.\n\nA: (1) Figure 2 shows the difference between the graph convolution and graph deconvolution, which is stated in Section 3.2.2 Graph deconvolutions. The encoder does n-hop edge information aggregation from the input graphs and learns the latent representation of nodes. Then we first decode the node embedding to get the n-hop aggregated information on edges by node-to-edge layer and then we further decode the n-hop aggregated information by n-layers back to get the output adjacency matrix.\n(2) we have modified our paper in Section 3.2.2, e.g, by adding \u201cTo get the nth hop information Aij, row filter decodes all the (n+1)-th hop information of outgoing edges of Vi and column filter decodes all the (n+1)-th hop information of incoming edges of Vj.\u201d\n \n ----------------------\nQ: The end of section 3.3 is a bit vague and lacks enough detail to reproduce.\n\nA: (1) Function of discriminator: In the training phase, the discriminator aims to classify the generated graphs and the real target graphs. The translator and discriminator are trained together, and the final goal is that the discriminator cannot distinguish the generated graphs and real target graphs.\n(2) Construction: The discriminator is constructed by an edge-to-edge layer and edge-to-node layer, which learns features from graphs like the translator. Then the extracted features of each node are together mapped into a fully connected layer, embedding the graph into a vector which is inputted into a SoftMax classifier to output the probability of the graph to be a target graph or generated graph. We have modified this section for better readability. We have released our codes in the paper and it will give an additional guarantee that the experimental results can be easily reproduced.\n \n----------------------\nQ: \u201cour GT-GAN is able to provide a scalable (i.e., O(N2)) algorithm that can generate general graphs.\u201d - what sizes have you tested this up to?\n\nA: We have test size up to 300."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SJgG2EBDpQ", "original": null, "number": 8, "cdate": 1542046890057, "ddate": null, "tcdate": 1542046890057, "tmdate": 1542046890057, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "S1l4P2HuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Explanations for concerns\uff1aPart II", "comment": "\n-------------------------\nQ: Although you comment on other graphs approaches being limited to very small graphs, you do not test your approach on graphs with over 150 nodes. These would also seem to be very small graphs in comparison to real-world graphs. Further evaluation on larger graphs would seem to be essential - how long would it take on graphs with 10^6 nodes?\n\nA: (1) Our testing experiments do test graphs on size 300 (i.e., user authentication dataset), but is not yet able to handle the scale of millions of nodes. Handling millions of nodes in graph is much more difficult than handling millions of pixels in images. This is because in graph data the nodes connect arbitrarily, and the adjacency matrix is required to code the connectivity among nodes which are at least quadratic to the number of nodes. And this is partially why most of the existing work on graph generative learning domain focus on only a very small graph. And also because of this, we test small-size graphs in this paper because most of the comparison methods can typically only handle dozens of nodes or fewer graphs. Compared to them, our model handles nontrivially \u201clarger graph\u201d (6-10 times larger than most existing methods).\n(2) Many important real-world applications are small graphs and may need graph generation. To list a few (with graph size from dozens to thousands): chemical molecules, cyber network (in our paper), electrical circuits, and semantic network (where nodes are words and links are their semantic correlations).\n \n-------------------------\nQ: The real-world dataset seems rather odd and not fully explored. Given that you have this data it is surprising that you didn\u2019t complete the loop by showing that you could take data from before a hack attempt and show that you could predict that in the future you had a hack attempt. Perhaps this is due to the fact that you didn\u2019t have the ground-truth data in here to show a graph going from good to bad? But if not, it would have been good to have shown, either through this data or some other, how your approach does match in with real-world results.\n\nA: (1) The real-world dataset and its application are authoritative and motivated this research. The dataset is recent, authoritative, and provided by the prestigious \u201cLos Alamos National Laboratory\u201d (https://csr.lanl.gov/data/cyber1/ ). The research problem behind this dataset raised up their needs to predict the future hacking behavior of a user with no historical hacking behavior has been a highly practical but prohibitively challenging. Such application strong motivates this new domain of graph translation where we transfer the hacking behavior from those users with historical hacking behavior in different network structure.\n(2) The dataset has been fully explored by a loop. We indeed have predicted the hack attempt in the future and validated it against ground truth with accuracy metrics such as accuracy, F1, Precision, and recall explained in the last paragraph of Section 4.2.3. Specifically,  we use half of users\u2019 good graphs (data before attack) and real hacker graphs (data after attack) to train the translator. We then generate graphs with this translator for the other users. To evaluate indirectly, we use the generated graphs and good graphs to train an attacker prediction model (classifier) for each user, if it can recognize his real hacker graphs, this prediction model works in real-world and our translator is good.\n(3) We have also shown the case studies on \u201ca graph going from good to bad\u201d. Specifically, in Figures 7 and 8 in Appendix C, we have shown: 1) the \u201cgood\u201d graph, 2)the \u201chacked\u201d graph generated by our methods, and 3) the ground-truth \u201chacked\u201d graph. And the results show that our methods can well predict the hack attempts. During our experiments, we have observed numerous such case studies and put them as representatives.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SyeBGNHD6m", "original": null, "number": 7, "cdate": 1542046733190, "ddate": null, "tcdate": 1542046733190, "tmdate": 1542046733190, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "S1l4P2HuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Explanations for concerns\uff1aPart I", "comment": "\nDear Reviewer:\n \nThanks very much for your comments and questions. We would like to first explain your concerns.\n\n-------------------------\nQ: Equation 2 is used to minimize the distance between graphs from X and graphs in Y. Yet, the main metric which is used to evaluate the paper is this distance. This would seem to give an unfair advantage to your approach. I would also be concerned about the fact that later you use this for stating if a graph represents good or hacker activity. If you have drawn translated graphs towards real graphs, how do you know that you haven\u2019t pulled a good graph closer to a hacker graph? This is more concerning considering work which came out of NIPS which suggested that GAN\u2019s tend to favour producing similar output rather than spreading it evenly over the domain.\n \nA: (1) We minimize the distance between generated graphs and real target graphs in Y, not graphs from X and graphs from Y. The comparison methods also minimize the distance between their generated graphs and real graphs, which are the same as us.\n(2) We have done both direct and indirect evaluations in all the datasets, which comprehensively demonstrate the good performance of the proposed methods. Moreover, the indirect evaluations (see Table 2-4) do not use distance to evaluate the performance but the classification metrics (precision, recall, ACU and F1-score). Even in direct evaluation, the metrics contain degree distribution distance, MSE of adjacent matrix comparison and repository comparing, while the loss function in equation 2 is applied only on the MSE of the adjacent matrix of graphs.\n\nFor the statement \u201cNIPS which suggested that GAN\u2019s tend to favor producing similar output\u201d. Our answers are three-fold:\n(1) As we have done an extensive survey and did not find papers using GAN for graph generation yet, we doubt if the experience in GAN on other data still applies exactly the same for graph data. This is because as we know, graphs are highly different types of data than images which are continuous-valued (e.g., RGB). In graphs, nodes can have arbitrary connectivity.\n(2) Moreover, our experiments suggest that there is indeed nontrivial variance in the generated graphs. As shown in Figure 5 in Appendix, we can see the difference among the degree distributions of different graphs we generated is obvious.\n(3) We can tune the dropout ratio to control the degree of variation of generated output. The noise is introduced by the dropout function in each convolution layer, which functions by randomly ignoring certain ratio of neuron\u2019s output of a network.\n \n-------------------------\nQ: It isn\u2019t entirely clear what your results are trying to show. Presumably P, R, AUC and F1 are generated from the results produced from your Discriminator? Were each of the other approaches optimized against your discriminator or not? Also, it is unclear as to what the Gold Standard method is - we\u2019re only told that it\u2019s a classifier, but what type and how constructed?\n\nA: (1) P, R, AUC and F1 are metrics to indirectly evaluate the generated graphs by comparison models.  Since all the comparison methods in our paper are generative models which generate graphs, and hence our experiment is to evaluate how good the generated graphs are. One way to evaluate this is by \u201cindirect evaluation\u201d, where we use the graphs generated by different comparison methods as training data to train a classifier (all are based on KCNN for fairness), and then compare which classifier is better. The flowchart of the indirect evaluation is shown in Figure 9, in Appendix D.\n(2) \u201cGold standard\u201d is the classifier trained directly by the real target graphs, instead of generated graphs. As it directly uses the real graphs to train the classifier (still based on KCNN), it is expected to get the best performance. Therefore, \u201cgold standard\u201d method acts as the \u201cbest-possible-performer\u201d, and is used as a benchmark to evaluate all the different generative models on how \u201creal\u201d the graphs they can generate: the closer (and better) their performance is to the \u201cgold standard\u201d one, the \u201cmore real\u201d their generated graphs are.\n \n-------------------------\nQ: Your approach seems to be \u2018fixed\u2019 in the set of nodes which are in both in the input and output graphs - needing to be the same. This would seem significantly limiting as graphs are rarely of the same node set.\n\nA: Yes, we admit that our model has a limitation in dealing with the variable-size input graphs. This limitation largely exists in the existing deep graph learning methods, especially those based on graph convolution. This problem itself is a challenging open problem that requires significant future efforts in the community. However, the focus of our work in this paper is the translation mapping establishment, optimization, and evaluation. We are indeed considering one of our next extensions to deal with this problem. Thanks for the comments."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SJlugpiVaX", "original": null, "number": 5, "cdate": 1541876975981, "ddate": null, "tcdate": 1541876975981, "tmdate": 1541877172346, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SkemSsMfpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Clarifications of some points: Part II", "comment": "\n-----------------------------------------------------------------------\nQ\uff1aA lot of clarity is required on the choice of evaluation metric; for example, choice of distance measure?  What is the L1 norm applied on?\n\nA: Answer about Evaluation metrics: \n(1) We want to evaluate if the generated graphs are scale-free graphs in the direct evaluation for dataset scale-free graphs. If the degree distribution of generated graphs is the same to the degree distribution of real target graphs, the generated graphs are good.\n(2) There are many classical evaluation metrics focusing on measuring the similarities or distance of two distributions. The four metrics in this paper are among the most authoritative and commonly used ones in existing works, e.g., [2][3][4][5].\n\nAnswer about L1 norm: \n(1) L1 norm is applied to the weight adjacent matrix of the graph. Our methodology is achieved by a trade-off between L1 loss and adversarial loss (GAN-D). Specifically, L1 makes generated graphs share the same rough outline of sparsity pattern like generated graphs, while under this outline, adversarial loss allows them to vary to some degree.\n(2) L1 norm is commonly used in GAN in relevant domains, e.g., in image-translation domain, for example, reference [1] (with 600+ citations) and reference [6] (with 1300+ citations). They have done extensive experiments to show the advantage of such a strategy. (3) The experiment demonstrates its effectiveness. Specifically, the proposed GT-GAN that uses L1 norm outperformed all the other comparison methods shown in Table 2,3 and 4.\n\n-------[2] Schieber, T. A., Carpi, L., D\u00edaz-Guilera, A., Pardalos, P. M., Masoller, C., & Ravetti, M. G. (2017). Quantification of network structural dissimilarities. Nature Communications, 8, 13928.\n-------[3] Bauckhage, C., Kersting, K., & Hadiji, F. (2015, July). Parameterizing the Distance Distribution of Undirected Networks. In UAI (pp. 121-130).\n-------[4] Chiang, S., Cassese, A., Guindani, M., Vannucci, M., Yeh, H. J., Haneef, Z., & Stern, J. M. (2016). Time-dependence of graph theory metrics in functional connectivity analysis. NeuroImage, 125, 601-615.\n-------[5] You, J., Ying, R., Ren, X., Hamilton, W. L., & Leskovec, J. (2018). GraphRNN: A Deep Generative Model for Graphs. arXiv preprint arXiv:1802.08773.\n-------[6] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.\n \n-----------------------------------------------------------------------\nQ\uff1aI did not completely follow the arguments towards directed graph deconvolution operators. There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work.\n\nA: (1) Our decoder is symmetric to the encoder in their architectures. The encoder does n-hop edge information aggregation from the input graphs and learns the latent representation of nodes. Then, we first decode the node embedding to get the n-hop aggregated information on edges by node-to-edge layer and then we further decode the n-hop aggregated information layer by layer by n-layers back to get the output adjacency matrix.\n(2) Different from image deconvolution, for each hidden channel, we have two filters vertical to each other, i.e., one is a column vector while the other is a row vector. To get the nth hop information of edge <i,j>, row filter decodes all the (n+1)-th hop information of outgoing edges of node i and column filter decodes all the (n+1)-th hop information of incoming edges of node j.\n(3) To make our description clearer, we have updated our paper in Section 3.2.2, e.g, by adding \u201cTo get the nth hop information Aij, row filter decodes all the (n+1)-th hop information of outgoing edges of Vi and column filter decodes all the (n+1)-th hop information of incoming edges of Vj.\u201d\n \n-----------------------------------------------------------------------\nQ: Typo:. The \u201cInf\u201d in Tabel 1\n\nA: As explained in Section 4.2.4 \u201cResults on Scale-Free Graphs\u201d, the \u201cInf\u201d in Tabel 1 represents the distance more than 1000.\n\nWe really hope that we have explained every confused point clearly and please let us know if there are any other points.\nThank you once again for your reviews."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "Hyeo_6oEaQ", "original": null, "number": 6, "cdate": 1541877107408, "ddate": null, "tcdate": 1541877107408, "tmdate": 1541877107408, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SkemSsMfpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Clarifications of some points: Part I", "comment": "\nDear Reviewer:\n\nThank you very much for your comments and suggestions. We would like to answer your questions in detail as follows:\n \n-----------------------------------------------------------------------\nQ: The authors claim that their method is applicable for large graphs. However, it seems the experiments do not seem to support this.\n\nA: (1) We did not mention that we handle \u201clarge graph\u201d, but instead we only mention that we handle \u201clarger\u201d graph. In the domain of graph generation, currently, the proposed graph generative models can typically only deal with graphs with dozens of nodes or less (except GraphRNN which can scale to 300). Compared to them, our model handles relatively \u201clarger graph\u201d (6-10 times larger than most existing methods).\n(2) Translation in graphs is a new topic and we have not found many datasets in very large scale, so we do not test on much larger nodes. But the scalability experiments can still show the superiority of our model compared to others.\n(3) We typically test small-size graphs because most of the comparison methods can only handle small-size graphs.\n \n-----------------------------------------------------------------------\nQ: It is not clear how the noise is introduced in the graphs. I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.\n\nA: Thanks for the review comment.\n(1) The noise is introduced by the dropout function in each convolution layer. Dropout functions by randomly ignore 50% of neuron\u2019s output of a network in our mode by a uniform distribution.\n(2) The way we add noise is well-recognized and commonly-used in generative deep learning models[1]. The noises added in GANs aim to enable the diversities in the generated graphs to avoid the problem that GANs tend to favor producing same output rather than spreading it evenly over the domain.\n(3) We have shown the analysis of the translation quality against noise in Figures 4 and 5. In Figure 5 (see in the supplementary material), each logarithm plot in each column show the power-law trend of each randomly generated graph, which will look linear in such a logarithm plot. It can be seen that the generated graphs show the similar randomness pattern as the real graphs. Moreover, the larger the graph is (see the graph size of 150), the smaller the randomness is, and the clearer the power-law trend is, which verifies that the translation quality of our method.\n  ------[1] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.\n\n-----------------------------------------------------------------------\nQ: It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph. Do we know how does the connectedness of the input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity? Towards this, how does the computational complexity scale wrt to the connectedness?\n\nA: (1) Similar to all the existing graph deep generative learning methods for generic graphs, we do not have additional assumptions on the graphs. The domain of graph deep generative learning methods typically do not require to distinguish or preprocess specific topological types of graphs before applying it, no matter it is strongly- or weakly- connected graph, complete graph, planar graph, scale-free graph, or graphs that have other specific patterns. This is actually one of the core advantages of deep learning based models where the graph patterns are not extracted or pre-identified manually by the human but automatically discovered by the end-to-end deep models.\n(2) This paper has given the time complexity in the worst case: O(n^2) as shown in 3.4. The worst case happens when the graph is a complete graph. The time complexity of a strongly-connected graph will not be worse than that.\n "}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "H1lkUZCQp7", "original": null, "number": 4, "cdate": 1541820743254, "ddate": null, "tcdate": 1541820743254, "tmdate": 1541820796671, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "ryl8Oq-53X", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Clarifications of confused points, and the philosophy behind the architecture: Part II ", "comment": "\n-----------------------------------------------------------------\nQ: Third, and slightly related to the previous point, why do you need a conditional GAN discriminator, if you already model similarity by L1? Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of traditional domains. Instead, here you seem to suggest using L1 and GAN to do basically the same thing, or with significant overlap anyways. This is confusing to me. Please explain the logic for this architectural choice.\n\nA:(1) The logic of using both of them has been explained in the answer to the last question.\n(2) The logic has been well-utilized and verified in the image-translation domain. Again please see the details in the answer to the last question.\n(3)  Our ablation experiment also demonstrates the similar advantage of using both losses for graph translation than only using L1 loss. Specifically, the proposed GT-GAN that uses both loses outperformed the S-Generator that only uses L1 loss on all three datasets by 10% in accuracy on average as shown in Table 2,3 and 4.\n\n-----------------------------------------------------------------\nQ: Four, could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behavior, and label accordingly? That said I am not 100% sure of this problem setting.\n\nA: Yes, \u201cgold standard\u201d method is directly trained based on real target graphs instead of generated ones. Specifically, as you know, all the comparison methods in our paper are generative models which generate graphs, and our experiment is to evaluate how real the generated graphs are. One way to evaluate this is by \u201cindirect evaluation\u201d, where we use the graphs generated by different comparison methods as training data to train a classifier based on KCNN (see reference (Nikolentzos, et al.,2017) in the paper), and then compare which model generates \u201cmore-real graphs\u201d by testing their corresponding trained classifier on test set which consists of real graphs. In \u201cgold standard\u201d method, it directly uses the real graphs to train the classifier (still based on KCNN), so it is expected to get the best performance. Therefore, \u201cgold standard\u201d method acts as the \u201cbest-possible-performer\u201d, and is used as a benchmark to evaluate all the different generative models on how \u201creal\u201d the graphs they can generate: the closer (and better) their performance is to the \u201cgold standard\u201d one, the \u201cmore real\u201d their generated graphs are.\n\nWe hope we were able to answer everything to your satisfaction, please let us know if there are any more open points.\n\nThank you once again!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SJlERxAm67", "original": null, "number": 3, "cdate": 1541820619712, "ddate": null, "tcdate": 1541820619712, "tmdate": 1541820619712, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "ryl8Oq-53X", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "Clarifications of confused points, and the philosophy behind the architecture: Part I", "comment": "Dear Reviewer: \n\nThanks very much for your comments and questions. We would like to explain them in detail and modify our paper accordingly.\n\n----------------------------------------------------------------------------\nQ: First, the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".\n\nA: General architecture: The whole framework includes a translator and a discriminator.\n(1) Translator. Translator consists of an encoder, a decoder, and a skip network, which first learn the representation of the graph and then decode it back to the target graph. See details in the third part of the answer.\n(2) Discriminator. Our discriminator aims to classify the generated graphs and the real target graphs given the input graph. \n(3) The translator and discriminator are trained together, and the final goal is that the discriminator cannot distinguish the generated graphs and real target graphs. After training such a model, the translator will be used in the test phase.\n \nThe logic behind edge-to-edge convolution:\n(1) Generally speaking, the purpose of edge-to-edge convolution layers is to aggregate the neighborhood information of nodes. Specifically, the n-th edge-to-edge convolution layer aggregates the n-th hop connection information of nodes related to each edge.\n(2) Different from image convolution, for each hidden channel, we have two filters, one is a column vector while the other is a row vector. To learn the nth hop information of edge <i,j>, row filter aggregates all the (n-1)-th hop information of outgoing edges of node i and column filter aggregates all the (n-1)-th hop information of incoming edges of node j. \n(3)  Edge-to-edge layers are important to extract some higher-level graph features, e.g., the n-hop reachability from a node to another; n-hop in-degree and out-degree, and many other higher-order patterns.\n\nDifferent blocks in the graph translator:\nTranslator consists of an encoder, a decoder, and a skip network. \n(1) Encoder. The encoder does n-hop edge information aggregation from the input graphs using edge-to-edge layers and then uses the edge-to-node layer to learn the latent representation of nodes. \n(2) Decoder. Reversely, the graph decoder first uses node-to-edge layers to decode the node representations to aggregated edge information and then further decode that into adjacency matrix, which is the final generated graphs. \n(3) Skip-network. Over the encoder-decoder framework, we also added skip-network (the black line of Fig.1) which can directly map the edge aggregation information in every hop from the input graph to the output graph so that can preserve the local information in every resolution (i.e., every hop).\n \n----------------------------------------------------------------------------\nQ: how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.\n\nA: (1) L1 norm is applied to the weight matrix. Our methodology is still general enough which is achieved by a trade-off between L1 loss and adversarial loss (GAN-D), which jointly enforces Gy and T(Gx) to follow a similar topological pattern but may not necessarily the same. Specifically, L1 makes T(Gx) share the same rough outline of sparsity pattern like Gy, while under this outline, adversarial loss allows the T(Gx) to vary to some degree.\n(2) Combining L1 loss and adversarial loss is well-recognized and validated. Works on image-translation have proposed and utilized L1 loss and adversarial loss jointly in GAN, for example, reference [1] (with 600+ citations) and reference [2] (with 1300+ citations). They have done extensive experiments to show the advantage of such a strategy. Furthermore, in our experiments, we found the performance when using L1 loss and adversarial loss jointly is better than using either of them.\n------[1] Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2536-2544).\n------[2] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint."}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}, {"id": "SkemSsMfpQ", "original": null, "number": 3, "cdate": 1541708602993, "ddate": null, "tcdate": 1541708602993, "tmdate": 1541708602993, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "content": {"title": "Novel idea but requesting clarifications. ", "review": "The paper presents a novel idea of generating discrete data such as graphs that is conditional on input data to control the graph structure that is being generated.\n\nGiven an input graph, the proposed method infers a target graph by learning their underlying translation mapping by using new graph convolution and deconvolution\nlayers to learn the global and local translation mapping.\n\nThe idea of learning generic shared common and latent implicit patterns across different graph structure is brilliant.\n\nTheir method learns a distribution over graphs conditioned on the input graph whilst allowing the network to learn latent and implicit properties. \n\nThe authors claim that their method is applicable for large graphs. However, it seems the experiments do not seem to support this. \n\nIt is not clear how the noise is introduced in the graphs. I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph. \n\nIt is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.\nDo we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity? Towards this, how does the computational complexity scale wrt to the connectedness?\n\nA lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?  What is the L1 norm applied on? \n\nI did not completely follow the arguments towards directed graph deconvolution operators. There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work\n\nTypo:. The \u201cInf\u201d in Tabel 1 \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "cdate": 1542234258243, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918670, "tmdate": 1552335918670, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryl8Oq-53X", "original": null, "number": 2, "cdate": 1541180013645, "ddate": null, "tcdate": 1541180013645, "tmdate": 1541533246710, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "content": {"title": "Good problem setting, interesting results, needs more clarifications.", "review": "This paper addresses the important / open problem of graph generation, and specifically in a conditional/transductive setting.\n\nGraph generations is a new topic, it is difficult, and has many important applications, for instance generating new molecules for drug development.\n\nAs stated by the authors, this is a relatively open field: there are not many papers in this area, with most approaches today resorting to domain specific encodinings, or \"flattening\" of graphs into sequences to then allow for the use recurrence (like in MT); this which per se is an rather coarse approximation to graph topology representations, thus fully motivating the need for new solutions that take graph-structure into account.\n\nThe setting / application of this method to graph synthesis of suspicious behaviours of network users, to detect intrusion, effectively a Zero-shot problem, is super interesting.\n\nThe main architectural contribution of this paper are graph-deconvolutions, practically a graph-equivalent of CNN's depth-to-space - achieved by means of transposed structural matrix multiplication of the hidden GNN (graph-NN) activation - simple, reasonable and effective.\n\nWhile better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.\n\nResults are provided on relatively new tasks so it's hard to compare fully to previous methods, but the authors do make an attempt to provide comparisons on synthetic graphs and intrusion detection data. The authors do published their code on GitHub with a link to the datasets as well.\n\nAs previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of \"edge-to-edge\" convolutions and generally the architectural choice related to the conditional GAN discriminator. Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.\n\nThank you!\n\nps // next my previous public comments, in detail, repeated ...\n\n--\n\n- the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".\n\n- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful. \n\n- why do you need a conditional GAN discriminator, if you already model similarity by L1? Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of trasductional domains. Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways. This is confusing to me. Please explain the logic for this architectural choice.\n\n-  could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "cdate": 1542234258243, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918670, "tmdate": 1552335918670, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1l4P2HuhX", "original": null, "number": 1, "cdate": 1541065819639, "ddate": null, "tcdate": 1541065819639, "tmdate": 1541533246462, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "content": {"title": "Interesting work with some odd issues on implementation and results", "review": "The paper presents an approach for translating graphs in one domain to graphs in the same domain using a GAN approach. A graph Translator approach is defined and a number of synthetic data sets and one real-world data set are used to evaluate the approach. Most of the paper is written well, though there are some odd sentence structure issues in places. The paper could do with a thorough check for grammatical and spelling mistakes. For example you miss-spell NVIDIA.\n\nThe main concerns with the work:\n1) Equation 2 is used to minimise the distance between graphs from X and graphs in Y. Yet, the main metric which is used to evaluate the paper is this distance. This would seem to give an unfair advantage to your approach. I would also be concerned about the fact that later you use this for stating if a graph represents good or hacker activity. If you have drawn translated graphs towards real graphs, how do you know that you haven\u2019t pulled a good graph closer to a hacker graph? This is more concerning considering work which came out of NIPS which suggested that GAN\u2019s tend to favour producing similar output rather than spreading it evenly over the domain.\n\n2) It isn\u2019t entirely clear what your results are trying to show. Presumably P, R, AUC and F1 are generated from the results produced from your Discriminator? Were each of the other approaches optimised against your discriminator or not? Also, it is unclear as to what the Gold Standard method is - we\u2019re only told that its a classifier, but what type and how constructed?\n\n3) Your approach seems to be \u2018fixed\u2019 in the set of nodes which are in both in the input and output graphs - needing to be the same. This would seem significantly limiting as graphs are rarely of the same node set.\n\n4) Although you comment on other graphs approaches being limited to very small graphs, you do not test your approach on graphs with over 150 nodes. These would also seem to be very small graphs in comparison to real-world graphs. Further evaluation on larger graphs would seem to be essential - how long would it take on graphs with 10^6 nodes?\n\n5) The real-world dataset seems rather odd and not fully explored. Given that you have this data it is surprising that you didn\u2019t complete the loop by showing that you could take data from before a hack attempt and show that you could predict that in the future you had a hack attempt. Perhaps this is due to the fact that you didn\u2019t have the ground-truth data in here to show a graph going from good to bad? But if not it would have been good to have shown, either through this data or some other, how your approach does match in with real-world results.\n\nGiven the points above, I would be very concerned on an approach which used the above to identify a future hacking attempt.\n\nSome more specific comments on the paper:\n- \"The tremendous success of deep generative models on generating continuous data like image and audio\u201d - it is not clear what this continuous data is.\n\n- Hard to parse : \u201cwhich barely can be available for the accounts worth being monitored.\u201d\n\n- \u201cThis requires us to learn the generic distribution of theft behaviors from historical attacks and synthesize the possible malicious authentication graphs for the other accounts conditioning on their current computer networks\u201d - given that these historical attacks are (hopefully) rare, is there enough data here to construct a model?\n\n- Please define GCNN\n\n- \u201cOur GT-GAN is highly extensible where underlying building blocks, GCNN and distance measure in discriminator, can be replaced by other techniques such as (Kipf & Welling, 2017; Arjovsky et al., 2017) or their extensions.\u201d - this sounds more like a feature of what you have contributed rather than a contribution in its own right.\n\n- In the context of synthetic data, what is ground-truth?\n\n- Hard to parse \u201cModern deep learning techniques operating on graphs is a new trending topic in recent years.\u201d\n\n- Hard to parse \u201cHowever, these methods are highly tailored to only address the graph generation in a specific type of applications such as molecules generation\u201d \n\n- Hard to parse \u201cExisting works are basically all proposed in the most recent year,\u201d\n\n- \u201cTypically we focus on learning the translation from one topological patterns to the other one\u201d -> \u201cTypically we focus on learning the translation from one topological pattern to the other\u201d\n\n- It\u2019s not clear in equation 1 how you represent G_X. Only much later is it mentioned about adjacency matrix.\n\n- Hard to parse \u201cDifferent and more difficult than graph generation designed only for learning the distribution of graph representations, for graph translation one needs to learn not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously.\u201c\n\n- Hard to parse \u201cgraph translation requires to learn\u201d\n\n- Hard to parse \u201cin most of them the input signal is given over node with a static set of edge and their weights fixed for all samples\u201d\n\n- \u201cwe propose an graph\u201d -> \u201cwe propose a graph\u201d\n\n- Hard to parse \u201cThe two components of the formula refers to direction filters as talked above\u201d\n\n- Hard to parse \u201cNext, graph translator requires to\u201d\n\n- \u201cas shown in Equations equation 7 and Equations equation 6,\u201d -> \u201cas shown in Equation 6 and Equation 7\u201d\n\n- Hard to parse \u201cThe challenge is that we need not only to learn the\u201d\n\n- Figure 2 would seem to need more explanation. \n\n- The end of section 3.3 is a bit vague and lacks enough detail to reproduce.\n\n- \u201cour GT-GAN is able to provide a scalable (i.e., O(N2)) algorithm that can generate general graphs.\u201d - what sizes have you tested this up to?\n\n- Hard to parse \u201cwe randomly add another kjEj edges on it to form the target graph\u201d\n\n- \u201cThe goal is to forecast and synthesize the future potential malicious authentication graphs of the users without any historical malicious behaviors, by the graph translator from normal to malicious graph trained based on the users with historical malicious-behavior records.\u201d - This isn\u2019t entirely clear. Are you trying to create new malicious graphs or show that a current graph will eventually go malicious?\n\n- \u201cAll the comparison methods are directly trained by the malicious graphs without the conditions of input graphs as they can only do graph generation instead of translation.\u201d - not clear. For the synthetic data sets how did you choose which ones were malicious?\n\n- \u201cGraphRNN is tested with graph size within 150. GraphGMG, GraphVAE is tested within size 10 and RandomVAE is tested on graphs within size 150.\u201d -> \u201cGraphRNN and RandomVAE are tested with graph up to size 150. GraphGMG, GraphVAE is tested with graphs up to  size 10.\u201d\n\n- \u201cHere, beyond label imbalance, we are interested in \u201clabel missing\u201d which is more challenging.\u201d - \u201cmissing labels\u201d?\n\n- \u201cIn addition, we have also trained a \u201cgold standard\u201d classifier based on input graphs and real target\ngraphs.\u201d - need to say more about this.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Review", "cdate": 1542234258243, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918670, "tmdate": 1552335918670, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyx-b0Vtnm", "original": null, "number": 1, "cdate": 1541127672621, "ddate": null, "tcdate": 1541127672621, "tmdate": 1541127672621, "tddate": null, "forum": "SJz6MnC5YQ", "replyto": "SJz6MnC5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "content": {"title": "A few of comments and request for clarifications", "comment": "Dear Authors, thank you for your submission. \n\nThe problem setting is very interesting, especially the problem of malicious graph activity synthesis \"forecast and synthesize the future potential malicious authentication graphs of the users without any historical malicious behaviors, by the graph translator from normal to malicious graph trained based on the users with historical malicious-behavior records.\".\n\nThat said, I have few points that need clarity:\n\nFirst, the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".\n\nSecond, how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful. \n\nThird, and slightly related to the previous point, why do you need a conditional GAN discriminator, if you already model similarity by L1? Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of trasductional domains. Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways. This is confusing to me. Please explain the logic for this architectural choice.\n\nFour, could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.\n\nThank you!\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1309/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP GRAPH TRANSLATION", "abstract": "The tremendous success of deep generative models on generating continuous data\nlike image and audio has been achieved; however, few deep graph generative models\nhave been proposed to generate discrete data such as graphs. The recently proposed\napproaches are typically unconditioned generative models which have no\ncontrol over modes of the graphs being generated. Differently, in this paper, we\nare interested in a new problem named Deep Graph Translation: given an input\ngraph, the goal is to infer a target graph by learning their underlying translation\nmapping. Graph translation could be highly desirable in many applications such\nas disaster management and rare event forecasting, where the rare and abnormal\ngraph patterns (e.g., traffic congestions and terrorism events) will be inferred prior\nto their occurrence even without historical data on the abnormal patterns for this\nspecific graph (e.g., a road network or human contact network). To this end, we\npropose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN)\nwhich translates one mode of the input graphs to its target mode. GT-GAN consists\nof a graph translator where we propose new graph convolution and deconvolution\nlayers to learn the global and local translation mapping. A new conditional\ngraph discriminator has also been proposed to classify target graphs by conditioning\non input graphs. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate the effectiveness and scalability of the proposed GT-GAN.", "keywords": [], "authorids": ["xguo7@gmu.edu", "lwu@email.wm.edu", "lzhao9@gmu.edu"], "authors": ["Xiaojie Guo", "Lingfei Wu", "Liang Zhao"], "pdf": "/pdf/e9283840b94e1153d88d1c5f8874d4bc0490cddf.pdf", "paperhash": "guo|deep_graph_translation", "_bibtex": "@misc{\nguo2019deep,\ntitle={{DEEP} {GRAPH} {TRANSLATION}},\nauthor={Xiaojie Guo and Lingfei Wu and Liang Zhao},\nyear={2019},\nurl={https://openreview.net/forum?id=SJz6MnC5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1309/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608218, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJz6MnC5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1309/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1309/Authors|ICLR.cc/2019/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1309/Reviewers", "ICLR.cc/2019/Conference/Paper1309/Authors", "ICLR.cc/2019/Conference/Paper1309/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608218}}}], "count": 19}