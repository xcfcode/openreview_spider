{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519433029198, "tcdate": 1509138734412, "number": 1123, "cdate": 1518730157277, "id": "rk07ZXZRb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rk07ZXZRb", "original": "BJjz-XbCZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260100721, "tcdate": 1517249213946, "number": 31, "cdate": 1517249213931, "id": "Bk8aGk6rM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This is a paper introducing a hierarchical RL method which incorporates the learning of a latent space, which enables the sharing of learned skills.\n\nThe reviewers unanimously rate this as a good paper. They suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework. To some extent, the authors have addressed this concern in the rebuttal.\n", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516230791587, "tcdate": 1516230791587, "number": 6, "cdate": 1516230791587, "id": "Hke9_IpVf", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "Hk2Ttk_NM", "signatures": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "content": {"title": "Manuscript updated", "comment": "We would you like to notify the reviewer that the pdf has been updated with the requested changes including the new experiment with the embedding pre-trained on all 6 tasks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}, {"tddate": null, "ddate": null, "tmdate": 1516230502922, "tcdate": 1516230502922, "number": 5, "cdate": 1516230502922, "id": "H1kdvIp4M", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "signatures": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "content": {"title": "Manuscript updated", "comment": "Dear reviewers, \nWe would like to let you know that we have updated the manuscript with the changes requested in your reviews. Thank you again for your feedback. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515874768094, "tcdate": 1511738020715, "number": 1, "cdate": 1511738020715, "id": "r1aiqauxf", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "signatures": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3"], "readers": ["everyone"], "content": {"title": " I find the method to be theoretically interesting and valuable to the learning community. However, the experiments are not entirely convincing.", "rating": "7: Good paper, accept", "review": "In this paper, (previous states, action) pairs and task ids are embedded into the same latent space with the goal of generalizing and sharing across skill variations. Once the embedding space is learned, policies can be modified by passing in sampled or learned embeddings.\n\nNovelty and Significance: To my knowledge, using a variational approach to embedding robot skills is novel. Significantly, the embedding is learned from off-policy trajectories, indicating feasibility on a real-world setting. The manipulation experiments show nice results on non-trivial tasks. However, no comparisons are shown against prior work in multitask or transfer learning. Additionally, the tasks used to train the embedding space were tailored exactly to the target task, making it unclear that this method will work generally.\n\nQuestions:\n- I am not sure how to interpret Figure 3. Do you use Bernoulli in the experiments?\n- How many task IDs are used for each experiment? 2?\n- Are the manipulation experiments learned with the off-policy variant?\n- Figure 4b needs the colors to be labeled. Video clips of the samples would be a plus.\n- (Major) For the experiments, only exactly the useful set of tasks is used to train the embedding. What happens if a single latent space is learned from all the tasks, and Spring-wall, L-wall, and Rail-push are each learned from the same embedding. \n\nI find the method to be theoretically interesting and valuable to the learning community. However, the experiments are not entirely convincing.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642386008, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer1"], "reply": {"forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642386008}}}, {"tddate": null, "ddate": null, "tmdate": 1515874755674, "tcdate": 1515874755674, "number": 4, "cdate": 1515874755674, "id": "Hk2Ttk_NM", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "r1aiqauxf", "signatures": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3"], "content": {"title": "Revision of Review", "comment": "I would really like to see an experiment where an embedding space is trained on a wider variety of tasks rather than just what is needed to generalize to the target task. However, I find that this paper is a valuable contribution to ICLR, and I think that it should be accepted.\n\nAs ICLR allows the authors to upload a new pdf, I do not understand why the author response only said they would make changes in the final version (especially for things like labeling a figure). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642386066, "tcdate": 1511818178218, "number": 2, "cdate": 1511818178218, "id": "Hk9a7-qlG", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "signatures": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting work at the intersection of reinforcement learning and variational inference", "rating": "7: Good paper, accept", "review": "The submission tackles an important problem of learning and transferring multiple motor skills. The approach relies on using an embedding space defined by latent variables and entropy-regularized policy gradient / variational inference formulation that encourages diversity and identifiability in latent space.\n\nThe exposition is clear and the method is well-motivated. I see no issues with the mathematical correctness of the claims made in the paper. The experimental results are both instructive of how the algorithm operates (in the particle example), and contain impressive robotic results. I appreciated the experiments that investigated cases where true number of tasks and the parameter T differ, showing that the approach is robust to choice of T.\n\nThe submission focuses particularly on discrete tasks and learning to sequence discrete tasks (as training requires a one-hot task ID input). I would like a bit of discussion on whether parameterized skills (that have continuous space of target location, or environment parameters, for example) can be supported in the current formulation, and what would be necessary if not.\n\nOverall, I believe this is in interesting piece of work at a fruitful intersection of reinforcement learning and variational inference, and I believe would be of interest to ICLR community.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642386008, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer1"], "reply": {"forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642386008}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642386026, "tcdate": 1512088092392, "number": 3, "cdate": 1512088092392, "id": "HkEQMXAxz", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "signatures": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills. This is a good paper.", "rating": "7: Good paper, accept", "review": "The paper presents a new approach for hierarchical reinforcement learning which aims at learning a versatile set of skills. The paper uses a variational bound for entropy regularized RL to learn a versatile latent space which represents the skill to execute. The variational bound is used to diversify the learned skills as well as to make the skills identifyable from their state trajectories. The algorithm is tested on a simple point mass task and on simulated robot manipulation tasks.\n\nThis is a very intersting paper which is also very well written. I like the presented approach of learning the skill embeddings using the variational lower bound. It represents one of the most principled approches for hierarchical RL. \n\nPros: \n- Interesting new approach for hiearchical reinforcement learning that focuses on skill versatility\n- The variational lower bound is one of the most principled formulations for hierarchical RL that I have seen so far\n- The results are convincing\n\nCons:\n- More comparisons against other DRL algorithms such as TRPO and PPO would be useful\n\nSummary: This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills. This is a good paper.\n\nMore comments:\n- There are several papers that focus on learning versatile skills in the context of movement primitive libraries, see [1],[2],[3]. These papers should be discussed.\n\n[1] Daniel, C.; Neumann, G.; Kroemer, O.; Peters, J. (2016). Hierarchical Relative Entropy Policy Search, Journal of Machine Learning Research (JMLR),\n[2] End, F.; Akrour, R.; Peters, J.; Neumann, G. (2017). Layered Direct Policy Search for Learning Hierarchical Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).\n[3] Gabriel, A.; Akrour, R.; Peters, J.; Neumann, G. (2017). Empowered Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642386008, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1123/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1123/AnonReviewer1"], "reply": {"forum": "rk07ZXZRb", "replyto": "rk07ZXZRb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642386008}}}, {"tddate": null, "ddate": null, "tmdate": 1515167880916, "tcdate": 1515167880916, "number": 3, "cdate": 1515167880916, "id": "S1b5e76mM", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "r1aiqauxf", "signatures": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "content": {"title": "Response to Reviewer 3", "comment": "We are grateful for the insightful comments and suggestions.\n\nPlease find the answers to the inline questions below, we will clarify all of these points in the final version of the paper.\n- I am not sure how to interpret Figure 3. Do you use Bernoulli in the experiments? \n- A Bernoulli distribution is only used for for Figure 3 to demonstrate that our method can work with other distributions.\n\n- How many task IDs are used for each experiment? 2?\n- Yes, T was set to 2 for the manipulation experiments.\n\n- Are the manipulation experiments learned with the off-policy variant?\n- That is correct. All experiments were performed in an off-policy setting. This decision was made due to the higher sample-efficiency of the off-policy methods.\n\n- Figure 4b needs the colors to be labeled. Video clips of the samples would be a plus.\n- We will add the labels and address this problem in the final version of the paper\n\nRegarding the last question on training the embedding space on all of the tasks; we are currently working on this experiment and are planning to include it in the final version of the paper. It is worth noting that the multi-task RL training can be challenging (especially with poorly scaled rewards) and it maintains as an open problem that is beyond the scope of this work. Our method presents a solution to a problem of finding an embedding space that enables re-using, interpolating and sequencing previously learned skills, with the assumption that the RL agent was able to learn them in the first place. However, we strongly believe that the off-policy setup presented in this work has much more flexibility that its on-policy equivalents as to how to address the multi-task RL problem.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}, {"tddate": null, "ddate": null, "tmdate": 1515167808575, "tcdate": 1515167808575, "number": 2, "cdate": 1515167808575, "id": "rJFSl767z", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "Hk9a7-qlG", "signatures": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their comments and suggestions.\n\nOur method does indeed support parameterized skills as suggested by the reviewer. For instance, the low-level policy could receive an embedding conditioned on a continuous target location instead of the task ID (given a suitable embedding space). It is also not limited to the multi-task setting, i.e., the number of tasks T used for training can be set to 1 (as explored in the point-mass experiments). We will add this to the discussion to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}, {"tddate": null, "ddate": null, "tmdate": 1515167750151, "tcdate": 1515167750151, "number": 1, "cdate": 1515167750151, "id": "Hy0-eQp7G", "invitation": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "forum": "rk07ZXZRb", "replyto": "HkEQMXAxz", "signatures": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1123/Authors"], "content": {"title": "Response to Reviewer 1", "comment": "We very much appreciate the reviewer\u2019s comments and suggestions. \n\nRegarding the comparison to other on-policy methods such as TRPO or PPO, we would like to emphasize that the presented approach is mostly independent of the underlying RL learning algorithm. In fact, it will be easier to implement our approach in the on-policy setup. The off-policy setup with experience replay that we are considering requires additional care due to the embedding variable which we also maintain in the replay buffer. In Section 5, we present all the modifications necessary to running our method in the more data-efficient off-policy setup, which we believe is crucial to running it on the real robots in the future.\n\nWe would also like to thank the reviewer for pointing out the additional references - we will be very happy to include them. While some of the high-level ideas are related, there are differences both in the formulation and the algorithmic framework. An important aspect of our work is that we show how to apply entropy-regularized RL with latent variables when working with neural networks and in an off-policy setting, avoiding both the burden of using a limited number of hand-crafted features and allowing for data-efficient learning.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning an Embedding Space for Transferable Robot Skills", "abstract": "We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.\n", "pdf": "/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf", "paperhash": "hausman|learning_an_embedding_space_for_transferable_robot_skills", "_bibtex": "@inproceedings{\nhausman2018learning,\ntitle={Learning an Embedding Space for Transferable Robot Skills},\nauthor={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rk07ZXZRb},\n}", "keywords": ["Deep Reinforcement Learning", "Variational Inference", "Control", "Robotics"], "authors": ["Karol Hausman", "Jost Tobias Springenberg", "Ziyu Wang", "Nicolas Heess", "Martin Riedmiller"], "authorids": ["hausmankarol@gmail.com", "springenberg@google.com", "ziyu@google.com", "heess@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723998, "id": "ICLR.cc/2018/Conference/-/Paper1123/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk07ZXZRb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1123/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1123/Authors|ICLR.cc/2018/Conference/Paper1123/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1123/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1123/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1123/Reviewers", "ICLR.cc/2018/Conference/Paper1123/Authors", "ICLR.cc/2018/Conference/Paper1123/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723998}}}], "count": 11}