{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396689000, "tcdate": 1486396689000, "number": 1, "id": "SJtGTGLOg", "invitation": "ICLR.cc/2017/conference/-/paper581/acceptance", "forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the relationships of this work to existing work in literature (both in terms of a discussion to clarify the novelty, and in terms of more complete empirical comparisons). Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396689488, "id": "ICLR.cc/2017/conference/-/paper581/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396689488}}}, {"tddate": null, "tmdate": 1483645965335, "tcdate": 1483645965335, "number": 4, "id": "BJBfN73Hl", "invitation": "ICLR.cc/2017/conference/-/paper581/public/comment", "forum": "B1-Hhnslg", "replyto": "Hk8Jice4g", "signatures": ["~Jake_Snell1"], "readers": ["everyone"], "writers": ["~Jake_Snell1"], "content": {"title": "Clarifications regarding results and related work", "comment": "Thank you very much for your review. Regarding the CUB result, we ran experiments where we replaced the AlexNet features with GoogLeNet features and obtained a result of 54.6% accuracy (please see Table 3 of our paper revision dated Dec 6, 2016). We believe this truly represents the state of the art at this point in time. Regarding Omniglot and miniImagenet, we originally reported the non-FCE/fine-tuned matching networks results because we wanted to keep the comparison limited to the basic models themselves. However, we agree that we should be thorough in our reporting and will include these results as well.\n\nRegarding the relevant literature, both our model and LMNNs are based on Neighborhood Component Analysis and we believe that our model is more similar to NCA than to LMNN, as we do not use a margin-based loss and do not pre-select a set of target neighbors. We believe that our model is closest in spirit to nonlinear NCA (from follow-up work by Salakhutdinov and Hinton, 2007). We are certainly happy to include your provided references from the metric learning literature as they are indeed relevant. We will also include a reference to the learning to rank literature as it relates to siamese networks, although we feel that this criticism applies to the current one-shot learning literature as opposed to just our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287512283, "id": "ICLR.cc/2017/conference/-/paper581/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1-Hhnslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper581/reviewers", "ICLR.cc/2017/conference/paper581/areachairs"], "cdate": 1485287512283}}}, {"tddate": null, "tmdate": 1483645296319, "tcdate": 1483645296319, "number": 3, "id": "B1uOZQnHg", "invitation": "ICLR.cc/2017/conference/-/paper581/public/comment", "forum": "B1-Hhnslg", "replyto": "Sk8_1xWVx", "signatures": ["~Jake_Snell1"], "readers": ["everyone"], "writers": ["~Jake_Snell1"], "content": {"title": "Thanks for your review", "comment": "Thanks for taking the time to review our paper. Including pseudocode is a valuable suggestion -- we will update the paper to include this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287512283, "id": "ICLR.cc/2017/conference/-/paper581/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1-Hhnslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper581/reviewers", "ICLR.cc/2017/conference/paper581/areachairs"], "cdate": 1485287512283}}}, {"tddate": null, "tmdate": 1483645110661, "tcdate": 1483645110661, "number": 2, "id": "rkJagQ2re", "invitation": "ICLR.cc/2017/conference/-/paper581/public/comment", "forum": "B1-Hhnslg", "replyto": "ByaTpJBVx", "signatures": ["~Jake_Snell1"], "readers": ["everyone"], "writers": ["~Jake_Snell1"], "content": {"title": "Clarification of relationship to matching networks", "comment": "We thank Reviewer 2 for reviewing our paper. This review\u2019s main criticism is that our approach is too similar to the matching networks model proposed by Vinyals et al. 2016. We will update the related work section of our paper to clarify the relationship between prototypical networks and matching nets. In the meantime, we would like to take this opportunity to elaborate upon some of the benefits of our approach relative to matching networks: better computational efficiency, a simpler form for the classifier, and a straightforward extension to the zero-shot setting.\n\nFirst and foremost, prototypical networks are computationally more efficient than matching networks. If there are K classes, each with N support examples, then computing distances for a query point will take O(K) time for prototypical networks vs. O(KN) for matching networks. This is indeed a 5x speedup for the 5-shot scenario, which though not enormous is also not insignificant. Even slightly larger scales such as N=10 or N=100 would lead to useful performance gains. We see favorable applications of our approach to tasks such as document tagging where it is important to quickly label new documents without needing to compute distances to multiple support examples per tag.\n\nSecond is the simpler expression of our classifier (see Section 3.2 of our paper) compared to the non-parametric form of matching networks. Not only does this require less storage (O(K) vs. O(KN)) but it also sets the stage for future work that explores the prediction of alternate classifiers beyond the linear ones we investigate here.\n\nFinally, our approach affords a straightforward extension to the zero-shot setting that matching networks do not. The matching networks approach is fundamentally about embedding support examples whereas prototypical networks focuses on learning an embedded representation for each class. For the few-shot scenario we chose our representation to be the mean of embedded points, while for the zero-shot scenario we utilized the embedded metadata. Such a generalization of the matching networks approach to zero-shot classification does not exist."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287512283, "id": "ICLR.cc/2017/conference/-/paper581/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1-Hhnslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper581/reviewers", "ICLR.cc/2017/conference/paper581/areachairs"], "cdate": 1485287512283}}}, {"tddate": null, "tmdate": 1482124806842, "tcdate": 1482124740625, "number": 3, "id": "ByaTpJBVx", "invitation": "ICLR.cc/2017/conference/-/paper581/official/review", "forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "signatures": ["ICLR.cc/2017/conference/paper581/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper581/AnonReviewer2"], "content": {"title": "Extension of matching networks but cannot see big advantages", "rating": "5: Marginally below acceptance threshold", "review": "The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535009, "id": "ICLR.cc/2017/conference/-/paper581/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper581/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper581/AnonReviewer4", "ICLR.cc/2017/conference/paper581/AnonReviewer3", "ICLR.cc/2017/conference/paper581/AnonReviewer2"], "reply": {"forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535009}}}, {"tddate": null, "tmdate": 1481863022214, "tcdate": 1481863022214, "number": 2, "id": "Sk8_1xWVx", "invitation": "ICLR.cc/2017/conference/-/paper581/official/review", "forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "signatures": ["ICLR.cc/2017/conference/paper581/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper581/AnonReviewer3"], "content": {"title": "Simple but useful extension of matching networks", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.\n\nAlthough the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535009, "id": "ICLR.cc/2017/conference/-/paper581/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper581/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper581/AnonReviewer4", "ICLR.cc/2017/conference/paper581/AnonReviewer3", "ICLR.cc/2017/conference/paper581/AnonReviewer2"], "reply": {"forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535009}}}, {"tddate": null, "tmdate": 1481841374415, "tcdate": 1481841374408, "number": 1, "id": "Hk8Jice4g", "invitation": "ICLR.cc/2017/conference/-/paper581/official/review", "forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "signatures": ["ICLR.cc/2017/conference/paper581/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper581/AnonReviewer4"], "content": {"title": "Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.", "rating": "4: Ok but not good enough - rejection", "review": "*** Paper Summary ***\n\nThis paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.\n\n*** Review ***\n\nThe paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. \n\nThe related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class \n\nI am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?\n\nOverall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.\n\n*** References ***\n\nLarge Margin Nearest Neighbors. Weinberger et al, 2005\nFrom RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010\nA Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535009, "id": "ICLR.cc/2017/conference/-/paper581/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper581/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper581/AnonReviewer4", "ICLR.cc/2017/conference/paper581/AnonReviewer3", "ICLR.cc/2017/conference/paper581/AnonReviewer2"], "reply": {"forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535009}}}, {"tddate": null, "tmdate": 1481061181578, "tcdate": 1481061181571, "number": 1, "id": "HyIBmnNQe", "invitation": "ICLR.cc/2017/conference/-/paper581/public/comment", "forum": "B1-Hhnslg", "replyto": "H1pDow0Gg", "signatures": ["~Jake_Snell1"], "readers": ["everyone"], "writers": ["~Jake_Snell1"], "content": {"title": "CUB section updated with new results", "comment": "Hi,\n\nThanks for your question. In light of the linked paper by Reed et al. 2016, we have updated the zero-shot CUB section of our paper with new results. We trained our model on GoogLeNet extracted features, as the DA-SJE/DS-SJE approach does. We continue to use the 312-dimensional continuous attribute vectors. We achieve accuracy of 54.6%, compared to DA-SJE which gets 50.9% on top of attribute vectors.\n\nCheers,\nJake"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287512283, "id": "ICLR.cc/2017/conference/-/paper581/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1-Hhnslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper581/reviewers", "ICLR.cc/2017/conference/paper581/areachairs"], "cdate": 1485287512283}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481060777491, "tcdate": 1478376505434, "number": 581, "id": "B1-Hhnslg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1-Hhnslg", "signatures": ["~Richard_Zemel1"], "readers": ["everyone"], "content": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480649573264, "tcdate": 1480649573260, "number": 1, "id": "H1pDow0Gg", "invitation": "ICLR.cc/2017/conference/-/paper581/pre-review/question", "forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "signatures": ["ICLR.cc/2017/conference/paper581/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper581/AnonReviewer3"], "content": {"title": "Additional baselines for CUB", "question": "There is another paper reporting zero-shot classification on CUB that appears to use the same protocol and get a better (state-of-the-art) result, using text features instead of attributes:\n\nhttps://arxiv.org/pdf/1605.05395v1.pdf\n\nAlso, in the cited paper \"Evaluation of Output Embeddings for Fine-Grained Image Classification\", the best result is actually 50.1% using GoogLeNet features instead of AlexNet. Do you know how the proposed model performs with GoogLeNet features, or another more modern architecture such as an open-sourced VGG model or Inception model? Was there a reason for using AlexNet? Since the baselines in table 3 are not state of the art, it is not clear why AlexNet features would be used instead of something more modern.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Networks for Few-shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.", "pdf": "/pdf/1e1d8a1884188e3ce0db825def307a25c844c868.pdf", "TL;DR": "We learn a metric space in which few-shot classification can be performed by computing Euclidean distances to a single prototype representative of each class.", "paperhash": "snell|prototypical_networks_for_fewshot_learning", "conflicts": ["toronto.edu", "twitter.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel"], "authorids": ["jsnell@cs.toronto.edu", "kswersky@twitter.com", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959203403, "id": "ICLR.cc/2017/conference/-/paper581/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper581/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper581/AnonReviewer3"], "reply": {"forum": "B1-Hhnslg", "replyto": "B1-Hhnslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper581/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959203403}}}], "count": 10}