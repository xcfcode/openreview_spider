{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396567213, "tcdate": 1486396567213, "number": 1, "id": "SJJs3z8Oe", "invitation": "ICLR.cc/2017/conference/-/paper412/acceptance", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The area chair agrees with the reviewers that this paper is not of sufficient quality for ICLR. The experimental results are weak (there might be even be some issues with the experimental methodology) and it is not at all clear whether the translation model benefits from the image data. The authors did not address the final reviews."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396567783, "id": "ICLR.cc/2017/conference/-/paper412/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396567783}}}, {"tddate": null, "tmdate": 1481931883853, "tcdate": 1481931883853, "number": 3, "id": "SJNO3xMVg", "invitation": "ICLR.cc/2017/conference/-/paper412/official/review", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/conference/paper412/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper412/AnonReviewer2"], "content": {"title": "Promising research direction but not quite there", "rating": "3: Clear rejection", "review": "This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. \n\nAs pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595677, "id": "ICLR.cc/2017/conference/-/paper412/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper412/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper412/AnonReviewer3", "ICLR.cc/2017/conference/paper412/AnonReviewer1", "ICLR.cc/2017/conference/paper412/AnonReviewer2"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595677}}}, {"tddate": null, "tmdate": 1481928986218, "tcdate": 1481928986218, "number": 2, "id": "Bkgf7ZeM4e", "invitation": "ICLR.cc/2017/conference/-/paper412/official/review", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/conference/paper412/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper412/AnonReviewer1"], "content": {"title": "There are major issues", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. \n\nThe idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.\n\nExperimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. \n\nThe qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.\n\nOverall, there are major issues with both the approach and the execution of the paper.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595677, "id": "ICLR.cc/2017/conference/-/paper412/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper412/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper412/AnonReviewer3", "ICLR.cc/2017/conference/paper412/AnonReviewer1", "ICLR.cc/2017/conference/paper412/AnonReviewer2"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595677}}}, {"tddate": null, "tmdate": 1481920008105, "tcdate": 1481919843929, "number": 1, "id": "BJ2wTaWNx", "invitation": "ICLR.cc/2017/conference/-/paper412/official/review", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/conference/paper412/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper412/AnonReviewer3"], "content": {"title": "Unclear motivation & unconvincing results", "rating": "3: Clear rejection", "review": "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595677, "id": "ICLR.cc/2017/conference/-/paper412/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper412/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper412/AnonReviewer3", "ICLR.cc/2017/conference/paper412/AnonReviewer1", "ICLR.cc/2017/conference/paper412/AnonReviewer2"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595677}}}, {"tddate": null, "tmdate": 1480886403243, "tcdate": 1480886403239, "number": 7, "id": "BkiFO-Gml", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "Hk0_CEbQe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "C is for Constrained", "comment": "C means \"Constrained\", i.e. the model was only trained with the given training corpus and not without some other resources (that would be a U: Unconstrained)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480836477918, "tcdate": 1480836477913, "number": 6, "id": "rJLKBSbmg", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "rkklCqe7g", "signatures": ["~Joji_Toyama1"], "readers": ["everyone"], "writers": ["~Joji_Toyama1"], "content": {"title": "Thanks for your further comment.", "comment": "We are not participating the competition, therefore it is not a competition submission of course, and we just evaluate our models with published dataset. So we thought it is not a problem to do further analysis to the model which scores the best in the test dataset.\n\nI hope this can be the answer for your question. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480836125216, "tcdate": 1480836125210, "number": 5, "id": "SyB7NBW7e", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "HkrDj-1Xe", "signatures": ["~Joji_Toyama1"], "readers": ["everyone"], "writers": ["~Joji_Toyama1"], "content": {"title": "Thanks for your comment.", "comment": "Thanks for your question. \nWe do not have the obvious evidence which shows our model is actually capturing useful semantics but we have some clues. We conclude that our model is capturing the semantic of the sentence because of the fact that our model scores the higher METEOR score compared to baselines, and some translation examples in qualitative analysis (such as Figure 6). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480834678488, "tcdate": 1480834678482, "number": 4, "id": "Hk0_CEbQe", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "BJUyn9gmg", "signatures": ["~Joji_Toyama1"], "readers": ["everyone"], "writers": ["~Joji_Toyama1"], "content": {"title": "Thanks for your comment.", "comment": "According to 1, we know this paper and it came about a week after we submitted our paper in openreview.\n\nAccording to 2, we should correct the reference as you commented. We will revise the paper. Thanks for advice.\n\nAccording to 3, we denote NMT as the monomodal translation using dl4mt baseline and we trained it by ourself. \nWe are sure the score reported by Huang et al is with -norm. There is also their scores reported in (http://www.statmt.org/wmt16/pdf/W16-2346.pdf), which is without norm and the scores reported in Huang et al are higher than those reported in (http://www.statmt.org/wmt16/pdf/W16-2346.pdf), which indicates that the report in Huang et al is with -norm. The reason why we did not put the score without norm is that we were not sure what \"CMU_1 MNMT_C\" denotes (We were not sure what \"C\" indicates. We may be missing the part explaining about it though). \nAbout your claim that we should put other competitors score such as Moses, what we compare our model to is limited to only end-to-end neural machine models because we want to show that the neural machine translation can actually benefit from images in our way, but as you told, we should have put other competitor's score for richer comparison.\n\nThanks for your valuable comment!!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480793574962, "tcdate": 1480793574958, "number": 3, "id": "rkklCqe7g", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "Syh3nLemx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Small addition", "comment": "Regarding this,\n\nIf you were participating to the challenge where the test set ground-truth sentences were not accessible except for official evaluators, you would have selected G+O-TXT for your best system and not G. So I think it is not good to further base your qualitative analysis on G vs VNMT and not G+O-TXT vs VNMT. Again if that was a competition submission, you wouldn't have access to your test scores and you wouldn't do your qualitative analysis using 'G'.."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480793053969, "tcdate": 1480793053963, "number": 2, "id": "BJUyn9gmg", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "General comments", "comment": "1. Just for information, regarding your claim \"We also present the first translation task with which one uses a parallel corpus and images in training, while using a source corpus in translating\", there's a recent paper called \"Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot\" which also proposes the very same idea (https://arxiv.org/pdf/1611.04503v1.pdf) but I don't know which one comes earlier.\n\n2. It would be nice to clarify that when you refer to Multi30k and the WMT16 challenge results, you actually refer to multimodal machine translation task (Task 1) where you have 1 English and 1 German descriptions and not the second one where you have 5 for each.\n\n3. Your results table is not very clear. What is NMT? Is it a monomodal(textual) dl4mt baseline that you trained yourself? You report the METEOR scores with '-norm' parameter but are you sure that Huang et al' reported with -norm as well? -norm gives substantially higher METEOR scores and the primary competition metric was METEOR without -norm. Also you may want to include the competition winner and Moses baseline in your results for a richer comparison.\n\n4. At the end of section 4.1, BLUE -> BLEU\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480776884065, "tcdate": 1480776884060, "number": 1, "id": "Syh3nLemx", "invitation": "ICLR.cc/2017/conference/-/paper412/public/comment", "forum": "B1G9tvcgx", "replyto": "Sy4luWyQx", "signatures": ["~Joji_Toyama1"], "readers": ["everyone"], "writers": ["~Joji_Toyama1"], "content": {"title": "Thanks for your comment.", "comment": "In our paper, we propose 4 different architectures, G, G+O-AVG, G+O-RNN, and G+O-TXT. Their difference is how the image information is integrated into a latent variable. \n\nIn the validation, we tune the best hyper parameters for each architecture with the validation dataset. The selection of architecture is not a part of hyper parameters setting.Then, we evaluate each architecture with test data.\n\nAccording to your comment that we should select only G+O-TXT for test, we thought it is not a problem to show all architecture's results since model selection is not part of the parameter tuning. Figure 4 suggests that G model's validation scores during the training iteration is higher than others in most cases. It can be the explanation why G scores higher than G+O-TXT in the test.\n\nI hope this can be the answer for your question. If you have further questions, it is welcomed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588915, "id": "ICLR.cc/2017/conference/-/paper412/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1G9tvcgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper412/reviewers", "ICLR.cc/2017/conference/paper412/areachairs"], "cdate": 1485287588915}}}, {"tddate": null, "tmdate": 1480690525136, "tcdate": 1480690525132, "number": 2, "id": "HkrDj-1Xe", "invitation": "ICLR.cc/2017/conference/-/paper412/pre-review/question", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/conference/paper412/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper412/AnonReviewer2"], "content": {"title": "Latent representations", "question": "I like your underlying motivation of using \"a latent\nvariable containing an underlying semantic extracted from texts and images\".  Do you have evidence that your particular model is indeed capturing useful semantics, for example with some qualitative demonstrations?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959297514, "id": "ICLR.cc/2017/conference/-/paper412/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper412/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper412/AnonReviewer1", "ICLR.cc/2017/conference/paper412/AnonReviewer2"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959297514}}}, {"tddate": null, "tmdate": 1480689644528, "tcdate": 1480689644524, "number": 1, "id": "Sy4luWyQx", "invitation": "ICLR.cc/2017/conference/-/paper412/pre-review/question", "forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "signatures": ["ICLR.cc/2017/conference/paper412/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper412/AnonReviewer1"], "content": {"title": "Development and test set scores", "question": "Can you please tell how exactly you performed model selection? If you tuned the hyperparameters using the development (validation) set, then you should only use the model that performed best on it. From Table 1 it seems like this model is G+O+TXT, and the test scores of this model are only marginally better than the baseline (0.6 METEOR, 0.2 BLEU)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959297514, "id": "ICLR.cc/2017/conference/-/paper412/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper412/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper412/AnonReviewer1", "ICLR.cc/2017/conference/paper412/AnonReviewer2"], "reply": {"forum": "B1G9tvcgx", "replyto": "B1G9tvcgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper412/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959297514}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480308397214, "tcdate": 1478289802796, "number": 412, "id": "B1G9tvcgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1G9tvcgx", "signatures": ["~Joji_Toyama1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "pdf": "/pdf/b810ed72c87234e45aa415ec557c77be911f52ab.pdf", "paperhash": "toyama|neural_machine_translation_with_latent_semantic_of_image_and_text", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 14}